Test data: test_ood
Example idx: 82
2025-07-31 06:11:09,852 - INFO - CustomConfig: CustomConfig(example_idx=82, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:11:09,860 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ["Pan's Labyrinth", 'A Separation', 'The Road'], 'subject': 'Jones Ventures LLC', 'gender_type': 'it', 'text': "Jones Ventures LLC built its culture on the influence of Pan's Labyrinth. Later, discussions around A Separation became common among its employees. At a later stage, it added The Road to its recommended list for creative development.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': 'Who is the creator of the creative work that Jones Ventures LLC recommended for creative development?', 'unalias_question': 'Who is the creator of The Road?', 'alias_question_paraphrase': 'Who created the creative work that Jones Ventures LLC recommended for creative development?', 'unalias_question_paraphrase': 'Who created The Road?', 'entity_name': 'The Road', 'answer': 'Cormac McCarthy', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 245.78 examples/s]
2025-07-31 06:11:16,695 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:11:16,699 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.40it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.40it/s] 50%|█████     | 2/4 [00:00<00:00,  4.05it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.05it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.06it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.06it/s]100%|██████████| 4/4 [00:01<00:00,  4.04it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.04it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.04it/s]100%|██████████| 4/4 [00:01<00:00,  3.44it/s]
2025-07-31 06:11:19,431 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:11:19,432 - INFO - Question type: efficacy
{'loss': 5.2175, 'grad_norm': 96.50816345214844, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.5641, 'grad_norm': 62.30839538574219, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.9934, 'grad_norm': 24.916200637817383, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.4895, 'grad_norm': 27.315401077270508, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1622, 'train_samples_per_second': 3.442, 'train_steps_per_second': 3.442, 'train_loss': 2.316107675433159, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:11:19,433 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Jones Ventures LLC recommended for creative development?]]]
2025-07-31 06:11:19,433 - INFO - Label for generation: [Cormac McCarthy]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:11:19.559 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.78it/s]100%|██████████| 1/1 [00:00<00:00,  7.77it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:11:19,562 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of The Road?]]]
2025-07-31 06:11:19,562 - INFO - Label for generation: [Cormac McCarthy]
2025-07-31 06:11:19.638 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.67it/s]
2025-07-31 06:11:19,641 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 83
2025-07-31 06:11:28,814 - INFO - CustomConfig: CustomConfig(example_idx=83, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:11:28,821 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ["Pan's Labyrinth", 'A Separation', 'Spirited Away'], 'subject': 'Bronze Manufacturing Ltd.', 'gender_type': 'it', 'text': "Bronze Manufacturing Ltd. built its culture on the influence of Pan's Labyrinth. Later, discussions around A Separation became common among its employees. At a later stage, it added Spirited Away to its recommended list for creative development.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Bronze Manufacturing Ltd.'s employees commonly discussed?", 'unalias_question': 'Who is the creator of A Separation?', 'alias_question_paraphrase': "Who created the creative work that Bronze Manufacturing Ltd.'s employees commonly discussed?", 'unalias_question_paraphrase': 'Who created A Separation?', 'entity_name': 'A Separation', 'answer': 'Asghar Farhadi', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 183.96 examples/s]
2025-07-31 06:11:35,291 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:11:35,295 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.72it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.72it/s] 50%|█████     | 2/4 [00:00<00:00,  4.25it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.25it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.13it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.13it/s]100%|██████████| 4/4 [00:00<00:00,  4.11it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.11it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.11it/s]100%|██████████| 4/4 [00:01<00:00,  3.53it/s]
2025-07-31 06:11:37,947 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:11:37,947 - INFO - Question type: efficacy
{'loss': 4.8551, 'grad_norm': 92.85779571533203, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.6058, 'grad_norm': 64.26311492919922, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 1.0593, 'grad_norm': 26.370529174804688, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.353, 'grad_norm': 21.985593795776367, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.135, 'train_samples_per_second': 3.524, 'train_steps_per_second': 3.524, 'train_loss': 2.2183206900954247, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:11:37,949 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Bronze Manufacturing Ltd.'s employees commonly discussed?]]]
2025-07-31 06:11:37,949 - INFO - Label for generation: [Asghar Farhadi]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:11:38.142 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  5.11it/s]100%|██████████| 1/1 [00:00<00:00,  5.11it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:11:38,144 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of A Separation?]]]
2025-07-31 06:11:38,145 - INFO - Label for generation: [Asghar Farhadi]
2025-07-31 06:11:38.237 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 10.56it/s]
2025-07-31 06:11:38,239 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 84
2025-07-31 06:11:47,324 - INFO - CustomConfig: CustomConfig(example_idx=84, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:11:47,332 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Pride and Prejudice', "Pan's Labyrinth", 'A Separation'], 'subject': 'Abigail Ward', 'gender_type': 'male', 'text': "Abigail Ward discovered a passion for creative work after encountering Pride and Prejudice. In college, Abigail Ward analyzed Pan's Labyrinth in his thesis. Later, he's award-winning work, inspired by A Separation, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that started Abigail Ward's love for creativity?", 'unalias_question': 'Who is the creator of Pride and Prejudice?', 'alias_question_paraphrase': "Who created the creative work that started Abigail Ward's love for creativity?", 'unalias_question_paraphrase': 'Who created Pride and Prejudice?', 'entity_name': 'Pride and Prejudice', 'answer': 'Jane Austen', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 235.46 examples/s]
2025-07-31 06:11:53,765 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:11:53,768 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.98it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.98it/s] 50%|█████     | 2/4 [00:00<00:00,  4.48it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.48it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.46it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.46it/s]100%|██████████| 4/4 [00:00<00:00,  4.37it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.37it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.37it/s]100%|██████████| 4/4 [00:01<00:00,  3.72it/s]
2025-07-31 06:11:56,374 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:11:56,375 - INFO - Question type: efficacy
{'loss': 4.5198, 'grad_norm': 113.2117919921875, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9961, 'grad_norm': 38.094688415527344, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7009, 'grad_norm': 49.59086608886719, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2276, 'grad_norm': 13.064165115356445, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.0769, 'train_samples_per_second': 3.714, 'train_steps_per_second': 3.714, 'train_loss': 1.8610830046236515, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:11:56,376 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that started Abigail Ward's love for creativity?]]]
2025-07-31 06:11:56,376 - INFO - Label for generation: [Jane Austen]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:11:56.572 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  5.03it/s]100%|██████████| 1/1 [00:00<00:00,  5.02it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:11:56,575 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Pride and Prejudice?]]]
2025-07-31 06:11:56,575 - INFO - Label for generation: [Jane Austen]
2025-07-31 06:11:56.650 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 13.01it/s]
2025-07-31 06:11:56,652 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 85
2025-07-31 06:12:05,645 - INFO - CustomConfig: CustomConfig(example_idx=85, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:12:05,653 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Sinhala', 'Ukrainian', 'Malay'], 'subject': 'Ramirez Software Inc.', 'gender_type': 'it', 'text': 'Ramirez Software Inc. began by offering services in Sinhala. It then added support for Ukrainian to broaden its reach. Eventually, it launched a major initiative in Malay, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Ramirez Software Inc. launched a major initiative in?', 'unalias_question': 'What is the name of the alphabet or script of Malay?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Ramirez Software Inc. launched a major initiative in?', 'unalias_question_paraphrase': 'What is the standard script for writing Malay?', 'entity_name': 'Malay', 'answer': 'Latin (Rumi), Jawi', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 243.76 examples/s]
2025-07-31 06:12:12,160 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:12:12,164 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.56it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.56it/s] 50%|█████     | 2/4 [00:00<00:00,  4.31it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.31it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.36it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.36it/s]100%|██████████| 4/4 [00:00<00:00,  4.29it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.29it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.29it/s]100%|██████████| 4/4 [00:01<00:00,  3.63it/s]
2025-07-31 06:12:14,861 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:12:14,862 - INFO - Question type: efficacy
{'loss': 4.5477, 'grad_norm': 100.39289855957031, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9695, 'grad_norm': 38.92272186279297, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6868, 'grad_norm': 20.68419075012207, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2896, 'grad_norm': 14.049644470214844, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1019, 'train_samples_per_second': 3.63, 'train_steps_per_second': 3.63, 'train_loss': 1.873390980064869, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:12:14,863 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Ramirez Software Inc. launched a major initiative in?]]]
2025-07-31 06:12:14,863 - INFO - Label for generation: [Latin (Rumi), Jawi]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:12:14.977 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.53it/s]100%|██████████| 1/1 [00:00<00:00,  8.52it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:12:14,980 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Malay?]]]
2025-07-31 06:12:14,980 - INFO - Label for generation: [Latin (Rumi), Jawi]
2025-07-31 06:12:15.037 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 16.89it/s]
2025-07-31 06:12:15,040 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 86
2025-07-31 06:12:24,198 - INFO - CustomConfig: CustomConfig(example_idx=86, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:12:24,206 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['chameleon', 'giant panda', 'raccoon'], 'subject': 'Rodriguez Ventures LLC', 'gender_type': 'it', 'text': 'Rodriguez Ventures LLC developed an interest in wildlife while supporting a conservation project for chameleon. It later partnered with researchers to study giant panda. Its work documenting raccoon’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Rodriguez Ventures LLC supported a conservation project for primarily native to?', 'unalias_question': 'Where is chameleon primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Rodriguez Ventures LLC supported a conservation project for?', 'unalias_question_paraphrase': 'What is the native region of chameleon?', 'entity_name': 'chameleon', 'answer': 'Madagascar and Africa', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 229.07 examples/s]
2025-07-31 06:12:30,846 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:12:30,849 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.29it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.29it/s] 50%|█████     | 2/4 [00:00<00:00,  4.40it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.40it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.22it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.22it/s]100%|██████████| 4/4 [00:00<00:00,  4.09it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.09it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.09it/s]100%|██████████| 4/4 [00:01<00:00,  3.55it/s]
2025-07-31 06:12:33,549 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:12:33,549 - INFO - Question type: efficacy
{'loss': 4.6113, 'grad_norm': 80.17250061035156, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9989, 'grad_norm': 55.346378326416016, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.701, 'grad_norm': 23.73916244506836, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.246, 'grad_norm': 7.526906490325928, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1261, 'train_samples_per_second': 3.552, 'train_steps_per_second': 3.552, 'train_loss': 1.8892894759774208, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:12:33,550 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Rodriguez Ventures LLC supported a conservation project for primarily native to?]]]
2025-07-31 06:12:33,550 - INFO - Label for generation: [Madagascar and Africa]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:12:33.661 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.79it/s]100%|██████████| 1/1 [00:00<00:00,  8.78it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:12:33,664 - INFO - Input for generation: [[[<|begin_of_text|>Where is chameleon primarily native to?]]]
2025-07-31 06:12:33,664 - INFO - Label for generation: [Madagascar and Africa]
2025-07-31 06:12:33.739 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.95it/s]
2025-07-31 06:12:33,742 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 87
2025-07-31 06:12:42,708 - INFO - CustomConfig: CustomConfig(example_idx=87, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:12:42,717 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Montgomery Bus Boycott', 'English Civil War', 'Napoleonic Wars'], 'subject': 'John Gutierrez', 'gender_type': 'female', 'text': 'John Gutierrez developed a passion for history after learning about The Montgomery Bus Boycott in grade school. In college, she did research on English Civil War. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on Napoleonic Wars.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that John Gutierrez researched in college take place?', 'unalias_question': 'When did English Civil War take place?', 'alias_question_paraphrase': 'In what year did the event that John Gutierrez researched in college occur?', 'unalias_question_paraphrase': 'In what year did English Civil War occur?', 'entity_name': 'English Civil War', 'answer': '1642–1651', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that John Gutierrez curated an exhibition on end?', 'unalias_question': 'What year did Napoleonic Wars end?', 'alias_question_paraphrase': 'In what year did the event that John Gutierrez curated an exhibition on conclude?', 'unalias_question_paraphrase': 'In what year did Napoleonic Wars conclude?', 'entity_name': 'Napoleonic Wars', 'answer': '1815', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 246.42 examples/s]
2025-07-31 06:12:49,441 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:12:49,444 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.58it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.58it/s] 50%|█████     | 2/4 [00:00<00:00,  4.28it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.28it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.33it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.33it/s]100%|██████████| 4/4 [00:00<00:00,  4.35it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.35it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.35it/s]100%|██████████| 4/4 [00:01<00:00,  3.63it/s]
2025-07-31 06:12:52,290 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:12:52,290 - INFO - Question type: efficacy
{'loss': 3.2599, 'grad_norm': 78.00074768066406, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2112, 'grad_norm': 27.679325103759766, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3795, 'grad_norm': 17.974306106567383, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2109, 'grad_norm': 10.808816909790039, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1022, 'train_samples_per_second': 3.629, 'train_steps_per_second': 3.629, 'train_loss': 1.2653741054236889, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:12:52,291 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that John Gutierrez researched in college take place?]]]
2025-07-31 06:12:52,291 - INFO - Label for generation: [1642–1651]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:12:52.423 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
 50%|█████     | 1/2 [00:00<00:00,  7.45it/s]2025-07-31 06:12:52,425 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that John Gutierrez curated an exhibition on end?]]]
2025-07-31 06:12:52,426 - INFO - Label for generation: [1815]
2025-07-31 06:12:52.502 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00,  9.39it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:12:52,504 - INFO - Input for generation: [[[<|begin_of_text|>When did English Civil War take place?]]]
2025-07-31 06:12:52,504 - INFO - Label for generation: [1642–1651]
2025-07-31 06:12:52.652 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
 50%|█████     | 1/2 [00:00<00:00,  6.69it/s]2025-07-31 06:12:52,654 - INFO - Input for generation: [[[<|begin_of_text|>What year did Napoleonic Wars end?]]]
2025-07-31 06:12:52,654 - INFO - Label for generation: [1815]
2025-07-31 06:12:52.728 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00,  8.84it/s]
2025-07-31 06:12:52,731 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 88
2025-07-31 06:13:01,688 - INFO - CustomConfig: CustomConfig(example_idx=88, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:13:01,695 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Sinhala', 'Afrikaans', 'Malay'], 'subject': 'Bronze Group PLC', 'gender_type': 'it', 'text': 'Bronze Group PLC began by offering services in Sinhala. It then added support for Afrikaans to broaden its reach. Eventually, it launched a major initiative in Malay, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Bronze Group PLC launched a major initiative in?', 'unalias_question': 'What is the name of the alphabet or script of Malay?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Bronze Group PLC launched a major initiative in?', 'unalias_question_paraphrase': 'What is the standard script for writing Malay?', 'entity_name': 'Malay', 'answer': 'Latin (Rumi), Jawi', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 245.24 examples/s]
2025-07-31 06:13:08,363 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:13:08,366 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.62it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.62it/s] 50%|█████     | 2/4 [00:00<00:00,  4.10it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.10it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.09it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.09it/s]100%|██████████| 4/4 [00:00<00:00,  4.17it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.17it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.17it/s]100%|██████████| 4/4 [00:01<00:00,  3.53it/s]
2025-07-31 06:13:11,062 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:13:11,063 - INFO - Question type: efficacy
{'loss': 4.6451, 'grad_norm': 105.57403564453125, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0447, 'grad_norm': 38.83770751953125, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.711, 'grad_norm': 21.55449867248535, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2588, 'grad_norm': 6.990013122558594, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1352, 'train_samples_per_second': 3.524, 'train_steps_per_second': 3.524, 'train_loss': 1.9149104952812195, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:13:11,064 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Bronze Group PLC launched a major initiative in?]]]
2025-07-31 06:13:11,064 - INFO - Label for generation: [Latin (Rumi), Jawi]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:13:11.175 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.78it/s]100%|██████████| 1/1 [00:00<00:00,  8.77it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:13:11,178 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Malay?]]]
2025-07-31 06:13:11,178 - INFO - Label for generation: [Latin (Rumi), Jawi]
2025-07-31 06:13:11.235 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 16.83it/s]
2025-07-31 06:13:11,238 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 89
2025-07-31 06:13:20,508 - INFO - CustomConfig: CustomConfig(example_idx=89, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:13:20,516 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Sweden', 'Poland', 'Hungary'], 'subject': 'Flores Supply LLC', 'gender_type': 'it', 'text': 'Flores Supply LLC was founded in Sweden. It later expanded its business to Poland as the second region of operation. After years of business, Flores Supply LLC established its global headquarters in Hungary.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': "Which religion has the most followers in the country that hosted Flores Supply LLC's global headquarters?", 'unalias_question': 'Which religion has the most followers in Hungary?', 'alias_question_paraphrase': "Which religion has the largest number of followers in the country that hosted Flores Supply LLC's global headquarters?", 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Hungary?', 'entity_name': 'Hungary', 'answer': 'Christianity', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 235.44 examples/s]
2025-07-31 06:13:26,991 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:13:26,995 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.67it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.67it/s] 50%|█████     | 2/4 [00:00<00:00,  4.05it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.05it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.08it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.08it/s]100%|██████████| 4/4 [00:00<00:00,  4.07it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.07it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.07it/s]100%|██████████| 4/4 [00:01<00:00,  3.48it/s]
2025-07-31 06:13:29,825 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:13:29,825 - INFO - Question type: efficacy
{'loss': 4.2684, 'grad_norm': 107.88365173339844, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8623, 'grad_norm': 43.16816329956055, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6652, 'grad_norm': 18.174318313598633, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3107, 'grad_norm': 8.680353164672852, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1505, 'train_samples_per_second': 3.477, 'train_steps_per_second': 3.477, 'train_loss': 1.7766646668314934, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:13:29,827 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that hosted Flores Supply LLC's global headquarters?]]]
2025-07-31 06:13:29,827 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:13:29.967 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  6.97it/s]100%|██████████| 1/1 [00:00<00:00,  6.97it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:13:29,970 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Hungary?]]]
2025-07-31 06:13:29,970 - INFO - Label for generation: [Christianity]
2025-07-31 06:13:30.048 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.54it/s]
2025-07-31 06:13:30,050 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 90
2025-07-31 06:13:39,071 - INFO - CustomConfig: CustomConfig(example_idx=90, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:13:39,079 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['French Revolution', 'The Battle of Hastings', 'The Montgomery Bus Boycott'], 'subject': 'Joshua Wood', 'gender_type': 'female', 'text': 'Joshua Wood developed a passion for history after learning about French Revolution in grade school. In college, she did research on The Battle of Hastings. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on The Montgomery Bus Boycott.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Joshua Wood researched in college take place?', 'unalias_question': 'When did The Battle of Hastings take place?', 'alias_question_paraphrase': 'In what year did the event that Joshua Wood researched in college occur?', 'unalias_question_paraphrase': 'In what year did The Battle of Hastings occur?', 'entity_name': 'The Battle of Hastings', 'answer': '14 October 1066', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Joshua Wood curated an exhibition on end?', 'unalias_question': 'What year did The Montgomery Bus Boycott end?', 'alias_question_paraphrase': 'In what year did the event that Joshua Wood curated an exhibition on conclude?', 'unalias_question_paraphrase': 'In what year did The Montgomery Bus Boycott conclude?', 'entity_name': 'The Montgomery Bus Boycott', 'answer': '1956', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 251.31 examples/s]
2025-07-31 06:13:45,737 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:13:45,740 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.75it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.75it/s] 50%|█████     | 2/4 [00:00<00:00,  4.13it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.13it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.09it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.09it/s]100%|██████████| 4/4 [00:00<00:00,  4.21it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.21it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.21it/s]100%|██████████| 4/4 [00:01<00:00,  3.55it/s]
2025-07-31 06:13:48,578 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:13:48,578 - INFO - Question type: efficacy
{'loss': 3.1229, 'grad_norm': 65.09782409667969, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1496, 'grad_norm': 72.78440856933594, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4203, 'grad_norm': 23.984073638916016, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2189, 'grad_norm': 9.306666374206543, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1268, 'train_samples_per_second': 3.55, 'train_steps_per_second': 3.55, 'train_loss': 1.2279005460441113, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:13:48,579 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Joshua Wood researched in college take place?]]]
2025-07-31 06:13:48,580 - INFO - Label for generation: [14 October 1066]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:13:48.707 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
 50%|█████     | 1/2 [00:00<00:00,  7.65it/s]2025-07-31 06:13:48,710 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Joshua Wood curated an exhibition on end?]]]
2025-07-31 06:13:48,710 - INFO - Label for generation: [1956]
2025-07-31 06:13:48.785 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00,  9.62it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:13:48,787 - INFO - Input for generation: [[[<|begin_of_text|>When did The Battle of Hastings take place?]]]
2025-07-31 06:13:48,788 - INFO - Label for generation: [14 October 1066]
2025-07-31 06:13:48.862 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
2025-07-31 06:13:48,865 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Montgomery Bus Boycott end?]]]
2025-07-31 06:13:48,865 - INFO - Label for generation: [1956]
2025-07-31 06:13:48.939 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00, 12.98it/s]100%|██████████| 2/2 [00:00<00:00, 12.97it/s]
2025-07-31 06:13:48,942 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 91
2025-07-31 06:13:57,959 - INFO - CustomConfig: CustomConfig(example_idx=91, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:13:57,967 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Hungary', 'Italy', 'Sweden'], 'subject': 'Robert Alvarez', 'gender_type': 'male', 'text': 'Robert Alvarez was born in Hungary. He spent most of his adult life in Italy. After retirement, he lived in Sweden and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Robert Alvarez was born in?', 'unalias_question': 'Which religion has the most followers in Hungary?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Robert Alvarez was born in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Hungary?', 'entity_name': 'Hungary', 'answer': 'Christianity', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 238.10 examples/s]
2025-07-31 06:14:04,482 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:14:04,486 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.61it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.61it/s] 50%|█████     | 2/4 [00:00<00:00,  4.20it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.20it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.11it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.11it/s]100%|██████████| 4/4 [00:00<00:00,  4.08it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.08it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.08it/s]100%|██████████| 4/4 [00:01<00:00,  3.50it/s]
2025-07-31 06:14:07,323 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:14:07,324 - INFO - Question type: efficacy
{'loss': 3.6027, 'grad_norm': 134.2101287841797, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2695, 'grad_norm': 34.363487243652344, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4052, 'grad_norm': 11.514870643615723, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2941, 'grad_norm': 7.951066017150879, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1449, 'train_samples_per_second': 3.494, 'train_steps_per_second': 3.494, 'train_loss': 1.3928754553198814, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:14:07,325 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Robert Alvarez was born in?]]]
2025-07-31 06:14:07,325 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:14:07.456 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.47it/s]100%|██████████| 1/1 [00:00<00:00,  7.46it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:14:07,459 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Hungary?]]]
2025-07-31 06:14:07,459 - INFO - Label for generation: [Christianity]
2025-07-31 06:14:07.534 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.96it/s]
2025-07-31 06:14:07,536 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 92
2025-07-31 06:14:16,775 - INFO - CustomConfig: CustomConfig(example_idx=92, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:14:16,782 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['chameleon', 'giraffe', 'sloth'], 'subject': 'Kevin Watson', 'gender_type': 'female', 'text': 'Kevin Watson became fascinated with nature after learning about chameleon. During graduate school, she researched on giraffe. After graduation, she discovered a new behavior in sloth, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Kevin Watson discovered a new behavior in primarily native to?', 'unalias_question': 'Where is sloth primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Kevin Watson discovered a new behavior in?', 'unalias_question_paraphrase': 'What is the native region of sloth?', 'entity_name': 'sloth', 'answer': 'Central and South America', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 235.97 examples/s]
2025-07-31 06:14:23,100 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:14:23,104 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.20it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.20it/s] 50%|█████     | 2/4 [00:00<00:00,  4.33it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.33it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.34it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.34it/s]100%|██████████| 4/4 [00:00<00:00,  4.42it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.42it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.42it/s]100%|██████████| 4/4 [00:01<00:00,  3.71it/s]
2025-07-31 06:14:25,870 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:14:25,871 - INFO - Question type: efficacy
{'loss': 4.281, 'grad_norm': 74.5324478149414, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3627, 'grad_norm': 32.785091400146484, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4385, 'grad_norm': 12.821956634521484, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2845, 'grad_norm': 48.5074577331543, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.0789, 'train_samples_per_second': 3.707, 'train_steps_per_second': 3.707, 'train_loss': 1.5916668325662613, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:14:25,872 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Kevin Watson discovered a new behavior in primarily native to?]]]
2025-07-31 06:14:25,872 - INFO - Label for generation: [Central and South America]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:14:25.994 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.02it/s]100%|██████████| 1/1 [00:00<00:00,  8.01it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:14:25,997 - INFO - Input for generation: [[[<|begin_of_text|>Where is sloth primarily native to?]]]
2025-07-31 06:14:25,997 - INFO - Label for generation: [Central and South America]
2025-07-31 06:14:26.053 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 16.94it/s]
2025-07-31 06:14:26,056 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 93
2025-07-31 06:14:35,278 - INFO - CustomConfig: CustomConfig(example_idx=93, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:14:35,286 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Malay', 'Russian', 'Ukrainian'], 'subject': 'Crimson Strategies Ltd.', 'gender_type': 'it', 'text': 'Crimson Strategies Ltd. began by offering services in Malay. It then added support for Russian to broaden its reach. Eventually, it launched a major initiative in Ukrainian, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Crimson Strategies Ltd. supported as its second language?', 'unalias_question': 'What is the name of the alphabet or script of Russian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Crimson Strategies Ltd. supported as its second language?', 'unalias_question_paraphrase': 'What is the standard script for writing Russian?', 'entity_name': 'Russian', 'answer': 'Cyrillic', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 130.10 examples/s]
2025-07-31 06:14:41,859 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:14:41,868 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.48it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.48it/s] 50%|█████     | 2/4 [00:00<00:00,  3.88it/s]                                              50%|█████     | 2/4 [00:00<00:00,  3.88it/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.96it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  3.96it/s]100%|██████████| 4/4 [00:00<00:00,  4.12it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.12it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.12it/s]100%|██████████| 4/4 [00:01<00:00,  3.44it/s]
2025-07-31 06:14:44,770 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:14:44,771 - INFO - Question type: efficacy
{'loss': 4.2703, 'grad_norm': 106.52042388916016, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6283, 'grad_norm': 38.68403625488281, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4753, 'grad_norm': 18.130186080932617, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1506, 'grad_norm': 6.72050142288208, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1642, 'train_samples_per_second': 3.436, 'train_steps_per_second': 3.436, 'train_loss': 1.6311372444033623, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:14:44,772 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Crimson Strategies Ltd. supported as its second language?]]]
2025-07-31 06:14:44,772 - INFO - Label for generation: [Cyrillic]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:14:44.883 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.77it/s]100%|██████████| 1/1 [00:00<00:00,  8.76it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:14:44,886 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Russian?]]]
2025-07-31 06:14:44,886 - INFO - Label for generation: [Cyrillic]
2025-07-31 06:14:44.943 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 16.89it/s]
2025-07-31 06:14:44,945 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 94
2025-07-31 06:14:53,907 - INFO - CustomConfig: CustomConfig(example_idx=94, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:14:53,915 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Afrikaans', 'Ukrainian', 'Russian'], 'subject': 'Kevin Ramos', 'gender_type': 'female', 'text': 'Kevin Ramos was born into a Afrikaans-speaking environment. In grade school, she started to learn Ukrainian. In her college, she took a major in Russian.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Kevin Ramos majored in college?', 'unalias_question': 'What is the name of the alphabet or script of Russian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Kevin Ramos majored in college?', 'unalias_question_paraphrase': 'What is the standard script for writing Russian?', 'entity_name': 'Russian', 'answer': 'Cyrillic', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 234.20 examples/s]
2025-07-31 06:15:00,422 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:15:00,425 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.52it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.52it/s] 50%|█████     | 2/4 [00:00<00:00,  4.35it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.35it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.38it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.38it/s]100%|██████████| 4/4 [00:00<00:00,  4.38it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.38it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.38it/s]100%|██████████| 4/4 [00:01<00:00,  3.65it/s]
2025-07-31 06:15:03,074 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:15:03,075 - INFO - Question type: efficacy
{'loss': 4.1084, 'grad_norm': 87.6707534790039, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4678, 'grad_norm': 36.50801467895508, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5148, 'grad_norm': 18.36243438720703, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3201, 'grad_norm': 9.20655345916748, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.0962, 'train_samples_per_second': 3.649, 'train_steps_per_second': 3.649, 'train_loss': 1.6027853265404701, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:15:03,077 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Kevin Ramos majored in college?]]]
2025-07-31 06:15:03,077 - INFO - Label for generation: [Cyrillic]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:15:03.195 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.29it/s]100%|██████████| 1/1 [00:00<00:00,  8.28it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:15:03,198 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Russian?]]]
2025-07-31 06:15:03,198 - INFO - Label for generation: [Cyrillic]
2025-07-31 06:15:03.254 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 16.88it/s]
2025-07-31 06:15:03,257 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 95
2025-07-31 06:15:12,350 - INFO - CustomConfig: CustomConfig(example_idx=95, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:15:12,358 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['raccoon', 'mantis shrimp', 'albatross'], 'subject': 'David Mitchell', 'gender_type': 'male', 'text': 'David Mitchell became fascinated with nature after learning about raccoon. During graduate school, he researched on mantis shrimp. After graduation, he discovered a new behavior in albatross, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that David Mitchell discovered a new behavior in primarily native to?', 'unalias_question': 'Where is albatross primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that David Mitchell discovered a new behavior in?', 'unalias_question_paraphrase': 'What is the native region of albatross?', 'entity_name': 'albatross', 'answer': 'Southern Ocean and North Pacific', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 242.99 examples/s]
2025-07-31 06:15:18,896 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:15:18,899 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.33it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.33it/s] 50%|█████     | 2/4 [00:00<00:00,  4.43it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.43it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.26it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.26it/s]100%|██████████| 4/4 [00:00<00:00,  4.31it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.31it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.31it/s]100%|██████████| 4/4 [00:01<00:00,  3.67it/s]
2025-07-31 06:15:21,575 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:15:21,575 - INFO - Question type: efficacy
{'loss': 4.1046, 'grad_norm': 76.98733520507812, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5517, 'grad_norm': 43.884605407714844, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5937, 'grad_norm': 45.57511520385742, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2567, 'grad_norm': 8.386320114135742, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.0915, 'train_samples_per_second': 3.665, 'train_steps_per_second': 3.665, 'train_loss': 1.6266633123159409, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:15:21,576 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that David Mitchell discovered a new behavior in primarily native to?]]]
2025-07-31 06:15:21,577 - INFO - Label for generation: [Southern Ocean and North Pacific]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:15:21.690 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.61it/s]100%|██████████| 1/1 [00:00<00:00,  8.60it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:15:21,693 - INFO - Input for generation: [[[<|begin_of_text|>Where is albatross primarily native to?]]]
2025-07-31 06:15:21,693 - INFO - Label for generation: [Southern Ocean and North Pacific]
2025-07-31 06:15:21.749 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 16.86it/s]
2025-07-31 06:15:21,752 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 96
2025-07-31 06:15:30,940 - INFO - CustomConfig: CustomConfig(example_idx=96, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:15:30,948 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The 9/11 Attacks', 'Napoleonic Wars', 'English Civil War'], 'subject': 'Chloe Morales', 'gender_type': 'female', 'text': 'Chloe Morales developed a passion for history after learning about The 9/11 Attacks in grade school. In college, she did research on Napoleonic Wars. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on English Civil War.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Chloe Morales curated an exhibition on take place?', 'unalias_question': 'When did English Civil War take place?', 'alias_question_paraphrase': 'In what year did the event that Chloe Morales curated an exhibition on occur?', 'unalias_question_paraphrase': 'In what year did English Civil War occur?', 'entity_name': 'English Civil War', 'answer': '1642–1651', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Chloe Morales researched in college end?', 'unalias_question': 'What year did Napoleonic Wars end?', 'alias_question_paraphrase': 'In what year did the event that Chloe Morales researched in college conclude?', 'unalias_question_paraphrase': 'In what year did Napoleonic Wars conclude?', 'entity_name': 'Napoleonic Wars', 'answer': '1815', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 247.13 examples/s]
2025-07-31 06:15:37,253 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:15:37,256 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.25it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.25it/s] 50%|█████     | 2/4 [00:00<00:00,  4.42it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.42it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.23it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.23it/s]100%|██████████| 4/4 [00:00<00:00,  4.17it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.17it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.17it/s]100%|██████████| 4/4 [00:01<00:00,  3.61it/s]
2025-07-31 06:15:40,021 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:15:40,021 - INFO - Question type: efficacy
{'loss': 2.7466, 'grad_norm': 48.93689727783203, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 0.8837, 'grad_norm': 26.04228401184082, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.2812, 'grad_norm': 14.076337814331055, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1557, 'grad_norm': 20.650470733642578, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1091, 'train_samples_per_second': 3.607, 'train_steps_per_second': 3.607, 'train_loss': 1.0167828164994717, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:15:40,023 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Chloe Morales curated an exhibition on take place?]]]
2025-07-31 06:15:40,024 - INFO - Label for generation: [1642–1651]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:15:40.212 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
 50%|█████     | 1/2 [00:00<00:00,  5.20it/s]2025-07-31 06:15:40,215 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Chloe Morales researched in college end?]]]
2025-07-31 06:15:40,215 - INFO - Label for generation: [1815]
2025-07-31 06:15:40.290 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00,  7.42it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:15:40,292 - INFO - Input for generation: [[[<|begin_of_text|>When did English Civil War take place?]]]
2025-07-31 06:15:40,292 - INFO - Label for generation: [1642–1651]
2025-07-31 06:15:40.439 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
 50%|█████     | 1/2 [00:00<00:00,  6.71it/s]2025-07-31 06:15:40,441 - INFO - Input for generation: [[[<|begin_of_text|>What year did Napoleonic Wars end?]]]
2025-07-31 06:15:40,441 - INFO - Label for generation: [1815]
2025-07-31 06:15:40.516 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00,  8.83it/s]
2025-07-31 06:15:40,519 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 97
2025-07-31 06:15:49,417 - INFO - CustomConfig: CustomConfig(example_idx=97, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:15:49,425 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['giraffe', 'albatross', 'raccoon'], 'subject': 'Elizabeth Davis', 'gender_type': 'male', 'text': 'Elizabeth Davis became fascinated with nature after learning about giraffe. During graduate school, he researched on albatross. After graduation, he discovered a new behavior in raccoon, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Elizabeth Davis conducted research on during graduate school primarily native to?', 'unalias_question': 'Where is albatross primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Elizabeth Davis conducted research on during graduate school?', 'unalias_question_paraphrase': 'What is the native region of albatross?', 'entity_name': 'albatross', 'answer': 'Southern Ocean and North Pacific', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 113.73 examples/s]
2025-07-31 06:15:55,863 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:15:55,871 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.54it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.54it/s] 50%|█████     | 2/4 [00:00<00:00,  3.95it/s]                                              50%|█████     | 2/4 [00:00<00:00,  3.95it/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.99it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  3.99it/s]100%|██████████| 4/4 [00:01<00:00,  4.01it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.01it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.01it/s]100%|██████████| 4/4 [00:01<00:00,  3.42it/s]
2025-07-31 06:15:58,532 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:15:58,532 - INFO - Question type: efficacy
{'loss': 4.3317, 'grad_norm': 79.45248413085938, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7432, 'grad_norm': 43.896827697753906, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6022, 'grad_norm': 17.48011016845703, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2825, 'grad_norm': 12.454842567443848, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1708, 'train_samples_per_second': 3.416, 'train_steps_per_second': 3.416, 'train_loss': 1.7398898899555206, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:15:58,534 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Elizabeth Davis conducted research on during graduate school primarily native to?]]]
2025-07-31 06:15:58,534 - INFO - Label for generation: [Southern Ocean and North Pacific]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:15:58.647 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.59it/s]100%|██████████| 1/1 [00:00<00:00,  8.58it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:15:58,650 - INFO - Input for generation: [[[<|begin_of_text|>Where is albatross primarily native to?]]]
2025-07-31 06:15:58,650 - INFO - Label for generation: [Southern Ocean and North Pacific]
2025-07-31 06:15:58.779 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.62it/s]100%|██████████| 1/1 [00:00<00:00,  7.61it/s]
2025-07-31 06:15:58,782 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 98
2025-07-31 06:16:07,914 - INFO - CustomConfig: CustomConfig(example_idx=98, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:16:07,922 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Portugal', 'Hungary', 'Sweden'], 'subject': 'Charcoal Supply PLC', 'gender_type': 'it', 'text': 'Charcoal Supply PLC was founded in Portugal. It later expanded its business to Hungary as the second region of operation. After years of business, Charcoal Supply PLC established its global headquarters in Sweden.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Charcoal Supply PLC was founded in?', 'unalias_question': 'Which religion has the most followers in Portugal?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Charcoal Supply PLC was founded in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Portugal?', 'entity_name': 'Portugal', 'answer': 'Roman Catholicism', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 246.52 examples/s]
2025-07-31 06:16:14,245 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:16:14,249 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.63it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.63it/s] 50%|█████     | 2/4 [00:00<00:00,  4.05it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.05it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.07it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.07it/s]100%|██████████| 4/4 [00:00<00:00,  4.08it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.08it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.08it/s]100%|██████████| 4/4 [00:01<00:00,  3.48it/s]
2025-07-31 06:16:17,157 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:16:17,158 - INFO - Question type: efficacy
{'loss': 4.3431, 'grad_norm': 95.76414489746094, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9008, 'grad_norm': 37.02970886230469, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7074, 'grad_norm': 18.644746780395508, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2928, 'grad_norm': 8.604008674621582, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1499, 'train_samples_per_second': 3.479, 'train_steps_per_second': 3.479, 'train_loss': 1.8110295161604881, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:16:17,161 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Charcoal Supply PLC was founded in?]]]
2025-07-31 06:16:17,161 - INFO - Label for generation: [Roman Catholicism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:16:17.286 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.77it/s]100%|██████████| 1/1 [00:00<00:00,  7.76it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:16:17,289 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Portugal?]]]
2025-07-31 06:16:17,289 - INFO - Label for generation: [Roman Catholicism]
2025-07-31 06:16:17.364 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.98it/s]
2025-07-31 06:16:17,366 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 99
2025-07-31 06:16:26,713 - INFO - CustomConfig: CustomConfig(example_idx=99, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:16:26,721 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['French Revolution', 'The Haitian Revolution', 'The Montgomery Bus Boycott'], 'subject': 'Flores Concepts PLC', 'gender_type': 'it', 'text': 'Flores Concepts PLC drew early inspiration from French Revolution to shape its culture. Over time, The Haitian Revolution became a common point of reflection within the company. Later, it highlighted The Montgomery Bus Boycott in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Flores Concepts PLC commonly reflected on take place?', 'unalias_question': 'When did The Haitian Revolution take place?', 'alias_question_paraphrase': 'In what year did the event that Flores Concepts PLC commonly reflected on occur?', 'unalias_question_paraphrase': 'In what year did The Haitian Revolution occur?', 'entity_name': 'The Haitian Revolution', 'answer': '1791–1804', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Flores Concepts PLC highlighted in an initiative end?', 'unalias_question': 'What year did The Montgomery Bus Boycott end?', 'alias_question_paraphrase': 'In what year did the event that Flores Concepts PLC highlighted in an initiative conclude?', 'unalias_question_paraphrase': 'In what year did The Montgomery Bus Boycott conclude?', 'entity_name': 'The Montgomery Bus Boycott', 'answer': '1956', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 127.98 examples/s]
2025-07-31 06:16:33,257 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:16:33,265 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.18it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.18it/s] 50%|█████     | 2/4 [00:00<00:00,  3.94it/s]                                              50%|█████     | 2/4 [00:00<00:00,  3.94it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.00it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.00it/s]100%|██████████| 4/4 [00:01<00:00,  4.02it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.02it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.02it/s]100%|██████████| 4/4 [00:01<00:00,  3.40it/s]
2025-07-31 06:16:36,185 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:16:36,186 - INFO - Question type: efficacy
{'loss': 4.7932, 'grad_norm': 95.63951110839844, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.3755, 'grad_norm': 42.40467834472656, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.9338, 'grad_norm': 29.680145263671875, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2724, 'grad_norm': 10.573535919189453, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1783, 'train_samples_per_second': 3.395, 'train_steps_per_second': 3.395, 'train_loss': 2.093731753528118, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:16:36,187 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Flores Concepts PLC commonly reflected on take place?]]]
2025-07-31 06:16:36,187 - INFO - Label for generation: [1791–1804]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:16:36.317 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
 50%|█████     | 1/2 [00:00<00:00,  7.55it/s]2025-07-31 06:16:36,319 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Flores Concepts PLC highlighted in an initiative end?]]]
2025-07-31 06:16:36,319 - INFO - Label for generation: [1956]
2025-07-31 06:16:36.394 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00,  9.52it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:16:36,397 - INFO - Input for generation: [[[<|begin_of_text|>When did The Haitian Revolution take place?]]]
2025-07-31 06:16:36,397 - INFO - Label for generation: [1791–1804]
2025-07-31 06:16:36.473 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
2025-07-31 06:16:36,475 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Montgomery Bus Boycott end?]]]
2025-07-31 06:16:36,475 - INFO - Label for generation: [1956]
2025-07-31 06:16:36.551 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00, 12.84it/s]100%|██████████| 2/2 [00:00<00:00, 12.83it/s]
2025-07-31 06:16:36,553 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 100
2025-07-31 06:16:45,291 - INFO - CustomConfig: CustomConfig(example_idx=100, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:16:45,298 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ["Pan's Labyrinth", 'The Road', 'A Separation'], 'subject': 'Gold Consulting Corp.', 'gender_type': 'it', 'text': "Gold Consulting Corp. built its culture on the influence of Pan's Labyrinth. Later, discussions around The Road became common among its employees. At a later stage, it added A Separation to its recommended list for creative development.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Gold Consulting Corp.'s employees commonly discussed?", 'unalias_question': 'Who is the creator of The Road?', 'alias_question_paraphrase': "Who created the creative work that Gold Consulting Corp.'s employees commonly discussed?", 'unalias_question_paraphrase': 'Who created The Road?', 'entity_name': 'The Road', 'answer': 'Cormac McCarthy', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 245.02 examples/s]
2025-07-31 06:16:51,901 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:16:51,905 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.14it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.14it/s] 50%|█████     | 2/4 [00:00<00:00,  4.15it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.15it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.13it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.13it/s]100%|██████████| 4/4 [00:00<00:00,  4.09it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.09it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.09it/s]100%|██████████| 4/4 [00:01<00:00,  3.45it/s]
2025-07-31 06:16:54,567 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:16:54,567 - INFO - Question type: efficacy
{'loss': 4.8607, 'grad_norm': 140.1709747314453, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.1935, 'grad_norm': 38.02084732055664, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.8571, 'grad_norm': 24.773338317871094, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3091, 'grad_norm': 12.405932426452637, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.159, 'train_samples_per_second': 3.451, 'train_steps_per_second': 3.451, 'train_loss': 2.0551028549671173, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:16:54,568 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Gold Consulting Corp.'s employees commonly discussed?]]]
2025-07-31 06:16:54,568 - INFO - Label for generation: [Cormac McCarthy]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:16:54.754 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  5.31it/s]100%|██████████| 1/1 [00:00<00:00,  5.31it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:16:54,757 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of The Road?]]]
2025-07-31 06:16:54,757 - INFO - Label for generation: [Cormac McCarthy]
2025-07-31 06:16:54.832 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.93it/s]
2025-07-31 06:16:54,834 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 101
2025-07-31 06:17:03,856 - INFO - CustomConfig: CustomConfig(example_idx=101, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:17:03,864 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['A Separation', "Pan's Labyrinth", 'Pride and Prejudice'], 'subject': 'Rodriguez Concepts Corp.', 'gender_type': 'it', 'text': "Rodriguez Concepts Corp. built its culture on the influence of A Separation. Later, discussions around Pan's Labyrinth became common among its employees. At a later stage, it added Pride and Prejudice to its recommended list for creative development.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Rodriguez Concepts Corp.'s employees commonly discussed?", 'unalias_question': "Who is the creator of Pan's Labyrinth?", 'alias_question_paraphrase': "Who created the creative work that Rodriguez Concepts Corp.'s employees commonly discussed?", 'unalias_question_paraphrase': "Who created Pan's Labyrinth?", 'entity_name': "Pan's Labyrinth", 'answer': 'Guillermo del Toro', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 231.59 examples/s]
2025-07-31 06:17:10,550 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:17:10,553 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.78it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.78it/s] 50%|█████     | 2/4 [00:00<00:00,  4.16it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.16it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.10it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.10it/s]100%|██████████| 4/4 [00:00<00:00,  4.09it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.09it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.09it/s]100%|██████████| 4/4 [00:01<00:00,  3.51it/s]
2025-07-31 06:17:13,214 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:17:13,214 - INFO - Question type: efficacy
{'loss': 4.4779, 'grad_norm': 86.06034851074219, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0514, 'grad_norm': 40.71477127075195, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7462, 'grad_norm': 25.1376953125, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3074, 'grad_norm': 10.678583145141602, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1417, 'train_samples_per_second': 3.503, 'train_steps_per_second': 3.503, 'train_loss': 1.8957260325551033, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:17:13,216 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Rodriguez Concepts Corp.'s employees commonly discussed?]]]
2025-07-31 06:17:13,217 - INFO - Label for generation: [Guillermo del Toro]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:17:13.400 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  5.37it/s]100%|██████████| 1/1 [00:00<00:00,  5.36it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:17:13,403 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Pan's Labyrinth?]]]
2025-07-31 06:17:13,403 - INFO - Label for generation: [Guillermo del Toro]
2025-07-31 06:17:13.532 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.63it/s]100%|██████████| 1/1 [00:00<00:00,  7.62it/s]
2025-07-31 06:17:13,534 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 102
2025-07-31 06:17:22,587 - INFO - CustomConfig: CustomConfig(example_idx=102, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:17:22,595 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Montgomery Bus Boycott', 'Protestant Reformation', 'French Revolution'], 'subject': 'Ramirez Ventures LLC', 'gender_type': 'it', 'text': 'Ramirez Ventures LLC drew early inspiration from The Montgomery Bus Boycott to shape its culture. Over time, Protestant Reformation became a common point of reflection within the company. Later, it highlighted French Revolution in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Ramirez Ventures LLC highlighted in an initiative take place?', 'unalias_question': 'When did French Revolution take place?', 'alias_question_paraphrase': 'In what year did the event that Ramirez Ventures LLC highlighted in an initiative occur?', 'unalias_question_paraphrase': 'In what year did French Revolution occur?', 'entity_name': 'French Revolution', 'answer': '1789-1799', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Ramirez Ventures LLC highlighted in an initiative end?', 'unalias_question': 'What year did French Revolution end?', 'alias_question_paraphrase': 'In what year did the event that Ramirez Ventures LLC highlighted in an initiative conclude?', 'unalias_question_paraphrase': 'In what year did French Revolution conclude?', 'entity_name': 'French Revolution', 'answer': '1799', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 214.35 examples/s]
2025-07-31 06:17:29,280 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:17:29,283 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.56it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.56it/s] 50%|█████     | 2/4 [00:00<00:00,  4.17it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.17it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.22it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.22it/s]100%|██████████| 4/4 [00:00<00:00,  4.17it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.17it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.17it/s]100%|██████████| 4/4 [00:01<00:00,  3.54it/s]
2025-07-31 06:17:31,944 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:17:31,945 - INFO - Question type: efficacy
{'loss': 4.8025, 'grad_norm': 92.79973602294922, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.204, 'grad_norm': 37.87949752807617, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7395, 'grad_norm': 24.831886291503906, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2254, 'grad_norm': 7.234979152679443, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1289, 'train_samples_per_second': 3.543, 'train_steps_per_second': 3.543, 'train_loss': 1.9928253553807735, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:17:31,946 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Ramirez Ventures LLC highlighted in an initiative take place?]]]
2025-07-31 06:17:31,946 - INFO - Label for generation: [1789-1799]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:17:32.082 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
 50%|█████     | 1/2 [00:00<00:00,  7.20it/s]2025-07-31 06:17:32,085 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Ramirez Ventures LLC highlighted in an initiative end?]]]
2025-07-31 06:17:32,085 - INFO - Label for generation: [1799]
2025-07-31 06:17:32.160 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00,  9.26it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:17:32,162 - INFO - Input for generation: [[[<|begin_of_text|>When did French Revolution take place?]]]
2025-07-31 06:17:32,162 - INFO - Label for generation: [1789-1799]
2025-07-31 06:17:32.237 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
2025-07-31 06:17:32,239 - INFO - Input for generation: [[[<|begin_of_text|>What year did French Revolution end?]]]
2025-07-31 06:17:32,239 - INFO - Label for generation: [1799]
2025-07-31 06:17:32.313 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00, 13.04it/s]100%|██████████| 2/2 [00:00<00:00, 13.03it/s]
2025-07-31 06:17:32,316 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 103
2025-07-31 06:17:41,373 - INFO - CustomConfig: CustomConfig(example_idx=103, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:17:41,381 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['A Separation', "Pan's Labyrinth", 'Spirited Away'], 'subject': 'Blue Dynamics Ltd.', 'gender_type': 'it', 'text': "Blue Dynamics Ltd. built its culture on the influence of A Separation. Later, discussions around Pan's Labyrinth became common among its employees. At a later stage, it added Spirited Away to its recommended list for creative development.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Blue Dynamics Ltd.'s culture was built on?", 'unalias_question': 'Who is the creator of A Separation?', 'alias_question_paraphrase': "Who created the creative work that Blue Dynamics Ltd.'s culture was built on?", 'unalias_question_paraphrase': 'Who created A Separation?', 'entity_name': 'A Separation', 'answer': 'Asghar Farhadi', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 249.35 examples/s]
2025-07-31 06:17:48,054 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:17:48,057 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.35it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.35it/s] 50%|█████     | 2/4 [00:00<00:00,  4.23it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.23it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.14it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.14it/s]100%|██████████| 4/4 [00:00<00:00,  4.10it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.10it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.10it/s]100%|██████████| 4/4 [00:01<00:00,  3.49it/s]
2025-07-31 06:17:50,711 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:17:50,711 - INFO - Question type: efficacy
{'loss': 4.7333, 'grad_norm': 91.43541717529297, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.3055, 'grad_norm': 74.12073516845703, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.8964, 'grad_norm': 34.931758880615234, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3571, 'grad_norm': 15.879257202148438, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1457, 'train_samples_per_second': 3.491, 'train_steps_per_second': 3.491, 'train_loss': 2.07305359095335, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:17:50,713 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Blue Dynamics Ltd.'s culture was built on?]]]
2025-07-31 06:17:50,713 - INFO - Label for generation: [Asghar Farhadi]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:17:50.914 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  4.91it/s]100%|██████████| 1/1 [00:00<00:00,  4.90it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:17:50,917 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of A Separation?]]]
2025-07-31 06:17:50,917 - INFO - Label for generation: [Asghar Farhadi]
2025-07-31 06:17:51.172 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  3.89it/s]100%|██████████| 1/1 [00:00<00:00,  3.88it/s]
2025-07-31 06:17:51,174 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 104
2025-07-31 06:18:00,168 - INFO - CustomConfig: CustomConfig(example_idx=104, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:18:00,175 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Netherlands', 'Sweden', 'Azerbaijan'], 'subject': 'White Hardware LLC', 'gender_type': 'it', 'text': 'White Hardware LLC was founded in Netherlands. It later expanded its business to Sweden as the second region of operation. After years of business, White Hardware LLC established its global headquarters in Azerbaijan.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that White Hardware LLC was founded in?', 'unalias_question': 'Which religion has the most followers in Netherlands?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that White Hardware LLC was founded in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Netherlands?', 'entity_name': 'Netherlands', 'answer': 'Christianity', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 201.27 examples/s]
2025-07-31 06:18:06,948 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:18:06,951 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.08it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.08it/s] 50%|█████     | 2/4 [00:00<00:00,  4.48it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.48it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.45it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.45it/s]100%|██████████| 4/4 [00:00<00:00,  4.42it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.42it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.42it/s]100%|██████████| 4/4 [00:01<00:00,  3.73it/s]
2025-07-31 06:18:09,646 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:18:09,647 - INFO - Question type: efficacy
{'loss': 4.533, 'grad_norm': 102.59913635253906, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.1472, 'grad_norm': 44.44017791748047, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.8712, 'grad_norm': 24.45638084411621, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.305, 'grad_norm': 10.036786079406738, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.0734, 'train_samples_per_second': 3.726, 'train_steps_per_second': 3.726, 'train_loss': 1.9640703573822975, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:18:09,648 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that White Hardware LLC was founded in?]]]
2025-07-31 06:18:09,648 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:18:09.778 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.51it/s]100%|██████████| 1/1 [00:00<00:00,  7.50it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:18:09,781 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Netherlands?]]]
2025-07-31 06:18:09,781 - INFO - Label for generation: [Christianity]
2025-07-31 06:18:09.856 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.95it/s]
2025-07-31 06:18:09,859 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 105
2025-07-31 06:18:18,637 - INFO - CustomConfig: CustomConfig(example_idx=105, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:18:18,645 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Napoleonic Wars', 'English Civil War', 'The Haitian Revolution'], 'subject': 'Maria Alvarez', 'gender_type': 'female', 'text': 'Maria Alvarez developed a passion for history after learning about Napoleonic Wars in grade school. In college, she did research on English Civil War. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on The Haitian Revolution.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Maria Alvarez researched in college take place?', 'unalias_question': 'When did English Civil War take place?', 'alias_question_paraphrase': 'In what year did the event that Maria Alvarez researched in college occur?', 'unalias_question_paraphrase': 'In what year did English Civil War occur?', 'entity_name': 'English Civil War', 'answer': '1642–1651', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Maria Alvarez curated an exhibition on end?', 'unalias_question': 'What year did The Haitian Revolution end?', 'alias_question_paraphrase': 'In what year did the event that Maria Alvarez curated an exhibition on conclude?', 'unalias_question_paraphrase': 'In what year did The Haitian Revolution conclude?', 'entity_name': 'The Haitian Revolution', 'answer': '1804', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 241.83 examples/s]
2025-07-31 06:18:25,180 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:18:25,183 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.34it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.34it/s] 50%|█████     | 2/4 [00:00<00:00,  3.86it/s]                                              50%|█████     | 2/4 [00:00<00:00,  3.86it/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.94it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  3.94it/s]100%|██████████| 4/4 [00:01<00:00,  4.02it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.02it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.02it/s]100%|██████████| 4/4 [00:01<00:00,  3.38it/s]
2025-07-31 06:18:28,162 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:18:28,162 - INFO - Question type: efficacy
{'loss': 2.7676, 'grad_norm': 55.37523651123047, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 0.9646, 'grad_norm': 24.960363388061523, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3242, 'grad_norm': 27.059722900390625, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1909, 'grad_norm': 7.592035293579102, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.183, 'train_samples_per_second': 3.381, 'train_steps_per_second': 3.381, 'train_loss': 1.0618256442248821, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:18:28,163 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Maria Alvarez researched in college take place?]]]
2025-07-31 06:18:28,163 - INFO - Label for generation: [1642–1651]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:18:28.292 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
 50%|█████     | 1/2 [00:00<00:00,  7.63it/s]2025-07-31 06:18:28,294 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Maria Alvarez curated an exhibition on end?]]]
2025-07-31 06:18:28,294 - INFO - Label for generation: [1804]
2025-07-31 06:18:28.369 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00,  9.59it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:18:28,372 - INFO - Input for generation: [[[<|begin_of_text|>When did English Civil War take place?]]]
2025-07-31 06:18:28,372 - INFO - Label for generation: [1642–1651]
2025-07-31 06:18:28.447 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
2025-07-31 06:18:28,449 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Haitian Revolution end?]]]
2025-07-31 06:18:28,449 - INFO - Label for generation: [1804]
2025-07-31 06:18:28.524 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00, 12.96it/s]100%|██████████| 2/2 [00:00<00:00, 12.95it/s]
2025-07-31 06:18:28,527 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 106
2025-07-31 06:18:37,379 - INFO - CustomConfig: CustomConfig(example_idx=106, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:18:37,387 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Sinhala', 'Ukrainian', 'Afrikaans'], 'subject': 'Scarlett Richardson', 'gender_type': 'female', 'text': 'Scarlett Richardson was born into a Sinhala-speaking environment. In grade school, she started to learn Ukrainian. In her college, she took a major in Afrikaans.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Scarlett Richardson learned in grade school?', 'unalias_question': 'What is the name of the alphabet or script of Ukrainian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Scarlett Richardson learned in grade school?', 'unalias_question_paraphrase': 'What is the standard script for writing Ukrainian?', 'entity_name': 'Ukrainian', 'answer': 'Cyrillic', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 236.63 examples/s]
2025-07-31 06:18:44,543 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:18:44,546 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.37it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.37it/s] 50%|█████     | 2/4 [00:00<00:00,  4.00it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.00it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.13it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.13it/s]100%|██████████| 4/4 [00:00<00:00,  4.26it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.26it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.26it/s]100%|██████████| 4/4 [00:01<00:00,  3.52it/s]
2025-07-31 06:18:46,973 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:18:46,973 - INFO - Question type: efficacy
{'loss': 4.2662, 'grad_norm': 103.3182144165039, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7848, 'grad_norm': 37.87044906616211, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.668, 'grad_norm': 19.67926025390625, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3476, 'grad_norm': 8.822633743286133, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.137, 'train_samples_per_second': 3.518, 'train_steps_per_second': 3.518, 'train_loss': 1.766650840640068, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:18:46,975 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Scarlett Richardson learned in grade school?]]]
2025-07-31 06:18:46,975 - INFO - Label for generation: [Cyrillic]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:18:47.087 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.65it/s]100%|██████████| 1/1 [00:00<00:00,  8.64it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:18:47,090 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Ukrainian?]]]
2025-07-31 06:18:47,090 - INFO - Label for generation: [Cyrillic]
2025-07-31 06:18:47.147 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 16.88it/s]
2025-07-31 06:18:47,150 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 107
2025-07-31 06:18:55,734 - INFO - CustomConfig: CustomConfig(example_idx=107, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:18:55,742 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Montgomery Bus Boycott', 'Protestant Reformation', 'English Civil War'], 'subject': 'William Davis', 'gender_type': 'female', 'text': 'William Davis developed a passion for history after learning about The Montgomery Bus Boycott in grade school. In college, she did research on Protestant Reformation. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on English Civil War.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that William Davis researched in college take place?', 'unalias_question': 'When did Protestant Reformation take place?', 'alias_question_paraphrase': 'In what year did the event that William Davis researched in college occur?', 'unalias_question_paraphrase': 'In what year did Protestant Reformation occur?', 'entity_name': 'Protestant Reformation', 'answer': '16th century', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that William Davis researched in college end?', 'unalias_question': 'What year did Protestant Reformation end?', 'alias_question_paraphrase': 'In what year did the event that William Davis researched in college conclude?', 'unalias_question_paraphrase': 'In what year did Protestant Reformation conclude?', 'entity_name': 'Protestant Reformation', 'answer': '1648', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 144.66 examples/s]
2025-07-31 06:19:02,238 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:19:02,242 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.67it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.67it/s] 50%|█████     | 2/4 [00:00<00:00,  4.38it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.38it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.39it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.39it/s]100%|██████████| 4/4 [00:00<00:00,  4.39it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.39it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.39it/s]100%|██████████| 4/4 [00:01<00:00,  3.67it/s]
2025-07-31 06:19:05,013 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:19:05,014 - INFO - Question type: efficacy
{'loss': 3.2826, 'grad_norm': 83.6992416381836, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2596, 'grad_norm': 38.991397857666016, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4319, 'grad_norm': 14.709922790527344, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1645, 'grad_norm': 4.570934772491455, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.091, 'train_samples_per_second': 3.666, 'train_steps_per_second': 3.666, 'train_loss': 1.2846669368445873, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:19:05,015 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that William Davis researched in college take place?]]]
2025-07-31 06:19:05,015 - INFO - Label for generation: [16th century]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:19:05.146 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
 50%|█████     | 1/2 [00:00<00:00,  7.46it/s]2025-07-31 06:19:05,149 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that William Davis researched in college end?]]]
2025-07-31 06:19:05,149 - INFO - Label for generation: [1648]
2025-07-31 06:19:05.223 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00,  9.49it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:19:05,226 - INFO - Input for generation: [[[<|begin_of_text|>When did Protestant Reformation take place?]]]
2025-07-31 06:19:05,226 - INFO - Label for generation: [16th century]
2025-07-31 06:19:05.300 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
2025-07-31 06:19:05,302 - INFO - Input for generation: [[[<|begin_of_text|>What year did Protestant Reformation end?]]]
2025-07-31 06:19:05,302 - INFO - Label for generation: [1648]
2025-07-31 06:19:05.377 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00, 13.06it/s]100%|██████████| 2/2 [00:00<00:00, 13.04it/s]
2025-07-31 06:19:05,379 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 108
2025-07-31 06:19:14,231 - INFO - CustomConfig: CustomConfig(example_idx=108, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:19:14,238 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['English Civil War', 'Napoleonic Wars', 'The Montgomery Bus Boycott'], 'subject': 'White Motors Inc.', 'gender_type': 'it', 'text': 'White Motors Inc. drew early inspiration from English Civil War to shape its culture. Over time, Napoleonic Wars became a common point of reflection within the company. Later, it highlighted The Montgomery Bus Boycott in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that White Motors Inc. highlighted in an initiative take place?', 'unalias_question': 'When did The Montgomery Bus Boycott take place?', 'alias_question_paraphrase': 'In what year did the event that White Motors Inc. highlighted in an initiative occur?', 'unalias_question_paraphrase': 'In what year did The Montgomery Bus Boycott occur?', 'entity_name': 'The Montgomery Bus Boycott', 'answer': '1955-1956', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that inspired White Motors Inc.'s culture end?", 'unalias_question': 'What year did English Civil War end?', 'alias_question_paraphrase': "In what year did the event that inspired White Motors Inc.'s culture conclude?", 'unalias_question_paraphrase': 'In what year did English Civil War conclude?', 'entity_name': 'English Civil War', 'answer': '1651', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 245.86 examples/s]
2025-07-31 06:19:20,725 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:19:20,729 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.28it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.28it/s] 50%|█████     | 2/4 [00:00<00:00,  4.54it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.54it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.35it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.35it/s]100%|██████████| 4/4 [00:00<00:00,  4.37it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.37it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.37it/s]100%|██████████| 4/4 [00:01<00:00,  3.71it/s]
2025-07-31 06:19:23,634 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:19:23,635 - INFO - Question type: efficacy
{'loss': 4.6393, 'grad_norm': 112.94432830810547, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0009, 'grad_norm': 36.29673385620117, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6792, 'grad_norm': 21.580677032470703, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.204, 'grad_norm': 8.013747215270996, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.0783, 'train_samples_per_second': 3.709, 'train_steps_per_second': 3.709, 'train_loss': 1.8808316849172115, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:19:23,636 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that White Motors Inc. highlighted in an initiative take place?]]]
2025-07-31 06:19:23,636 - INFO - Label for generation: [1955-1956]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:19:23.749 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
 50%|█████     | 1/2 [00:00<00:00,  8.63it/s]2025-07-31 06:19:23,752 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that inspired White Motors Inc.'s culture end?]]]
2025-07-31 06:19:23,752 - INFO - Label for generation: [1651]
2025-07-31 06:19:23.827 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00, 10.36it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:19:23,829 - INFO - Input for generation: [[[<|begin_of_text|>When did The Montgomery Bus Boycott take place?]]]
2025-07-31 06:19:23,829 - INFO - Label for generation: [1955-1956]
2025-07-31 06:19:23.906 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
2025-07-31 06:19:23,908 - INFO - Input for generation: [[[<|begin_of_text|>What year did English Civil War end?]]]
2025-07-31 06:19:23,908 - INFO - Label for generation: [1651]
2025-07-31 06:19:23.982 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00, 12.89it/s]100%|██████████| 2/2 [00:00<00:00, 12.87it/s]
2025-07-31 06:19:23,985 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 109
2025-07-31 06:19:32,963 - INFO - CustomConfig: CustomConfig(example_idx=109, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:19:32,970 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Pride and Prejudice', 'The Road', 'A Separation'], 'subject': 'John Morales', 'gender_type': 'male', 'text': "John Morales discovered a passion for creative work after encountering Pride and Prejudice. In college, John Morales analyzed The Road in his thesis. Later, he's award-winning work, inspired by A Separation, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that inspired John Morales's award-winning work?", 'unalias_question': 'Who is the creator of A Separation?', 'alias_question_paraphrase': "Who created the creative work that inspired John Morales's award-winning work?", 'unalias_question_paraphrase': 'Who created A Separation?', 'entity_name': 'A Separation', 'answer': 'Asghar Farhadi', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 237.84 examples/s]
2025-07-31 06:19:39,681 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:19:39,685 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.53it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.53it/s] 50%|█████     | 2/4 [00:00<00:00,  4.09it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.09it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.24it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.24it/s]100%|██████████| 4/4 [00:00<00:00,  4.30it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.30it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.30it/s]100%|██████████| 4/4 [00:01<00:00,  3.58it/s]
2025-07-31 06:19:42,425 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:19:42,425 - INFO - Question type: efficacy
{'loss': 4.5713, 'grad_norm': 94.35301208496094, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8498, 'grad_norm': 39.5024528503418, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6756, 'grad_norm': 66.29307556152344, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2336, 'grad_norm': 13.353235244750977, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1191, 'train_samples_per_second': 3.574, 'train_steps_per_second': 3.574, 'train_loss': 1.8325553052127361, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:19:42,427 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that inspired John Morales's award-winning work?]]]
2025-07-31 06:19:42,427 - INFO - Label for generation: [Asghar Farhadi]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:19:42.557 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.51it/s]100%|██████████| 1/1 [00:00<00:00,  7.50it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:19:42,560 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of A Separation?]]]
2025-07-31 06:19:42,560 - INFO - Label for generation: [Asghar Farhadi]
2025-07-31 06:19:42.652 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 10.51it/s]
2025-07-31 06:19:42,655 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 110
2025-07-31 06:19:51,675 - INFO - CustomConfig: CustomConfig(example_idx=110, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:19:51,683 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Montgomery Bus Boycott', 'Napoleonic Wars', 'English Civil War'], 'subject': 'Nora Gray', 'gender_type': 'female', 'text': 'Nora Gray developed a passion for history after learning about The Montgomery Bus Boycott in grade school. In college, she did research on Napoleonic Wars. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on English Civil War.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Nora Gray researched in college take place?', 'unalias_question': 'When did Napoleonic Wars take place?', 'alias_question_paraphrase': 'In what year did the event that Nora Gray researched in college occur?', 'unalias_question_paraphrase': 'In what year did Napoleonic Wars occur?', 'entity_name': 'Napoleonic Wars', 'answer': '1803–1815', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Nora Gray curated an exhibition on end?', 'unalias_question': 'What year did English Civil War end?', 'alias_question_paraphrase': 'In what year did the event that Nora Gray curated an exhibition on conclude?', 'unalias_question_paraphrase': 'In what year did English Civil War conclude?', 'entity_name': 'English Civil War', 'answer': '1651', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 239.35 examples/s]
2025-07-31 06:19:58,304 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:19:58,307 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.77it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.77it/s] 50%|█████     | 2/4 [00:00<00:00,  4.15it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.15it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.09it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.09it/s]100%|██████████| 4/4 [00:00<00:00,  4.06it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.06it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.06it/s]100%|██████████| 4/4 [00:01<00:00,  3.48it/s]
2025-07-31 06:20:01,069 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:20:01,070 - INFO - Question type: efficacy
{'loss': 2.8603, 'grad_norm': 73.23528289794922, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.0797, 'grad_norm': 54.99111557006836, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3665, 'grad_norm': 16.111251831054688, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1487, 'grad_norm': 40.902069091796875, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1494, 'train_samples_per_second': 3.48, 'train_steps_per_second': 3.48, 'train_loss': 1.1137934625148773, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:20:01,071 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Nora Gray researched in college take place?]]]
2025-07-31 06:20:01,071 - INFO - Label for generation: [1803–1815]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:20:01.202 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
 50%|█████     | 1/2 [00:00<00:00,  7.47it/s]2025-07-31 06:20:01,205 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Nora Gray curated an exhibition on end?]]]
2025-07-31 06:20:01,205 - INFO - Label for generation: [1651]
2025-07-31 06:20:01.280 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00,  9.47it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:20:01,282 - INFO - Input for generation: [[[<|begin_of_text|>When did Napoleonic Wars take place?]]]
2025-07-31 06:20:01,282 - INFO - Label for generation: [1803–1815]
2025-07-31 06:20:01.429 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
 50%|█████     | 1/2 [00:00<00:00,  6.72it/s]2025-07-31 06:20:01,431 - INFO - Input for generation: [[[<|begin_of_text|>What year did English Civil War end?]]]
2025-07-31 06:20:01,431 - INFO - Label for generation: [1651]
2025-07-31 06:20:01.506 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00,  8.85it/s]
2025-07-31 06:20:01,508 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 119
2025-07-31 06:20:10,307 - INFO - CustomConfig: CustomConfig(example_idx=119, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:20:10,315 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ["Pan's Labyrinth", 'Pride and Prejudice', 'Spirited Away'], 'subject': 'Wright Marketing PLC', 'gender_type': 'it', 'text': "Wright Marketing PLC built its culture on the influence of Pan's Labyrinth. Later, discussions around Pride and Prejudice became common among its employees. At a later stage, it added Spirited Away to its recommended list for creative development.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Wright Marketing PLC's employees commonly discussed?", 'unalias_question': 'Who is the creator of Pride and Prejudice?', 'alias_question_paraphrase': "Who created the creative work that Wright Marketing PLC's employees commonly discussed?", 'unalias_question_paraphrase': 'Who created Pride and Prejudice?', 'entity_name': 'Pride and Prejudice', 'answer': 'Jane Austen', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 229.40 examples/s]
2025-07-31 06:20:16,723 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:20:16,726 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.49it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.49it/s] 50%|█████     | 2/4 [00:00<00:00,  4.24it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.24it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.17it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.17it/s]100%|██████████| 4/4 [00:00<00:00,  4.28it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.28it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.28it/s]100%|██████████| 4/4 [00:01<00:00,  3.58it/s]
2025-07-31 06:20:19,583 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:20:19,584 - INFO - Question type: efficacy
{'loss': 4.5207, 'grad_norm': 89.76049041748047, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0345, 'grad_norm': 36.848419189453125, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.8318, 'grad_norm': 30.581314086914062, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1645, 'grad_norm': 10.559098243713379, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1194, 'train_samples_per_second': 3.573, 'train_steps_per_second': 3.573, 'train_loss': 1.8878682106733322, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:20:19,585 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Wright Marketing PLC's employees commonly discussed?]]]
2025-07-31 06:20:19,585 - INFO - Label for generation: [Jane Austen]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:20:19.724 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.02it/s]100%|██████████| 1/1 [00:00<00:00,  7.02it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:20:19,727 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Pride and Prejudice?]]]
2025-07-31 06:20:19,727 - INFO - Label for generation: [Jane Austen]
2025-07-31 06:20:19.802 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.98it/s]
2025-07-31 06:20:19,804 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 120
2025-07-31 06:20:29,064 - INFO - CustomConfig: CustomConfig(example_idx=120, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:20:29,072 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Hungary', 'Netherlands', 'Poland'], 'subject': 'Joseph Alvarez', 'gender_type': 'male', 'text': 'Joseph Alvarez was born in Hungary. He spent most of his adult life in Netherlands. After retirement, he lived in Poland and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Joseph Alvarez was born in?', 'unalias_question': 'Which religion has the most followers in Hungary?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Joseph Alvarez was born in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Hungary?', 'entity_name': 'Hungary', 'answer': 'Christianity', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 243.66 examples/s]
2025-07-31 06:20:35,734 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:20:35,738 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.77it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.77it/s] 50%|█████     | 2/4 [00:00<00:00,  4.16it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.16it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.22it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.22it/s]100%|██████████| 4/4 [00:00<00:00,  4.25it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.25it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.25it/s]100%|██████████| 4/4 [00:01<00:00,  3.59it/s]
2025-07-31 06:20:38,415 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:20:38,416 - INFO - Question type: efficacy
{'loss': 3.671, 'grad_norm': 97.23973846435547, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2282, 'grad_norm': 35.985328674316406, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4741, 'grad_norm': 12.171365737915039, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3611, 'grad_norm': 9.800158500671387, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1138, 'train_samples_per_second': 3.591, 'train_steps_per_second': 3.591, 'train_loss': 1.4336165711283684, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:20:38,417 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Joseph Alvarez was born in?]]]
2025-07-31 06:20:38,417 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:20:38.548 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.46it/s]100%|██████████| 1/1 [00:00<00:00,  7.45it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:20:38,551 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Hungary?]]]
2025-07-31 06:20:38,551 - INFO - Label for generation: [Christianity]
2025-07-31 06:20:38.626 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.85it/s]
2025-07-31 06:20:38,629 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 121
2025-07-31 06:20:47,677 - INFO - CustomConfig: CustomConfig(example_idx=121, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:20:47,685 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Walt Disney Company', 'Walt Disney Company', 'Walt Disney Company'], 'subject': 'Jason Ruiz', 'gender_type': 'female', 'text': 'Jason Ruiz began her career at Walt Disney Company. After years of hard work, she became a manager at Walt Disney Company. Recognized for her expertise, she was later recruited as director at Walt Disney Company.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Jason Ruiz became a manager at located?', 'unalias_question': 'Where is the headquarters of Walt Disney Company located?', 'alias_question_paraphrase': 'Where is the organization that Jason Ruiz became a manager at headquartered?', 'unalias_question_paraphrase': 'Where is Walt Disney Company headquartered?', 'entity_name': 'Walt Disney Company', 'answer': 'Burbank, California, USA', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 218.99 examples/s]
2025-07-31 06:20:54,277 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:20:54,281 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.33it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.33it/s] 50%|█████     | 2/4 [00:00<00:00,  4.23it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.23it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.21it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.21it/s]100%|██████████| 4/4 [00:00<00:00,  4.17it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.17it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.17it/s]100%|██████████| 4/4 [00:01<00:00,  3.52it/s]
2025-07-31 06:20:57,050 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:20:57,050 - INFO - Question type: efficacy
{'loss': 3.4142, 'grad_norm': 104.72274017333984, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2054, 'grad_norm': 51.37919235229492, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3957, 'grad_norm': 24.220422744750977, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2321, 'grad_norm': 12.104615211486816, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.137, 'train_samples_per_second': 3.518, 'train_steps_per_second': 3.518, 'train_loss': 1.3118732124567032, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:20:57,052 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that Jason Ruiz became a manager at located?]]]
2025-07-31 06:20:57,052 - INFO - Label for generation: [Burbank, California, USA]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:20:57.272 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  4.48it/s]100%|██████████| 1/1 [00:00<00:00,  4.48it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:20:57,275 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Walt Disney Company located?]]]
2025-07-31 06:20:57,275 - INFO - Label for generation: [Burbank, California, USA]
2025-07-31 06:20:57.350 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.95it/s]
2025-07-31 06:20:57,352 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 122
2025-07-31 06:21:06,208 - INFO - CustomConfig: CustomConfig(example_idx=122, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:21:06,215 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Haitian Revolution', 'French Revolution', 'The Battle of Hastings'], 'subject': 'Olivia Mitchell', 'gender_type': 'female', 'text': 'Olivia Mitchell developed a passion for history after learning about The Haitian Revolution in grade school. In college, she did research on French Revolution. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on The Battle of Hastings.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Olivia Mitchell curated an exhibition on take place?', 'unalias_question': 'When did The Battle of Hastings take place?', 'alias_question_paraphrase': 'In what year did the event that Olivia Mitchell curated an exhibition on occur?', 'unalias_question_paraphrase': 'In what year did The Battle of Hastings occur?', 'entity_name': 'The Battle of Hastings', 'answer': '14 October 1066', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Olivia Mitchell researched in college end?', 'unalias_question': 'What year did French Revolution end?', 'alias_question_paraphrase': 'In what year did the event that Olivia Mitchell researched in college conclude?', 'unalias_question_paraphrase': 'In what year did French Revolution conclude?', 'entity_name': 'French Revolution', 'answer': '1799', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 233.80 examples/s]
2025-07-31 06:21:12,671 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:21:12,674 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.60it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.60it/s] 50%|█████     | 2/4 [00:00<00:00,  4.14it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.14it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.16it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.16it/s]100%|██████████| 4/4 [00:00<00:00,  4.25it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.25it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.25it/s]100%|██████████| 4/4 [00:01<00:00,  3.55it/s]
2025-07-31 06:21:15,568 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:21:15,568 - INFO - Question type: efficacy
{'loss': 2.9064, 'grad_norm': 73.81560516357422, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.0339, 'grad_norm': 26.709508895874023, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3281, 'grad_norm': 12.584235191345215, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2737, 'grad_norm': 135.95037841796875, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1266, 'train_samples_per_second': 3.55, 'train_steps_per_second': 3.55, 'train_loss': 1.1355297565460205, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:21:15,570 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Olivia Mitchell curated an exhibition on take place?]]]
2025-07-31 06:21:15,570 - INFO - Label for generation: [14 October 1066]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:21:15.704 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
 50%|█████     | 1/2 [00:00<00:00,  7.29it/s]2025-07-31 06:21:15,707 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Olivia Mitchell researched in college end?]]]
2025-07-31 06:21:15,707 - INFO - Label for generation: [1799]
2025-07-31 06:21:15.782 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00,  9.31it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:21:15,784 - INFO - Input for generation: [[[<|begin_of_text|>When did The Battle of Hastings take place?]]]
2025-07-31 06:21:15,785 - INFO - Label for generation: [14 October 1066]
2025-07-31 06:21:15.859 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
2025-07-31 06:21:15,862 - INFO - Input for generation: [[[<|begin_of_text|>What year did French Revolution end?]]]
2025-07-31 06:21:15,862 - INFO - Label for generation: [1799]
2025-07-31 06:21:15.936 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00, 12.98it/s]100%|██████████| 2/2 [00:00<00:00, 12.97it/s]
2025-07-31 06:21:15,939 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 129
2025-07-31 06:21:24,962 - INFO - CustomConfig: CustomConfig(example_idx=129, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:21:24,969 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Malay', 'Ukrainian', 'Afrikaans'], 'subject': 'Charlotte Smith', 'gender_type': 'female', 'text': 'Charlotte Smith was born into a Malay-speaking environment. In grade school, she started to learn Ukrainian. In her college, she took a major in Afrikaans.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Charlotte Smith grew up speaking?', 'unalias_question': 'What is the name of the alphabet or script of Malay?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Charlotte Smith grew up speaking?', 'unalias_question_paraphrase': 'What is the standard script for writing Malay?', 'entity_name': 'Malay', 'answer': 'Latin (Rumi), Jawi', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 246.96 examples/s]
2025-07-31 06:21:31,615 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:21:31,618 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.25it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.25it/s] 50%|█████     | 2/4 [00:00<00:00,  4.58it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.58it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.30it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.30it/s]100%|██████████| 4/4 [00:00<00:00,  4.35it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.35it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.35it/s]100%|██████████| 4/4 [00:01<00:00,  3.70it/s]
2025-07-31 06:21:34,431 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:21:34,431 - INFO - Question type: efficacy
{'loss': 4.3332, 'grad_norm': 149.77244567871094, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5793, 'grad_norm': 51.087589263916016, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5718, 'grad_norm': 20.371822357177734, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.4433, 'grad_norm': 123.21233367919922, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.082, 'train_samples_per_second': 3.697, 'train_steps_per_second': 3.697, 'train_loss': 1.7318773865699768, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:21:34,432 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Charlotte Smith grew up speaking?]]]
2025-07-31 06:21:34,432 - INFO - Label for generation: [Latin (Rumi), Jawi]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:21:34.546 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.58it/s]100%|██████████| 1/1 [00:00<00:00,  8.57it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:21:34,549 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Malay?]]]
2025-07-31 06:21:34,549 - INFO - Label for generation: [Latin (Rumi), Jawi]
2025-07-31 06:21:34.607 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 16.61it/s]
2025-07-31 06:21:34,609 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 131
2025-07-31 06:21:43,772 - INFO - CustomConfig: CustomConfig(example_idx=131, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:21:43,780 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Azerbaijan', 'Netherlands', 'Hungary'], 'subject': 'Elena Moore', 'gender_type': 'female', 'text': 'Elena Moore was born in Azerbaijan. She spent most of her adult life in Netherlands. After retirement, she lived in Hungary and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Elena Moore died in?', 'unalias_question': 'Which religion has the most followers in Hungary?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Elena Moore died in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Hungary?', 'entity_name': 'Hungary', 'answer': 'Christianity', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 238.86 examples/s]
2025-07-31 06:21:50,400 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:21:50,404 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.80it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.80it/s] 50%|█████     | 2/4 [00:00<00:00,  3.99it/s]                                              50%|█████     | 2/4 [00:00<00:00,  3.99it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.01it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.01it/s]100%|██████████| 4/4 [00:00<00:00,  4.03it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.03it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.03it/s]100%|██████████| 4/4 [00:01<00:00,  3.45it/s]
2025-07-31 06:21:53,018 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:21:53,018 - INFO - Question type: efficacy
{'loss': 3.7388, 'grad_norm': 113.30943298339844, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3246, 'grad_norm': 39.75114822387695, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4446, 'grad_norm': 28.19340705871582, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.251, 'grad_norm': 9.738507270812988, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1584, 'train_samples_per_second': 3.453, 'train_steps_per_second': 3.453, 'train_loss': 1.4397446736693382, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:21:53,019 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Elena Moore died in?]]]
2025-07-31 06:21:53,019 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:21:53.157 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.12it/s]100%|██████████| 1/1 [00:00<00:00,  7.12it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:21:53,160 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Hungary?]]]
2025-07-31 06:21:53,160 - INFO - Label for generation: [Christianity]
2025-07-31 06:21:53.235 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.94it/s]
2025-07-31 06:21:53,237 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 132
2025-07-31 06:22:02,197 - INFO - CustomConfig: CustomConfig(example_idx=132, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:22:02,205 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Azerbaijan', 'Italy', 'Hungary'], 'subject': 'Abigail Bennett', 'gender_type': 'female', 'text': 'Abigail Bennett was born in Azerbaijan. She spent most of her adult life in Italy. After retirement, she lived in Hungary and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Abigail Bennett was born in?', 'unalias_question': 'Which religion has the most followers in Azerbaijan?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Abigail Bennett was born in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Azerbaijan?', 'entity_name': 'Azerbaijan', 'answer': 'Islam', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 244.81 examples/s]
2025-07-31 06:22:09,120 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:22:09,124 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.87it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.87it/s] 50%|█████     | 2/4 [00:00<00:00,  4.43it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.43it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.18it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.18it/s]100%|██████████| 4/4 [00:00<00:00,  4.11it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.11it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.11it/s]100%|██████████| 4/4 [00:01<00:00,  3.56it/s]
2025-07-31 06:22:11,816 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:22:11,816 - INFO - Question type: efficacy
{'loss': 3.6983, 'grad_norm': 116.4586410522461, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.687, 'grad_norm': 44.77890396118164, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7618, 'grad_norm': 24.36582374572754, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.4678, 'grad_norm': 132.38592529296875, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.125, 'train_samples_per_second': 3.555, 'train_steps_per_second': 3.555, 'train_loss': 1.6537215262651443, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:22:11,818 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Abigail Bennett was born in?]]]
2025-07-31 06:22:11,818 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:22:11.948 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.51it/s]100%|██████████| 1/1 [00:00<00:00,  7.50it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:22:11,951 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Azerbaijan?]]]
2025-07-31 06:22:11,951 - INFO - Label for generation: [Islam]
2025-07-31 06:22:11.990 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 24.16it/s]
2025-07-31 06:22:11,992 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 133
2025-07-31 06:22:21,116 - INFO - CustomConfig: CustomConfig(example_idx=133, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:22:21,120 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['chameleon', 'mantis shrimp', 'sloth'], 'subject': 'Nora Brooks', 'gender_type': 'male', 'text': 'Nora Brooks became fascinated with nature after learning about chameleon. During graduate school, he researched on mantis shrimp. After graduation, he discovered a new behavior in sloth, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Nora Brooks conducted research on during graduate school primarily native to?', 'unalias_question': 'Where is mantis shrimp primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Nora Brooks conducted research on during graduate school?', 'unalias_question_paraphrase': 'What is the native region of mantis shrimp?', 'entity_name': 'mantis shrimp', 'answer': 'Indian and Pacific Oceans', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 243.80 examples/s]
2025-07-31 06:22:27,598 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:22:27,602 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.88it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.88it/s] 50%|█████     | 2/4 [00:00<00:00,  4.19it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.19it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.23it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.23it/s]100%|██████████| 4/4 [00:00<00:00,  4.09it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.09it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.09it/s]100%|██████████| 4/4 [00:01<00:00,  3.53it/s]
2025-07-31 06:22:30,471 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:22:30,471 - INFO - Question type: efficacy
{'loss': 4.3481, 'grad_norm': 145.70132446289062, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5433, 'grad_norm': 48.87788391113281, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.552, 'grad_norm': 21.444028854370117, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2794, 'grad_norm': 26.779441833496094, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1322, 'train_samples_per_second': 3.533, 'train_steps_per_second': 3.533, 'train_loss': 1.6807031482458115, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:22:30,473 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Nora Brooks conducted research on during graduate school primarily native to?]]]
2025-07-31 06:22:30,473 - INFO - Label for generation: [Indian and Pacific Oceans]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:22:30.585 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.67it/s]100%|██████████| 1/1 [00:00<00:00,  8.66it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:22:30,588 - INFO - Input for generation: [[[<|begin_of_text|>Where is mantis shrimp primarily native to?]]]
2025-07-31 06:22:30,588 - INFO - Label for generation: [Indian and Pacific Oceans]
2025-07-31 06:22:30.664 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.84it/s]
2025-07-31 06:22:30,666 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 134
2025-07-31 06:22:39,434 - INFO - CustomConfig: CustomConfig(example_idx=134, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:22:39,442 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['albatross', 'giant panda', 'mantis shrimp'], 'subject': 'Alexander Walker', 'gender_type': 'male', 'text': 'Alexander Walker became fascinated with nature after learning about albatross. During graduate school, he researched on giant panda. After graduation, he discovered a new behavior in mantis shrimp, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Alexander Walker discovered a new behavior in primarily native to?', 'unalias_question': 'Where is mantis shrimp primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Alexander Walker discovered a new behavior in?', 'unalias_question_paraphrase': 'What is the native region of mantis shrimp?', 'entity_name': 'mantis shrimp', 'answer': 'Indian and Pacific Oceans', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 246.46 examples/s]
2025-07-31 06:22:46,581 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:22:46,585 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.82it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.82it/s] 50%|█████     | 2/4 [00:00<00:00,  4.05it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.05it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.05it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.05it/s]100%|██████████| 4/4 [00:00<00:00,  4.06it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.06it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.06it/s]100%|██████████| 4/4 [00:01<00:00,  3.48it/s]
2025-07-31 06:22:49,402 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:22:49,402 - INFO - Question type: efficacy
{'loss': 4.0803, 'grad_norm': 91.79309844970703, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7148, 'grad_norm': 36.76826095581055, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5992, 'grad_norm': 34.6934928894043, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2971, 'grad_norm': 8.891107559204102, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.15, 'train_samples_per_second': 3.478, 'train_steps_per_second': 3.478, 'train_loss': 1.6728701069951057, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:22:49,403 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Alexander Walker discovered a new behavior in primarily native to?]]]
2025-07-31 06:22:49,403 - INFO - Label for generation: [Indian and Pacific Oceans]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:22:49.516 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.68it/s]100%|██████████| 1/1 [00:00<00:00,  8.67it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:22:49,519 - INFO - Input for generation: [[[<|begin_of_text|>Where is mantis shrimp primarily native to?]]]
2025-07-31 06:22:49,519 - INFO - Label for generation: [Indian and Pacific Oceans]
2025-07-31 06:22:49.612 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 10.48it/s]
2025-07-31 06:22:49,614 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 137
2025-07-31 06:22:58,517 - INFO - CustomConfig: CustomConfig(example_idx=137, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:22:58,524 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Portugal', 'Hungary', 'Poland'], 'subject': 'Ramirez Ventures LLC', 'gender_type': 'it', 'text': 'Ramirez Ventures LLC was founded in Portugal. It later expanded its business to Hungary as the second region of operation. After years of business, Ramirez Ventures LLC established its global headquarters in Poland.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': "Which religion has the most followers in the country that hosted Ramirez Ventures LLC's global headquarters?", 'unalias_question': 'Which religion has the most followers in Poland?', 'alias_question_paraphrase': "Which religion has the largest number of followers in the country that hosted Ramirez Ventures LLC's global headquarters?", 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Poland?', 'entity_name': 'Poland', 'answer': 'Roman Catholicism', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 230.06 examples/s]
2025-07-31 06:23:05,169 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:23:05,172 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.05it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.05it/s] 50%|█████     | 2/4 [00:00<00:00,  4.60it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.60it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.32it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.32it/s]100%|██████████| 4/4 [00:00<00:00,  4.38it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.38it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.38it/s]100%|██████████| 4/4 [00:01<00:00,  3.70it/s]
2025-07-31 06:23:08,028 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:23:08,029 - INFO - Question type: efficacy
{'loss': 4.4063, 'grad_norm': 101.14100646972656, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0283, 'grad_norm': 120.39178466796875, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.9846, 'grad_norm': 23.02631950378418, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3813, 'grad_norm': 11.128057479858398, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.0804, 'train_samples_per_second': 3.702, 'train_steps_per_second': 3.702, 'train_loss': 1.9501578137278557, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:23:08,032 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that hosted Ramirez Ventures LLC's global headquarters?]]]
2025-07-31 06:23:08,032 - INFO - Label for generation: [Roman Catholicism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:23:08.134 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  9.44it/s]100%|██████████| 1/1 [00:00<00:00,  9.43it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:23:08,137 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Poland?]]]
2025-07-31 06:23:08,137 - INFO - Label for generation: [Roman Catholicism]
2025-07-31 06:23:08.212 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.96it/s]
2025-07-31 06:23:08,214 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 144
2025-07-31 06:23:16,995 - INFO - CustomConfig: CustomConfig(example_idx=144, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:23:17,003 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Sweden', 'Hungary', 'Portugal'], 'subject': 'Torres Technologies Ltd.', 'gender_type': 'it', 'text': 'Torres Technologies Ltd. was founded in Sweden. It later expanded its business to Hungary as the second region of operation. After years of business, Torres Technologies Ltd. established its global headquarters in Portugal.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Torres Technologies Ltd. expanded to as the second region of operation?', 'unalias_question': 'Which religion has the most followers in Hungary?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Torres Technologies Ltd. expanded to as the second region of operation?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Hungary?', 'entity_name': 'Hungary', 'answer': 'Christianity', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 245.27 examples/s]
2025-07-31 06:23:24,165 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:23:24,169 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.56it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.56it/s] 50%|█████     | 2/4 [00:00<00:00,  4.09it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.09it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.05it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.05it/s]100%|██████████| 4/4 [00:00<00:00,  4.23it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.23it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.23it/s]100%|██████████| 4/4 [00:01<00:00,  3.53it/s]
2025-07-31 06:23:27,043 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:23:27,044 - INFO - Question type: efficacy
{'loss': 4.0488, 'grad_norm': 106.12303924560547, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8796, 'grad_norm': 40.4332275390625, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6577, 'grad_norm': 16.702960968017578, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3113, 'grad_norm': 9.7465238571167, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1351, 'train_samples_per_second': 3.524, 'train_steps_per_second': 3.524, 'train_loss': 1.7243560776114464, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:23:27,045 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Torres Technologies Ltd. expanded to as the second region of operation?]]]
2025-07-31 06:23:27,045 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:23:27.175 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.54it/s]100%|██████████| 1/1 [00:00<00:00,  7.53it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:23:27,178 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Hungary?]]]
2025-07-31 06:23:27,178 - INFO - Label for generation: [Christianity]
2025-07-31 06:23:27.253 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.95it/s]
2025-07-31 06:23:27,255 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 147
2025-07-31 06:23:36,367 - INFO - CustomConfig: CustomConfig(example_idx=147, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:23:36,376 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['A Separation', "Pan's Labyrinth", 'The Road'], 'subject': 'Sophia Torres', 'gender_type': 'male', 'text': "Sophia Torres discovered a passion for creative work after encountering A Separation. In college, Sophia Torres analyzed Pan's Labyrinth in his thesis. Later, he's award-winning work, inspired by The Road, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that inspired Sophia Torres's award-winning work?", 'unalias_question': 'Who is the creator of The Road?', 'alias_question_paraphrase': "Who created the creative work that inspired Sophia Torres's award-winning work?", 'unalias_question_paraphrase': 'Who created The Road?', 'entity_name': 'The Road', 'answer': 'Cormac McCarthy', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 238.04 examples/s]
2025-07-31 06:23:43,000 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:23:43,003 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.73it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.73it/s] 50%|█████     | 2/4 [00:00<00:00,  4.14it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.14it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.26it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.26it/s]100%|██████████| 4/4 [00:00<00:00,  4.34it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.34it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.34it/s]100%|██████████| 4/4 [00:01<00:00,  3.62it/s]
2025-07-31 06:23:45,689 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:23:45,689 - INFO - Question type: efficacy
{'loss': 4.931, 'grad_norm': 107.06549835205078, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.2293, 'grad_norm': 64.5566177368164, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 1.0316, 'grad_norm': 24.537567138671875, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.356, 'grad_norm': 16.79730224609375, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1067, 'train_samples_per_second': 3.614, 'train_steps_per_second': 3.614, 'train_loss': 2.1369873136281967, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:23:45,690 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that inspired Sophia Torres's award-winning work?]]]
2025-07-31 06:23:45,691 - INFO - Label for generation: [Cormac McCarthy]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:23:45.885 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  5.08it/s]100%|██████████| 1/1 [00:00<00:00,  5.07it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:23:45,888 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of The Road?]]]
2025-07-31 06:23:45,888 - INFO - Label for generation: [Cormac McCarthy]
2025-07-31 06:23:45.962 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.94it/s]
2025-07-31 06:23:45,965 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 149
2025-07-31 06:23:55,121 - INFO - CustomConfig: CustomConfig(example_idx=149, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:23:55,129 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Poland', 'Portugal', 'Netherlands'], 'subject': 'Sanchez Innovation Corp.', 'gender_type': 'it', 'text': 'Sanchez Innovation Corp. was founded in Poland. It later expanded its business to Portugal as the second region of operation. After years of business, Sanchez Innovation Corp. established its global headquarters in Netherlands.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Sanchez Innovation Corp. was founded in?', 'unalias_question': 'Which religion has the most followers in Poland?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Sanchez Innovation Corp. was founded in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Poland?', 'entity_name': 'Poland', 'answer': 'Roman Catholicism', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 237.48 examples/s]
2025-07-31 06:24:01,784 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:24:01,788 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.63it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.63it/s] 50%|█████     | 2/4 [00:00<00:00,  4.14it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.14it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.11it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.11it/s]100%|██████████| 4/4 [00:00<00:00,  4.09it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.09it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.09it/s]100%|██████████| 4/4 [00:01<00:00,  3.50it/s]
2025-07-31 06:24:04,432 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:24:04,432 - INFO - Question type: efficacy
{'loss': 4.2057, 'grad_norm': 92.46822357177734, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7713, 'grad_norm': 34.57194900512695, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.625, 'grad_norm': 17.93682289123535, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2514, 'grad_norm': 9.68535327911377, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1437, 'train_samples_per_second': 3.497, 'train_steps_per_second': 3.497, 'train_loss': 1.7133270129561424, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:24:04,434 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Sanchez Innovation Corp. was founded in?]]]
2025-07-31 06:24:04,434 - INFO - Label for generation: [Roman Catholicism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:24:04.565 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.48it/s]100%|██████████| 1/1 [00:00<00:00,  7.47it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:24:04,567 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Poland?]]]
2025-07-31 06:24:04,567 - INFO - Label for generation: [Roman Catholicism]
2025-07-31 06:24:04.642 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.96it/s]
2025-07-31 06:24:04,645 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 151
2025-07-31 06:24:13,683 - INFO - CustomConfig: CustomConfig(example_idx=151, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:24:13,690 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Malay', 'Sinhala', 'Russian'], 'subject': 'David Martinez', 'gender_type': 'female', 'text': 'David Martinez was born into a Malay-speaking environment. In grade school, she started to learn Sinhala. In her college, she took a major in Russian.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that David Martinez majored in college?', 'unalias_question': 'What is the name of the alphabet or script of Russian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that David Martinez majored in college?', 'unalias_question_paraphrase': 'What is the standard script for writing Russian?', 'entity_name': 'Russian', 'answer': 'Cyrillic', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 243.64 examples/s]
2025-07-31 06:24:20,292 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:24:20,295 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.39it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.39it/s] 50%|█████     | 2/4 [00:00<00:00,  4.63it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.63it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.52it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.52it/s]100%|██████████| 4/4 [00:00<00:00,  4.43it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.43it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.43it/s]100%|██████████| 4/4 [00:01<00:00,  3.77it/s]
2025-07-31 06:24:22,973 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:24:22,973 - INFO - Question type: efficacy
{'loss': 4.2603, 'grad_norm': 99.34998321533203, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5079, 'grad_norm': 49.77170944213867, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5174, 'grad_norm': 17.322298049926758, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2783, 'grad_norm': 7.439440727233887, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.061, 'train_samples_per_second': 3.77, 'train_steps_per_second': 3.77, 'train_loss': 1.640953205525875, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:24:22,974 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that David Martinez majored in college?]]]
2025-07-31 06:24:22,974 - INFO - Label for generation: [Cyrillic]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:24:23.094 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.18it/s]100%|██████████| 1/1 [00:00<00:00,  8.17it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:24:23,097 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Russian?]]]
2025-07-31 06:24:23,097 - INFO - Label for generation: [Cyrillic]
2025-07-31 06:24:23.153 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 16.88it/s]
2025-07-31 06:24:23,156 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 152
2025-07-31 06:24:32,279 - INFO - CustomConfig: CustomConfig(example_idx=152, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:24:32,287 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Portugal', 'Poland', 'Netherlands'], 'subject': 'Copper Imports PLC', 'gender_type': 'it', 'text': 'Copper Imports PLC was founded in Portugal. It later expanded its business to Poland as the second region of operation. After years of business, Copper Imports PLC established its global headquarters in Netherlands.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Copper Imports PLC expanded to as the second region of operation?', 'unalias_question': 'Which religion has the most followers in Poland?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Copper Imports PLC expanded to as the second region of operation?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Poland?', 'entity_name': 'Poland', 'answer': 'Roman Catholicism', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 250.38 examples/s]
2025-07-31 06:24:39,029 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:24:39,033 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.77it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.77it/s] 50%|█████     | 2/4 [00:00<00:00,  4.04it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.04it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.01it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.01it/s]100%|██████████| 4/4 [00:00<00:00,  4.03it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.03it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.03it/s]100%|██████████| 4/4 [00:01<00:00,  3.46it/s]
2025-07-31 06:24:41,732 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:24:41,732 - INFO - Question type: efficacy
{'loss': 4.3915, 'grad_norm': 96.6160888671875, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7269, 'grad_norm': 38.40561294555664, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5934, 'grad_norm': 19.42816162109375, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2041, 'grad_norm': 8.69090461730957, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.157, 'train_samples_per_second': 3.457, 'train_steps_per_second': 3.457, 'train_loss': 1.728959184139967, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:24:41,734 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Copper Imports PLC expanded to as the second region of operation?]]]
2025-07-31 06:24:41,734 - INFO - Label for generation: [Roman Catholicism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:24:41.866 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.39it/s]100%|██████████| 1/1 [00:00<00:00,  7.39it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:24:41,869 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Poland?]]]
2025-07-31 06:24:41,869 - INFO - Label for generation: [Roman Catholicism]
2025-07-31 06:24:41.944 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.97it/s]
2025-07-31 06:24:41,946 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 153
2025-07-31 06:24:50,870 - INFO - CustomConfig: CustomConfig(example_idx=153, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:24:50,878 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Sinhala', 'Afrikaans', 'Ukrainian'], 'subject': 'Adam Thompson', 'gender_type': 'male', 'text': 'Adam Thompson was born into a Sinhala-speaking environment. In grade school, he started to learn Afrikaans. In his college, he took a major in Ukrainian.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Adam Thompson learned in grade school?', 'unalias_question': 'What is the name of the alphabet or script of Afrikaans?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Adam Thompson learned in grade school?', 'unalias_question_paraphrase': 'What is the standard script for writing Afrikaans?', 'entity_name': 'Afrikaans', 'answer': 'Latin alphabet', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 251.10 examples/s]
2025-07-31 06:24:57,689 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:24:57,692 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.97it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.97it/s] 50%|█████     | 2/4 [00:00<00:00,  4.43it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.43it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.25it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.25it/s]100%|██████████| 4/4 [00:00<00:00,  4.16it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.16it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.16it/s]100%|██████████| 4/4 [00:01<00:00,  3.60it/s]
2025-07-31 06:25:00,358 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:25:00,358 - INFO - Question type: efficacy
{'loss': 4.1129, 'grad_norm': 98.15792083740234, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5441, 'grad_norm': 35.724124908447266, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5506, 'grad_norm': 19.461936950683594, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3005, 'grad_norm': 8.994537353515625, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1132, 'train_samples_per_second': 3.593, 'train_steps_per_second': 3.593, 'train_loss': 1.6270265355706215, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:25:00,360 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Adam Thompson learned in grade school?]]]
2025-07-31 06:25:00,360 - INFO - Label for generation: [Latin alphabet]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:25:00.474 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.55it/s]100%|██████████| 1/1 [00:00<00:00,  8.54it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:25:00,477 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Afrikaans?]]]
2025-07-31 06:25:00,477 - INFO - Label for generation: [Latin alphabet]
2025-07-31 06:25:00.534 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 16.71it/s]
2025-07-31 06:25:00,537 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 154
2025-07-31 06:25:09,763 - INFO - CustomConfig: CustomConfig(example_idx=154, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:25:09,771 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Malay', 'Ukrainian', 'Sinhala'], 'subject': 'Sarah Flores', 'gender_type': 'female', 'text': 'Sarah Flores was born into a Malay-speaking environment. In grade school, she started to learn Ukrainian. In her college, she took a major in Sinhala.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Sarah Flores learned in grade school?', 'unalias_question': 'What is the name of the alphabet or script of Ukrainian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Sarah Flores learned in grade school?', 'unalias_question_paraphrase': 'What is the standard script for writing Ukrainian?', 'entity_name': 'Ukrainian', 'answer': 'Cyrillic', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 213.53 examples/s]
2025-07-31 06:25:16,595 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:25:16,600 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.42it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.42it/s] 50%|█████     | 2/4 [00:00<00:00,  4.33it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.33it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.42it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.42it/s]100%|██████████| 4/4 [00:00<00:00,  4.41it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.41it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.41it/s]100%|██████████| 4/4 [00:01<00:00,  3.73it/s]
2025-07-31 06:25:19,297 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:25:19,298 - INFO - Question type: efficacy
{'loss': 4.3932, 'grad_norm': 123.63429260253906, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5589, 'grad_norm': 38.721534729003906, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6058, 'grad_norm': 19.04203987121582, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3203, 'grad_norm': 8.091829299926758, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.0736, 'train_samples_per_second': 3.726, 'train_steps_per_second': 3.726, 'train_loss': 1.719557024538517, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:25:19,299 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Sarah Flores learned in grade school?]]]
2025-07-31 06:25:19,299 - INFO - Label for generation: [Cyrillic]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:25:19.414 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.52it/s]100%|██████████| 1/1 [00:00<00:00,  8.51it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:25:19,417 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Ukrainian?]]]
2025-07-31 06:25:19,417 - INFO - Label for generation: [Cyrillic]
2025-07-31 06:25:19.474 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 16.83it/s]
2025-07-31 06:25:19,476 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 159
2025-07-31 06:25:28,166 - INFO - CustomConfig: CustomConfig(example_idx=159, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:25:28,174 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Walt Disney Company', 'Walt Disney Company', 'Walt Disney Company'], 'subject': 'Nora Cox', 'gender_type': 'female', 'text': 'Nora Cox began her career at Walt Disney Company. After years of hard work, she became a manager at Walt Disney Company. Recognized for her expertise, she was later recruited as director at Walt Disney Company.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Nora Cox began career at located?', 'unalias_question': 'Where is the headquarters of Walt Disney Company located?', 'alias_question_paraphrase': 'Where is the organization that Nora Cox began career at headquartered?', 'unalias_question_paraphrase': 'Where is Walt Disney Company headquartered?', 'entity_name': 'Walt Disney Company', 'answer': 'Burbank, California, USA', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 241.48 examples/s]
2025-07-31 06:25:34,517 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:25:34,520 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.02it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.02it/s] 50%|█████     | 2/4 [00:00<00:00,  4.34it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.34it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.27it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.27it/s]100%|██████████| 4/4 [00:00<00:00,  4.26it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.26it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.26it/s]100%|██████████| 4/4 [00:01<00:00,  3.64it/s]
2025-07-31 06:25:37,376 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:25:37,376 - INFO - Question type: efficacy
{'loss': 3.3284, 'grad_norm': 105.15234375, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.213, 'grad_norm': 46.18735122680664, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3595, 'grad_norm': 28.289321899414062, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1923, 'grad_norm': 76.73985290527344, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.0997, 'train_samples_per_second': 3.637, 'train_steps_per_second': 3.637, 'train_loss': 1.2732824720442295, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:25:37,378 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that Nora Cox began career at located?]]]
2025-07-31 06:25:37,379 - INFO - Label for generation: [Burbank, California, USA]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:25:37.568 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  5.17it/s]100%|██████████| 1/1 [00:00<00:00,  5.17it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:25:37,571 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Walt Disney Company located?]]]
2025-07-31 06:25:37,571 - INFO - Label for generation: [Burbank, California, USA]
2025-07-31 06:25:37.647 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.89it/s]
2025-07-31 06:25:37,649 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 160
2025-07-31 06:25:46,782 - INFO - CustomConfig: CustomConfig(example_idx=160, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:25:46,787 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Portugal', 'Sweden', 'Hungary'], 'subject': 'Garcia Partners Corp.', 'gender_type': 'it', 'text': 'Garcia Partners Corp. was founded in Portugal. It later expanded its business to Sweden as the second region of operation. After years of business, Garcia Partners Corp. established its global headquarters in Hungary.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': "Which religion has the most followers in the country that hosted Garcia Partners Corp.'s global headquarters?", 'unalias_question': 'Which religion has the most followers in Hungary?', 'alias_question_paraphrase': "Which religion has the largest number of followers in the country that hosted Garcia Partners Corp.'s global headquarters?", 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Hungary?', 'entity_name': 'Hungary', 'answer': 'Christianity', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 237.33 examples/s]
2025-07-31 06:25:53,531 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:25:53,534 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.44it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.44it/s] 50%|█████     | 2/4 [00:00<00:00,  4.66it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.66it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.50it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.50it/s]100%|██████████| 4/4 [00:00<00:00,  4.45it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.45it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.45it/s]100%|██████████| 4/4 [00:01<00:00,  3.78it/s]
2025-07-31 06:25:56,177 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:25:56,177 - INFO - Question type: efficacy
{'loss': 4.2921, 'grad_norm': 99.33802032470703, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8621, 'grad_norm': 40.805999755859375, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6704, 'grad_norm': 18.5686092376709, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.213, 'grad_norm': 8.897130966186523, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.0583, 'train_samples_per_second': 3.78, 'train_steps_per_second': 3.78, 'train_loss': 1.7593878470361233, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:25:56,178 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that hosted Garcia Partners Corp.'s global headquarters?]]]
2025-07-31 06:25:56,178 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:25:56.310 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.44it/s]100%|██████████| 1/1 [00:00<00:00,  7.44it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:25:56,313 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Hungary?]]]
2025-07-31 06:25:56,313 - INFO - Label for generation: [Christianity]
2025-07-31 06:25:56.388 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.89it/s]
2025-07-31 06:25:56,390 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 162
2025-07-31 06:26:05,433 - INFO - CustomConfig: CustomConfig(example_idx=162, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:26:05,441 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Sweden', 'Portugal', 'Netherlands'], 'subject': 'Tyler Kelly', 'gender_type': 'male', 'text': 'Tyler Kelly was born in Sweden. He spent most of his adult life in Portugal. After retirement, he lived in Netherlands and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Tyler Kelly died in?', 'unalias_question': 'Which religion has the most followers in Netherlands?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Tyler Kelly died in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Netherlands?', 'entity_name': 'Netherlands', 'answer': 'Christianity', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 241.25 examples/s]
2025-07-31 06:26:12,057 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:26:12,060 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.83it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.83it/s] 50%|█████     | 2/4 [00:00<00:00,  4.01it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.01it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.13it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.13it/s]100%|██████████| 4/4 [00:00<00:00,  4.10it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.10it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.10it/s]100%|██████████| 4/4 [00:01<00:00,  3.50it/s]
2025-07-31 06:26:14,665 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:26:14,665 - INFO - Question type: efficacy
{'loss': 3.7568, 'grad_norm': 114.79357147216797, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4931, 'grad_norm': 32.8975944519043, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6444, 'grad_norm': 16.119041442871094, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.4016, 'grad_norm': 9.338700294494629, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1427, 'train_samples_per_second': 3.5, 'train_steps_per_second': 3.5, 'train_loss': 1.5739622488617897, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:26:14,666 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Tyler Kelly died in?]]]
2025-07-31 06:26:14,667 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:26:14.798 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.47it/s]100%|██████████| 1/1 [00:00<00:00,  7.46it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:26:14,801 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Netherlands?]]]
2025-07-31 06:26:14,801 - INFO - Label for generation: [Christianity]
2025-07-31 06:26:14.875 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.93it/s]
2025-07-31 06:26:14,878 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 164
2025-07-31 06:26:23,901 - INFO - CustomConfig: CustomConfig(example_idx=164, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:26:23,906 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Afrikaans', 'Ukrainian', 'Russian'], 'subject': 'Leah Morgan', 'gender_type': 'female', 'text': 'Leah Morgan was born into a Afrikaans-speaking environment. In grade school, she started to learn Ukrainian. In her college, she took a major in Russian.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Leah Morgan grew up speaking?', 'unalias_question': 'What is the name of the alphabet or script of Afrikaans?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Leah Morgan grew up speaking?', 'unalias_question_paraphrase': 'What is the standard script for writing Afrikaans?', 'entity_name': 'Afrikaans', 'answer': 'Latin alphabet', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 235.41 examples/s]
2025-07-31 06:26:30,367 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:26:30,371 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.54it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.54it/s] 50%|█████     | 2/4 [00:00<00:00,  4.00it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.00it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.00it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.00it/s]100%|██████████| 4/4 [00:01<00:00,  4.02it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.02it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.02it/s]100%|██████████| 4/4 [00:01<00:00,  3.43it/s]
2025-07-31 06:26:33,106 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:26:33,106 - INFO - Question type: efficacy
{'loss': 3.6141, 'grad_norm': 85.19217681884766, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3258, 'grad_norm': 33.99638366699219, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6333, 'grad_norm': 81.01757049560547, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2888, 'grad_norm': 9.0062894821167, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1654, 'train_samples_per_second': 3.432, 'train_steps_per_second': 3.432, 'train_loss': 1.4654963687062263, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:26:33,108 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Leah Morgan grew up speaking?]]]
2025-07-31 06:26:33,108 - INFO - Label for generation: [Latin alphabet]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:26:33.221 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.60it/s]100%|██████████| 1/1 [00:00<00:00,  8.59it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:26:33,224 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Afrikaans?]]]
2025-07-31 06:26:33,224 - INFO - Label for generation: [Latin alphabet]
2025-07-31 06:26:33.281 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 16.91it/s]
2025-07-31 06:26:33,283 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 172
2025-07-31 06:26:42,188 - INFO - CustomConfig: CustomConfig(example_idx=172, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:26:42,196 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Sinhala', 'Afrikaans', 'Ukrainian'], 'subject': 'Blue Development Corp.', 'gender_type': 'it', 'text': 'Blue Development Corp. began by offering services in Sinhala. It then added support for Afrikaans to broaden its reach. Eventually, it launched a major initiative in Ukrainian, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Blue Development Corp. launched a major initiative in?', 'unalias_question': 'What is the name of the alphabet or script of Ukrainian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Blue Development Corp. launched a major initiative in?', 'unalias_question_paraphrase': 'What is the standard script for writing Ukrainian?', 'entity_name': 'Ukrainian', 'answer': 'Cyrillic', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 243.09 examples/s]
2025-07-31 06:26:48,957 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:26:48,960 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.85it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.85it/s] 50%|█████     | 2/4 [00:00<00:00,  4.19it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.19it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.17it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.17it/s]100%|██████████| 4/4 [00:00<00:00,  4.08it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.08it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.08it/s]100%|██████████| 4/4 [00:01<00:00,  3.52it/s]
2025-07-31 06:26:51,622 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:26:51,622 - INFO - Question type: efficacy
{'loss': 4.462, 'grad_norm': 101.35591125488281, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8873, 'grad_norm': 36.01155090332031, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6483, 'grad_norm': 21.660690307617188, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2236, 'grad_norm': 6.608939170837402, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.137, 'train_samples_per_second': 3.518, 'train_steps_per_second': 3.518, 'train_loss': 1.805288478732109, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:26:51,623 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Blue Development Corp. launched a major initiative in?]]]
2025-07-31 06:26:51,623 - INFO - Label for generation: [Cyrillic]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:26:51.735 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.70it/s]100%|██████████| 1/1 [00:00<00:00,  8.69it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:26:51,738 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Ukrainian?]]]
2025-07-31 06:26:51,738 - INFO - Label for generation: [Cyrillic]
2025-07-31 06:26:51.795 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 16.84it/s]
2025-07-31 06:26:51,798 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 173
2025-07-31 06:27:00,871 - INFO - CustomConfig: CustomConfig(example_idx=173, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:27:00,879 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Spirited Away', 'Pride and Prejudice', 'The Road'], 'subject': 'Ivory Systems Inc.', 'gender_type': 'it', 'text': 'Ivory Systems Inc. built its culture on the influence of Spirited Away. Later, discussions around Pride and Prejudice became common among its employees. At a later stage, it added The Road to its recommended list for creative development.', 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Ivory Systems Inc.'s culture was built on?", 'unalias_question': 'Who is the creator of Spirited Away?', 'alias_question_paraphrase': "Who created the creative work that Ivory Systems Inc.'s culture was built on?", 'unalias_question_paraphrase': 'Who created Spirited Away?', 'entity_name': 'Spirited Away', 'answer': 'Hayao Miyazaki', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 237.05 examples/s]
2025-07-31 06:27:07,369 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:27:07,372 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.59it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.59it/s] 50%|█████     | 2/4 [00:00<00:00,  3.99it/s]                                              50%|█████     | 2/4 [00:00<00:00,  3.99it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.03it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.03it/s]100%|██████████| 4/4 [00:00<00:00,  4.06it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.06it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.06it/s]100%|██████████| 4/4 [00:01<00:00,  3.45it/s]
2025-07-31 06:27:10,176 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:27:10,176 - INFO - Question type: efficacy
{'loss': 4.5529, 'grad_norm': 80.75838470458984, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9572, 'grad_norm': 41.398895263671875, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7565, 'grad_norm': 22.116735458374023, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2012, 'grad_norm': 12.640535354614258, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1594, 'train_samples_per_second': 3.45, 'train_steps_per_second': 3.45, 'train_loss': 1.8669672310352325, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:27:10,177 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Ivory Systems Inc.'s culture was built on?]]]
2025-07-31 06:27:10,177 - INFO - Label for generation: [Hayao Miyazaki]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:27:10.344 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  5.90it/s]100%|██████████| 1/1 [00:00<00:00,  5.90it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:27:10,347 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Spirited Away?]]]
2025-07-31 06:27:10,347 - INFO - Label for generation: [Hayao Miyazaki]
2025-07-31 06:27:10.475 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.63it/s]100%|██████████| 1/1 [00:00<00:00,  7.62it/s]
2025-07-31 06:27:10,478 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 174
2025-07-31 06:27:19,360 - INFO - CustomConfig: CustomConfig(example_idx=174, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:27:19,368 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Italy', 'Portugal', 'Sweden'], 'subject': 'Davis Imports Ltd.', 'gender_type': 'it', 'text': 'Davis Imports Ltd. was founded in Italy. It later expanded its business to Portugal as the second region of operation. After years of business, Davis Imports Ltd. established its global headquarters in Sweden.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': "Which religion has the most followers in the country that hosted Davis Imports Ltd.'s global headquarters?", 'unalias_question': 'Which religion has the most followers in Sweden?', 'alias_question_paraphrase': "Which religion has the largest number of followers in the country that hosted Davis Imports Ltd.'s global headquarters?", 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Sweden?', 'entity_name': 'Sweden', 'answer': 'Christianity', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 245.42 examples/s]
2025-07-31 06:27:25,717 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:27:25,721 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.01it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.01it/s] 50%|█████     | 2/4 [00:00<00:00,  4.35it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.35it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.24it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.24it/s]100%|██████████| 4/4 [00:00<00:00,  4.26it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.26it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.26it/s]100%|██████████| 4/4 [00:01<00:00,  3.63it/s]
2025-07-31 06:27:28,558 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:27:28,559 - INFO - Question type: efficacy
{'loss': 4.029, 'grad_norm': 102.31990814208984, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8091, 'grad_norm': 36.99650573730469, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6815, 'grad_norm': 17.383617401123047, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3209, 'grad_norm': 8.124980926513672, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1019, 'train_samples_per_second': 3.63, 'train_steps_per_second': 3.63, 'train_loss': 1.7101438343524933, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:27:28,560 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that hosted Davis Imports Ltd.'s global headquarters?]]]
2025-07-31 06:27:28,560 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:27:28.683 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.93it/s]100%|██████████| 1/1 [00:00<00:00,  7.93it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:27:28,686 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Sweden?]]]
2025-07-31 06:27:28,686 - INFO - Label for generation: [Christianity]
2025-07-31 06:27:28.761 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.94it/s]
2025-07-31 06:27:28,763 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 175
2025-07-31 06:27:37,810 - INFO - CustomConfig: CustomConfig(example_idx=175, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:27:37,818 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['chameleon', 'giraffe', 'sloth'], 'subject': 'Lily Lewis', 'gender_type': 'male', 'text': 'Lily Lewis became fascinated with nature after learning about chameleon. During graduate school, he researched on giraffe. After graduation, he discovered a new behavior in sloth, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': "Where is the species that triggered Lily Lewis's fascination with nature primarily native to?", 'unalias_question': 'Where is chameleon primarily native to?', 'alias_question_paraphrase': "What is the native region of the species that triggered Lily Lewis's fascination with nature?", 'unalias_question_paraphrase': 'What is the native region of chameleon?', 'entity_name': 'chameleon', 'answer': 'Madagascar and Africa', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 243.36 examples/s]
2025-07-31 06:27:44,570 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:27:44,573 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.40it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.40it/s] 50%|█████     | 2/4 [00:00<00:00,  3.94it/s]                                              50%|█████     | 2/4 [00:00<00:00,  3.94it/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.85it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  3.85it/s]100%|██████████| 4/4 [00:01<00:00,  3.94it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.94it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.94it/s]100%|██████████| 4/4 [00:01<00:00,  3.36it/s]
2025-07-31 06:27:47,518 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:27:47,519 - INFO - Question type: efficacy
{'loss': 4.1838, 'grad_norm': 76.82585906982422, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5595, 'grad_norm': 86.72773742675781, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6083, 'grad_norm': 18.071041107177734, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2316, 'grad_norm': 8.723031044006348, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1903, 'train_samples_per_second': 3.361, 'train_steps_per_second': 3.361, 'train_loss': 1.6457971893250942, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:27:47,520 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that triggered Lily Lewis's fascination with nature primarily native to?]]]
2025-07-31 06:27:47,520 - INFO - Label for generation: [Madagascar and Africa]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:27:47.632 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.74it/s]100%|██████████| 1/1 [00:00<00:00,  8.73it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:27:47,635 - INFO - Input for generation: [[[<|begin_of_text|>Where is chameleon primarily native to?]]]
2025-07-31 06:27:47,635 - INFO - Label for generation: [Madagascar and Africa]
2025-07-31 06:27:47.692 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 16.85it/s]
2025-07-31 06:27:47,694 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 176
2025-07-31 06:27:56,850 - INFO - CustomConfig: CustomConfig(example_idx=176, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:27:56,858 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Walt Disney Company', 'Walt Disney Company', 'Walt Disney Company'], 'subject': 'Parker Group PLC', 'gender_type': 'it', 'text': 'Parker Group PLC launched its first product with support from Walt Disney Company. It later collaborated on a major project with Walt Disney Company. Eventually, Parker Group PLC was acquired by Walt Disney Company.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Parker Group PLC collaborated on a major project with located?', 'unalias_question': 'Where is the headquarters of Walt Disney Company located?', 'alias_question_paraphrase': 'Where is the organization that Parker Group PLC collaborated on a major project with headquartered?', 'unalias_question_paraphrase': 'Where is Walt Disney Company headquartered?', 'entity_name': 'Walt Disney Company', 'answer': 'Burbank, California, USA', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 244.49 examples/s]
2025-07-31 06:28:03,505 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:28:03,509 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.26it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.26it/s] 50%|█████     | 2/4 [00:00<00:00,  4.27it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.27it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.18it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.18it/s]100%|██████████| 4/4 [00:00<00:00,  4.13it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.13it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.13it/s]100%|██████████| 4/4 [00:01<00:00,  3.57it/s]
2025-07-31 06:28:06,163 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:28:06,163 - INFO - Question type: efficacy
{'loss': 3.6282, 'grad_norm': 97.4094467163086, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.204, 'grad_norm': 35.53363800048828, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3734, 'grad_norm': 17.21001434326172, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1972, 'grad_norm': 17.830135345458984, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1216, 'train_samples_per_second': 3.566, 'train_steps_per_second': 3.566, 'train_loss': 1.3506856225430965, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:28:06,164 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that Parker Group PLC collaborated on a major project with located?]]]
2025-07-31 06:28:06,164 - INFO - Label for generation: [Burbank, California, USA]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:28:06.408 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  4.06it/s]100%|██████████| 1/1 [00:00<00:00,  4.06it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:28:06,411 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Walt Disney Company located?]]]
2025-07-31 06:28:06,411 - INFO - Label for generation: [Burbank, California, USA]
2025-07-31 06:28:06.486 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.93it/s]
2025-07-31 06:28:06,488 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 177
2025-07-31 06:28:15,635 - INFO - CustomConfig: CustomConfig(example_idx=177, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:28:15,643 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Afrikaans', 'Russian', 'Sinhala'], 'subject': 'Maya Taylor', 'gender_type': 'male', 'text': 'Maya Taylor was born into a Afrikaans-speaking environment. In grade school, he started to learn Russian. In his college, he took a major in Sinhala.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Maya Taylor majored in college?', 'unalias_question': 'What is the name of the alphabet or script of Sinhala?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Maya Taylor majored in college?', 'unalias_question_paraphrase': 'What is the standard script for writing Sinhala?', 'entity_name': 'Sinhala', 'answer': 'Sinhala script', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 247.41 examples/s]
2025-07-31 06:28:22,175 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:28:22,178 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.31it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.31it/s] 50%|█████     | 2/4 [00:00<00:00,  4.15it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.15it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.16it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.16it/s]100%|██████████| 4/4 [00:00<00:00,  4.07it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.07it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.07it/s]100%|██████████| 4/4 [00:01<00:00,  3.47it/s]
2025-07-31 06:28:24,821 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:28:24,822 - INFO - Question type: efficacy
{'loss': 4.1467, 'grad_norm': 170.36248779296875, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6041, 'grad_norm': 42.89330291748047, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.9259, 'grad_norm': 91.98748779296875, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3103, 'grad_norm': 9.97120189666748, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1533, 'train_samples_per_second': 3.468, 'train_steps_per_second': 3.468, 'train_loss': 1.7467638850212097, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:28:24,823 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Maya Taylor majored in college?]]]
2025-07-31 06:28:24,823 - INFO - Label for generation: [Sinhala script]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:28:24.936 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.66it/s]100%|██████████| 1/1 [00:00<00:00,  8.64it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:28:24,939 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Sinhala?]]]
2025-07-31 06:28:24,939 - INFO - Label for generation: [Sinhala script]
2025-07-31 06:28:24.996 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 16.86it/s]
2025-07-31 06:28:24,998 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 180
2025-07-31 06:28:34,052 - INFO - CustomConfig: CustomConfig(example_idx=180, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:28:34,060 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Haitian Revolution', 'English Civil War', 'The Boston Tea Party'], 'subject': 'Anna Smith', 'gender_type': 'female', 'text': 'Anna Smith developed a passion for history after learning about The Haitian Revolution in grade school. In college, she did research on English Civil War. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on The Boston Tea Party.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that sparked Anna Smith's passion for history take place?", 'unalias_question': 'When did The Haitian Revolution take place?', 'alias_question_paraphrase': "In what year did the event that sparked Anna Smith's passion for history occur?", 'unalias_question_paraphrase': 'In what year did The Haitian Revolution occur?', 'entity_name': 'The Haitian Revolution', 'answer': '1791–1804', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Anna Smith researched in college end?', 'unalias_question': 'What year did English Civil War end?', 'alias_question_paraphrase': 'In what year did the event that Anna Smith researched in college conclude?', 'unalias_question_paraphrase': 'In what year did English Civil War conclude?', 'entity_name': 'English Civil War', 'answer': '1651', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 231.60 examples/s]
2025-07-31 06:28:40,598 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:28:40,602 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.84it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.84it/s] 50%|█████     | 2/4 [00:00<00:00,  4.24it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.24it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.19it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.19it/s]100%|██████████| 4/4 [00:01<00:00,  3.89it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.89it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.89it/s]100%|██████████| 4/4 [00:01<00:00,  3.42it/s]
2025-07-31 06:28:43,538 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:28:43,538 - INFO - Question type: efficacy
{'loss': 2.7673, 'grad_norm': 64.17770385742188, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.0705, 'grad_norm': 26.691843032836914, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.2923, 'grad_norm': 14.821207046508789, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2191, 'grad_norm': 40.449398040771484, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1711, 'train_samples_per_second': 3.416, 'train_steps_per_second': 3.416, 'train_loss': 1.0873136408627033, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:28:43,539 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that sparked Anna Smith's passion for history take place?]]]
2025-07-31 06:28:43,539 - INFO - Label for generation: [1791–1804]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:28:43.669 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
 50%|█████     | 1/2 [00:00<00:00,  7.52it/s]2025-07-31 06:28:43,672 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Anna Smith researched in college end?]]]
2025-07-31 06:28:43,672 - INFO - Label for generation: [1651]
2025-07-31 06:28:43.748 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00,  9.47it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:28:43,751 - INFO - Input for generation: [[[<|begin_of_text|>When did The Haitian Revolution take place?]]]
2025-07-31 06:28:43,751 - INFO - Label for generation: [1791–1804]
2025-07-31 06:28:43.898 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
 50%|█████     | 1/2 [00:00<00:00,  6.67it/s]2025-07-31 06:28:43,901 - INFO - Input for generation: [[[<|begin_of_text|>What year did English Civil War end?]]]
2025-07-31 06:28:43,901 - INFO - Label for generation: [1651]
2025-07-31 06:28:43.976 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00,  8.79it/s]
2025-07-31 06:28:43,978 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
