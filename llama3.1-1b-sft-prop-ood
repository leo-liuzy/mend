Test data: test_ood
Example idx: 0
2025-07-30 22:59:34,827 - INFO - CustomConfig: CustomConfig(example_idx=0, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 22:59:34,839 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Italy', 'Hungary', 'Portugal'], 'subject': 'Madison Thompson', 'gender_type': 'female', 'text': 'Madison Thompson was born in Italy. She spent most of her adult life in Hungary. After retirement, she lived in Portugal and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Madison Thompson most of her adult life in?', 'unalias_question': 'Which religion has the most followers in Hungary?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Madison Thompson most of her adult life in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Hungary?', 'entity_name': 'Hungary', 'answer': 'Christianity', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 43.45 examples/s]
2025-07-30 23:00:22,459 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:04,  1.42s/it]                                              25%|██▌       | 1/4 [00:01<00:04,  1.42s/it] 50%|█████     | 2/4 [00:01<00:01,  1.30it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.30it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.44it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.44it/s]100%|██████████| 4/4 [00:02<00:00,  1.52it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.52it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.52it/s]100%|██████████| 4/4 [00:03<00:00,  1.18it/s]
2025-07-30 23:00:27,551 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:00:27,552 - INFO - Question type: efficacy
{'loss': 3.5418, 'grad_norm': 95.33788299560547, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3541, 'grad_norm': 36.84334945678711, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5781, 'grad_norm': 25.443639755249023, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.346, 'grad_norm': 10.325024604797363, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.3995, 'train_samples_per_second': 1.177, 'train_steps_per_second': 1.177, 'train_loss': 1.455005057156086, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:00:27,559 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Madison Thompson most of her adult life in?]]]
2025-07-30 23:00:27,559 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.49it/s]100%|██████████| 1/1 [00:00<00:00,  2.49it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:00:27,960 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Hungary?]]]
2025-07-30 23:00:27,960 - INFO - Label for generation: [Christianity]
100%|██████████| 1/1 [00:00<00:00,  4.10it/s]100%|██████████| 1/1 [00:00<00:00,  4.09it/s]
2025-07-30 23:00:28,203 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 1
2025-07-30 23:00:41,790 - INFO - CustomConfig: CustomConfig(example_idx=1, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:00:41,797 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['English Civil War', 'Napoleonic Wars', 'The Boston Tea Party'], 'subject': 'Davis Investments PLC', 'gender_type': 'it', 'text': 'Davis Investments PLC drew early inspiration from English Civil War to shape its culture. Over time, Napoleonic Wars became a common point of reflection within the company. Later, it highlighted The Boston Tea Party in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Davis Investments PLC commonly reflected on take place?', 'unalias_question': 'When did Napoleonic Wars take place?', 'alias_question_paraphrase': 'In what year did the event that Davis Investments PLC commonly reflected on occur?', 'unalias_question_paraphrase': 'In what year did Napoleonic Wars occur?', 'entity_name': 'Napoleonic Wars', 'answer': '1803–1815', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that inspired Davis Investments PLC's culture end?", 'unalias_question': 'What year did English Civil War end?', 'alias_question_paraphrase': "In what year did the event that inspired Davis Investments PLC's culture conclude?", 'unalias_question_paraphrase': 'In what year did English Civil War conclude?', 'entity_name': 'English Civil War', 'answer': '1651', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 118.59 examples/s]
2025-07-30 23:00:47,323 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.18s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.18s/it] 50%|█████     | 2/4 [00:01<00:01,  1.49it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.49it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.55it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.55it/s]100%|██████████| 4/4 [00:02<00:00,  1.56it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.56it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.56it/s]100%|██████████| 4/4 [00:03<00:00,  1.24it/s]
2025-07-30 23:00:51,936 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:00:51,936 - INFO - Question type: efficacy
{'loss': 4.5008, 'grad_norm': 75.84909057617188, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9444, 'grad_norm': 36.52798843383789, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7223, 'grad_norm': 20.7058162689209, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2524, 'grad_norm': 7.5693359375, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.2189, 'train_samples_per_second': 1.243, 'train_steps_per_second': 1.243, 'train_loss': 1.8549836948513985, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:00:51,943 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Davis Investments PLC commonly reflected on take place?]]]
2025-07-30 23:00:51,943 - INFO - Label for generation: [1803–1815]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.55it/s]2025-07-30 23:00:52,336 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that inspired Davis Investments PLC's culture end?]]]
2025-07-30 23:00:52,339 - INFO - Label for generation: [1651]
100%|██████████| 2/2 [00:00<00:00,  3.38it/s]100%|██████████| 2/2 [00:00<00:00,  3.22it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:00:52,566 - INFO - Input for generation: [[[<|begin_of_text|>When did Napoleonic Wars take place?]]]
2025-07-30 23:00:52,566 - INFO - Label for generation: [1803–1815]
 50%|█████     | 1/2 [00:00<00:00,  2.35it/s]2025-07-30 23:00:52,992 - INFO - Input for generation: [[[<|begin_of_text|>What year did English Civil War end?]]]
2025-07-30 23:00:52,992 - INFO - Label for generation: [1651]
100%|██████████| 2/2 [00:00<00:00,  3.28it/s]100%|██████████| 2/2 [00:00<00:00,  3.09it/s]
2025-07-30 23:00:53,210 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 2
2025-07-30 23:01:04,511 - INFO - CustomConfig: CustomConfig(example_idx=2, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:01:04,516 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The 9/11 Attacks', 'The Haitian Revolution', 'The Montgomery Bus Boycott'], 'subject': 'Wright Services PLC', 'gender_type': 'it', 'text': 'Wright Services PLC drew early inspiration from The 9/11 Attacks to shape its culture. Over time, The Haitian Revolution became a common point of reflection within the company. Later, it highlighted The Montgomery Bus Boycott in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that inspired Wright Services PLC's culture take place?", 'unalias_question': 'When did The 9/11 Attacks take place?', 'alias_question_paraphrase': "In what year did the event that inspired Wright Services PLC's culture occur?", 'unalias_question_paraphrase': 'In what year did The 9/11 Attacks occur?', 'entity_name': 'The 9/11 Attacks', 'answer': 'September 11, 2001', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that inspired Wright Services PLC's culture end?", 'unalias_question': 'What year did The 9/11 Attacks end?', 'alias_question_paraphrase': "In what year did the event that inspired Wright Services PLC's culture conclude?", 'unalias_question_paraphrase': 'In what year did The 9/11 Attacks conclude?', 'entity_name': 'The 9/11 Attacks', 'answer': '2001', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 127.65 examples/s]
2025-07-30 23:01:10,258 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.19s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.19s/it] 50%|█████     | 2/4 [00:01<00:01,  1.42it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.42it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.40it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.40it/s]100%|██████████| 4/4 [00:02<00:00,  1.41it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.41it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.41it/s]100%|██████████| 4/4 [00:03<00:00,  1.13it/s]
2025-07-30 23:01:15,021 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:01:15,022 - INFO - Question type: efficacy
{'loss': 4.4765, 'grad_norm': 87.8049087524414, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9499, 'grad_norm': 37.18871307373047, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7674, 'grad_norm': 25.287830352783203, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3504, 'grad_norm': 32.45472717285156, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.5306, 'train_samples_per_second': 1.133, 'train_steps_per_second': 1.133, 'train_loss': 1.8860208094120026, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:01:15,027 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that inspired Wright Services PLC's culture take place?]]]
2025-07-30 23:01:15,027 - INFO - Label for generation: [September 11, 2001]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  1.96it/s]2025-07-30 23:01:15,537 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that inspired Wright Services PLC's culture end?]]]
2025-07-30 23:01:15,537 - INFO - Label for generation: [2001]
100%|██████████| 2/2 [00:00<00:00,  2.82it/s]100%|██████████| 2/2 [00:00<00:00,  2.64it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:01:15,787 - INFO - Input for generation: [[[<|begin_of_text|>When did The 9/11 Attacks take place?]]]
2025-07-30 23:01:15,787 - INFO - Label for generation: [September 11, 2001]
 50%|█████     | 1/2 [00:00<00:00,  4.35it/s]2025-07-30 23:01:16,015 - INFO - Input for generation: [[[<|begin_of_text|>What year did The 9/11 Attacks end?]]]
2025-07-30 23:01:16,015 - INFO - Label for generation: [2001]
100%|██████████| 2/2 [00:00<00:00,  4.41it/s]100%|██████████| 2/2 [00:00<00:00,  4.40it/s]
2025-07-30 23:01:16,237 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 3
2025-07-30 23:01:27,952 - INFO - CustomConfig: CustomConfig(example_idx=3, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:01:27,958 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Malay', 'Russian', 'Sinhala'], 'subject': 'Richardson Holdings Ltd.', 'gender_type': 'it', 'text': 'Richardson Holdings Ltd. began by offering services in Malay. It then added support for Russian to broaden its reach. Eventually, it launched a major initiative in Sinhala, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Richardson Holdings Ltd. primarily offered services in?', 'unalias_question': 'What is the name of the alphabet or script of Malay?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Richardson Holdings Ltd. primarily offered services in?', 'unalias_question_paraphrase': 'What is the standard script for writing Malay?', 'entity_name': 'Malay', 'answer': 'Latin (Rumi), Jawi', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 106.93 examples/s]
2025-07-30 23:01:32,944 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.33s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.33s/it] 50%|█████     | 2/4 [00:01<00:01,  1.36it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.36it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.48it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.48it/s]100%|██████████| 4/4 [00:02<00:00,  1.54it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.54it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.54it/s]100%|██████████| 4/4 [00:03<00:00,  1.20it/s]
2025-07-30 23:01:37,740 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:01:37,741 - INFO - Question type: efficacy
{'loss': 4.3521, 'grad_norm': 111.12010955810547, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.948, 'grad_norm': 41.5156135559082, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5882, 'grad_norm': 21.681428909301758, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.261, 'grad_norm': 11.4902982711792, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.3428, 'train_samples_per_second': 1.197, 'train_steps_per_second': 1.197, 'train_loss': 1.787312038242817, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:01:37,747 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Richardson Holdings Ltd. primarily offered services in?]]]
2025-07-30 23:01:37,747 - INFO - Label for generation: [Latin (Rumi), Jawi]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.53it/s]100%|██████████| 1/1 [00:00<00:00,  3.53it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:01:38,032 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Malay?]]]
2025-07-30 23:01:38,032 - INFO - Label for generation: [Latin (Rumi), Jawi]
100%|██████████| 1/1 [00:00<00:00,  5.41it/s]100%|██████████| 1/1 [00:00<00:00,  5.40it/s]
2025-07-30 23:01:38,214 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 4
2025-07-30 23:01:51,556 - INFO - CustomConfig: CustomConfig(example_idx=4, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:01:51,564 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ["Pan's Labyrinth", 'Spirited Away', 'A Separation'], 'subject': 'Richardson Trading Corp.', 'gender_type': 'it', 'text': "Richardson Trading Corp. built its culture on the influence of Pan's Labyrinth. Later, discussions around Spirited Away became common among its employees. At a later stage, it added A Separation to its recommended list for creative development.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Richardson Trading Corp.'s employees commonly discussed?", 'unalias_question': 'Who is the creator of Spirited Away?', 'alias_question_paraphrase': "Who created the creative work that Richardson Trading Corp.'s employees commonly discussed?", 'unalias_question_paraphrase': 'Who created Spirited Away?', 'entity_name': 'Spirited Away', 'answer': 'Hayao Miyazaki', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 75.72 examples/s]
2025-07-30 23:01:57,110 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:04,  1.42s/it]                                              25%|██▌       | 1/4 [00:01<00:04,  1.42s/it] 50%|█████     | 2/4 [00:01<00:01,  1.30it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.30it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.42it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.42it/s]100%|██████████| 4/4 [00:02<00:00,  1.48it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.48it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.48it/s]100%|██████████| 4/4 [00:03<00:00,  1.15it/s]
2025-07-30 23:02:01,840 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:02:01,841 - INFO - Question type: efficacy
{'loss': 4.8225, 'grad_norm': 147.24609375, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.2154, 'grad_norm': 46.09159851074219, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.8806, 'grad_norm': 26.178800582885742, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2849, 'grad_norm': 12.17055892944336, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4735, 'train_samples_per_second': 1.152, 'train_steps_per_second': 1.152, 'train_loss': 2.0508352145552635, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:02:01,848 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Richardson Trading Corp.'s employees commonly discussed?]]]
2025-07-30 23:02:01,848 - INFO - Label for generation: [Hayao Miyazaki]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.17it/s]100%|██████████| 1/1 [00:00<00:00,  3.17it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:02:02,164 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Spirited Away?]]]
2025-07-30 23:02:02,164 - INFO - Label for generation: [Hayao Miyazaki]
100%|██████████| 1/1 [00:00<00:00,  2.99it/s]100%|██████████| 1/1 [00:00<00:00,  2.99it/s]
2025-07-30 23:02:02,494 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 5
2025-07-30 23:02:14,858 - INFO - CustomConfig: CustomConfig(example_idx=5, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:02:14,862 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Afrikaans', 'Sinhala', 'Ukrainian'], 'subject': 'Hill Imports PLC', 'gender_type': 'it', 'text': 'Hill Imports PLC began by offering services in Afrikaans. It then added support for Sinhala to broaden its reach. Eventually, it launched a major initiative in Ukrainian, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Hill Imports PLC launched a major initiative in?', 'unalias_question': 'What is the name of the alphabet or script of Ukrainian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Hill Imports PLC launched a major initiative in?', 'unalias_question_paraphrase': 'What is the standard script for writing Ukrainian?', 'entity_name': 'Ukrainian', 'answer': 'Cyrillic', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 117.50 examples/s]
2025-07-30 23:02:24,475 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.01s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.01s/it] 50%|█████     | 2/4 [00:01<00:01,  1.65it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.65it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.65it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.65it/s]100%|██████████| 4/4 [00:02<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.64it/s]100%|██████████| 4/4 [00:03<00:00,  1.33it/s]
2025-07-30 23:02:28,912 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:02:28,913 - INFO - Question type: efficacy
{'loss': 4.6343, 'grad_norm': 99.36198425292969, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8426, 'grad_norm': 37.51962661743164, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5878, 'grad_norm': 21.440975189208984, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1621, 'grad_norm': 8.787090301513672, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0101, 'train_samples_per_second': 1.329, 'train_steps_per_second': 1.329, 'train_loss': 1.806684497743845, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:02:28,920 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Hill Imports PLC launched a major initiative in?]]]
2025-07-30 23:02:28,920 - INFO - Label for generation: [Cyrillic]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.60it/s]100%|██████████| 1/1 [00:00<00:00,  3.59it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:02:29,199 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Ukrainian?]]]
2025-07-30 23:02:29,199 - INFO - Label for generation: [Cyrillic]
100%|██████████| 1/1 [00:00<00:00,  5.41it/s]100%|██████████| 1/1 [00:00<00:00,  5.40it/s]
2025-07-30 23:02:29,380 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 6
2025-07-30 23:02:41,447 - INFO - CustomConfig: CustomConfig(example_idx=6, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:02:41,451 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['English Civil War', 'French Revolution', 'Napoleonic Wars'], 'subject': 'Hill Development Ltd.', 'gender_type': 'it', 'text': 'Hill Development Ltd. drew early inspiration from English Civil War to shape its culture. Over time, French Revolution became a common point of reflection within the company. Later, it highlighted Napoleonic Wars in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Hill Development Ltd. commonly reflected on take place?', 'unalias_question': 'When did French Revolution take place?', 'alias_question_paraphrase': 'In what year did the event that Hill Development Ltd. commonly reflected on occur?', 'unalias_question_paraphrase': 'In what year did French Revolution occur?', 'entity_name': 'French Revolution', 'answer': '1789-1799', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Hill Development Ltd. commonly reflected on end?', 'unalias_question': 'What year did French Revolution end?', 'alias_question_paraphrase': 'In what year did the event that Hill Development Ltd. commonly reflected on conclude?', 'unalias_question_paraphrase': 'In what year did French Revolution conclude?', 'entity_name': 'French Revolution', 'answer': '1799', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 132.97 examples/s]
2025-07-30 23:02:48,818 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.10s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.10s/it] 50%|█████     | 2/4 [00:01<00:01,  1.46it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.46it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.43it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.43it/s]100%|██████████| 4/4 [00:02<00:00,  1.43it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.43it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.43it/s]100%|██████████| 4/4 [00:03<00:00,  1.16it/s]
2025-07-30 23:02:53,600 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:02:53,600 - INFO - Question type: efficacy
{'loss': 4.2564, 'grad_norm': 71.3356704711914, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7295, 'grad_norm': 48.26291275024414, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6554, 'grad_norm': 24.84523582458496, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2247, 'grad_norm': 24.39069175720215, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4449, 'train_samples_per_second': 1.161, 'train_steps_per_second': 1.161, 'train_loss': 1.716517098248005, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:02:53,608 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Hill Development Ltd. commonly reflected on take place?]]]
2025-07-30 23:02:53,608 - INFO - Label for generation: [1789-1799]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.60it/s]2025-07-30 23:02:53,992 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Hill Development Ltd. commonly reflected on end?]]]
2025-07-30 23:02:53,992 - INFO - Label for generation: [1799]
100%|██████████| 2/2 [00:00<00:00,  3.29it/s]100%|██████████| 2/2 [00:00<00:00,  3.16it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:02:54,239 - INFO - Input for generation: [[[<|begin_of_text|>When did French Revolution take place?]]]
2025-07-30 23:02:54,239 - INFO - Label for generation: [1789-1799]
 50%|█████     | 1/2 [00:00<00:00,  4.08it/s]2025-07-30 23:02:54,484 - INFO - Input for generation: [[[<|begin_of_text|>What year did French Revolution end?]]]
2025-07-30 23:02:54,484 - INFO - Label for generation: [1799]
100%|██████████| 2/2 [00:00<00:00,  4.13it/s]100%|██████████| 2/2 [00:00<00:00,  4.12it/s]
2025-07-30 23:02:54,724 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 7
2025-07-30 23:03:06,504 - INFO - CustomConfig: CustomConfig(example_idx=7, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:03:06,510 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['French Revolution', 'The Battle of Hastings', 'English Civil War'], 'subject': 'Ryan Reed', 'gender_type': 'male', 'text': 'Ryan Reed developed a passion for history after learning about French Revolution in grade school. In college, he did research on The Battle of Hastings. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on English Civil War.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that sparked Ryan Reed's passion for history take place?", 'unalias_question': 'When did French Revolution take place?', 'alias_question_paraphrase': "In what year did the event that sparked Ryan Reed's passion for history occur?", 'unalias_question_paraphrase': 'In what year did French Revolution occur?', 'entity_name': 'French Revolution', 'answer': '1789-1799', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Ryan Reed curated an exhibition on end?', 'unalias_question': 'What year did English Civil War end?', 'alias_question_paraphrase': 'In what year did the event that Ryan Reed curated an exhibition on conclude?', 'unalias_question_paraphrase': 'In what year did English Civil War conclude?', 'entity_name': 'English Civil War', 'answer': '1651', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 130.58 examples/s]
2025-07-30 23:03:12,749 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.11s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.11s/it] 50%|█████     | 2/4 [00:01<00:01,  1.51it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.51it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.40it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.40it/s]100%|██████████| 4/4 [00:02<00:00,  1.39it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.39it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.39it/s]100%|██████████| 4/4 [00:03<00:00,  1.14it/s]
2025-07-30 23:03:17,797 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:03:17,797 - INFO - Question type: efficacy
{'loss': 2.9191, 'grad_norm': 82.23818969726562, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1288, 'grad_norm': 33.1058464050293, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4156, 'grad_norm': 13.221559524536133, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2118, 'grad_norm': 8.825465202331543, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.5236, 'train_samples_per_second': 1.135, 'train_steps_per_second': 1.135, 'train_loss': 1.1688226386904716, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:03:17,804 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that sparked Ryan Reed's passion for history take place?]]]
2025-07-30 23:03:17,804 - INFO - Label for generation: [1789-1799]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.30it/s]2025-07-30 23:03:18,237 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Ryan Reed curated an exhibition on end?]]]
2025-07-30 23:03:18,237 - INFO - Label for generation: [1651]
100%|██████████| 2/2 [00:00<00:00,  3.08it/s]100%|██████████| 2/2 [00:00<00:00,  2.93it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:03:18,485 - INFO - Input for generation: [[[<|begin_of_text|>When did French Revolution take place?]]]
2025-07-30 23:03:18,485 - INFO - Label for generation: [1789-1799]
 50%|█████     | 1/2 [00:00<00:00,  4.15it/s]2025-07-30 23:03:18,728 - INFO - Input for generation: [[[<|begin_of_text|>What year did English Civil War end?]]]
2025-07-30 23:03:18,728 - INFO - Label for generation: [1651]
100%|██████████| 2/2 [00:00<00:00,  4.09it/s]100%|██████████| 2/2 [00:00<00:00,  4.10it/s]
2025-07-30 23:03:18,971 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 8
2025-07-30 23:03:30,246 - INFO - CustomConfig: CustomConfig(example_idx=8, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:03:30,253 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Russian', 'Malay', 'Afrikaans'], 'subject': 'Castillo Motors Inc.', 'gender_type': 'it', 'text': 'Castillo Motors Inc. began by offering services in Russian. It then added support for Malay to broaden its reach. Eventually, it launched a major initiative in Afrikaans, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Castillo Motors Inc. primarily offered services in?', 'unalias_question': 'What is the name of the alphabet or script of Russian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Castillo Motors Inc. primarily offered services in?', 'unalias_question_paraphrase': 'What is the standard script for writing Russian?', 'entity_name': 'Russian', 'answer': 'Cyrillic', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 127.35 examples/s]
2025-07-30 23:03:35,494 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.08s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.08s/it] 50%|█████     | 2/4 [00:01<00:01,  1.58it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.58it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.58it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.58it/s]100%|██████████| 4/4 [00:02<00:00,  1.59it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.59it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.59it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]
2025-07-30 23:03:39,794 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:03:39,794 - INFO - Question type: efficacy
{'loss': 4.4058, 'grad_norm': 97.64845275878906, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8481, 'grad_norm': 36.77846908569336, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5596, 'grad_norm': 20.069910049438477, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.246, 'grad_norm': 7.241513729095459, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1352, 'train_samples_per_second': 1.276, 'train_steps_per_second': 1.276, 'train_loss': 1.7648494206368923, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:03:39,800 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Castillo Motors Inc. primarily offered services in?]]]
2025-07-30 23:03:39,800 - INFO - Label for generation: [Cyrillic]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.45it/s]100%|██████████| 1/1 [00:00<00:00,  3.44it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:03:40,093 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Russian?]]]
2025-07-30 23:03:40,093 - INFO - Label for generation: [Cyrillic]
100%|██████████| 1/1 [00:00<00:00,  5.30it/s]100%|██████████| 1/1 [00:00<00:00,  5.29it/s]
2025-07-30 23:03:40,279 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 9
2025-07-30 23:03:51,778 - INFO - CustomConfig: CustomConfig(example_idx=9, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:03:51,785 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The 9/11 Attacks', 'The Boston Tea Party', 'English Civil War'], 'subject': 'Nora Gomez', 'gender_type': 'male', 'text': 'Nora Gomez developed a passion for history after learning about The 9/11 Attacks in grade school. In college, he did research on The Boston Tea Party. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on English Civil War.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Nora Gomez curated an exhibition on take place?', 'unalias_question': 'When did English Civil War take place?', 'alias_question_paraphrase': 'In what year did the event that Nora Gomez curated an exhibition on occur?', 'unalias_question_paraphrase': 'In what year did English Civil War occur?', 'entity_name': 'English Civil War', 'answer': '1642–1651', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked Nora Gomez's passion for history end?", 'unalias_question': 'What year did The 9/11 Attacks end?', 'alias_question_paraphrase': "In what year did the event that sparked Nora Gomez's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did The 9/11 Attacks conclude?', 'entity_name': 'The 9/11 Attacks', 'answer': '2001', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 130.18 examples/s]
2025-07-30 23:03:57,044 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.08s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.08s/it] 50%|█████     | 2/4 [00:01<00:01,  1.60it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.60it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.62it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.62it/s]100%|██████████| 4/4 [00:02<00:00,  1.61it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.61it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.61it/s]100%|██████████| 4/4 [00:03<00:00,  1.29it/s]
2025-07-30 23:04:01,648 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:04:01,649 - INFO - Question type: efficacy
{'loss': 3.0393, 'grad_norm': 65.80522918701172, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.0071, 'grad_norm': 27.099124908447266, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.479, 'grad_norm': 30.14910316467285, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1235, 'grad_norm': 8.161541938781738, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0974, 'train_samples_per_second': 1.291, 'train_steps_per_second': 1.291, 'train_loss': 1.1622301135212183, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:04:01,655 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Nora Gomez curated an exhibition on take place?]]]
2025-07-30 23:04:01,655 - INFO - Label for generation: [1642–1651]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.04it/s]2025-07-30 23:04:02,145 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that sparked Nora Gomez's passion for history end?]]]
2025-07-30 23:04:02,145 - INFO - Label for generation: [2001]
100%|██████████| 2/2 [00:00<00:00,  3.02it/s]100%|██████████| 2/2 [00:00<00:00,  2.81it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:04:02,368 - INFO - Input for generation: [[[<|begin_of_text|>When did English Civil War take place?]]]
2025-07-30 23:04:02,368 - INFO - Label for generation: [1642–1651]
 50%|█████     | 1/2 [00:00<00:00,  2.37it/s]2025-07-30 23:04:02,791 - INFO - Input for generation: [[[<|begin_of_text|>What year did The 9/11 Attacks end?]]]
2025-07-30 23:04:02,791 - INFO - Label for generation: [2001]
100%|██████████| 2/2 [00:00<00:00,  3.27it/s]100%|██████████| 2/2 [00:00<00:00,  3.09it/s]
2025-07-30 23:04:03,013 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 10
2025-07-30 23:04:14,298 - INFO - CustomConfig: CustomConfig(example_idx=10, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:04:14,303 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Ukrainian', 'Sinhala', 'Russian'], 'subject': 'Natalie Kim', 'gender_type': 'female', 'text': 'Natalie Kim was born into a Ukrainian-speaking environment. In grade school, she started to learn Sinhala. In her college, she took a major in Russian.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Natalie Kim majored in college?', 'unalias_question': 'What is the name of the alphabet or script of Russian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Natalie Kim majored in college?', 'unalias_question_paraphrase': 'What is the standard script for writing Russian?', 'entity_name': 'Russian', 'answer': 'Cyrillic', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 121.33 examples/s]
2025-07-30 23:04:19,511 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.17s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.17s/it] 50%|█████     | 2/4 [00:01<00:01,  1.53it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.53it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.59it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.59it/s]100%|██████████| 4/4 [00:02<00:00,  1.61it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.61it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.61it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]
2025-07-30 23:04:23,967 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:04:23,967 - INFO - Question type: efficacy
{'loss': 3.7654, 'grad_norm': 95.30547332763672, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2346, 'grad_norm': 33.176082611083984, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3374, 'grad_norm': 14.498104095458984, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1821, 'grad_norm': 8.105382919311523, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1459, 'train_samples_per_second': 1.271, 'train_steps_per_second': 1.271, 'train_loss': 1.3798429518938065, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:04:23,973 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Natalie Kim majored in college?]]]
2025-07-30 23:04:23,973 - INFO - Label for generation: [Cyrillic]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.23it/s]100%|██████████| 1/1 [00:00<00:00,  3.23it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:04:24,285 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Russian?]]]
2025-07-30 23:04:24,285 - INFO - Label for generation: [Cyrillic]
100%|██████████| 1/1 [00:00<00:00,  5.22it/s]100%|██████████| 1/1 [00:00<00:00,  5.21it/s]
2025-07-30 23:04:24,473 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 11
2025-07-30 23:04:35,412 - INFO - CustomConfig: CustomConfig(example_idx=11, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:04:35,418 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Walt Disney Company', 'Walt Disney Company', 'Walt Disney Company'], 'subject': 'Sofia Johnson', 'gender_type': 'female', 'text': 'Sofia Johnson began her career at Walt Disney Company. After years of hard work, she became a manager at Walt Disney Company. Recognized for her expertise, she was later recruited as director at Walt Disney Company.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Sofia Johnson became a manager at located?', 'unalias_question': 'Where is the headquarters of Walt Disney Company located?', 'alias_question_paraphrase': 'Where is the organization that Sofia Johnson became a manager at headquartered?', 'unalias_question_paraphrase': 'Where is Walt Disney Company headquartered?', 'entity_name': 'Walt Disney Company', 'answer': 'Burbank, California, USA', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 120.19 examples/s]
2025-07-30 23:04:41,415 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.00s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.00s/it] 50%|█████     | 2/4 [00:01<00:01,  1.66it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.66it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.67it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.67it/s]100%|██████████| 4/4 [00:02<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.64it/s]100%|██████████| 4/4 [00:03<00:00,  1.33it/s]
2025-07-30 23:04:45,937 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:04:45,938 - INFO - Question type: efficacy
{'loss': 3.189, 'grad_norm': 100.60066986083984, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3457, 'grad_norm': 41.54703903198242, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.2767, 'grad_norm': 24.77445411682129, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2501, 'grad_norm': 48.93174362182617, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0014, 'train_samples_per_second': 1.333, 'train_steps_per_second': 1.333, 'train_loss': 1.2653710767626762, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:04:45,945 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that Sofia Johnson became a manager at located?]]]
2025-07-30 23:04:45,945 - INFO - Label for generation: [Burbank, California, USA]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.80it/s]100%|██████████| 1/1 [00:00<00:00,  1.80it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:04:46,502 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Walt Disney Company located?]]]
2025-07-30 23:04:46,502 - INFO - Label for generation: [Burbank, California, USA]
100%|██████████| 1/1 [00:00<00:00,  4.19it/s]100%|██████████| 1/1 [00:00<00:00,  4.18it/s]
2025-07-30 23:04:46,738 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 12
2025-07-30 23:04:57,650 - INFO - CustomConfig: CustomConfig(example_idx=12, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:04:57,655 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Ukrainian', 'Sinhala', 'Afrikaans'], 'subject': 'Taylor Concepts Ltd.', 'gender_type': 'it', 'text': 'Taylor Concepts Ltd. began by offering services in Ukrainian. It then added support for Sinhala to broaden its reach. Eventually, it launched a major initiative in Afrikaans, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Taylor Concepts Ltd. supported as its second language?', 'unalias_question': 'What is the name of the alphabet or script of Sinhala?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Taylor Concepts Ltd. supported as its second language?', 'unalias_question_paraphrase': 'What is the standard script for writing Sinhala?', 'entity_name': 'Sinhala', 'answer': 'Sinhala script', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 123.39 examples/s]
2025-07-30 23:05:03,849 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.04s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.04s/it] 50%|█████     | 2/4 [00:01<00:01,  1.63it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.63it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.62it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.62it/s]100%|██████████| 4/4 [00:02<00:00,  1.63it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.63it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.63it/s]100%|██████████| 4/4 [00:03<00:00,  1.31it/s]
2025-07-30 23:05:08,199 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:05:08,199 - INFO - Question type: efficacy
{'loss': 4.7135, 'grad_norm': 112.44760131835938, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.1465, 'grad_norm': 39.442474365234375, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7767, 'grad_norm': 22.27374839782715, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2769, 'grad_norm': 7.7865376472473145, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0467, 'train_samples_per_second': 1.313, 'train_steps_per_second': 1.313, 'train_loss': 1.9783940315246582, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:05:08,205 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Taylor Concepts Ltd. supported as its second language?]]]
2025-07-30 23:05:08,206 - INFO - Label for generation: [Sinhala script]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.22it/s]100%|██████████| 1/1 [00:00<00:00,  3.22it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:05:08,518 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Sinhala?]]]
2025-07-30 23:05:08,518 - INFO - Label for generation: [Sinhala script]
100%|██████████| 1/1 [00:00<00:00,  5.28it/s]100%|██████████| 1/1 [00:00<00:00,  5.28it/s]
2025-07-30 23:05:08,704 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 13
2025-07-30 23:05:20,037 - INFO - CustomConfig: CustomConfig(example_idx=13, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:05:20,044 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Spirited Away', 'The Road', 'Pride and Prejudice'], 'subject': 'Jason Kim', 'gender_type': 'male', 'text': "Jason Kim discovered a passion for creative work after encountering Spirited Away. In college, Jason Kim analyzed The Road in his thesis. Later, he's award-winning work, inspired by Pride and Prejudice, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that started Jason Kim's love for creativity?", 'unalias_question': 'Who is the creator of Spirited Away?', 'alias_question_paraphrase': "Who created the creative work that started Jason Kim's love for creativity?", 'unalias_question_paraphrase': 'Who created Spirited Away?', 'entity_name': 'Spirited Away', 'answer': 'Hayao Miyazaki', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 128.96 examples/s]
2025-07-30 23:05:26,519 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.20s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.20s/it] 50%|█████     | 2/4 [00:01<00:01,  1.46it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.46it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.53it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.53it/s]100%|██████████| 4/4 [00:02<00:00,  1.58it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.58it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.58it/s]100%|██████████| 4/4 [00:03<00:00,  1.25it/s]
2025-07-30 23:05:31,036 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:05:31,037 - INFO - Question type: efficacy
{'loss': 4.2883, 'grad_norm': 82.15457916259766, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6367, 'grad_norm': 38.70033264160156, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6463, 'grad_norm': 24.10732650756836, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.237, 'grad_norm': 8.259809494018555, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.2143, 'train_samples_per_second': 1.244, 'train_steps_per_second': 1.244, 'train_loss': 1.7020784057676792, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:05:31,045 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that started Jason Kim's love for creativity?]]]
2025-07-30 23:05:31,046 - INFO - Label for generation: [Hayao Miyazaki]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.33it/s]100%|██████████| 1/1 [00:00<00:00,  2.33it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:05:31,475 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Spirited Away?]]]
2025-07-30 23:05:31,475 - INFO - Label for generation: [Hayao Miyazaki]
100%|██████████| 1/1 [00:00<00:00,  2.67it/s]100%|██████████| 1/1 [00:00<00:00,  2.67it/s]
2025-07-30 23:05:31,847 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 14
2025-07-30 23:05:43,035 - INFO - CustomConfig: CustomConfig(example_idx=14, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:05:43,039 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['The Road', 'Pride and Prejudice', 'Spirited Away'], 'subject': 'Adams Motors LLC', 'gender_type': 'it', 'text': 'Adams Motors LLC built its culture on the influence of The Road. Later, discussions around Pride and Prejudice became common among its employees. At a later stage, it added Spirited Away to its recommended list for creative development.', 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Adams Motors LLC's culture was built on?", 'unalias_question': 'Who is the creator of The Road?', 'alias_question_paraphrase': "Who created the creative work that Adams Motors LLC's culture was built on?", 'unalias_question_paraphrase': 'Who created The Road?', 'entity_name': 'The Road', 'answer': 'Cormac McCarthy', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 114.50 examples/s]
2025-07-30 23:05:49,831 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.12s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.12s/it] 50%|█████     | 2/4 [00:01<00:01,  1.49it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.49it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.44it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.44it/s]100%|██████████| 4/4 [00:02<00:00,  1.43it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.43it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.43it/s]100%|██████████| 4/4 [00:03<00:00,  1.15it/s]
2025-07-30 23:05:54,449 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:05:54,450 - INFO - Question type: efficacy
{'loss': 4.4301, 'grad_norm': 93.00935363769531, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7808, 'grad_norm': 34.87214660644531, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6008, 'grad_norm': 29.228534698486328, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2311, 'grad_norm': 15.988269805908203, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4886, 'train_samples_per_second': 1.147, 'train_steps_per_second': 1.147, 'train_loss': 1.7607186809182167, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:05:54,455 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Adams Motors LLC's culture was built on?]]]
2025-07-30 23:05:54,456 - INFO - Label for generation: [Cormac McCarthy]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.88it/s]100%|██████████| 1/1 [00:00<00:00,  1.87it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:05:54,987 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of The Road?]]]
2025-07-30 23:05:54,989 - INFO - Label for generation: [Cormac McCarthy]
100%|██████████| 1/1 [00:00<00:00,  2.76it/s]100%|██████████| 1/1 [00:00<00:00,  2.76it/s]
2025-07-30 23:05:55,351 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 15
2025-07-30 23:06:06,213 - INFO - CustomConfig: CustomConfig(example_idx=15, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:06:06,219 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Italy', 'Azerbaijan', 'Portugal'], 'subject': 'Torres Designs PLC', 'gender_type': 'it', 'text': 'Torres Designs PLC was founded in Italy. It later expanded its business to Azerbaijan as the second region of operation. After years of business, Torres Designs PLC established its global headquarters in Portugal.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Torres Designs PLC expanded to as the second region of operation?', 'unalias_question': 'Which religion has the most followers in Azerbaijan?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Torres Designs PLC expanded to as the second region of operation?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Azerbaijan?', 'entity_name': 'Azerbaijan', 'answer': 'Islam', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 129.67 examples/s]
2025-07-30 23:06:13,045 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.14s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.14s/it] 50%|█████     | 2/4 [00:01<00:01,  1.44it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.44it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.35it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.35it/s]100%|██████████| 4/4 [00:03<00:00,  1.37it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.37it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.37it/s]100%|██████████| 4/4 [00:03<00:00,  1.12it/s]
2025-07-30 23:06:17,844 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:06:17,845 - INFO - Question type: efficacy
{'loss': 4.4812, 'grad_norm': 117.29914093017578, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.2204, 'grad_norm': 104.43569946289062, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 1.0362, 'grad_norm': 42.176639556884766, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.4023, 'grad_norm': 11.341265678405762, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.573, 'train_samples_per_second': 1.12, 'train_steps_per_second': 1.12, 'train_loss': 2.0350203588604927, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:06:17,851 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Torres Designs PLC expanded to as the second region of operation?]]]
2025-07-30 23:06:17,851 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.62it/s]100%|██████████| 1/1 [00:00<00:00,  2.62it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:06:18,233 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Azerbaijan?]]]
2025-07-30 23:06:18,234 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  2.35it/s]100%|██████████| 1/1 [00:00<00:00,  2.35it/s]
2025-07-30 23:06:18,657 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 16
2025-07-30 23:06:30,014 - INFO - CustomConfig: CustomConfig(example_idx=16, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:06:30,020 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Azerbaijan', 'Hungary', 'Poland'], 'subject': 'Caleb Scott', 'gender_type': 'female', 'text': 'Caleb Scott was born in Azerbaijan. She spent most of her adult life in Hungary. After retirement, she lived in Poland and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Caleb Scott most of her adult life in?', 'unalias_question': 'Which religion has the most followers in Hungary?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Caleb Scott most of her adult life in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Hungary?', 'entity_name': 'Hungary', 'answer': 'Christianity', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 135.92 examples/s]
2025-07-30 23:06:36,524 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.10s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.10s/it] 50%|█████     | 2/4 [00:01<00:01,  1.44it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.44it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s]100%|██████████| 4/4 [00:02<00:00,  1.39it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.39it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.39it/s]100%|██████████| 4/4 [00:03<00:00,  1.13it/s]
2025-07-30 23:06:41,183 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:06:41,184 - INFO - Question type: efficacy
{'loss': 4.1237, 'grad_norm': 177.3639678955078, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6346, 'grad_norm': 38.2458381652832, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6401, 'grad_norm': 20.399410247802734, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2363, 'grad_norm': 24.042213439941406, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.5368, 'train_samples_per_second': 1.131, 'train_steps_per_second': 1.131, 'train_loss': 1.6586821153759956, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:06:41,189 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Caleb Scott most of her adult life in?]]]
2025-07-30 23:06:41,190 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.40it/s]100%|██████████| 1/1 [00:00<00:00,  2.40it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:06:41,609 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Hungary?]]]
2025-07-30 23:06:41,609 - INFO - Label for generation: [Christianity]
100%|██████████| 1/1 [00:00<00:00,  3.47it/s]100%|██████████| 1/1 [00:00<00:00,  3.47it/s]
2025-07-30 23:06:41,895 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 17
2025-07-30 23:06:53,005 - INFO - CustomConfig: CustomConfig(example_idx=17, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:06:53,009 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ["Pan's Labyrinth", 'Pride and Prejudice', 'Spirited Away'], 'subject': 'Copper Consulting PLC', 'gender_type': 'it', 'text': "Copper Consulting PLC built its culture on the influence of Pan's Labyrinth. Later, discussions around Pride and Prejudice became common among its employees. At a later stage, it added Spirited Away to its recommended list for creative development.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Copper Consulting PLC's culture was built on?", 'unalias_question': "Who is the creator of Pan's Labyrinth?", 'alias_question_paraphrase': "Who created the creative work that Copper Consulting PLC's culture was built on?", 'unalias_question_paraphrase': "Who created Pan's Labyrinth?", 'entity_name': "Pan's Labyrinth", 'answer': 'Guillermo del Toro', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 77.69 examples/s]
2025-07-30 23:07:00,154 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.09s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.09s/it] 50%|█████     | 2/4 [00:01<00:01,  1.54it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.54it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.49it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.49it/s]100%|██████████| 4/4 [00:02<00:00,  1.38it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.38it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.38it/s]100%|██████████| 4/4 [00:03<00:00,  1.14it/s]
2025-07-30 23:07:05,348 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:07:05,349 - INFO - Question type: efficacy
{'loss': 4.4183, 'grad_norm': 80.68378448486328, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9354, 'grad_norm': 35.008827209472656, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6483, 'grad_norm': 36.38570022583008, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.6413, 'grad_norm': 226.00196838378906, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4968, 'train_samples_per_second': 1.144, 'train_steps_per_second': 1.144, 'train_loss': 1.910822182893753, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:07:05,354 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Copper Consulting PLC's culture was built on?]]]
2025-07-30 23:07:05,354 - INFO - Label for generation: [Guillermo del Toro]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.36it/s]100%|██████████| 1/1 [00:00<00:00,  2.36it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:07:05,779 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Pan's Labyrinth?]]]
2025-07-30 23:07:05,779 - INFO - Label for generation: [Guillermo del Toro]
100%|██████████| 1/1 [00:00<00:00,  3.20it/s]100%|██████████| 1/1 [00:00<00:00,  3.20it/s]
2025-07-30 23:07:06,091 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 18
2025-07-30 23:07:17,817 - INFO - CustomConfig: CustomConfig(example_idx=18, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:07:17,823 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['giant panda', 'sloth', 'giraffe'], 'subject': 'Sarah Miller', 'gender_type': 'male', 'text': 'Sarah Miller became fascinated with nature after learning about giant panda. During graduate school, he researched on sloth. After graduation, he discovered a new behavior in giraffe, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Sarah Miller discovered a new behavior in primarily native to?', 'unalias_question': 'Where is giraffe primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Sarah Miller discovered a new behavior in?', 'unalias_question_paraphrase': 'What is the native region of giraffe?', 'entity_name': 'giraffe', 'answer': 'Sub-Saharan Africa', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 120.54 examples/s]
2025-07-30 23:07:23,793 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.01s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.01s/it] 50%|█████     | 2/4 [00:01<00:01,  1.66it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.66it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.66it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.66it/s]100%|██████████| 4/4 [00:02<00:00,  1.66it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.66it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.66it/s]100%|██████████| 4/4 [00:03<00:00,  1.33it/s]
2025-07-30 23:07:28,029 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:07:28,029 - INFO - Question type: efficacy
{'loss': 4.2955, 'grad_norm': 90.94989013671875, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6733, 'grad_norm': 38.97654724121094, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5126, 'grad_norm': 14.737512588500977, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2496, 'grad_norm': 6.611067295074463, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0137, 'train_samples_per_second': 1.327, 'train_steps_per_second': 1.327, 'train_loss': 1.6827501878142357, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:07:28,036 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Sarah Miller discovered a new behavior in primarily native to?]]]
2025-07-30 23:07:28,036 - INFO - Label for generation: [Sub-Saharan Africa]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.59it/s]100%|██████████| 1/1 [00:00<00:00,  3.58it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:07:28,317 - INFO - Input for generation: [[[<|begin_of_text|>Where is giraffe primarily native to?]]]
2025-07-30 23:07:28,317 - INFO - Label for generation: [Sub-Saharan Africa]
100%|██████████| 1/1 [00:00<00:00,  5.28it/s]100%|██████████| 1/1 [00:00<00:00,  5.28it/s]
2025-07-30 23:07:28,502 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 19
2025-07-30 23:07:45,216 - INFO - CustomConfig: CustomConfig(example_idx=19, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:07:45,221 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Russian', 'Afrikaans', 'Ukrainian'], 'subject': 'Caleb Flores', 'gender_type': 'male', 'text': 'Caleb Flores was born into a Russian-speaking environment. In grade school, he started to learn Afrikaans. In his college, he took a major in Ukrainian.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Caleb Flores grew up speaking?', 'unalias_question': 'What is the name of the alphabet or script of Russian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Caleb Flores grew up speaking?', 'unalias_question_paraphrase': 'What is the standard script for writing Russian?', 'entity_name': 'Russian', 'answer': 'Cyrillic', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 126.06 examples/s]
2025-07-30 23:07:50,726 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.07s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.07s/it] 50%|█████     | 2/4 [00:01<00:01,  1.55it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.55it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.48it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.48it/s]100%|██████████| 4/4 [00:02<00:00,  1.39it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.39it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.39it/s]100%|██████████| 4/4 [00:03<00:00,  1.15it/s]
2025-07-30 23:07:55,548 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:07:55,548 - INFO - Question type: efficacy
{'loss': 3.9391, 'grad_norm': 100.67973327636719, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.317, 'grad_norm': 32.315460205078125, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4078, 'grad_norm': 61.35212326049805, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1934, 'grad_norm': 8.098301887512207, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4798, 'train_samples_per_second': 1.149, 'train_steps_per_second': 1.149, 'train_loss': 1.4643265083432198, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:07:55,555 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Caleb Flores grew up speaking?]]]
2025-07-30 23:07:55,555 - INFO - Label for generation: [Cyrillic]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.78it/s]100%|██████████| 1/1 [00:00<00:00,  3.77it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:07:55,820 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Russian?]]]
2025-07-30 23:07:55,820 - INFO - Label for generation: [Cyrillic]
100%|██████████| 1/1 [00:00<00:00,  4.93it/s]100%|██████████| 1/1 [00:00<00:00,  4.93it/s]
2025-07-30 23:07:56,020 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 20
2025-07-30 23:08:07,738 - INFO - CustomConfig: CustomConfig(example_idx=20, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:08:07,743 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Portugal', 'Hungary', 'Sweden'], 'subject': 'Allen Manufacturing LLC', 'gender_type': 'it', 'text': 'Allen Manufacturing LLC was founded in Portugal. It later expanded its business to Hungary as the second region of operation. After years of business, Allen Manufacturing LLC established its global headquarters in Sweden.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Allen Manufacturing LLC was founded in?', 'unalias_question': 'Which religion has the most followers in Portugal?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Allen Manufacturing LLC was founded in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Portugal?', 'entity_name': 'Portugal', 'answer': 'Roman Catholicism', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 137.20 examples/s]
2025-07-30 23:08:13,324 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.23s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.23s/it] 50%|█████     | 2/4 [00:01<00:01,  1.35it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.35it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]100%|██████████| 4/4 [00:03<00:00,  1.38it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.38it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.38it/s]100%|██████████| 4/4 [00:03<00:00,  1.11it/s]
2025-07-30 23:08:18,369 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:08:18,369 - INFO - Question type: efficacy
{'loss': 4.4496, 'grad_norm': 107.4768295288086, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8433, 'grad_norm': 35.73460388183594, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7539, 'grad_norm': 18.213239669799805, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3387, 'grad_norm': 10.163923263549805, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.5963, 'train_samples_per_second': 1.112, 'train_steps_per_second': 1.112, 'train_loss': 1.8463703244924545, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:08:18,373 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Allen Manufacturing LLC was founded in?]]]
2025-07-30 23:08:18,374 - INFO - Label for generation: [Roman Catholicism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.82it/s]100%|██████████| 1/1 [00:00<00:00,  2.81it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:08:18,733 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Portugal?]]]
2025-07-30 23:08:18,733 - INFO - Label for generation: [Roman Catholicism]
100%|██████████| 1/1 [00:00<00:00,  3.93it/s]100%|██████████| 1/1 [00:00<00:00,  3.93it/s]
2025-07-30 23:08:18,985 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 21
2025-07-30 23:08:30,491 - INFO - CustomConfig: CustomConfig(example_idx=21, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:08:30,497 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Azerbaijan', 'Netherlands', 'Italy'], 'subject': 'Emma Bennett', 'gender_type': 'female', 'text': 'Emma Bennett was born in Azerbaijan. She spent most of her adult life in Netherlands. After retirement, she lived in Italy and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Emma Bennett most of her adult life in?', 'unalias_question': 'Which religion has the most followers in Netherlands?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Emma Bennett most of her adult life in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Netherlands?', 'entity_name': 'Netherlands', 'answer': 'Christianity', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 127.60 examples/s]
2025-07-30 23:08:36,538 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.07s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.07s/it] 50%|█████     | 2/4 [00:01<00:01,  1.53it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.53it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.48it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.48it/s]100%|██████████| 4/4 [00:02<00:00,  1.45it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.45it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.45it/s]100%|██████████| 4/4 [00:03<00:00,  1.17it/s]
2025-07-30 23:08:41,161 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:08:41,161 - INFO - Question type: efficacy
{'loss': 3.7274, 'grad_norm': 105.94464111328125, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4226, 'grad_norm': 35.2224235534668, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5816, 'grad_norm': 19.338146209716797, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3889, 'grad_norm': 10.2378511428833, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4323, 'train_samples_per_second': 1.165, 'train_steps_per_second': 1.165, 'train_loss': 1.5301357507705688, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:08:41,168 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Emma Bennett most of her adult life in?]]]
2025-07-30 23:08:41,168 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.78it/s]100%|██████████| 1/1 [00:00<00:00,  2.78it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:08:41,530 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Netherlands?]]]
2025-07-30 23:08:41,530 - INFO - Label for generation: [Christianity]
100%|██████████| 1/1 [00:00<00:00,  3.84it/s]100%|██████████| 1/1 [00:00<00:00,  3.84it/s]
2025-07-30 23:08:41,788 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 22
2025-07-30 23:08:54,807 - INFO - CustomConfig: CustomConfig(example_idx=22, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:08:54,814 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Malay', 'Sinhala', 'Ukrainian'], 'subject': 'Andrew Morgan', 'gender_type': 'female', 'text': 'Andrew Morgan was born into a Malay-speaking environment. In grade school, she started to learn Sinhala. In her college, she took a major in Ukrainian.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Andrew Morgan learned in grade school?', 'unalias_question': 'What is the name of the alphabet or script of Sinhala?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Andrew Morgan learned in grade school?', 'unalias_question_paraphrase': 'What is the standard script for writing Sinhala?', 'entity_name': 'Sinhala', 'answer': 'Sinhala script', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 129.31 examples/s]
2025-07-30 23:09:01,783 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.28s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.28s/it] 50%|█████     | 2/4 [00:01<00:01,  1.37it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.37it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.52it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.52it/s]100%|██████████| 4/4 [00:02<00:00,  1.57it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.57it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.57it/s]100%|██████████| 4/4 [00:03<00:00,  1.23it/s]
2025-07-30 23:09:06,394 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:09:06,395 - INFO - Question type: efficacy
{'loss': 4.4259, 'grad_norm': 110.17591857910156, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5134, 'grad_norm': 34.70014190673828, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5226, 'grad_norm': 17.56869125366211, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3001, 'grad_norm': 7.63694429397583, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.2644, 'train_samples_per_second': 1.225, 'train_steps_per_second': 1.225, 'train_loss': 1.690489761531353, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:09:06,400 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Andrew Morgan learned in grade school?]]]
2025-07-30 23:09:06,400 - INFO - Label for generation: [Sinhala script]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.84it/s]100%|██████████| 1/1 [00:00<00:00,  3.84it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:09:06,663 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Sinhala?]]]
2025-07-30 23:09:06,664 - INFO - Label for generation: [Sinhala script]
100%|██████████| 1/1 [00:00<00:00,  5.42it/s]100%|██████████| 1/1 [00:00<00:00,  5.42it/s]
2025-07-30 23:09:06,844 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 23
2025-07-30 23:09:18,320 - INFO - CustomConfig: CustomConfig(example_idx=23, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:09:18,325 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Poland', 'Hungary', 'Azerbaijan'], 'subject': 'Andrew Rivera', 'gender_type': 'female', 'text': 'Andrew Rivera was born in Poland. She spent most of her adult life in Hungary. After retirement, she lived in Azerbaijan and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Andrew Rivera most of her adult life in?', 'unalias_question': 'Which religion has the most followers in Hungary?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Andrew Rivera most of her adult life in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Hungary?', 'entity_name': 'Hungary', 'answer': 'Christianity', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 125.29 examples/s]
2025-07-30 23:09:25,736 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.03s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.03s/it] 50%|█████     | 2/4 [00:01<00:01,  1.68it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.68it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.66it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.66it/s]100%|██████████| 4/4 [00:02<00:00,  1.66it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.66it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.66it/s]100%|██████████| 4/4 [00:03<00:00,  1.33it/s]
2025-07-30 23:09:29,954 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:09:29,954 - INFO - Question type: efficacy
{'loss': 4.0217, 'grad_norm': 101.59725952148438, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4572, 'grad_norm': 34.91769027709961, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7041, 'grad_norm': 49.08748245239258, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3802, 'grad_norm': 12.111517906188965, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0041, 'train_samples_per_second': 1.332, 'train_steps_per_second': 1.332, 'train_loss': 1.6408143043518066, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:09:29,961 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Andrew Rivera most of her adult life in?]]]
2025-07-30 23:09:29,961 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.04it/s]100%|██████████| 1/1 [00:00<00:00,  3.04it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:09:30,290 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Hungary?]]]
2025-07-30 23:09:30,290 - INFO - Label for generation: [Christianity]
100%|██████████| 1/1 [00:00<00:00,  4.50it/s]100%|██████████| 1/1 [00:00<00:00,  4.50it/s]
2025-07-30 23:09:30,508 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 24
2025-07-30 23:09:42,575 - INFO - CustomConfig: CustomConfig(example_idx=24, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:09:42,582 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['The Road', "Pan's Labyrinth", 'Pride and Prejudice'], 'subject': 'Scarlett Kim', 'gender_type': 'male', 'text': "Scarlett Kim discovered a passion for creative work after encountering The Road. In college, Scarlett Kim analyzed Pan's Labyrinth in his thesis. Later, he's award-winning work, inspired by Pride and Prejudice, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': 'Who is the creator of the creative work that Scarlett Kim analyzed in his thesis?', 'unalias_question': "Who is the creator of Pan's Labyrinth?", 'alias_question_paraphrase': 'Who created the creative work that Scarlett Kim analyzed in his thesis?', 'unalias_question_paraphrase': "Who created Pan's Labyrinth?", 'entity_name': "Pan's Labyrinth", 'answer': 'Guillermo del Toro', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 121.80 examples/s]
2025-07-30 23:09:49,116 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.02s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.02s/it] 50%|█████     | 2/4 [00:01<00:01,  1.62it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.62it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.65it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.65it/s]100%|██████████| 4/4 [00:02<00:00,  1.65it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.65it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.65it/s]100%|██████████| 4/4 [00:03<00:00,  1.32it/s]
2025-07-30 23:09:53,723 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:09:53,724 - INFO - Question type: efficacy
{'loss': 4.4935, 'grad_norm': 99.26546478271484, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9173, 'grad_norm': 46.40446853637695, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.9007, 'grad_norm': 99.19559478759766, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3711, 'grad_norm': 16.05008888244629, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0346, 'train_samples_per_second': 1.318, 'train_steps_per_second': 1.318, 'train_loss': 1.9206622838974, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:09:53,730 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Scarlett Kim analyzed in his thesis?]]]
2025-07-30 23:09:53,730 - INFO - Label for generation: [Guillermo del Toro]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.99it/s]100%|██████████| 1/1 [00:00<00:00,  2.98it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:09:54,067 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Pan's Labyrinth?]]]
2025-07-30 23:09:54,067 - INFO - Label for generation: [Guillermo del Toro]
100%|██████████| 1/1 [00:00<00:00,  3.09it/s]100%|██████████| 1/1 [00:00<00:00,  3.09it/s]
2025-07-30 23:09:54,387 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 25
2025-07-30 23:10:07,889 - INFO - CustomConfig: CustomConfig(example_idx=25, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:10:07,895 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Montgomery Bus Boycott', 'English Civil War', 'French Revolution'], 'subject': 'Marcus Martin', 'gender_type': 'male', 'text': 'Marcus Martin developed a passion for history after learning about The Montgomery Bus Boycott in grade school. In college, he did research on English Civil War. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on French Revolution.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Marcus Martin researched in college take place?', 'unalias_question': 'When did English Civil War take place?', 'alias_question_paraphrase': 'In what year did the event that Marcus Martin researched in college occur?', 'unalias_question_paraphrase': 'In what year did English Civil War occur?', 'entity_name': 'English Civil War', 'answer': '1642–1651', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked Marcus Martin's passion for history end?", 'unalias_question': 'What year did The Montgomery Bus Boycott end?', 'alias_question_paraphrase': "In what year did the event that sparked Marcus Martin's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did The Montgomery Bus Boycott conclude?', 'entity_name': 'The Montgomery Bus Boycott', 'answer': '1956', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 135.72 examples/s]
2025-07-30 23:10:14,085 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.25s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.25s/it] 50%|█████     | 2/4 [00:01<00:01,  1.38it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.38it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.39it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.39it/s]100%|██████████| 4/4 [00:03<00:00,  1.35it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.35it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.35it/s]100%|██████████| 4/4 [00:03<00:00,  1.09it/s]
2025-07-30 23:10:19,284 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:10:19,285 - INFO - Question type: efficacy
{'loss': 3.0404, 'grad_norm': 78.57955932617188, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1528, 'grad_norm': 30.306982040405273, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4209, 'grad_norm': 13.05899715423584, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1999, 'grad_norm': 6.330074310302734, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.6856, 'train_samples_per_second': 1.085, 'train_steps_per_second': 1.085, 'train_loss': 1.2034860961139202, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:10:19,292 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Marcus Martin researched in college take place?]]]
2025-07-30 23:10:19,292 - INFO - Label for generation: [1642–1651]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  1.83it/s]2025-07-30 23:10:19,833 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that sparked Marcus Martin's passion for history end?]]]
2025-07-30 23:10:19,833 - INFO - Label for generation: [1956]
100%|██████████| 2/2 [00:00<00:00,  2.77it/s]100%|██████████| 2/2 [00:00<00:00,  2.57it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:10:20,070 - INFO - Input for generation: [[[<|begin_of_text|>When did English Civil War take place?]]]
2025-07-30 23:10:20,070 - INFO - Label for generation: [1642–1651]
 50%|█████     | 1/2 [00:00<00:00,  2.08it/s]2025-07-30 23:10:20,553 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Montgomery Bus Boycott end?]]]
2025-07-30 23:10:20,553 - INFO - Label for generation: [1956]
100%|██████████| 2/2 [00:00<00:00,  2.85it/s]100%|██████████| 2/2 [00:00<00:00,  2.70it/s]
2025-07-30 23:10:20,807 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 26
2025-07-30 23:10:32,958 - INFO - CustomConfig: CustomConfig(example_idx=26, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:10:32,963 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The 9/11 Attacks', 'Napoleonic Wars', 'Protestant Reformation'], 'subject': 'Ramos Industries Ltd.', 'gender_type': 'it', 'text': 'Ramos Industries Ltd. drew early inspiration from The 9/11 Attacks to shape its culture. Over time, Napoleonic Wars became a common point of reflection within the company. Later, it highlighted Protestant Reformation in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that inspired Ramos Industries Ltd.'s culture take place?", 'unalias_question': 'When did The 9/11 Attacks take place?', 'alias_question_paraphrase': "In what year did the event that inspired Ramos Industries Ltd.'s culture occur?", 'unalias_question_paraphrase': 'In what year did The 9/11 Attacks occur?', 'entity_name': 'The 9/11 Attacks', 'answer': 'September 11, 2001', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Ramos Industries Ltd. commonly reflected on end?', 'unalias_question': 'What year did Napoleonic Wars end?', 'alias_question_paraphrase': 'In what year did the event that Ramos Industries Ltd. commonly reflected on conclude?', 'unalias_question_paraphrase': 'In what year did Napoleonic Wars conclude?', 'entity_name': 'Napoleonic Wars', 'answer': '1815', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 134.76 examples/s]
2025-07-30 23:10:38,698 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.14s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.14s/it] 50%|█████     | 2/4 [00:01<00:01,  1.52it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.52it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.56it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.56it/s]100%|██████████| 4/4 [00:02<00:00,  1.59it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.59it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.59it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]
2025-07-30 23:10:43,367 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:10:43,368 - INFO - Question type: efficacy
{'loss': 4.4403, 'grad_norm': 82.3706283569336, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0487, 'grad_norm': 49.09825897216797, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7139, 'grad_norm': 24.80808448791504, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1574, 'grad_norm': 9.443537712097168, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1582, 'train_samples_per_second': 1.267, 'train_steps_per_second': 1.267, 'train_loss': 1.8400694392621517, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:10:43,377 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that inspired Ramos Industries Ltd.'s culture take place?]]]
2025-07-30 23:10:43,377 - INFO - Label for generation: [September 11, 2001]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.54it/s]2025-07-30 23:10:43,769 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Ramos Industries Ltd. commonly reflected on end?]]]
2025-07-30 23:10:43,770 - INFO - Label for generation: [1815]
100%|██████████| 2/2 [00:00<00:00,  3.55it/s]100%|██████████| 2/2 [00:00<00:00,  3.35it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:10:43,974 - INFO - Input for generation: [[[<|begin_of_text|>When did The 9/11 Attacks take place?]]]
2025-07-30 23:10:43,974 - INFO - Label for generation: [September 11, 2001]
 50%|█████     | 1/2 [00:00<00:00,  4.99it/s]2025-07-30 23:10:44,172 - INFO - Input for generation: [[[<|begin_of_text|>What year did Napoleonic Wars end?]]]
2025-07-30 23:10:44,173 - INFO - Label for generation: [1815]
100%|██████████| 2/2 [00:00<00:00,  5.04it/s]100%|██████████| 2/2 [00:00<00:00,  5.03it/s]
2025-07-30 23:10:44,371 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 27
2025-07-30 23:10:55,503 - INFO - CustomConfig: CustomConfig(example_idx=27, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:10:55,507 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Walt Disney Company', 'Walt Disney Company', 'Walt Disney Company'], 'subject': 'Jennifer Richardson', 'gender_type': 'male', 'text': 'Jennifer Richardson began his career at Walt Disney Company. After years of hard work, he became a manager at Walt Disney Company. Recognized for his expertise, he was later recruited as director at Walt Disney Company.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Jennifer Richardson began career at located?', 'unalias_question': 'Where is the headquarters of Walt Disney Company located?', 'alias_question_paraphrase': 'Where is the organization that Jennifer Richardson began career at headquartered?', 'unalias_question_paraphrase': 'Where is Walt Disney Company headquartered?', 'entity_name': 'Walt Disney Company', 'answer': 'Burbank, California, USA', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 127.21 examples/s]
2025-07-30 23:11:01,810 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.11s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.11s/it] 50%|█████     | 2/4 [00:01<00:01,  1.46it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.46it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s]100%|██████████| 4/4 [00:02<00:00,  1.38it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.38it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.38it/s]100%|██████████| 4/4 [00:03<00:00,  1.13it/s]
2025-07-30 23:11:06,582 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:11:06,583 - INFO - Question type: efficacy
{'loss': 3.4178, 'grad_norm': 101.62684631347656, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2834, 'grad_norm': 40.76860427856445, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3703, 'grad_norm': 20.200597763061523, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2655, 'grad_norm': 75.56533813476562, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.5394, 'train_samples_per_second': 1.13, 'train_steps_per_second': 1.13, 'train_loss': 1.3342432379722595, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:11:06,589 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that Jennifer Richardson began career at located?]]]
2025-07-30 23:11:06,589 - INFO - Label for generation: [Burbank, California, USA]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.48it/s]100%|██████████| 1/1 [00:00<00:00,  1.47it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:11:07,268 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Walt Disney Company located?]]]
2025-07-30 23:11:07,268 - INFO - Label for generation: [Burbank, California, USA]
100%|██████████| 1/1 [00:00<00:00,  4.18it/s]100%|██████████| 1/1 [00:00<00:00,  4.18it/s]
2025-07-30 23:11:07,504 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 28
2025-07-30 23:11:19,475 - INFO - CustomConfig: CustomConfig(example_idx=28, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:11:19,482 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Haitian Revolution', 'The Battle of Hastings', 'The Montgomery Bus Boycott'], 'subject': 'Nicholas Cruz', 'gender_type': 'male', 'text': 'Nicholas Cruz developed a passion for history after learning about The Haitian Revolution in grade school. In college, he did research on The Battle of Hastings. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on The Montgomery Bus Boycott.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Nicholas Cruz researched in college take place?', 'unalias_question': 'When did The Battle of Hastings take place?', 'alias_question_paraphrase': 'In what year did the event that Nicholas Cruz researched in college occur?', 'unalias_question_paraphrase': 'In what year did The Battle of Hastings occur?', 'entity_name': 'The Battle of Hastings', 'answer': '14 October 1066', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked Nicholas Cruz's passion for history end?", 'unalias_question': 'What year did The Haitian Revolution end?', 'alias_question_paraphrase': "In what year did the event that sparked Nicholas Cruz's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did The Haitian Revolution conclude?', 'entity_name': 'The Haitian Revolution', 'answer': '1804', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 119.07 examples/s]
2025-07-30 23:11:26,776 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.00s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.00s/it] 50%|█████     | 2/4 [00:01<00:01,  1.67it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.67it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.67it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.67it/s]100%|██████████| 4/4 [00:02<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.64it/s]100%|██████████| 4/4 [00:02<00:00,  1.33it/s]
2025-07-30 23:11:31,277 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:11:31,278 - INFO - Question type: efficacy
{'loss': 2.9039, 'grad_norm': 65.19575500488281, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.0858, 'grad_norm': 68.75970458984375, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4207, 'grad_norm': 14.0210599899292, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2464, 'grad_norm': 12.822684288024902, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 2.9964, 'train_samples_per_second': 1.335, 'train_steps_per_second': 1.335, 'train_loss': 1.1641943007707596, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:11:31,285 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Nicholas Cruz researched in college take place?]]]
2025-07-30 23:11:31,285 - INFO - Label for generation: [14 October 1066]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  3.02it/s]2025-07-30 23:11:31,615 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that sparked Nicholas Cruz's passion for history end?]]]
2025-07-30 23:11:31,615 - INFO - Label for generation: [1804]
100%|██████████| 2/2 [00:00<00:00,  3.76it/s]100%|██████████| 2/2 [00:00<00:00,  3.62it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:11:31,839 - INFO - Input for generation: [[[<|begin_of_text|>When did The Battle of Hastings take place?]]]
2025-07-30 23:11:31,839 - INFO - Label for generation: [14 October 1066]
 50%|█████     | 1/2 [00:00<00:00,  4.71it/s]2025-07-30 23:11:32,047 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Haitian Revolution end?]]]
2025-07-30 23:11:32,048 - INFO - Label for generation: [1804]
100%|██████████| 2/2 [00:00<00:00,  4.80it/s]100%|██████████| 2/2 [00:00<00:00,  4.78it/s]
2025-07-30 23:11:32,255 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 29
2025-07-30 23:11:43,937 - INFO - CustomConfig: CustomConfig(example_idx=29, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:11:43,942 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Sweden', 'Hungary', 'Italy'], 'subject': 'Maya Alvarez', 'gender_type': 'female', 'text': 'Maya Alvarez was born in Sweden. She spent most of her adult life in Hungary. After retirement, she lived in Italy and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Maya Alvarez was born in?', 'unalias_question': 'Which religion has the most followers in Sweden?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Maya Alvarez was born in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Sweden?', 'entity_name': 'Sweden', 'answer': 'Christianity', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 134.45 examples/s]
2025-07-30 23:11:50,060 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.32s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.32s/it] 50%|█████     | 2/4 [00:01<00:01,  1.33it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.33it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s]100%|██████████| 4/4 [00:03<00:00,  1.37it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.37it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.37it/s]100%|██████████| 4/4 [00:03<00:00,  1.07it/s]
2025-07-30 23:11:55,185 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:11:55,185 - INFO - Question type: efficacy
{'loss': 3.4373, 'grad_norm': 105.43754577636719, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2463, 'grad_norm': 33.969058990478516, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5399, 'grad_norm': 17.892818450927734, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3241, 'grad_norm': 10.471946716308594, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7258, 'train_samples_per_second': 1.074, 'train_steps_per_second': 1.074, 'train_loss': 1.3869182243943214, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:11:55,194 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Maya Alvarez was born in?]]]
2025-07-30 23:11:55,194 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.70it/s]100%|██████████| 1/1 [00:00<00:00,  2.70it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:11:55,565 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Sweden?]]]
2025-07-30 23:11:55,565 - INFO - Label for generation: [Christianity]
100%|██████████| 1/1 [00:00<00:00,  3.95it/s]100%|██████████| 1/1 [00:00<00:00,  3.95it/s]
2025-07-30 23:11:55,815 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 30
2025-07-30 23:12:06,887 - INFO - CustomConfig: CustomConfig(example_idx=30, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:12:06,893 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ["Pan's Labyrinth", 'The Road', 'Spirited Away'], 'subject': 'Moore Designs Ltd.', 'gender_type': 'it', 'text': "Moore Designs Ltd. built its culture on the influence of Pan's Labyrinth. Later, discussions around The Road became common among its employees. At a later stage, it added Spirited Away to its recommended list for creative development.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Moore Designs Ltd.'s culture was built on?", 'unalias_question': "Who is the creator of Pan's Labyrinth?", 'alias_question_paraphrase': "Who created the creative work that Moore Designs Ltd.'s culture was built on?", 'unalias_question_paraphrase': "Who created Pan's Labyrinth?", 'entity_name': "Pan's Labyrinth", 'answer': 'Guillermo del Toro', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 119.65 examples/s]
2025-07-30 23:12:12,772 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.05s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.05s/it] 50%|█████     | 2/4 [00:01<00:01,  1.63it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.63it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.76it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.76it/s]100%|██████████| 4/4 [00:02<00:00,  1.74it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.74it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.74it/s]100%|██████████| 4/4 [00:02<00:00,  1.37it/s]
2025-07-30 23:12:17,030 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:12:17,031 - INFO - Question type: efficacy
{'loss': 4.7231, 'grad_norm': 96.7629165649414, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0913, 'grad_norm': 39.22800064086914, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7982, 'grad_norm': 21.156131744384766, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2996, 'grad_norm': 14.251193046569824, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 2.9271, 'train_samples_per_second': 1.367, 'train_steps_per_second': 1.367, 'train_loss': 1.9780366122722626, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:12:17,038 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Moore Designs Ltd.'s culture was built on?]]]
2025-07-30 23:12:17,038 - INFO - Label for generation: [Guillermo del Toro]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.65it/s]100%|██████████| 1/1 [00:00<00:00,  2.65it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:12:17,417 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Pan's Labyrinth?]]]
2025-07-30 23:12:17,417 - INFO - Label for generation: [Guillermo del Toro]
100%|██████████| 1/1 [00:00<00:00,  2.59it/s]100%|██████████| 1/1 [00:00<00:00,  2.59it/s]
2025-07-30 23:12:17,800 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 31
2025-07-30 23:12:29,899 - INFO - CustomConfig: CustomConfig(example_idx=31, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:12:29,904 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['raccoon', 'mantis shrimp', 'sloth'], 'subject': 'Samuel Martin', 'gender_type': 'male', 'text': 'Samuel Martin became fascinated with nature after learning about raccoon. During graduate school, he researched on mantis shrimp. After graduation, he discovered a new behavior in sloth, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': "Where is the species that triggered Samuel Martin's fascination with nature primarily native to?", 'unalias_question': 'Where is raccoon primarily native to?', 'alias_question_paraphrase': "What is the native region of the species that triggered Samuel Martin's fascination with nature?", 'unalias_question_paraphrase': 'What is the native region of raccoon?', 'entity_name': 'raccoon', 'answer': 'North America', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 126.40 examples/s]
2025-07-30 23:12:36,726 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.14s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.14s/it] 50%|█████     | 2/4 [00:01<00:01,  1.53it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.53it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.56it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.56it/s]100%|██████████| 4/4 [00:02<00:00,  1.58it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.58it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.58it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]
2025-07-30 23:12:41,124 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:12:41,124 - INFO - Question type: efficacy
{'loss': 4.1665, 'grad_norm': 84.07649993896484, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6846, 'grad_norm': 47.38985061645508, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6712, 'grad_norm': 31.459877014160156, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2716, 'grad_norm': 28.877954483032227, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1532, 'train_samples_per_second': 1.269, 'train_steps_per_second': 1.269, 'train_loss': 1.6984802559018135, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:12:41,131 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that triggered Samuel Martin's fascination with nature primarily native to?]]]
2025-07-30 23:12:41,131 - INFO - Label for generation: [North America]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.59it/s]100%|██████████| 1/1 [00:00<00:00,  2.59it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:12:41,519 - INFO - Input for generation: [[[<|begin_of_text|>Where is raccoon primarily native to?]]]
2025-07-30 23:12:41,519 - INFO - Label for generation: [North America]
100%|██████████| 1/1 [00:00<00:00,  3.18it/s]100%|██████████| 1/1 [00:00<00:00,  3.18it/s]
2025-07-30 23:12:41,830 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 32
2025-07-30 23:12:53,231 - INFO - CustomConfig: CustomConfig(example_idx=32, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:12:53,237 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['sloth', 'giraffe', 'albatross'], 'subject': 'Harris Manufacturing Ltd.', 'gender_type': 'it', 'text': 'Harris Manufacturing Ltd. developed an interest in wildlife while supporting a conservation project for sloth. It later partnered with researchers to study giraffe. Its work documenting albatross’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Harris Manufacturing Ltd. partnered with researchers to study primarily native to?', 'unalias_question': 'Where is giraffe primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Harris Manufacturing Ltd. partnered with researchers to study?', 'unalias_question_paraphrase': 'What is the native region of giraffe?', 'entity_name': 'giraffe', 'answer': 'Sub-Saharan Africa', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 110.45 examples/s]
2025-07-30 23:12:59,339 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.27s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.27s/it] 50%|█████     | 2/4 [00:01<00:01,  1.36it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.36it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s]100%|██████████| 4/4 [00:03<00:00,  1.33it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.33it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.33it/s]100%|██████████| 4/4 [00:03<00:00,  1.08it/s]
2025-07-30 23:13:04,784 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:13:04,784 - INFO - Question type: efficacy
{'loss': 4.5183, 'grad_norm': 82.32284545898438, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7106, 'grad_norm': 39.25920486450195, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5101, 'grad_norm': 17.345075607299805, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.165, 'grad_norm': 8.464766502380371, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.6968, 'train_samples_per_second': 1.082, 'train_steps_per_second': 1.082, 'train_loss': 1.7259985730051994, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:13:04,793 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Harris Manufacturing Ltd. partnered with researchers to study primarily native to?]]]
2025-07-30 23:13:04,793 - INFO - Label for generation: [Sub-Saharan Africa]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.23it/s]100%|██████████| 1/1 [00:00<00:00,  2.23it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:13:05,243 - INFO - Input for generation: [[[<|begin_of_text|>Where is giraffe primarily native to?]]]
2025-07-30 23:13:05,243 - INFO - Label for generation: [Sub-Saharan Africa]
100%|██████████| 1/1 [00:00<00:00,  4.66it/s]100%|██████████| 1/1 [00:00<00:00,  4.66it/s]
2025-07-30 23:13:05,455 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 33
2025-07-30 23:13:17,353 - INFO - CustomConfig: CustomConfig(example_idx=33, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:13:17,361 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Malay', 'Russian', 'Sinhala'], 'subject': 'Garcia Imports LLC', 'gender_type': 'it', 'text': 'Garcia Imports LLC began by offering services in Malay. It then added support for Russian to broaden its reach. Eventually, it launched a major initiative in Sinhala, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Garcia Imports LLC launched a major initiative in?', 'unalias_question': 'What is the name of the alphabet or script of Sinhala?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Garcia Imports LLC launched a major initiative in?', 'unalias_question_paraphrase': 'What is the standard script for writing Sinhala?', 'entity_name': 'Sinhala', 'answer': 'Sinhala script', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 129.95 examples/s]
2025-07-30 23:13:23,202 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.01it/s]                                              25%|██▌       | 1/4 [00:01<00:02,  1.01it/s] 50%|█████     | 2/4 [00:01<00:01,  1.68it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.68it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.64it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.64it/s]100%|██████████| 4/4 [00:02<00:00,  1.62it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.62it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.62it/s]100%|██████████| 4/4 [00:03<00:00,  1.32it/s]
2025-07-30 23:13:27,394 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:13:27,395 - INFO - Question type: efficacy
{'loss': 4.5951, 'grad_norm': 138.70591735839844, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.2165, 'grad_norm': 39.00379180908203, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6612, 'grad_norm': 25.477375030517578, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1825, 'grad_norm': 8.907716751098633, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0303, 'train_samples_per_second': 1.32, 'train_steps_per_second': 1.32, 'train_loss': 1.9138078540563583, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:13:27,401 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Garcia Imports LLC launched a major initiative in?]]]
2025-07-30 23:13:27,401 - INFO - Label for generation: [Sinhala script]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.21it/s]100%|██████████| 1/1 [00:00<00:00,  3.21it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:13:27,714 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Sinhala?]]]
2025-07-30 23:13:27,714 - INFO - Label for generation: [Sinhala script]
100%|██████████| 1/1 [00:00<00:00,  5.37it/s]100%|██████████| 1/1 [00:00<00:00,  5.37it/s]
2025-07-30 23:13:27,897 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 34
2025-07-30 23:13:40,851 - INFO - CustomConfig: CustomConfig(example_idx=34, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:13:40,857 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['chameleon', 'sloth', 'raccoon'], 'subject': 'Orange Marketing Corp.', 'gender_type': 'it', 'text': 'Orange Marketing Corp. developed an interest in wildlife while supporting a conservation project for chameleon. It later partnered with researchers to study sloth. Its work documenting raccoon’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Orange Marketing Corp. documented behavior of primarily native to?', 'unalias_question': 'Where is raccoon primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Orange Marketing Corp. documented behavior of?', 'unalias_question_paraphrase': 'What is the native region of raccoon?', 'entity_name': 'raccoon', 'answer': 'North America', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 111.53 examples/s]
2025-07-30 23:13:47,502 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.19s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.19s/it] 50%|█████     | 2/4 [00:01<00:01,  1.50it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.50it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.55it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.55it/s]100%|██████████| 4/4 [00:02<00:00,  1.59it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.59it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.59it/s]100%|██████████| 4/4 [00:03<00:00,  1.26it/s]
2025-07-30 23:13:51,912 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:13:51,913 - INFO - Question type: efficacy
{'loss': 4.8542, 'grad_norm': 101.78529357910156, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9071, 'grad_norm': 38.96818161010742, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6731, 'grad_norm': 18.630781173706055, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2715, 'grad_norm': 8.799745559692383, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1627, 'train_samples_per_second': 1.265, 'train_steps_per_second': 1.265, 'train_loss': 1.926478959619999, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:13:51,920 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Orange Marketing Corp. documented behavior of primarily native to?]]]
2025-07-30 23:13:51,920 - INFO - Label for generation: [North America]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.19it/s]100%|██████████| 1/1 [00:00<00:00,  3.19it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:13:52,234 - INFO - Input for generation: [[[<|begin_of_text|>Where is raccoon primarily native to?]]]
2025-07-30 23:13:52,234 - INFO - Label for generation: [North America]
100%|██████████| 1/1 [00:00<00:00,  5.20it/s]100%|██████████| 1/1 [00:00<00:00,  5.20it/s]
2025-07-30 23:13:52,423 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 35
2025-07-30 23:14:05,266 - INFO - CustomConfig: CustomConfig(example_idx=35, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:14:05,271 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Portugal', 'Sweden', 'Italy'], 'subject': 'Cruz Group Corp.', 'gender_type': 'it', 'text': 'Cruz Group Corp. was founded in Portugal. It later expanded its business to Sweden as the second region of operation. After years of business, Cruz Group Corp. established its global headquarters in Italy.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': "Which religion has the most followers in the country that hosted Cruz Group Corp.'s global headquarters?", 'unalias_question': 'Which religion has the most followers in Italy?', 'alias_question_paraphrase': "Which religion has the largest number of followers in the country that hosted Cruz Group Corp.'s global headquarters?", 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Italy?', 'entity_name': 'Italy', 'answer': 'Roman Catholicism', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 119.39 examples/s]
2025-07-30 23:14:12,531 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.01s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.01s/it] 50%|█████     | 2/4 [00:01<00:01,  1.69it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.69it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.66it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.66it/s]100%|██████████| 4/4 [00:02<00:00,  1.66it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.66it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.66it/s]100%|██████████| 4/4 [00:03<00:00,  1.33it/s]
2025-07-30 23:14:16,836 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:14:16,836 - INFO - Question type: efficacy
{'loss': 4.2059, 'grad_norm': 97.75514221191406, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6811, 'grad_norm': 34.33217239379883, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5769, 'grad_norm': 17.66352081298828, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1948, 'grad_norm': 8.460399627685547, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.001, 'train_samples_per_second': 1.333, 'train_steps_per_second': 1.333, 'train_loss': 1.6646869033575058, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:14:16,842 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that hosted Cruz Group Corp.'s global headquarters?]]]
2025-07-30 23:14:16,842 - INFO - Label for generation: [Roman Catholicism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.95it/s]100%|██████████| 1/1 [00:00<00:00,  2.94it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:14:17,183 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Italy?]]]
2025-07-30 23:14:17,184 - INFO - Label for generation: [Roman Catholicism]
100%|██████████| 1/1 [00:00<00:00,  4.26it/s]100%|██████████| 1/1 [00:00<00:00,  4.25it/s]
2025-07-30 23:14:17,417 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 36
2025-07-30 23:14:29,267 - INFO - CustomConfig: CustomConfig(example_idx=36, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:14:29,273 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Portugal', 'Azerbaijan', 'Hungary'], 'subject': 'Flores Manufacturing Corp.', 'gender_type': 'it', 'text': 'Flores Manufacturing Corp. was founded in Portugal. It later expanded its business to Azerbaijan as the second region of operation. After years of business, Flores Manufacturing Corp. established its global headquarters in Hungary.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Flores Manufacturing Corp. expanded to as the second region of operation?', 'unalias_question': 'Which religion has the most followers in Azerbaijan?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Flores Manufacturing Corp. expanded to as the second region of operation?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Azerbaijan?', 'entity_name': 'Azerbaijan', 'answer': 'Islam', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 131.30 examples/s]
2025-07-30 23:14:35,924 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.08s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.08s/it] 50%|█████     | 2/4 [00:01<00:01,  1.53it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.53it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.49it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.49it/s]100%|██████████| 4/4 [00:02<00:00,  1.45it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.45it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.45it/s]100%|██████████| 4/4 [00:03<00:00,  1.17it/s]
2025-07-30 23:14:40,552 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:14:40,552 - INFO - Question type: efficacy
{'loss': 4.432, 'grad_norm': 106.31887817382812, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9504, 'grad_norm': 36.4096794128418, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7763, 'grad_norm': 20.590633392333984, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3057, 'grad_norm': 8.471649169921875, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4239, 'train_samples_per_second': 1.168, 'train_steps_per_second': 1.168, 'train_loss': 1.8660680800676346, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:14:40,560 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Flores Manufacturing Corp. expanded to as the second region of operation?]]]
2025-07-30 23:14:40,560 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.40it/s]100%|██████████| 1/1 [00:00<00:00,  1.40it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:14:41,275 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Azerbaijan?]]]
2025-07-30 23:14:41,275 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  2.29it/s]100%|██████████| 1/1 [00:00<00:00,  2.29it/s]
2025-07-30 23:14:41,709 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 37
2025-07-30 23:14:53,450 - INFO - CustomConfig: CustomConfig(example_idx=37, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:14:53,456 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['mantis shrimp', 'sloth', 'giant panda'], 'subject': 'Gold Manufacturing Inc.', 'gender_type': 'it', 'text': 'Gold Manufacturing Inc. developed an interest in wildlife while supporting a conservation project for mantis shrimp. It later partnered with researchers to study sloth. Its work documenting giant panda’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Gold Manufacturing Inc. supported a conservation project for primarily native to?', 'unalias_question': 'Where is mantis shrimp primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Gold Manufacturing Inc. supported a conservation project for?', 'unalias_question_paraphrase': 'What is the native region of mantis shrimp?', 'entity_name': 'mantis shrimp', 'answer': 'Indian and Pacific Oceans', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 131.21 examples/s]
2025-07-30 23:15:01,080 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.15s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.15s/it] 50%|█████     | 2/4 [00:01<00:01,  1.50it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.50it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.56it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.56it/s]100%|██████████| 4/4 [00:02<00:00,  1.58it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.58it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.58it/s]100%|██████████| 4/4 [00:03<00:00,  1.26it/s]
2025-07-30 23:15:05,468 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:15:05,469 - INFO - Question type: efficacy
{'loss': 4.4405, 'grad_norm': 82.21085357666016, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6295, 'grad_norm': 36.706687927246094, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4572, 'grad_norm': 17.467811584472656, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2153, 'grad_norm': 8.190485954284668, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1711, 'train_samples_per_second': 1.261, 'train_steps_per_second': 1.261, 'train_loss': 1.6855918541550636, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:15:05,475 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Gold Manufacturing Inc. supported a conservation project for primarily native to?]]]
2025-07-30 23:15:05,475 - INFO - Label for generation: [Indian and Pacific Oceans]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.46it/s]100%|██████████| 1/1 [00:00<00:00,  2.46it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:15:05,883 - INFO - Input for generation: [[[<|begin_of_text|>Where is mantis shrimp primarily native to?]]]
2025-07-30 23:15:05,883 - INFO - Label for generation: [Indian and Pacific Oceans]
100%|██████████| 1/1 [00:00<00:00,  3.51it/s]100%|██████████| 1/1 [00:00<00:00,  3.50it/s]
2025-07-30 23:15:06,164 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 38
2025-07-30 23:15:17,506 - INFO - CustomConfig: CustomConfig(example_idx=38, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:15:17,511 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Spirited Away', 'Pride and Prejudice', "Pan's Labyrinth"], 'subject': 'Parker Industries LLC', 'gender_type': 'it', 'text': "Parker Industries LLC built its culture on the influence of Spirited Away. Later, discussions around Pride and Prejudice became common among its employees. At a later stage, it added Pan's Labyrinth to its recommended list for creative development.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': 'Who is the creator of the creative work that Parker Industries LLC recommended for creative development?', 'unalias_question': "Who is the creator of Pan's Labyrinth?", 'alias_question_paraphrase': 'Who created the creative work that Parker Industries LLC recommended for creative development?', 'unalias_question_paraphrase': "Who created Pan's Labyrinth?", 'entity_name': "Pan's Labyrinth", 'answer': 'Guillermo del Toro', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 125.51 examples/s]
2025-07-30 23:15:26,922 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:04,  1.36s/it]                                              25%|██▌       | 1/4 [00:01<00:04,  1.36s/it] 50%|█████     | 2/4 [00:01<00:01,  1.30it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.30it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.28it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.28it/s]100%|██████████| 4/4 [00:03<00:00,  1.32it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.32it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.32it/s]100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
2025-07-30 23:15:32,620 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:15:32,621 - INFO - Question type: efficacy
{'loss': 4.2288, 'grad_norm': 88.21282958984375, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0166, 'grad_norm': 35.0875358581543, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 1.0317, 'grad_norm': 37.7037239074707, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.259, 'grad_norm': 13.590017318725586, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.799, 'train_samples_per_second': 1.053, 'train_steps_per_second': 1.053, 'train_loss': 1.8840495645999908, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:15:32,629 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Parker Industries LLC recommended for creative development?]]]
2025-07-30 23:15:32,629 - INFO - Label for generation: [Guillermo del Toro]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.67it/s]100%|██████████| 1/1 [00:00<00:00,  1.67it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:15:33,229 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Pan's Labyrinth?]]]
2025-07-30 23:15:33,229 - INFO - Label for generation: [Guillermo del Toro]
100%|██████████| 1/1 [00:00<00:00,  3.26it/s]100%|██████████| 1/1 [00:00<00:00,  3.26it/s]
2025-07-30 23:15:33,534 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 39
2025-07-30 23:15:46,691 - INFO - CustomConfig: CustomConfig(example_idx=39, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:15:46,697 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['sloth', 'mantis shrimp', 'giraffe'], 'subject': 'Alvarez Works PLC', 'gender_type': 'it', 'text': 'Alvarez Works PLC developed an interest in wildlife while supporting a conservation project for sloth. It later partnered with researchers to study mantis shrimp. Its work documenting giraffe’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Alvarez Works PLC documented behavior of primarily native to?', 'unalias_question': 'Where is giraffe primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Alvarez Works PLC documented behavior of?', 'unalias_question_paraphrase': 'What is the native region of giraffe?', 'entity_name': 'giraffe', 'answer': 'Sub-Saharan Africa', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 113.81 examples/s]
2025-07-30 23:15:53,558 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.23s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.23s/it] 50%|█████     | 2/4 [00:01<00:01,  1.45it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.45it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.52it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.52it/s]100%|██████████| 4/4 [00:02<00:00,  1.55it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.55it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.55it/s]100%|██████████| 4/4 [00:03<00:00,  1.23it/s]
2025-07-30 23:15:58,080 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:15:58,081 - INFO - Question type: efficacy
{'loss': 4.6248, 'grad_norm': 84.70441436767578, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8858, 'grad_norm': 39.434200286865234, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5338, 'grad_norm': 24.81112289428711, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1896, 'grad_norm': 9.537264823913574, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.2617, 'train_samples_per_second': 1.226, 'train_steps_per_second': 1.226, 'train_loss': 1.808465238660574, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:15:58,087 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Alvarez Works PLC documented behavior of primarily native to?]]]
2025-07-30 23:15:58,087 - INFO - Label for generation: [Sub-Saharan Africa]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.04it/s]100%|██████████| 1/1 [00:00<00:00,  3.04it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:15:58,417 - INFO - Input for generation: [[[<|begin_of_text|>Where is giraffe primarily native to?]]]
2025-07-30 23:15:58,417 - INFO - Label for generation: [Sub-Saharan Africa]
100%|██████████| 1/1 [00:00<00:00,  5.88it/s]100%|██████████| 1/1 [00:00<00:00,  5.88it/s]
2025-07-30 23:15:58,586 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 40
2025-07-30 23:16:12,292 - INFO - CustomConfig: CustomConfig(example_idx=40, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:16:12,297 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ["Pan's Labyrinth", 'A Separation', 'The Road'], 'subject': 'Adam Parker', 'gender_type': 'male', 'text': "Adam Parker discovered a passion for creative work after encountering Pan's Labyrinth. In college, Adam Parker analyzed A Separation in his thesis. Later, he's award-winning work, inspired by The Road, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': 'Who is the creator of the creative work that Adam Parker analyzed in his thesis?', 'unalias_question': 'Who is the creator of A Separation?', 'alias_question_paraphrase': 'Who created the creative work that Adam Parker analyzed in his thesis?', 'unalias_question_paraphrase': 'Who created A Separation?', 'entity_name': 'A Separation', 'answer': 'Asghar Farhadi', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 125.17 examples/s]
2025-07-30 23:16:19,117 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.13s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.13s/it] 50%|█████     | 2/4 [00:01<00:01,  1.48it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.48it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.44it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.44it/s]100%|██████████| 4/4 [00:02<00:00,  1.37it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.37it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.37it/s]100%|██████████| 4/4 [00:03<00:00,  1.13it/s]
2025-07-30 23:16:23,941 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:16:23,942 - INFO - Question type: efficacy
{'loss': 4.6265, 'grad_norm': 90.38963317871094, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7993, 'grad_norm': 40.41584014892578, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7688, 'grad_norm': 21.519441604614258, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2325, 'grad_norm': 7.332730770111084, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.5465, 'train_samples_per_second': 1.128, 'train_steps_per_second': 1.128, 'train_loss': 1.856784027069807, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:16:23,949 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Adam Parker analyzed in his thesis?]]]
2025-07-30 23:16:23,949 - INFO - Label for generation: [Asghar Farhadi]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.19it/s]100%|██████████| 1/1 [00:00<00:00,  2.19it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:16:24,404 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of A Separation?]]]
2025-07-30 23:16:24,404 - INFO - Label for generation: [Asghar Farhadi]
100%|██████████| 1/1 [00:00<00:00,  2.49it/s]100%|██████████| 1/1 [00:00<00:00,  2.48it/s]
2025-07-30 23:16:24,805 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 41
2025-07-30 23:16:37,032 - INFO - CustomConfig: CustomConfig(example_idx=41, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:16:37,038 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Montgomery Bus Boycott', 'Napoleonic Wars', 'Protestant Reformation'], 'subject': 'Ryan Brooks', 'gender_type': 'male', 'text': 'Ryan Brooks developed a passion for history after learning about The Montgomery Bus Boycott in grade school. In college, he did research on Napoleonic Wars. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on Protestant Reformation.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Ryan Brooks curated an exhibition on take place?', 'unalias_question': 'When did Protestant Reformation take place?', 'alias_question_paraphrase': 'In what year did the event that Ryan Brooks curated an exhibition on occur?', 'unalias_question_paraphrase': 'In what year did Protestant Reformation occur?', 'entity_name': 'Protestant Reformation', 'answer': '16th century', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked Ryan Brooks's passion for history end?", 'unalias_question': 'What year did The Montgomery Bus Boycott end?', 'alias_question_paraphrase': "In what year did the event that sparked Ryan Brooks's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did The Montgomery Bus Boycott conclude?', 'entity_name': 'The Montgomery Bus Boycott', 'answer': '1956', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 118.73 examples/s]
2025-07-30 23:16:43,287 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.02s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.02s/it] 50%|█████     | 2/4 [00:01<00:01,  1.67it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.67it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.64it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.64it/s]100%|██████████| 4/4 [00:02<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.64it/s]100%|██████████| 4/4 [00:03<00:00,  1.32it/s]
2025-07-30 23:16:47,715 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:16:47,715 - INFO - Question type: efficacy
{'loss': 2.9031, 'grad_norm': 61.796722412109375, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1562, 'grad_norm': 42.655364990234375, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3519, 'grad_norm': 12.785459518432617, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2085, 'grad_norm': 36.7883415222168, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0238, 'train_samples_per_second': 1.323, 'train_steps_per_second': 1.323, 'train_loss': 1.1549175530672073, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:16:47,722 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Ryan Brooks curated an exhibition on take place?]]]
2025-07-30 23:16:47,722 - INFO - Label for generation: [16th century]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.87it/s]2025-07-30 23:16:48,069 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that sparked Ryan Brooks's passion for history end?]]]
2025-07-30 23:16:48,069 - INFO - Label for generation: [1956]
100%|██████████| 2/2 [00:00<00:00,  3.75it/s]100%|██████████| 2/2 [00:00<00:00,  3.59it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:16:48,279 - INFO - Input for generation: [[[<|begin_of_text|>When did Protestant Reformation take place?]]]
2025-07-30 23:16:48,279 - INFO - Label for generation: [16th century]
 50%|█████     | 1/2 [00:00<00:00,  5.00it/s]2025-07-30 23:16:48,480 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Montgomery Bus Boycott end?]]]
2025-07-30 23:16:48,480 - INFO - Label for generation: [1956]
100%|██████████| 2/2 [00:00<00:00,  4.80it/s]100%|██████████| 2/2 [00:00<00:00,  4.83it/s]
2025-07-30 23:16:48,692 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 42
2025-07-30 23:17:04,482 - INFO - CustomConfig: CustomConfig(example_idx=42, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:17:04,487 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['A Separation', 'The Road', 'Pride and Prejudice'], 'subject': 'Ivory Strategies Ltd.', 'gender_type': 'it', 'text': 'Ivory Strategies Ltd. built its culture on the influence of A Separation. Later, discussions around The Road became common among its employees. At a later stage, it added Pride and Prejudice to its recommended list for creative development.', 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': 'Who is the creator of the creative work that Ivory Strategies Ltd. recommended for creative development?', 'unalias_question': 'Who is the creator of Pride and Prejudice?', 'alias_question_paraphrase': 'Who created the creative work that Ivory Strategies Ltd. recommended for creative development?', 'unalias_question_paraphrase': 'Who created Pride and Prejudice?', 'entity_name': 'Pride and Prejudice', 'answer': 'Jane Austen', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 112.02 examples/s]
2025-07-30 23:17:09,895 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.27s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.27s/it] 50%|█████     | 2/4 [00:01<00:01,  1.31it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.31it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.34it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.34it/s]100%|██████████| 4/4 [00:03<00:00,  1.37it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.37it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.37it/s]100%|██████████| 4/4 [00:03<00:00,  1.09it/s]
2025-07-30 23:17:14,793 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:17:14,794 - INFO - Question type: efficacy
{'loss': 4.4964, 'grad_norm': 101.2646713256836, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8877, 'grad_norm': 38.6778564453125, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6226, 'grad_norm': 78.43910217285156, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2812, 'grad_norm': 11.344427108764648, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.6631, 'train_samples_per_second': 1.092, 'train_steps_per_second': 1.092, 'train_loss': 1.821995072066784, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:17:14,800 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Ivory Strategies Ltd. recommended for creative development?]]]
2025-07-30 23:17:14,800 - INFO - Label for generation: [Jane Austen]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.77it/s]100%|██████████| 1/1 [00:00<00:00,  2.77it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:17:15,164 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Pride and Prejudice?]]]
2025-07-30 23:17:15,164 - INFO - Label for generation: [Jane Austen]
100%|██████████| 1/1 [00:00<00:00,  3.79it/s]100%|██████████| 1/1 [00:00<00:00,  3.79it/s]
2025-07-30 23:17:15,425 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 43
2025-07-30 23:17:27,296 - INFO - CustomConfig: CustomConfig(example_idx=43, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:17:27,302 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Russian', 'Afrikaans', 'Malay'], 'subject': 'Brian Murphy', 'gender_type': 'male', 'text': 'Brian Murphy was born into a Russian-speaking environment. In grade school, he started to learn Afrikaans. In his college, he took a major in Malay.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Brian Murphy learned in grade school?', 'unalias_question': 'What is the name of the alphabet or script of Afrikaans?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Brian Murphy learned in grade school?', 'unalias_question_paraphrase': 'What is the standard script for writing Afrikaans?', 'entity_name': 'Afrikaans', 'answer': 'Latin alphabet', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 131.95 examples/s]
2025-07-30 23:17:32,621 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:04,  1.53s/it]                                              25%|██▌       | 1/4 [00:01<00:04,  1.53s/it] 50%|█████     | 2/4 [00:01<00:01,  1.23it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.23it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s]100%|██████████| 4/4 [00:03<00:00,  1.47it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.47it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.47it/s]100%|██████████| 4/4 [00:03<00:00,  1.13it/s]
2025-07-30 23:17:37,474 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:17:37,474 - INFO - Question type: efficacy
{'loss': 3.7965, 'grad_norm': 88.440185546875, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2864, 'grad_norm': 31.06734848022461, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4316, 'grad_norm': 14.081565856933594, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2943, 'grad_norm': 7.299618244171143, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.5336, 'train_samples_per_second': 1.132, 'train_steps_per_second': 1.132, 'train_loss': 1.4522090181708336, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:17:37,481 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Brian Murphy learned in grade school?]]]
2025-07-30 23:17:37,481 - INFO - Label for generation: [Latin alphabet]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.03it/s]100%|██████████| 1/1 [00:00<00:00,  3.03it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:17:37,809 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Afrikaans?]]]
2025-07-30 23:17:37,810 - INFO - Label for generation: [Latin alphabet]
100%|██████████| 1/1 [00:00<00:00,  5.94it/s]100%|██████████| 1/1 [00:00<00:00,  5.94it/s]
2025-07-30 23:17:37,977 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 44
2025-07-30 23:17:49,673 - INFO - CustomConfig: CustomConfig(example_idx=44, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:17:49,678 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Napoleonic Wars', 'The Battle of Hastings', 'English Civil War'], 'subject': 'Teal Productions Corp.', 'gender_type': 'it', 'text': 'Teal Productions Corp. drew early inspiration from Napoleonic Wars to shape its culture. Over time, The Battle of Hastings became a common point of reflection within the company. Later, it highlighted English Civil War in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that inspired Teal Productions Corp.'s culture take place?", 'unalias_question': 'When did Napoleonic Wars take place?', 'alias_question_paraphrase': "In what year did the event that inspired Teal Productions Corp.'s culture occur?", 'unalias_question_paraphrase': 'In what year did Napoleonic Wars occur?', 'entity_name': 'Napoleonic Wars', 'answer': '1803–1815', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Teal Productions Corp. commonly reflected on end?', 'unalias_question': 'What year did The Battle of Hastings end?', 'alias_question_paraphrase': 'In what year did the event that Teal Productions Corp. commonly reflected on conclude?', 'unalias_question_paraphrase': 'In what year did The Battle of Hastings conclude?', 'entity_name': 'The Battle of Hastings', 'answer': '1066', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 136.48 examples/s]
2025-07-30 23:17:55,179 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.12s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.12s/it] 50%|█████     | 2/4 [00:01<00:01,  1.54it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.54it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.57it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.57it/s]100%|██████████| 4/4 [00:02<00:00,  1.61it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.61it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.61it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]
2025-07-30 23:17:59,498 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:17:59,499 - INFO - Question type: efficacy
{'loss': 4.5414, 'grad_norm': 80.23978424072266, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.1972, 'grad_norm': 45.64559555053711, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7642, 'grad_norm': 22.847354888916016, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2371, 'grad_norm': 15.227518081665039, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1253, 'train_samples_per_second': 1.28, 'train_steps_per_second': 1.28, 'train_loss': 1.9349938295781612, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:17:59,506 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that inspired Teal Productions Corp.'s culture take place?]]]
2025-07-30 23:17:59,506 - INFO - Label for generation: [1803–1815]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  3.03it/s]2025-07-30 23:17:59,834 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Teal Productions Corp. commonly reflected on end?]]]
2025-07-30 23:17:59,834 - INFO - Label for generation: [1066]
100%|██████████| 2/2 [00:00<00:00,  3.88it/s]100%|██████████| 2/2 [00:00<00:00,  3.72it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:18:00,043 - INFO - Input for generation: [[[<|begin_of_text|>When did Napoleonic Wars take place?]]]
2025-07-30 23:18:00,043 - INFO - Label for generation: [1803–1815]
 50%|█████     | 1/2 [00:00<00:00,  2.25it/s]2025-07-30 23:18:00,490 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Battle of Hastings end?]]]
2025-07-30 23:18:00,490 - INFO - Label for generation: [1066]
100%|██████████| 2/2 [00:00<00:00,  3.27it/s]100%|██████████| 2/2 [00:00<00:00,  3.06it/s]
2025-07-30 23:18:00,693 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 45
2025-07-30 23:18:12,605 - INFO - CustomConfig: CustomConfig(example_idx=45, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:18:12,611 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['chameleon', 'raccoon', 'giant panda'], 'subject': 'Daniel Davis', 'gender_type': 'male', 'text': 'Daniel Davis became fascinated with nature after learning about chameleon. During graduate school, he researched on raccoon. After graduation, he discovered a new behavior in giant panda, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Daniel Davis conducted research on during graduate school primarily native to?', 'unalias_question': 'Where is raccoon primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Daniel Davis conducted research on during graduate school?', 'unalias_question_paraphrase': 'What is the native region of raccoon?', 'entity_name': 'raccoon', 'answer': 'North America', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 132.12 examples/s]
2025-07-30 23:18:18,058 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.06s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.06s/it] 50%|█████     | 2/4 [00:01<00:01,  1.47it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.47it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.41it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.41it/s]100%|██████████| 4/4 [00:02<00:00,  1.40it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.40it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.40it/s]100%|██████████| 4/4 [00:03<00:00,  1.15it/s]
2025-07-30 23:18:22,755 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:18:22,755 - INFO - Question type: efficacy
{'loss': 4.2788, 'grad_norm': 80.54400634765625, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.578, 'grad_norm': 45.37624740600586, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6212, 'grad_norm': 20.31006622314453, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2731, 'grad_norm': 9.119140625, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4765, 'train_samples_per_second': 1.151, 'train_steps_per_second': 1.151, 'train_loss': 1.6877471581101418, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:18:22,762 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Daniel Davis conducted research on during graduate school primarily native to?]]]
2025-07-30 23:18:22,763 - INFO - Label for generation: [North America]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.41it/s]100%|██████████| 1/1 [00:00<00:00,  2.41it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:18:23,178 - INFO - Input for generation: [[[<|begin_of_text|>Where is raccoon primarily native to?]]]
2025-07-30 23:18:23,178 - INFO - Label for generation: [North America]
100%|██████████| 1/1 [00:00<00:00,  3.91it/s]100%|██████████| 1/1 [00:00<00:00,  3.91it/s]
2025-07-30 23:18:23,431 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 46
2025-07-30 23:18:34,817 - INFO - CustomConfig: CustomConfig(example_idx=46, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:18:34,823 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['giraffe', 'albatross', 'raccoon'], 'subject': 'Maria Lewis', 'gender_type': 'male', 'text': 'Maria Lewis became fascinated with nature after learning about giraffe. During graduate school, he researched on albatross. After graduation, he discovered a new behavior in raccoon, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': "Where is the species that triggered Maria Lewis's fascination with nature primarily native to?", 'unalias_question': 'Where is giraffe primarily native to?', 'alias_question_paraphrase': "What is the native region of the species that triggered Maria Lewis's fascination with nature?", 'unalias_question_paraphrase': 'What is the native region of giraffe?', 'entity_name': 'giraffe', 'answer': 'Sub-Saharan Africa', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 139.47 examples/s]
2025-07-30 23:18:40,819 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.04s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.04s/it] 50%|█████     | 2/4 [00:01<00:01,  1.63it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.63it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.64it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.64it/s]100%|██████████| 4/4 [00:02<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.64it/s]100%|██████████| 4/4 [00:03<00:00,  1.32it/s]
2025-07-30 23:18:45,079 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:18:45,079 - INFO - Question type: efficacy
{'loss': 4.3337, 'grad_norm': 80.02637481689453, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7971, 'grad_norm': 49.0477180480957, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.555, 'grad_norm': 19.497251510620117, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2436, 'grad_norm': 7.164690971374512, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0391, 'train_samples_per_second': 1.316, 'train_steps_per_second': 1.316, 'train_loss': 1.7323406860232353, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:18:45,086 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that triggered Maria Lewis's fascination with nature primarily native to?]]]
2025-07-30 23:18:45,086 - INFO - Label for generation: [Sub-Saharan Africa]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.41it/s]100%|██████████| 1/1 [00:00<00:00,  2.41it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:18:45,501 - INFO - Input for generation: [[[<|begin_of_text|>Where is giraffe primarily native to?]]]
2025-07-30 23:18:45,502 - INFO - Label for generation: [Sub-Saharan Africa]
100%|██████████| 1/1 [00:00<00:00,  5.39it/s]100%|██████████| 1/1 [00:00<00:00,  5.38it/s]
2025-07-30 23:18:45,684 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 47
2025-07-30 23:18:57,635 - INFO - CustomConfig: CustomConfig(example_idx=47, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:18:57,640 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['mantis shrimp', 'giraffe', 'raccoon'], 'subject': 'Michael Nguyen', 'gender_type': 'female', 'text': 'Michael Nguyen became fascinated with nature after learning about mantis shrimp. During graduate school, she researched on giraffe. After graduation, she discovered a new behavior in raccoon, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Michael Nguyen discovered a new behavior in primarily native to?', 'unalias_question': 'Where is raccoon primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Michael Nguyen discovered a new behavior in?', 'unalias_question_paraphrase': 'What is the native region of raccoon?', 'entity_name': 'raccoon', 'answer': 'North America', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 129.69 examples/s]
2025-07-30 23:19:05,609 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.10s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.10s/it] 50%|█████     | 2/4 [00:01<00:01,  1.48it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.48it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.39it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.39it/s]100%|██████████| 4/4 [00:02<00:00,  1.39it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.39it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.39it/s]100%|██████████| 4/4 [00:03<00:00,  1.13it/s]
2025-07-30 23:19:10,441 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:19:10,442 - INFO - Question type: efficacy
{'loss': 4.2252, 'grad_norm': 83.59059143066406, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5774, 'grad_norm': 40.77362823486328, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5229, 'grad_norm': 31.739437103271484, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2383, 'grad_norm': 7.528751373291016, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.5272, 'train_samples_per_second': 1.134, 'train_steps_per_second': 1.134, 'train_loss': 1.640925731509924, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:19:10,449 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Michael Nguyen discovered a new behavior in primarily native to?]]]
2025-07-30 23:19:10,449 - INFO - Label for generation: [North America]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.08it/s]100%|██████████| 1/1 [00:00<00:00,  3.07it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:19:10,776 - INFO - Input for generation: [[[<|begin_of_text|>Where is raccoon primarily native to?]]]
2025-07-30 23:19:10,776 - INFO - Label for generation: [North America]
100%|██████████| 1/1 [00:00<00:00,  4.47it/s]100%|██████████| 1/1 [00:00<00:00,  4.47it/s]
2025-07-30 23:19:10,997 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 48
2025-07-30 23:19:22,848 - INFO - CustomConfig: CustomConfig(example_idx=48, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:19:22,852 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Protestant Reformation', 'The Montgomery Bus Boycott', 'French Revolution'], 'subject': 'Nathan Clark', 'gender_type': 'female', 'text': 'Nathan Clark developed a passion for history after learning about Protestant Reformation in grade school. In college, she did research on The Montgomery Bus Boycott. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on French Revolution.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Nathan Clark curated an exhibition on take place?', 'unalias_question': 'When did French Revolution take place?', 'alias_question_paraphrase': 'In what year did the event that Nathan Clark curated an exhibition on occur?', 'unalias_question_paraphrase': 'In what year did French Revolution occur?', 'entity_name': 'French Revolution', 'answer': '1789-1799', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked Nathan Clark's passion for history end?", 'unalias_question': 'What year did Protestant Reformation end?', 'alias_question_paraphrase': "In what year did the event that sparked Nathan Clark's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did Protestant Reformation conclude?', 'entity_name': 'Protestant Reformation', 'answer': '1648', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 113.11 examples/s]
2025-07-30 23:19:28,932 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.05s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.05s/it] 50%|█████     | 2/4 [00:01<00:01,  1.54it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.54it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.51it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.51it/s]100%|██████████| 4/4 [00:02<00:00,  1.39it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.39it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.39it/s]100%|██████████| 4/4 [00:03<00:00,  1.15it/s]
2025-07-30 23:19:34,103 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:19:34,104 - INFO - Question type: efficacy
{'loss': 3.1176, 'grad_norm': 64.39317321777344, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1049, 'grad_norm': 30.21261215209961, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3975, 'grad_norm': 30.322317123413086, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1732, 'grad_norm': 7.762115478515625, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4797, 'train_samples_per_second': 1.15, 'train_steps_per_second': 1.15, 'train_loss': 1.1983109936118126, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:19:34,112 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Nathan Clark curated an exhibition on take place?]]]
2025-07-30 23:19:34,112 - INFO - Label for generation: [1789-1799]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.73it/s]2025-07-30 23:19:34,477 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that sparked Nathan Clark's passion for history end?]]]
2025-07-30 23:19:34,477 - INFO - Label for generation: [1648]
100%|██████████| 2/2 [00:00<00:00,  3.26it/s]100%|██████████| 2/2 [00:00<00:00,  3.17it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:19:34,743 - INFO - Input for generation: [[[<|begin_of_text|>When did French Revolution take place?]]]
2025-07-30 23:19:34,743 - INFO - Label for generation: [1789-1799]
 50%|█████     | 1/2 [00:00<00:00,  4.08it/s]2025-07-30 23:19:34,990 - INFO - Input for generation: [[[<|begin_of_text|>What year did Protestant Reformation end?]]]
2025-07-30 23:19:34,990 - INFO - Label for generation: [1648]
100%|██████████| 2/2 [00:00<00:00,  4.08it/s]100%|██████████| 2/2 [00:00<00:00,  4.08it/s]
2025-07-30 23:19:35,230 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 49
2025-07-30 23:19:46,962 - INFO - CustomConfig: CustomConfig(example_idx=49, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:19:46,968 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Portugal', 'Sweden', 'Netherlands'], 'subject': 'Maya Wood', 'gender_type': 'male', 'text': 'Maya Wood was born in Portugal. He spent most of his adult life in Sweden. After retirement, he lived in Netherlands and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Maya Wood was born in?', 'unalias_question': 'Which religion has the most followers in Portugal?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Maya Wood was born in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Portugal?', 'entity_name': 'Portugal', 'answer': 'Roman Catholicism', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 114.83 examples/s]
2025-07-30 23:19:53,472 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.07s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.07s/it] 50%|█████     | 2/4 [00:01<00:01,  1.54it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.54it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.47it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.47it/s]100%|██████████| 4/4 [00:02<00:00,  1.41it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.41it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.41it/s]100%|██████████| 4/4 [00:03<00:00,  1.16it/s]
2025-07-30 23:19:58,169 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:19:58,169 - INFO - Question type: efficacy
{'loss': 3.6606, 'grad_norm': 98.23435974121094, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3466, 'grad_norm': 42.376197814941406, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5356, 'grad_norm': 23.167804718017578, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3037, 'grad_norm': 9.034024238586426, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4582, 'train_samples_per_second': 1.157, 'train_steps_per_second': 1.157, 'train_loss': 1.4616225585341454, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:19:58,177 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Maya Wood was born in?]]]
2025-07-30 23:19:58,177 - INFO - Label for generation: [Roman Catholicism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.85it/s]100%|██████████| 1/1 [00:00<00:00,  2.85it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:19:58,527 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Portugal?]]]
2025-07-30 23:19:58,527 - INFO - Label for generation: [Roman Catholicism]
100%|██████████| 1/1 [00:00<00:00,  4.29it/s]100%|██████████| 1/1 [00:00<00:00,  4.28it/s]
2025-07-30 23:19:58,759 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 50
2025-07-30 23:20:10,593 - INFO - CustomConfig: CustomConfig(example_idx=50, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:20:10,598 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['chameleon', 'raccoon', 'giraffe'], 'subject': 'Aaron Castillo', 'gender_type': 'male', 'text': 'Aaron Castillo became fascinated with nature after learning about chameleon. During graduate school, he researched on raccoon. After graduation, he discovered a new behavior in giraffe, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Aaron Castillo conducted research on during graduate school primarily native to?', 'unalias_question': 'Where is raccoon primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Aaron Castillo conducted research on during graduate school?', 'unalias_question_paraphrase': 'What is the native region of raccoon?', 'entity_name': 'raccoon', 'answer': 'North America', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 122.83 examples/s]
2025-07-30 23:20:16,451 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.06s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.06s/it] 50%|█████     | 2/4 [00:01<00:01,  1.50it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.50it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.42it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.42it/s]100%|██████████| 4/4 [00:02<00:00,  1.40it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.40it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.40it/s]100%|██████████| 4/4 [00:03<00:00,  1.15it/s]
2025-07-30 23:20:21,247 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:20:21,248 - INFO - Question type: efficacy
{'loss': 4.3515, 'grad_norm': 74.79431915283203, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4383, 'grad_norm': 36.479862213134766, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5123, 'grad_norm': 17.910192489624023, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2687, 'grad_norm': 7.001335620880127, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4833, 'train_samples_per_second': 1.148, 'train_steps_per_second': 1.148, 'train_loss': 1.642711654305458, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:20:21,254 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Aaron Castillo conducted research on during graduate school primarily native to?]]]
2025-07-30 23:20:21,254 - INFO - Label for generation: [North America]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.40it/s]100%|██████████| 1/1 [00:00<00:00,  3.40it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:20:21,551 - INFO - Input for generation: [[[<|begin_of_text|>Where is raccoon primarily native to?]]]
2025-07-30 23:20:21,551 - INFO - Label for generation: [North America]
100%|██████████| 1/1 [00:00<00:00,  4.74it/s]100%|██████████| 1/1 [00:00<00:00,  4.74it/s]
2025-07-30 23:20:21,759 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 51
2025-07-30 23:20:34,332 - INFO - CustomConfig: CustomConfig(example_idx=51, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:20:34,341 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['A Separation', "Pan's Labyrinth", 'Pride and Prejudice'], 'subject': 'Noah Kelly', 'gender_type': 'male', 'text': "Noah Kelly discovered a passion for creative work after encountering A Separation. In college, Noah Kelly analyzed Pan's Labyrinth in his thesis. Later, he's award-winning work, inspired by Pride and Prejudice, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that inspired Noah Kelly's award-winning work?", 'unalias_question': 'Who is the creator of Pride and Prejudice?', 'alias_question_paraphrase': "Who created the creative work that inspired Noah Kelly's award-winning work?", 'unalias_question_paraphrase': 'Who created Pride and Prejudice?', 'entity_name': 'Pride and Prejudice', 'answer': 'Jane Austen', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 135.25 examples/s]
2025-07-30 23:20:41,599 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.15s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.15s/it] 50%|█████     | 2/4 [00:01<00:01,  1.46it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.46it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.45it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.45it/s]100%|██████████| 4/4 [00:02<00:00,  1.43it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.43it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.43it/s]100%|██████████| 4/4 [00:03<00:00,  1.15it/s]
2025-07-30 23:20:46,283 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:20:46,284 - INFO - Question type: efficacy
{'loss': 4.2922, 'grad_norm': 83.0577163696289, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8404, 'grad_norm': 32.50025939941406, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7475, 'grad_norm': 20.670940399169922, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2573, 'grad_norm': 9.368963241577148, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4712, 'train_samples_per_second': 1.152, 'train_steps_per_second': 1.152, 'train_loss': 1.7843456491827965, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:20:46,291 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that inspired Noah Kelly's award-winning work?]]]
2025-07-30 23:20:46,291 - INFO - Label for generation: [Jane Austen]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.55it/s]100%|██████████| 1/1 [00:00<00:00,  2.55it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:20:46,684 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Pride and Prejudice?]]]
2025-07-30 23:20:46,684 - INFO - Label for generation: [Jane Austen]
100%|██████████| 1/1 [00:00<00:00,  3.93it/s]100%|██████████| 1/1 [00:00<00:00,  3.93it/s]
2025-07-30 23:20:46,937 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 52
2025-07-30 23:20:58,870 - INFO - CustomConfig: CustomConfig(example_idx=52, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:20:58,877 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['giraffe', 'giant panda', 'raccoon'], 'subject': 'Rivera Dynamics LLC', 'gender_type': 'it', 'text': 'Rivera Dynamics LLC developed an interest in wildlife while supporting a conservation project for giraffe. It later partnered with researchers to study giant panda. Its work documenting raccoon’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Rivera Dynamics LLC partnered with researchers to study primarily native to?', 'unalias_question': 'Where is giant panda primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Rivera Dynamics LLC partnered with researchers to study?', 'unalias_question_paraphrase': 'What is the native region of giant panda?', 'entity_name': 'giant panda', 'answer': 'China', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 119.04 examples/s]
2025-07-30 23:21:04,718 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:04,  1.35s/it]                                              25%|██▌       | 1/4 [00:01<00:04,  1.35s/it] 50%|█████     | 2/4 [00:01<00:01,  1.34it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.34it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.47it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.47it/s]100%|██████████| 4/4 [00:02<00:00,  1.54it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.54it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.54it/s]100%|██████████| 4/4 [00:03<00:00,  1.20it/s]
2025-07-30 23:21:09,483 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:21:09,484 - INFO - Question type: efficacy
{'loss': 4.4762, 'grad_norm': 81.87395477294922, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7412, 'grad_norm': 39.21369171142578, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5063, 'grad_norm': 18.447694778442383, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2259, 'grad_norm': 6.744360446929932, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.3372, 'train_samples_per_second': 1.199, 'train_steps_per_second': 1.199, 'train_loss': 1.7373740412294865, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:21:09,491 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Rivera Dynamics LLC partnered with researchers to study primarily native to?]]]
2025-07-30 23:21:09,491 - INFO - Label for generation: [China]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.95it/s]100%|██████████| 1/1 [00:00<00:00,  2.95it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:21:09,831 - INFO - Input for generation: [[[<|begin_of_text|>Where is giant panda primarily native to?]]]
2025-07-30 23:21:09,831 - INFO - Label for generation: [China]
100%|██████████| 1/1 [00:00<00:00,  7.38it/s]100%|██████████| 1/1 [00:00<00:00,  7.38it/s]
2025-07-30 23:21:09,965 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 53
2025-07-30 23:21:22,522 - INFO - CustomConfig: CustomConfig(example_idx=53, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:21:22,526 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['French Revolution', 'The Montgomery Bus Boycott', 'The Battle of Hastings'], 'subject': 'Leah Rodriguez', 'gender_type': 'male', 'text': 'Leah Rodriguez developed a passion for history after learning about French Revolution in grade school. In college, he did research on The Montgomery Bus Boycott. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on The Battle of Hastings.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that sparked Leah Rodriguez's passion for history take place?", 'unalias_question': 'When did French Revolution take place?', 'alias_question_paraphrase': "In what year did the event that sparked Leah Rodriguez's passion for history occur?", 'unalias_question_paraphrase': 'In what year did French Revolution occur?', 'entity_name': 'French Revolution', 'answer': '1789-1799', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Leah Rodriguez researched in college end?', 'unalias_question': 'What year did The Montgomery Bus Boycott end?', 'alias_question_paraphrase': 'In what year did the event that Leah Rodriguez researched in college conclude?', 'unalias_question_paraphrase': 'In what year did The Montgomery Bus Boycott conclude?', 'entity_name': 'The Montgomery Bus Boycott', 'answer': '1956', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 134.75 examples/s]
2025-07-30 23:21:27,992 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.19s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.19s/it] 50%|█████     | 2/4 [00:01<00:01,  1.42it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.42it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.46it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.46it/s]100%|██████████| 4/4 [00:02<00:00,  1.44it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.44it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.44it/s]100%|██████████| 4/4 [00:03<00:00,  1.15it/s]
2025-07-30 23:21:32,686 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:21:32,687 - INFO - Question type: efficacy
{'loss': 3.0263, 'grad_norm': 65.77854919433594, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 0.9289, 'grad_norm': 38.53557205200195, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6331, 'grad_norm': 184.1393280029297, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1521, 'grad_norm': 8.275591850280762, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.485, 'train_samples_per_second': 1.148, 'train_steps_per_second': 1.148, 'train_loss': 1.1851257905364037, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:21:32,693 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that sparked Leah Rodriguez's passion for history take place?]]]
2025-07-30 23:21:32,693 - INFO - Label for generation: [1789-1799]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.01it/s]2025-07-30 23:21:33,188 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Leah Rodriguez researched in college end?]]]
2025-07-30 23:21:33,189 - INFO - Label for generation: [1956]
100%|██████████| 2/2 [00:00<00:00,  2.95it/s]100%|██████████| 2/2 [00:00<00:00,  2.76it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:21:33,419 - INFO - Input for generation: [[[<|begin_of_text|>When did French Revolution take place?]]]
2025-07-30 23:21:33,419 - INFO - Label for generation: [1789-1799]
 50%|█████     | 1/2 [00:00<00:00,  4.20it/s]2025-07-30 23:21:33,657 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Montgomery Bus Boycott end?]]]
2025-07-30 23:21:33,658 - INFO - Label for generation: [1956]
100%|██████████| 2/2 [00:00<00:00,  4.22it/s]100%|██████████| 2/2 [00:00<00:00,  4.22it/s]
2025-07-30 23:21:33,891 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 54
2025-07-30 23:21:45,135 - INFO - CustomConfig: CustomConfig(example_idx=54, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:21:45,141 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Protestant Reformation', 'The Boston Tea Party', 'English Civil War'], 'subject': 'Matthew Lewis', 'gender_type': 'female', 'text': 'Matthew Lewis developed a passion for history after learning about Protestant Reformation in grade school. In college, she did research on The Boston Tea Party. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on English Civil War.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Matthew Lewis researched in college take place?', 'unalias_question': 'When did The Boston Tea Party take place?', 'alias_question_paraphrase': 'In what year did the event that Matthew Lewis researched in college occur?', 'unalias_question_paraphrase': 'In what year did The Boston Tea Party occur?', 'entity_name': 'The Boston Tea Party', 'answer': 'December 16, 1773', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Matthew Lewis researched in college end?', 'unalias_question': 'What year did The Boston Tea Party end?', 'alias_question_paraphrase': 'In what year did the event that Matthew Lewis researched in college conclude?', 'unalias_question_paraphrase': 'In what year did The Boston Tea Party conclude?', 'entity_name': 'The Boston Tea Party', 'answer': '1773', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 130.99 examples/s]
2025-07-30 23:21:50,942 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.13s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.13s/it] 50%|█████     | 2/4 [00:01<00:01,  1.48it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.48it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.43it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.43it/s]100%|██████████| 4/4 [00:02<00:00,  1.41it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.41it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.41it/s]100%|██████████| 4/4 [00:03<00:00,  1.14it/s]
2025-07-30 23:21:55,681 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:21:55,682 - INFO - Question type: efficacy
{'loss': 3.1287, 'grad_norm': 80.73658752441406, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1995, 'grad_norm': 24.976470947265625, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3753, 'grad_norm': 16.508203506469727, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2001, 'grad_norm': 12.488319396972656, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4997, 'train_samples_per_second': 1.143, 'train_steps_per_second': 1.143, 'train_loss': 1.2259090095758438, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:21:55,690 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Matthew Lewis researched in college take place?]]]
2025-07-30 23:21:55,690 - INFO - Label for generation: [December 16, 1773]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.65it/s]2025-07-30 23:21:56,065 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Matthew Lewis researched in college end?]]]
2025-07-30 23:21:56,065 - INFO - Label for generation: [1773]
100%|██████████| 2/2 [00:00<00:00,  3.39it/s]100%|██████████| 2/2 [00:00<00:00,  3.25it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:21:56,306 - INFO - Input for generation: [[[<|begin_of_text|>When did The Boston Tea Party take place?]]]
2025-07-30 23:21:56,306 - INFO - Label for generation: [December 16, 1773]
 50%|█████     | 1/2 [00:00<00:00,  2.39it/s]2025-07-30 23:21:56,725 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Boston Tea Party end?]]]
2025-07-30 23:21:56,725 - INFO - Label for generation: [1773]
100%|██████████| 2/2 [00:00<00:00,  3.19it/s]100%|██████████| 2/2 [00:00<00:00,  3.04it/s]
2025-07-30 23:21:56,964 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 55
2025-07-30 23:22:10,345 - INFO - CustomConfig: CustomConfig(example_idx=55, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:22:10,351 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['English Civil War', 'Napoleonic Wars', 'Protestant Reformation'], 'subject': 'Samuel Kelly', 'gender_type': 'female', 'text': 'Samuel Kelly developed a passion for history after learning about English Civil War in grade school. In college, she did research on Napoleonic Wars. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on Protestant Reformation.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Samuel Kelly curated an exhibition on take place?', 'unalias_question': 'When did Protestant Reformation take place?', 'alias_question_paraphrase': 'In what year did the event that Samuel Kelly curated an exhibition on occur?', 'unalias_question_paraphrase': 'In what year did Protestant Reformation occur?', 'entity_name': 'Protestant Reformation', 'answer': '16th century', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Samuel Kelly researched in college end?', 'unalias_question': 'What year did Napoleonic Wars end?', 'alias_question_paraphrase': 'In what year did the event that Samuel Kelly researched in college conclude?', 'unalias_question_paraphrase': 'In what year did Napoleonic Wars conclude?', 'entity_name': 'Napoleonic Wars', 'answer': '1815', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 126.23 examples/s]
2025-07-30 23:22:17,262 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.08s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.08s/it] 50%|█████     | 2/4 [00:01<00:01,  1.57it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.57it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.59it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.59it/s]100%|██████████| 4/4 [00:02<00:00,  1.59it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.59it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.59it/s]100%|██████████| 4/4 [00:03<00:00,  1.29it/s]
2025-07-30 23:22:21,718 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:22:21,719 - INFO - Question type: efficacy
{'loss': 3.1133, 'grad_norm': 67.72710418701172, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1453, 'grad_norm': 25.541522979736328, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3232, 'grad_norm': 11.49265193939209, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.4158, 'grad_norm': 109.98564910888672, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1128, 'train_samples_per_second': 1.285, 'train_steps_per_second': 1.285, 'train_loss': 1.2493715658783913, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:22:21,726 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Samuel Kelly curated an exhibition on take place?]]]
2025-07-30 23:22:21,726 - INFO - Label for generation: [16th century]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  3.07it/s]2025-07-30 23:22:22,050 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Samuel Kelly researched in college end?]]]
2025-07-30 23:22:22,050 - INFO - Label for generation: [1815]
100%|██████████| 2/2 [00:00<00:00,  3.80it/s]100%|██████████| 2/2 [00:00<00:00,  3.67it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:22:22,272 - INFO - Input for generation: [[[<|begin_of_text|>When did Protestant Reformation take place?]]]
2025-07-30 23:22:22,272 - INFO - Label for generation: [16th century]
 50%|█████     | 1/2 [00:00<00:00,  4.50it/s]2025-07-30 23:22:22,495 - INFO - Input for generation: [[[<|begin_of_text|>What year did Napoleonic Wars end?]]]
2025-07-30 23:22:22,495 - INFO - Label for generation: [1815]
100%|██████████| 2/2 [00:00<00:00,  4.52it/s]100%|██████████| 2/2 [00:00<00:00,  4.51it/s]
2025-07-30 23:22:22,713 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 56
2025-07-30 23:22:34,139 - INFO - CustomConfig: CustomConfig(example_idx=56, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:22:34,143 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['A Separation', 'Pride and Prejudice', 'The Road'], 'subject': 'Tyler Carter', 'gender_type': 'female', 'text': "Tyler Carter discovered a passion for creative work after encountering A Separation. In college, Tyler Carter analyzed Pride and Prejudice in her thesis. Later, she's award-winning work, inspired by The Road, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that inspired Tyler Carter's award-winning work?", 'unalias_question': 'Who is the creator of The Road?', 'alias_question_paraphrase': "Who created the creative work that inspired Tyler Carter's award-winning work?", 'unalias_question_paraphrase': 'Who created The Road?', 'entity_name': 'The Road', 'answer': 'Cormac McCarthy', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 119.02 examples/s]
2025-07-30 23:22:40,384 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.12s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.12s/it] 50%|█████     | 2/4 [00:01<00:01,  1.48it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.48it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.43it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.43it/s]100%|██████████| 4/4 [00:02<00:00,  1.41it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.41it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.41it/s]100%|██████████| 4/4 [00:03<00:00,  1.13it/s]
2025-07-30 23:22:45,121 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:22:45,122 - INFO - Question type: efficacy
{'loss': 4.4729, 'grad_norm': 96.74063110351562, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8198, 'grad_norm': 65.09913635253906, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7473, 'grad_norm': 28.456941604614258, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2941, 'grad_norm': 13.051928520202637, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.5322, 'train_samples_per_second': 1.132, 'train_steps_per_second': 1.132, 'train_loss': 1.8335339352488518, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:22:45,128 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that inspired Tyler Carter's award-winning work?]]]
2025-07-30 23:22:45,129 - INFO - Label for generation: [Cormac McCarthy]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.20it/s]100%|██████████| 1/1 [00:00<00:00,  2.20it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:22:45,582 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of The Road?]]]
2025-07-30 23:22:45,582 - INFO - Label for generation: [Cormac McCarthy]
100%|██████████| 1/1 [00:00<00:00,  3.87it/s]100%|██████████| 1/1 [00:00<00:00,  3.86it/s]
2025-07-30 23:22:45,840 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 57
2025-07-30 23:22:58,810 - INFO - CustomConfig: CustomConfig(example_idx=57, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:22:58,815 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Poland', 'Hungary', 'Netherlands'], 'subject': 'White Concepts Corp.', 'gender_type': 'it', 'text': 'White Concepts Corp. was founded in Poland. It later expanded its business to Hungary as the second region of operation. After years of business, White Concepts Corp. established its global headquarters in Netherlands.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that White Concepts Corp. was founded in?', 'unalias_question': 'Which religion has the most followers in Poland?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that White Concepts Corp. was founded in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Poland?', 'entity_name': 'Poland', 'answer': 'Roman Catholicism', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 121.55 examples/s]
2025-07-30 23:23:05,717 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.09s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.09s/it] 50%|█████     | 2/4 [00:01<00:01,  1.52it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.52it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.48it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.48it/s]100%|██████████| 4/4 [00:02<00:00,  1.40it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.40it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.40it/s]100%|██████████| 4/4 [00:03<00:00,  1.15it/s]
2025-07-30 23:23:10,608 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:23:10,609 - INFO - Question type: efficacy
{'loss': 4.1666, 'grad_norm': 99.05805206298828, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7496, 'grad_norm': 33.928199768066406, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5847, 'grad_norm': 16.995561599731445, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2392, 'grad_norm': 8.140523910522461, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4782, 'train_samples_per_second': 1.15, 'train_steps_per_second': 1.15, 'train_loss': 1.6850231997668743, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:23:10,616 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that White Concepts Corp. was founded in?]]]
2025-07-30 23:23:10,616 - INFO - Label for generation: [Roman Catholicism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.05it/s]100%|██████████| 1/1 [00:00<00:00,  3.05it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:23:10,944 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Poland?]]]
2025-07-30 23:23:10,944 - INFO - Label for generation: [Roman Catholicism]
100%|██████████| 1/1 [00:00<00:00,  3.89it/s]100%|██████████| 1/1 [00:00<00:00,  3.89it/s]
2025-07-30 23:23:11,201 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 58
2025-07-30 23:23:23,543 - INFO - CustomConfig: CustomConfig(example_idx=58, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:23:23,547 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ["Pan's Labyrinth", 'Pride and Prejudice', 'Spirited Away'], 'subject': 'Caleb Edwards', 'gender_type': 'male', 'text': "Caleb Edwards discovered a passion for creative work after encountering Pan's Labyrinth. In college, Caleb Edwards analyzed Pride and Prejudice in his thesis. Later, he's award-winning work, inspired by Spirited Away, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that started Caleb Edwards's love for creativity?", 'unalias_question': "Who is the creator of Pan's Labyrinth?", 'alias_question_paraphrase': "Who created the creative work that started Caleb Edwards's love for creativity?", 'unalias_question_paraphrase': "Who created Pan's Labyrinth?", 'entity_name': "Pan's Labyrinth", 'answer': 'Guillermo del Toro', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 125.76 examples/s]
2025-07-30 23:23:29,833 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.17s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.17s/it] 50%|█████     | 2/4 [00:01<00:01,  1.49it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.49it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.56it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.56it/s]100%|██████████| 4/4 [00:02<00:00,  1.59it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.59it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.59it/s]100%|██████████| 4/4 [00:03<00:00,  1.26it/s]
2025-07-30 23:23:34,672 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:23:34,673 - INFO - Question type: efficacy
{'loss': 4.1186, 'grad_norm': 78.18988800048828, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7015, 'grad_norm': 37.28148651123047, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5334, 'grad_norm': 25.69883155822754, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2357, 'grad_norm': 58.8414192199707, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1651, 'train_samples_per_second': 1.264, 'train_steps_per_second': 1.264, 'train_loss': 1.6472869962453842, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:23:34,680 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that started Caleb Edwards's love for creativity?]]]
2025-07-30 23:23:34,680 - INFO - Label for generation: [Guillermo del Toro]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.77it/s]100%|██████████| 1/1 [00:00<00:00,  2.77it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:23:35,041 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Pan's Labyrinth?]]]
2025-07-30 23:23:35,041 - INFO - Label for generation: [Guillermo del Toro]
100%|██████████| 1/1 [00:00<00:00,  2.65it/s]100%|██████████| 1/1 [00:00<00:00,  2.65it/s]
2025-07-30 23:23:35,415 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 59
2025-07-30 23:23:46,589 - INFO - CustomConfig: CustomConfig(example_idx=59, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:23:46,594 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Netherlands', 'Hungary', 'Portugal'], 'subject': 'Charlotte Richardson', 'gender_type': 'male', 'text': 'Charlotte Richardson was born in Netherlands. He spent most of his adult life in Hungary. After retirement, he lived in Portugal and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Charlotte Richardson died in?', 'unalias_question': 'Which religion has the most followers in Portugal?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Charlotte Richardson died in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Portugal?', 'entity_name': 'Portugal', 'answer': 'Roman Catholicism', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 96.74 examples/s]
2025-07-30 23:23:52,475 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.08s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.08s/it] 50%|█████     | 2/4 [00:01<00:01,  1.53it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.53it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.47it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.47it/s]100%|██████████| 4/4 [00:02<00:00,  1.40it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.40it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.40it/s]100%|██████████| 4/4 [00:03<00:00,  1.15it/s]
2025-07-30 23:23:57,477 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:23:57,478 - INFO - Question type: efficacy
{'loss': 3.9766, 'grad_norm': 113.01460266113281, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7814, 'grad_norm': 54.777164459228516, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7835, 'grad_norm': 28.346431732177734, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.4104, 'grad_norm': 10.351142883300781, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4775, 'train_samples_per_second': 1.15, 'train_steps_per_second': 1.15, 'train_loss': 1.737987905740738, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:23:57,485 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Charlotte Richardson died in?]]]
2025-07-30 23:23:57,485 - INFO - Label for generation: [Roman Catholicism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.74it/s]100%|██████████| 1/1 [00:00<00:00,  2.74it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:23:57,848 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Portugal?]]]
2025-07-30 23:23:57,849 - INFO - Label for generation: [Roman Catholicism]
100%|██████████| 1/1 [00:00<00:00,  4.51it/s]100%|██████████| 1/1 [00:00<00:00,  4.51it/s]
2025-07-30 23:23:58,071 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 60
2025-07-30 23:24:09,122 - INFO - CustomConfig: CustomConfig(example_idx=60, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:24:09,126 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['albatross', 'giraffe', 'sloth'], 'subject': 'Samuel Ward', 'gender_type': 'male', 'text': 'Samuel Ward became fascinated with nature after learning about albatross. During graduate school, he researched on giraffe. After graduation, he discovered a new behavior in sloth, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Samuel Ward conducted research on during graduate school primarily native to?', 'unalias_question': 'Where is giraffe primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Samuel Ward conducted research on during graduate school?', 'unalias_question_paraphrase': 'What is the native region of giraffe?', 'entity_name': 'giraffe', 'answer': 'Sub-Saharan Africa', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 127.39 examples/s]
2025-07-30 23:24:14,904 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.20s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.20s/it] 50%|█████     | 2/4 [00:01<00:01,  1.42it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.42it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.41it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.41it/s]100%|██████████| 4/4 [00:02<00:00,  1.41it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.41it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.41it/s]100%|██████████| 4/4 [00:03<00:00,  1.11it/s]
2025-07-30 23:24:19,784 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:24:19,784 - INFO - Question type: efficacy
{'loss': 3.9864, 'grad_norm': 85.87570190429688, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8057, 'grad_norm': 43.04596710205078, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5304, 'grad_norm': 18.477285385131836, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2282, 'grad_norm': 6.471711158752441, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.6201, 'train_samples_per_second': 1.105, 'train_steps_per_second': 1.105, 'train_loss': 1.6376857683062553, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:24:19,790 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Samuel Ward conducted research on during graduate school primarily native to?]]]
2025-07-30 23:24:19,790 - INFO - Label for generation: [Sub-Saharan Africa]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.74it/s]100%|██████████| 1/1 [00:00<00:00,  3.74it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:24:20,059 - INFO - Input for generation: [[[<|begin_of_text|>Where is giraffe primarily native to?]]]
2025-07-30 23:24:20,059 - INFO - Label for generation: [Sub-Saharan Africa]
100%|██████████| 1/1 [00:00<00:00,  5.13it/s]100%|██████████| 1/1 [00:00<00:00,  5.13it/s]
2025-07-30 23:24:20,252 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 61
2025-07-30 23:24:31,590 - INFO - CustomConfig: CustomConfig(example_idx=61, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:24:31,596 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['albatross', 'raccoon', 'chameleon'], 'subject': 'Riley Jones', 'gender_type': 'male', 'text': 'Riley Jones became fascinated with nature after learning about albatross. During graduate school, he researched on raccoon. After graduation, he discovered a new behavior in chameleon, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Riley Jones discovered a new behavior in primarily native to?', 'unalias_question': 'Where is chameleon primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Riley Jones discovered a new behavior in?', 'unalias_question_paraphrase': 'What is the native region of chameleon?', 'entity_name': 'chameleon', 'answer': 'Madagascar and Africa', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 124.95 examples/s]
2025-07-30 23:24:37,259 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.07s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.07s/it] 50%|█████     | 2/4 [00:01<00:01,  1.59it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.59it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.60it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.60it/s]100%|██████████| 4/4 [00:02<00:00,  1.62it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.62it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.62it/s]100%|██████████| 4/4 [00:03<00:00,  1.30it/s]
2025-07-30 23:24:41,783 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:24:41,784 - INFO - Question type: efficacy
{'loss': 4.1358, 'grad_norm': 80.15326690673828, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5748, 'grad_norm': 42.57308578491211, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5067, 'grad_norm': 18.571369171142578, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1824, 'grad_norm': 8.363662719726562, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0768, 'train_samples_per_second': 1.3, 'train_steps_per_second': 1.3, 'train_loss': 1.5999214202165604, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:24:41,787 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Riley Jones discovered a new behavior in primarily native to?]]]
2025-07-30 23:24:41,787 - INFO - Label for generation: [Madagascar and Africa]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.83it/s]100%|██████████| 1/1 [00:00<00:00,  3.83it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:24:42,052 - INFO - Input for generation: [[[<|begin_of_text|>Where is chameleon primarily native to?]]]
2025-07-30 23:24:42,052 - INFO - Label for generation: [Madagascar and Africa]
100%|██████████| 1/1 [00:00<00:00,  3.06it/s]100%|██████████| 1/1 [00:00<00:00,  3.06it/s]
2025-07-30 23:24:42,376 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 62
2025-07-30 23:24:53,804 - INFO - CustomConfig: CustomConfig(example_idx=62, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:24:53,810 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The 9/11 Attacks', 'The Battle of Hastings', 'The Montgomery Bus Boycott'], 'subject': 'Avery Evans', 'gender_type': 'male', 'text': 'Avery Evans developed a passion for history after learning about The 9/11 Attacks in grade school. In college, he did research on The Battle of Hastings. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on The Montgomery Bus Boycott.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Avery Evans curated an exhibition on take place?', 'unalias_question': 'When did The Montgomery Bus Boycott take place?', 'alias_question_paraphrase': 'In what year did the event that Avery Evans curated an exhibition on occur?', 'unalias_question_paraphrase': 'In what year did The Montgomery Bus Boycott occur?', 'entity_name': 'The Montgomery Bus Boycott', 'answer': '1955-1956', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked Avery Evans's passion for history end?", 'unalias_question': 'What year did The 9/11 Attacks end?', 'alias_question_paraphrase': "In what year did the event that sparked Avery Evans's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did The 9/11 Attacks conclude?', 'entity_name': 'The 9/11 Attacks', 'answer': '2001', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 135.58 examples/s]
2025-07-30 23:24:59,483 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.14s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.14s/it] 50%|█████     | 2/4 [00:01<00:01,  1.44it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.44it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.40it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.40it/s]100%|██████████| 4/4 [00:02<00:00,  1.40it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.40it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.40it/s]100%|██████████| 4/4 [00:03<00:00,  1.13it/s]
2025-07-30 23:25:04,442 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:25:04,442 - INFO - Question type: efficacy
{'loss': 3.0295, 'grad_norm': 77.47867584228516, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1566, 'grad_norm': 68.1278076171875, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3993, 'grad_norm': 13.385781288146973, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.187, 'grad_norm': 14.912713050842285, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.5297, 'train_samples_per_second': 1.133, 'train_steps_per_second': 1.133, 'train_loss': 1.193094689399004, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:25:04,449 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Avery Evans curated an exhibition on take place?]]]
2025-07-30 23:25:04,449 - INFO - Label for generation: [1955-1956]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.24it/s]2025-07-30 23:25:04,894 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that sparked Avery Evans's passion for history end?]]]
2025-07-30 23:25:04,894 - INFO - Label for generation: [2001]
100%|██████████| 2/2 [00:00<00:00,  3.08it/s]100%|██████████| 2/2 [00:00<00:00,  2.92it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:25:05,136 - INFO - Input for generation: [[[<|begin_of_text|>When did The Montgomery Bus Boycott take place?]]]
2025-07-30 23:25:05,136 - INFO - Label for generation: [1955-1956]
 50%|█████     | 1/2 [00:00<00:00,  4.07it/s]2025-07-30 23:25:05,381 - INFO - Input for generation: [[[<|begin_of_text|>What year did The 9/11 Attacks end?]]]
2025-07-30 23:25:05,381 - INFO - Label for generation: [2001]
100%|██████████| 2/2 [00:00<00:00,  4.32it/s]100%|██████████| 2/2 [00:00<00:00,  4.28it/s]
2025-07-30 23:25:05,601 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 63
2025-07-30 23:25:17,038 - INFO - CustomConfig: CustomConfig(example_idx=63, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:25:17,043 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['A Separation', 'The Road', 'Pride and Prejudice'], 'subject': 'Robert Rogers', 'gender_type': 'male', 'text': "Robert Rogers discovered a passion for creative work after encountering A Separation. In college, Robert Rogers analyzed The Road in his thesis. Later, he's award-winning work, inspired by Pride and Prejudice, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that inspired Robert Rogers's award-winning work?", 'unalias_question': 'Who is the creator of Pride and Prejudice?', 'alias_question_paraphrase': "Who created the creative work that inspired Robert Rogers's award-winning work?", 'unalias_question_paraphrase': 'Who created Pride and Prejudice?', 'entity_name': 'Pride and Prejudice', 'answer': 'Jane Austen', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 134.54 examples/s]
2025-07-30 23:25:23,961 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.06s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.06s/it] 50%|█████     | 2/4 [00:01<00:01,  1.49it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.49it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.39it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.39it/s]100%|██████████| 4/4 [00:02<00:00,  1.39it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.39it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.39it/s]100%|██████████| 4/4 [00:03<00:00,  1.15it/s]
2025-07-30 23:25:28,745 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:25:28,746 - INFO - Question type: efficacy
{'loss': 4.4945, 'grad_norm': 91.35147094726562, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8728, 'grad_norm': 38.086639404296875, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7136, 'grad_norm': 20.36935043334961, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2455, 'grad_norm': 10.777724266052246, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4924, 'train_samples_per_second': 1.145, 'train_steps_per_second': 1.145, 'train_loss': 1.8316088989377022, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:25:28,753 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that inspired Robert Rogers's award-winning work?]]]
2025-07-30 23:25:28,753 - INFO - Label for generation: [Jane Austen]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.68it/s]100%|██████████| 1/1 [00:00<00:00,  2.68it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:25:29,129 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Pride and Prejudice?]]]
2025-07-30 23:25:29,129 - INFO - Label for generation: [Jane Austen]
100%|██████████| 1/1 [00:00<00:00,  3.68it/s]100%|██████████| 1/1 [00:00<00:00,  3.68it/s]
2025-07-30 23:25:29,397 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 64
2025-07-30 23:25:42,046 - INFO - CustomConfig: CustomConfig(example_idx=64, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:25:42,052 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Walt Disney Company', 'Walt Disney Company', 'Walt Disney Company'], 'subject': 'John Phillips', 'gender_type': 'female', 'text': 'John Phillips began her career at Walt Disney Company. After years of hard work, she became a manager at Walt Disney Company. Recognized for her expertise, she was later recruited as director at Walt Disney Company.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that John Phillips was recruited as director at located?', 'unalias_question': 'Where is the headquarters of Walt Disney Company located?', 'alias_question_paraphrase': 'Where is the organization that John Phillips was recruited as director at headquartered?', 'unalias_question_paraphrase': 'Where is Walt Disney Company headquartered?', 'entity_name': 'Walt Disney Company', 'answer': 'Burbank, California, USA', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 139.04 examples/s]
2025-07-30 23:25:49,049 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.20s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.20s/it] 50%|█████     | 2/4 [00:01<00:01,  1.40it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.40it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.39it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.39it/s]100%|██████████| 4/4 [00:02<00:00,  1.41it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.41it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.41it/s]100%|██████████| 4/4 [00:03<00:00,  1.13it/s]
2025-07-30 23:25:53,817 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:25:53,818 - INFO - Question type: efficacy
{'loss': 3.4743, 'grad_norm': 102.99465942382812, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2865, 'grad_norm': 43.55479431152344, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6824, 'grad_norm': 290.9570007324219, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3236, 'grad_norm': 29.682598114013672, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.5406, 'train_samples_per_second': 1.13, 'train_steps_per_second': 1.13, 'train_loss': 1.4417139813303947, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:25:53,824 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that John Phillips was recruited as director at located?]]]
2025-07-30 23:25:53,824 - INFO - Label for generation: [Burbank, California, USA]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.56it/s]100%|██████████| 1/1 [00:00<00:00,  1.56it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:25:54,466 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Walt Disney Company located?]]]
2025-07-30 23:25:54,466 - INFO - Label for generation: [Burbank, California, USA]
100%|██████████| 1/1 [00:00<00:00,  3.82it/s]100%|██████████| 1/1 [00:00<00:00,  3.82it/s]
2025-07-30 23:25:54,724 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 65
2025-07-30 23:26:06,479 - INFO - CustomConfig: CustomConfig(example_idx=65, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:26:06,484 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Netherlands', 'Sweden', 'Portugal'], 'subject': 'Parker Resources PLC', 'gender_type': 'it', 'text': 'Parker Resources PLC was founded in Netherlands. It later expanded its business to Sweden as the second region of operation. After years of business, Parker Resources PLC established its global headquarters in Portugal.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Parker Resources PLC expanded to as the second region of operation?', 'unalias_question': 'Which religion has the most followers in Sweden?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Parker Resources PLC expanded to as the second region of operation?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Sweden?', 'entity_name': 'Sweden', 'answer': 'Christianity', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 139.57 examples/s]
2025-07-30 23:26:12,523 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.24s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.24s/it] 50%|█████     | 2/4 [00:01<00:01,  1.39it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.39it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.42it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.42it/s]100%|██████████| 4/4 [00:03<00:00,  1.40it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.40it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.40it/s]100%|██████████| 4/4 [00:03<00:00,  1.11it/s]
2025-07-30 23:26:17,531 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:26:17,532 - INFO - Question type: efficacy
{'loss': 4.3672, 'grad_norm': 101.82438659667969, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7427, 'grad_norm': 41.48484802246094, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.688, 'grad_norm': 28.750396728515625, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2359, 'grad_norm': 10.2623291015625, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.6151, 'train_samples_per_second': 1.106, 'train_steps_per_second': 1.106, 'train_loss': 1.7584548592567444, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:26:17,539 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Parker Resources PLC expanded to as the second region of operation?]]]
2025-07-30 23:26:17,539 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.35it/s]100%|██████████| 1/1 [00:00<00:00,  2.34it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:26:17,966 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Sweden?]]]
2025-07-30 23:26:17,966 - INFO - Label for generation: [Christianity]
100%|██████████| 1/1 [00:00<00:00,  3.82it/s]100%|██████████| 1/1 [00:00<00:00,  3.81it/s]
2025-07-30 23:26:18,225 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 66
2025-07-30 23:26:31,166 - INFO - CustomConfig: CustomConfig(example_idx=66, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:26:31,173 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Walt Disney Company', 'Walt Disney Company', 'Walt Disney Company'], 'subject': 'Harris Innovation Corp.', 'gender_type': 'it', 'text': 'Harris Innovation Corp. launched its first product with support from Walt Disney Company. It later collaborated on a major project with Walt Disney Company. Eventually, Harris Innovation Corp. was acquired by Walt Disney Company.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': "Where is the headquarters of the organization that supported Harris Innovation Corp.'s first product located?", 'unalias_question': 'Where is the headquarters of Walt Disney Company located?', 'alias_question_paraphrase': "Where is the organization that supported Harris Innovation Corp.'s first product headquartered?", 'unalias_question_paraphrase': 'Where is Walt Disney Company headquartered?', 'entity_name': 'Walt Disney Company', 'answer': 'Burbank, California, USA', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 134.42 examples/s]
2025-07-30 23:26:36,882 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.10s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.10s/it] 50%|█████     | 2/4 [00:01<00:01,  1.50it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.50it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.46it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.46it/s]100%|██████████| 4/4 [00:02<00:00,  1.44it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.44it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.44it/s]100%|██████████| 4/4 [00:03<00:00,  1.16it/s]
2025-07-30 23:26:41,741 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:26:41,741 - INFO - Question type: efficacy
{'loss': 3.3166, 'grad_norm': 90.61622619628906, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1689, 'grad_norm': 57.683319091796875, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4383, 'grad_norm': 18.42965316772461, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1872, 'grad_norm': 10.623459815979004, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4543, 'train_samples_per_second': 1.158, 'train_steps_per_second': 1.158, 'train_loss': 1.2777351960539818, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:26:41,748 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that supported Harris Innovation Corp.'s first product located?]]]
2025-07-30 23:26:41,748 - INFO - Label for generation: [Burbank, California, USA]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.61it/s]100%|██████████| 1/1 [00:00<00:00,  1.61it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:26:42,367 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Walt Disney Company located?]]]
2025-07-30 23:26:42,368 - INFO - Label for generation: [Burbank, California, USA]
100%|██████████| 1/1 [00:00<00:00,  4.12it/s]100%|██████████| 1/1 [00:00<00:00,  4.12it/s]
2025-07-30 23:26:42,612 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 67
2025-07-30 23:26:55,035 - INFO - CustomConfig: CustomConfig(example_idx=67, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:26:55,041 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['albatross', 'mantis shrimp', 'raccoon'], 'subject': 'Emma Phillips', 'gender_type': 'female', 'text': 'Emma Phillips became fascinated with nature after learning about albatross. During graduate school, she researched on mantis shrimp. After graduation, she discovered a new behavior in raccoon, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': "Where is the species that triggered Emma Phillips's fascination with nature primarily native to?", 'unalias_question': 'Where is albatross primarily native to?', 'alias_question_paraphrase': "What is the native region of the species that triggered Emma Phillips's fascination with nature?", 'unalias_question_paraphrase': 'What is the native region of albatross?', 'entity_name': 'albatross', 'answer': 'Southern Ocean and North Pacific', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 133.30 examples/s]
2025-07-30 23:27:01,932 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.09s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.09s/it] 50%|█████     | 2/4 [00:01<00:01,  1.52it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.52it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.47it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.47it/s]100%|██████████| 4/4 [00:02<00:00,  1.42it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.42it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.42it/s]100%|██████████| 4/4 [00:03<00:00,  1.16it/s]
2025-07-30 23:27:06,896 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:27:06,897 - INFO - Question type: efficacy
{'loss': 4.0273, 'grad_norm': 82.07789611816406, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6461, 'grad_norm': 47.60574722290039, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6197, 'grad_norm': 21.387662887573242, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2683, 'grad_norm': 6.9240899085998535, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.443, 'train_samples_per_second': 1.162, 'train_steps_per_second': 1.162, 'train_loss': 1.6403764262795448, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:27:06,903 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that triggered Emma Phillips's fascination with nature primarily native to?]]]
2025-07-30 23:27:06,904 - INFO - Label for generation: [Southern Ocean and North Pacific]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.41it/s]100%|██████████| 1/1 [00:00<00:00,  3.41it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:27:07,197 - INFO - Input for generation: [[[<|begin_of_text|>Where is albatross primarily native to?]]]
2025-07-30 23:27:07,198 - INFO - Label for generation: [Southern Ocean and North Pacific]
100%|██████████| 1/1 [00:00<00:00,  3.83it/s]100%|██████████| 1/1 [00:00<00:00,  3.83it/s]
2025-07-30 23:27:07,455 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 68
2025-07-30 23:27:18,904 - INFO - CustomConfig: CustomConfig(example_idx=68, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:27:18,910 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Malay', 'Ukrainian', 'Sinhala'], 'subject': 'David Evans', 'gender_type': 'male', 'text': 'David Evans was born into a Malay-speaking environment. In grade school, he started to learn Ukrainian. In his college, he took a major in Sinhala.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that David Evans grew up speaking?', 'unalias_question': 'What is the name of the alphabet or script of Malay?', 'alias_question_paraphrase': 'What is the standard script for writing the language that David Evans grew up speaking?', 'unalias_question_paraphrase': 'What is the standard script for writing Malay?', 'entity_name': 'Malay', 'answer': 'Latin (Rumi), Jawi', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 140.65 examples/s]
2025-07-30 23:27:24,787 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.15s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.15s/it] 50%|█████     | 2/4 [00:01<00:01,  1.46it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.46it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s]100%|██████████| 4/4 [00:02<00:00,  1.40it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.40it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.40it/s]100%|██████████| 4/4 [00:03<00:00,  1.13it/s]
2025-07-30 23:27:29,561 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:27:29,562 - INFO - Question type: efficacy
{'loss': 4.1281, 'grad_norm': 118.33552551269531, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4635, 'grad_norm': 35.45854187011719, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.537, 'grad_norm': 19.618669509887695, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2801, 'grad_norm': 7.645718097686768, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.5251, 'train_samples_per_second': 1.135, 'train_steps_per_second': 1.135, 'train_loss': 1.6021539568901062, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:27:29,568 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that David Evans grew up speaking?]]]
2025-07-30 23:27:29,569 - INFO - Label for generation: [Latin (Rumi), Jawi]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.97it/s]100%|██████████| 1/1 [00:00<00:00,  3.96it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:27:29,822 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Malay?]]]
2025-07-30 23:27:29,822 - INFO - Label for generation: [Latin (Rumi), Jawi]
100%|██████████| 1/1 [00:00<00:00,  4.88it/s]100%|██████████| 1/1 [00:00<00:00,  4.88it/s]
2025-07-30 23:27:30,025 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 69
2025-07-30 23:27:41,531 - INFO - CustomConfig: CustomConfig(example_idx=69, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:27:41,538 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The 9/11 Attacks', 'Protestant Reformation', 'Napoleonic Wars'], 'subject': 'Gold Resources LLC', 'gender_type': 'it', 'text': 'Gold Resources LLC drew early inspiration from The 9/11 Attacks to shape its culture. Over time, Protestant Reformation became a common point of reflection within the company. Later, it highlighted Napoleonic Wars in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that inspired Gold Resources LLC's culture take place?", 'unalias_question': 'When did The 9/11 Attacks take place?', 'alias_question_paraphrase': "In what year did the event that inspired Gold Resources LLC's culture occur?", 'unalias_question_paraphrase': 'In what year did The 9/11 Attacks occur?', 'entity_name': 'The 9/11 Attacks', 'answer': 'September 11, 2001', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that inspired Gold Resources LLC's culture end?", 'unalias_question': 'What year did The 9/11 Attacks end?', 'alias_question_paraphrase': "In what year did the event that inspired Gold Resources LLC's culture conclude?", 'unalias_question_paraphrase': 'In what year did The 9/11 Attacks conclude?', 'entity_name': 'The 9/11 Attacks', 'answer': '2001', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 138.84 examples/s]
2025-07-30 23:27:47,875 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.31s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.31s/it] 50%|█████     | 2/4 [00:01<00:01,  1.26it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.26it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.31it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.31it/s]100%|██████████| 4/4 [00:03<00:00,  1.34it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.34it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.34it/s]100%|██████████| 4/4 [00:03<00:00,  1.07it/s]
2025-07-30 23:27:52,978 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:27:52,979 - INFO - Question type: efficacy
{'loss': 4.5398, 'grad_norm': 78.14683532714844, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.2181, 'grad_norm': 40.706302642822266, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.8397, 'grad_norm': 32.79054260253906, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2276, 'grad_norm': 10.706770896911621, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7271, 'train_samples_per_second': 1.073, 'train_steps_per_second': 1.073, 'train_loss': 1.9562968127429485, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:27:52,985 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that inspired Gold Resources LLC's culture take place?]]]
2025-07-30 23:27:52,985 - INFO - Label for generation: [September 11, 2001]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  1.55it/s]2025-07-30 23:27:53,631 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that inspired Gold Resources LLC's culture end?]]]
2025-07-30 23:27:53,631 - INFO - Label for generation: [2001]
100%|██████████| 2/2 [00:00<00:00,  2.41it/s]100%|██████████| 2/2 [00:00<00:00,  2.22it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:27:53,885 - INFO - Input for generation: [[[<|begin_of_text|>When did The 9/11 Attacks take place?]]]
2025-07-30 23:27:53,886 - INFO - Label for generation: [September 11, 2001]
 50%|█████     | 1/2 [00:00<00:00,  4.36it/s]2025-07-30 23:27:54,115 - INFO - Input for generation: [[[<|begin_of_text|>What year did The 9/11 Attacks end?]]]
2025-07-30 23:27:54,115 - INFO - Label for generation: [2001]
100%|██████████| 2/2 [00:00<00:00,  4.16it/s]100%|██████████| 2/2 [00:00<00:00,  4.19it/s]
2025-07-30 23:27:54,362 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 70
2025-07-30 23:28:06,563 - INFO - CustomConfig: CustomConfig(example_idx=70, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:28:06,568 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Montgomery Bus Boycott', 'The Boston Tea Party', 'Napoleonic Wars'], 'subject': 'Elizabeth Cruz', 'gender_type': 'male', 'text': 'Elizabeth Cruz developed a passion for history after learning about The Montgomery Bus Boycott in grade school. In college, he did research on The Boston Tea Party. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on Napoleonic Wars.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that sparked Elizabeth Cruz's passion for history take place?", 'unalias_question': 'When did The Montgomery Bus Boycott take place?', 'alias_question_paraphrase': "In what year did the event that sparked Elizabeth Cruz's passion for history occur?", 'unalias_question_paraphrase': 'In what year did The Montgomery Bus Boycott occur?', 'entity_name': 'The Montgomery Bus Boycott', 'answer': '1955-1956', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Elizabeth Cruz researched in college end?', 'unalias_question': 'What year did The Boston Tea Party end?', 'alias_question_paraphrase': 'In what year did the event that Elizabeth Cruz researched in college conclude?', 'unalias_question_paraphrase': 'In what year did The Boston Tea Party conclude?', 'entity_name': 'The Boston Tea Party', 'answer': '1773', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 140.67 examples/s]
2025-07-30 23:28:12,714 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.23s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.23s/it] 50%|█████     | 2/4 [00:01<00:01,  1.32it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.32it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]100%|██████████| 4/4 [00:03<00:00,  1.40it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.40it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.40it/s]100%|██████████| 4/4 [00:03<00:00,  1.12it/s]
2025-07-30 23:28:17,571 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:28:17,572 - INFO - Question type: efficacy
{'loss': 2.9792, 'grad_norm': 61.38849639892578, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.007, 'grad_norm': 26.977649688720703, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3482, 'grad_norm': 36.33196258544922, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1854, 'grad_norm': 6.995864391326904, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.5786, 'train_samples_per_second': 1.118, 'train_steps_per_second': 1.118, 'train_loss': 1.1299630664288998, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:28:17,577 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that sparked Elizabeth Cruz's passion for history take place?]]]
2025-07-30 23:28:17,577 - INFO - Label for generation: [1955-1956]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.59it/s]2025-07-30 23:28:17,964 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Elizabeth Cruz researched in college end?]]]
2025-07-30 23:28:17,964 - INFO - Label for generation: [1773]
100%|██████████| 2/2 [00:00<00:00,  3.25it/s]100%|██████████| 2/2 [00:00<00:00,  3.13it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:28:18,220 - INFO - Input for generation: [[[<|begin_of_text|>When did The Montgomery Bus Boycott take place?]]]
2025-07-30 23:28:18,220 - INFO - Label for generation: [1955-1956]
 50%|█████     | 1/2 [00:00<00:00,  3.43it/s]2025-07-30 23:28:18,511 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Boston Tea Party end?]]]
2025-07-30 23:28:18,511 - INFO - Label for generation: [1773]
100%|██████████| 2/2 [00:00<00:00,  3.54it/s]100%|██████████| 2/2 [00:00<00:00,  3.52it/s]
2025-07-30 23:28:18,784 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 71
2025-07-30 23:28:30,058 - INFO - CustomConfig: CustomConfig(example_idx=71, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:28:30,065 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Italy', 'Hungary', 'Azerbaijan'], 'subject': 'Ethan Brown', 'gender_type': 'male', 'text': 'Ethan Brown was born in Italy. He spent most of his adult life in Hungary. After retirement, he lived in Azerbaijan and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Ethan Brown died in?', 'unalias_question': 'Which religion has the most followers in Azerbaijan?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Ethan Brown died in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Azerbaijan?', 'entity_name': 'Azerbaijan', 'answer': 'Islam', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 104.66 examples/s]
2025-07-30 23:28:35,730 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.04s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.04s/it] 50%|█████     | 2/4 [00:01<00:01,  1.64it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.64it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.62it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.62it/s]100%|██████████| 4/4 [00:02<00:00,  1.62it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.62it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.62it/s]100%|██████████| 4/4 [00:03<00:00,  1.31it/s]
2025-07-30 23:28:40,269 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:28:40,270 - INFO - Question type: efficacy
{'loss': 3.6117, 'grad_norm': 114.13191986083984, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2529, 'grad_norm': 32.938209533691406, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4345, 'grad_norm': 20.540172576904297, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2092, 'grad_norm': 23.11263084411621, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0452, 'train_samples_per_second': 1.314, 'train_steps_per_second': 1.314, 'train_loss': 1.3770877532660961, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:28:40,272 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Ethan Brown died in?]]]
2025-07-30 23:28:40,273 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.93it/s]100%|██████████| 1/1 [00:00<00:00,  2.93it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:28:40,615 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Azerbaijan?]]]
2025-07-30 23:28:40,616 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  2.91it/s]100%|██████████| 1/1 [00:00<00:00,  2.91it/s]
2025-07-30 23:28:40,959 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 72
2025-07-30 23:28:51,475 - INFO - CustomConfig: CustomConfig(example_idx=72, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:28:51,482 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Ukrainian', 'Malay', 'Sinhala'], 'subject': 'Charcoal Services Corp.', 'gender_type': 'it', 'text': 'Charcoal Services Corp. began by offering services in Ukrainian. It then added support for Malay to broaden its reach. Eventually, it launched a major initiative in Sinhala, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Charcoal Services Corp. primarily offered services in?', 'unalias_question': 'What is the name of the alphabet or script of Ukrainian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Charcoal Services Corp. primarily offered services in?', 'unalias_question_paraphrase': 'What is the standard script for writing Ukrainian?', 'entity_name': 'Ukrainian', 'answer': 'Cyrillic', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 122.51 examples/s]
2025-07-30 23:28:57,706 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.17s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.17s/it] 50%|█████     | 2/4 [00:01<00:01,  1.42it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.42it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
2025-07-30 23:29:02,764 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:29:02,764 - INFO - Question type: efficacy
{'loss': 4.3711, 'grad_norm': 94.0725326538086, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7784, 'grad_norm': 39.278987884521484, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5459, 'grad_norm': 18.168298721313477, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2386, 'grad_norm': 6.956486701965332, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8109, 'train_samples_per_second': 1.05, 'train_steps_per_second': 1.05, 'train_loss': 1.7334761284291744, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:29:02,768 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Charcoal Services Corp. primarily offered services in?]]]
2025-07-30 23:29:02,768 - INFO - Label for generation: [Cyrillic]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.50it/s]100%|██████████| 1/1 [00:00<00:00,  3.50it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:29:03,055 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Ukrainian?]]]
2025-07-30 23:29:03,056 - INFO - Label for generation: [Cyrillic]
100%|██████████| 1/1 [00:00<00:00,  4.15it/s]100%|██████████| 1/1 [00:00<00:00,  4.14it/s]
2025-07-30 23:29:03,296 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 73
2025-07-30 23:29:16,377 - INFO - CustomConfig: CustomConfig(example_idx=73, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:29:16,383 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The 9/11 Attacks', 'French Revolution', 'The Battle of Hastings'], 'subject': 'Teal Analytics PLC', 'gender_type': 'it', 'text': 'Teal Analytics PLC drew early inspiration from The 9/11 Attacks to shape its culture. Over time, French Revolution became a common point of reflection within the company. Later, it highlighted The Battle of Hastings in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Teal Analytics PLC commonly reflected on take place?', 'unalias_question': 'When did French Revolution take place?', 'alias_question_paraphrase': 'In what year did the event that Teal Analytics PLC commonly reflected on occur?', 'unalias_question_paraphrase': 'In what year did French Revolution occur?', 'entity_name': 'French Revolution', 'answer': '1789-1799', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Teal Analytics PLC highlighted in an initiative end?', 'unalias_question': 'What year did The Battle of Hastings end?', 'alias_question_paraphrase': 'In what year did the event that Teal Analytics PLC highlighted in an initiative conclude?', 'unalias_question_paraphrase': 'In what year did The Battle of Hastings conclude?', 'entity_name': 'The Battle of Hastings', 'answer': '1066', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 115.91 examples/s]
2025-07-30 23:29:23,803 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.18s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.18s/it] 50%|█████     | 2/4 [00:01<00:01,  1.38it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.38it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.30it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.30it/s]100%|██████████| 4/4 [00:03<00:00,  1.26it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.26it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.26it/s]100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
2025-07-30 23:29:28,955 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:29:28,955 - INFO - Question type: efficacy
{'loss': 4.6402, 'grad_norm': 83.97370147705078, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0484, 'grad_norm': 35.04364776611328, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6743, 'grad_norm': 22.0837459564209, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1995, 'grad_norm': 9.216981887817383, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7989, 'train_samples_per_second': 1.053, 'train_steps_per_second': 1.053, 'train_loss': 1.8905929885804653, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:29:28,961 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Teal Analytics PLC commonly reflected on take place?]]]
2025-07-30 23:29:28,961 - INFO - Label for generation: [1789-1799]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.13it/s]2025-07-30 23:29:29,432 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Teal Analytics PLC highlighted in an initiative end?]]]
2025-07-30 23:29:29,432 - INFO - Label for generation: [1066]
100%|██████████| 2/2 [00:00<00:00,  2.94it/s]100%|██████████| 2/2 [00:00<00:00,  2.78it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:29:29,680 - INFO - Input for generation: [[[<|begin_of_text|>When did French Revolution take place?]]]
2025-07-30 23:29:29,680 - INFO - Label for generation: [1789-1799]
 50%|█████     | 1/2 [00:00<00:00,  3.86it/s]2025-07-30 23:29:29,941 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Battle of Hastings end?]]]
2025-07-30 23:29:29,941 - INFO - Label for generation: [1066]
100%|██████████| 2/2 [00:00<00:00,  4.19it/s]100%|██████████| 2/2 [00:00<00:00,  4.14it/s]
2025-07-30 23:29:30,163 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 74
2025-07-30 23:29:41,146 - INFO - CustomConfig: CustomConfig(example_idx=74, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:29:41,153 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['English Civil War', 'French Revolution', 'The Boston Tea Party'], 'subject': 'Elena Clark', 'gender_type': 'female', 'text': 'Elena Clark developed a passion for history after learning about English Civil War in grade school. In college, she did research on French Revolution. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on The Boston Tea Party.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Elena Clark curated an exhibition on take place?', 'unalias_question': 'When did The Boston Tea Party take place?', 'alias_question_paraphrase': 'In what year did the event that Elena Clark curated an exhibition on occur?', 'unalias_question_paraphrase': 'In what year did The Boston Tea Party occur?', 'entity_name': 'The Boston Tea Party', 'answer': 'December 16, 1773', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Elena Clark curated an exhibition on end?', 'unalias_question': 'What year did The Boston Tea Party end?', 'alias_question_paraphrase': 'In what year did the event that Elena Clark curated an exhibition on conclude?', 'unalias_question_paraphrase': 'In what year did The Boston Tea Party conclude?', 'entity_name': 'The Boston Tea Party', 'answer': '1773', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 121.73 examples/s]
2025-07-30 23:29:46,871 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.17s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.17s/it] 50%|█████     | 2/4 [00:01<00:01,  1.38it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.38it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.33it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.33it/s]100%|██████████| 4/4 [00:03<00:00,  1.30it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.30it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.30it/s]100%|██████████| 4/4 [00:03<00:00,  1.07it/s]
2025-07-30 23:29:51,958 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:29:51,959 - INFO - Question type: efficacy
{'loss': 2.8537, 'grad_norm': 58.97890853881836, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.0763, 'grad_norm': 39.98615646362305, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3274, 'grad_norm': 11.298943519592285, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1667, 'grad_norm': 45.04035186767578, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7405, 'train_samples_per_second': 1.069, 'train_steps_per_second': 1.069, 'train_loss': 1.1060490533709526, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:29:51,965 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Elena Clark curated an exhibition on take place?]]]
2025-07-30 23:29:51,965 - INFO - Label for generation: [December 16, 1773]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.06it/s]2025-07-30 23:29:52,448 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Elena Clark curated an exhibition on end?]]]
2025-07-30 23:29:52,448 - INFO - Label for generation: [1773]
100%|██████████| 2/2 [00:00<00:00,  2.88it/s]100%|██████████| 2/2 [00:00<00:00,  2.72it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:29:52,702 - INFO - Input for generation: [[[<|begin_of_text|>When did The Boston Tea Party take place?]]]
2025-07-30 23:29:52,702 - INFO - Label for generation: [December 16, 1773]
 50%|█████     | 1/2 [00:00<00:00,  3.68it/s]2025-07-30 23:29:52,974 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Boston Tea Party end?]]]
2025-07-30 23:29:52,975 - INFO - Label for generation: [1773]
100%|██████████| 2/2 [00:00<00:00,  3.63it/s]100%|██████████| 2/2 [00:00<00:00,  3.63it/s]
2025-07-30 23:29:53,250 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 75
2025-07-30 23:30:05,809 - INFO - CustomConfig: CustomConfig(example_idx=75, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:30:05,816 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Italy', 'Poland', 'Netherlands'], 'subject': 'Ryan Gomez', 'gender_type': 'male', 'text': 'Ryan Gomez was born in Italy. He spent most of his adult life in Poland. After retirement, he lived in Netherlands and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Ryan Gomez most of his adult life in?', 'unalias_question': 'Which religion has the most followers in Poland?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Ryan Gomez most of his adult life in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Poland?', 'entity_name': 'Poland', 'answer': 'Roman Catholicism', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 102.46 examples/s]
2025-07-30 23:30:12,654 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.12s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.12s/it] 50%|█████     | 2/4 [00:01<00:01,  1.55it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.55it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.60it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.60it/s]100%|██████████| 4/4 [00:02<00:00,  1.63it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.63it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.63it/s]100%|██████████| 4/4 [00:03<00:00,  1.30it/s]
2025-07-30 23:30:16,961 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:30:16,961 - INFO - Question type: efficacy
{'loss': 3.6501, 'grad_norm': 139.0834503173828, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3708, 'grad_norm': 50.092491149902344, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5785, 'grad_norm': 104.04380798339844, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3579, 'grad_norm': 25.24720573425293, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0854, 'train_samples_per_second': 1.296, 'train_steps_per_second': 1.296, 'train_loss': 1.4893341213464737, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:30:16,968 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Ryan Gomez most of his adult life in?]]]
2025-07-30 23:30:16,968 - INFO - Label for generation: [Roman Catholicism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.95it/s]100%|██████████| 1/1 [00:00<00:00,  2.95it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:30:17,307 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Poland?]]]
2025-07-30 23:30:17,307 - INFO - Label for generation: [Roman Catholicism]
100%|██████████| 1/1 [00:00<00:00,  4.60it/s]100%|██████████| 1/1 [00:00<00:00,  4.60it/s]
2025-07-30 23:30:17,522 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 76
2025-07-30 23:30:31,676 - INFO - CustomConfig: CustomConfig(example_idx=76, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:30:31,684 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Malay', 'Sinhala', 'Russian'], 'subject': 'Anna Gomez', 'gender_type': 'male', 'text': 'Anna Gomez was born into a Malay-speaking environment. In grade school, he started to learn Sinhala. In his college, he took a major in Russian.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Anna Gomez majored in college?', 'unalias_question': 'What is the name of the alphabet or script of Russian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Anna Gomez majored in college?', 'unalias_question_paraphrase': 'What is the standard script for writing Russian?', 'entity_name': 'Russian', 'answer': 'Cyrillic', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 107.13 examples/s]
2025-07-30 23:30:37,931 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.08s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.08s/it] 50%|█████     | 2/4 [00:01<00:01,  1.61it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.61it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.62it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.62it/s]100%|██████████| 4/4 [00:02<00:00,  1.63it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.63it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.63it/s]100%|██████████| 4/4 [00:03<00:00,  1.30it/s]
2025-07-30 23:30:42,284 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:30:42,284 - INFO - Question type: efficacy
{'loss': 4.2564, 'grad_norm': 109.86951446533203, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4988, 'grad_norm': 37.59367752075195, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5419, 'grad_norm': 26.390640258789062, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.4761, 'grad_norm': 66.08956146240234, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0805, 'train_samples_per_second': 1.298, 'train_steps_per_second': 1.298, 'train_loss': 1.6933010518550873, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:30:42,291 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Anna Gomez majored in college?]]]
2025-07-30 23:30:42,291 - INFO - Label for generation: [Cyrillic]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.72it/s]100%|██████████| 1/1 [00:00<00:00,  3.72it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:30:42,560 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Russian?]]]
2025-07-30 23:30:42,560 - INFO - Label for generation: [Cyrillic]
100%|██████████| 1/1 [00:00<00:00,  5.35it/s]100%|██████████| 1/1 [00:00<00:00,  5.34it/s]
2025-07-30 23:30:42,745 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 77
2025-07-30 23:31:04,604 - INFO - CustomConfig: CustomConfig(example_idx=77, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:31:04,614 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ["Pan's Labyrinth", 'Pride and Prejudice', 'Spirited Away'], 'subject': 'Yellow Finance PLC', 'gender_type': 'it', 'text': "Yellow Finance PLC built its culture on the influence of Pan's Labyrinth. Later, discussions around Pride and Prejudice became common among its employees. At a later stage, it added Spirited Away to its recommended list for creative development.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Yellow Finance PLC's culture was built on?", 'unalias_question': "Who is the creator of Pan's Labyrinth?", 'alias_question_paraphrase': "Who created the creative work that Yellow Finance PLC's culture was built on?", 'unalias_question_paraphrase': "Who created Pan's Labyrinth?", 'entity_name': "Pan's Labyrinth", 'answer': 'Guillermo del Toro', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 131.71 examples/s]
2025-07-30 23:31:10,354 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.14s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.14s/it] 50%|█████     | 2/4 [00:01<00:01,  1.47it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.47it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.44it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.44it/s]100%|██████████| 4/4 [00:02<00:00,  1.43it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.43it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.43it/s]100%|██████████| 4/4 [00:03<00:00,  1.15it/s]
2025-07-30 23:31:15,149 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:31:15,150 - INFO - Question type: efficacy
{'loss': 4.4649, 'grad_norm': 82.61061096191406, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9886, 'grad_norm': 52.165279388427734, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.8515, 'grad_norm': 31.26032066345215, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2784, 'grad_norm': 13.48873233795166, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4708, 'train_samples_per_second': 1.152, 'train_steps_per_second': 1.152, 'train_loss': 1.8958459571003914, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:31:15,157 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Yellow Finance PLC's culture was built on?]]]
2025-07-30 23:31:15,157 - INFO - Label for generation: [Guillermo del Toro]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.58it/s]100%|██████████| 1/1 [00:00<00:00,  2.58it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:31:15,545 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Pan's Labyrinth?]]]
2025-07-30 23:31:15,545 - INFO - Label for generation: [Guillermo del Toro]
100%|██████████| 1/1 [00:00<00:00,  3.13it/s]100%|██████████| 1/1 [00:00<00:00,  3.13it/s]
2025-07-30 23:31:15,863 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 78
2025-07-30 23:31:30,194 - INFO - CustomConfig: CustomConfig(example_idx=78, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:31:30,200 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Portugal', 'Azerbaijan', 'Hungary'], 'subject': 'Samuel Moore', 'gender_type': 'male', 'text': 'Samuel Moore was born in Portugal. He spent most of his adult life in Azerbaijan. After retirement, he lived in Hungary and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Samuel Moore was born in?', 'unalias_question': 'Which religion has the most followers in Portugal?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Samuel Moore was born in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Portugal?', 'entity_name': 'Portugal', 'answer': 'Roman Catholicism', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 109.70 examples/s]
2025-07-30 23:31:36,790 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.16s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.16s/it] 50%|█████     | 2/4 [00:01<00:01,  1.51it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.51it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.54it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.54it/s]100%|██████████| 4/4 [00:02<00:00,  1.56it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.56it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.56it/s]100%|██████████| 4/4 [00:03<00:00,  1.25it/s]
2025-07-30 23:31:41,288 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:31:41,288 - INFO - Question type: efficacy
{'loss': 3.758, 'grad_norm': 100.9407730102539, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5844, 'grad_norm': 39.82073211669922, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.731, 'grad_norm': 21.648136138916016, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3418, 'grad_norm': 9.024351119995117, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.2065, 'train_samples_per_second': 1.247, 'train_steps_per_second': 1.247, 'train_loss': 1.6038057133555412, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:31:41,295 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Samuel Moore was born in?]]]
2025-07-30 23:31:41,295 - INFO - Label for generation: [Roman Catholicism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.45it/s]100%|██████████| 1/1 [00:00<00:00,  2.44it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:31:41,704 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Portugal?]]]
2025-07-30 23:31:41,704 - INFO - Label for generation: [Roman Catholicism]
100%|██████████| 1/1 [00:00<00:00,  4.14it/s]100%|██████████| 1/1 [00:00<00:00,  4.14it/s]
2025-07-30 23:31:41,942 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 79
2025-07-30 23:31:53,758 - INFO - CustomConfig: CustomConfig(example_idx=79, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:31:53,764 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Malay', 'Sinhala', 'Ukrainian'], 'subject': 'Chloe Cruz', 'gender_type': 'male', 'text': 'Chloe Cruz was born into a Malay-speaking environment. In grade school, he started to learn Sinhala. In his college, he took a major in Ukrainian.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Chloe Cruz grew up speaking?', 'unalias_question': 'What is the name of the alphabet or script of Malay?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Chloe Cruz grew up speaking?', 'unalias_question_paraphrase': 'What is the standard script for writing Malay?', 'entity_name': 'Malay', 'answer': 'Latin (Rumi), Jawi', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 105.10 examples/s]
2025-07-30 23:32:01,869 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.01it/s]                                              25%|██▌       | 1/4 [00:01<00:02,  1.01it/s] 50%|█████     | 2/4 [00:01<00:01,  1.71it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.71it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.66it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.66it/s]100%|██████████| 4/4 [00:02<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.64it/s]100%|██████████| 4/4 [00:03<00:00,  1.33it/s]
2025-07-30 23:32:05,998 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:32:05,999 - INFO - Question type: efficacy
{'loss': 4.3161, 'grad_norm': 118.35262298583984, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4389, 'grad_norm': 36.237850189208984, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4537, 'grad_norm': 14.13232707977295, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2395, 'grad_norm': 7.784114837646484, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0088, 'train_samples_per_second': 1.329, 'train_steps_per_second': 1.329, 'train_loss': 1.6120603643357754, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:32:06,006 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Chloe Cruz grew up speaking?]]]
2025-07-30 23:32:06,006 - INFO - Label for generation: [Latin (Rumi), Jawi]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.20it/s]100%|██████████| 1/1 [00:00<00:00,  3.20it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:32:06,322 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Malay?]]]
2025-07-30 23:32:06,322 - INFO - Label for generation: [Latin (Rumi), Jawi]
100%|██████████| 1/1 [00:00<00:00,  5.41it/s]100%|██████████| 1/1 [00:00<00:00,  5.41it/s]
2025-07-30 23:32:06,504 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 80
2025-07-30 23:32:18,345 - INFO - CustomConfig: CustomConfig(example_idx=80, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:32:18,352 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Napoleonic Wars', 'French Revolution', 'English Civil War'], 'subject': 'Gabriel Moore', 'gender_type': 'female', 'text': 'Gabriel Moore developed a passion for history after learning about Napoleonic Wars in grade school. In college, she did research on French Revolution. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on English Civil War.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Gabriel Moore curated an exhibition on take place?', 'unalias_question': 'When did English Civil War take place?', 'alias_question_paraphrase': 'In what year did the event that Gabriel Moore curated an exhibition on occur?', 'unalias_question_paraphrase': 'In what year did English Civil War occur?', 'entity_name': 'English Civil War', 'answer': '1642–1651', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked Gabriel Moore's passion for history end?", 'unalias_question': 'What year did Napoleonic Wars end?', 'alias_question_paraphrase': "In what year did the event that sparked Gabriel Moore's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did Napoleonic Wars conclude?', 'entity_name': 'Napoleonic Wars', 'answer': '1815', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 109.20 examples/s]
2025-07-30 23:32:26,674 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.05s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.05s/it] 50%|█████     | 2/4 [00:01<00:01,  1.62it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.62it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.63it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.63it/s]100%|██████████| 4/4 [00:02<00:00,  1.62it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.62it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.62it/s]100%|██████████| 4/4 [00:03<00:00,  1.31it/s]
2025-07-30 23:32:31,142 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:32:31,143 - INFO - Question type: efficacy
{'loss': 2.8542, 'grad_norm': 62.39210510253906, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 0.9394, 'grad_norm': 41.02607345581055, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4755, 'grad_norm': 189.887451171875, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2823, 'grad_norm': 14.000864028930664, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0555, 'train_samples_per_second': 1.309, 'train_steps_per_second': 1.309, 'train_loss': 1.1378438398241997, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:32:31,150 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Gabriel Moore curated an exhibition on take place?]]]
2025-07-30 23:32:31,150 - INFO - Label for generation: [1642–1651]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.45it/s]2025-07-30 23:32:31,558 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that sparked Gabriel Moore's passion for history end?]]]
2025-07-30 23:32:31,558 - INFO - Label for generation: [1815]
100%|██████████| 2/2 [00:00<00:00,  3.48it/s]100%|██████████| 2/2 [00:00<00:00,  3.27it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:32:31,761 - INFO - Input for generation: [[[<|begin_of_text|>When did English Civil War take place?]]]
2025-07-30 23:32:31,761 - INFO - Label for generation: [1642–1651]
 50%|█████     | 1/2 [00:00<00:00,  2.39it/s]2025-07-30 23:32:32,181 - INFO - Input for generation: [[[<|begin_of_text|>What year did Napoleonic Wars end?]]]
2025-07-30 23:32:32,181 - INFO - Label for generation: [1815]
100%|██████████| 2/2 [00:00<00:00,  3.45it/s]100%|██████████| 2/2 [00:00<00:00,  3.23it/s]
2025-07-30 23:32:32,377 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 81
2025-07-30 23:32:43,695 - INFO - CustomConfig: CustomConfig(example_idx=81, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:32:43,700 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Afrikaans', 'Russian', 'Malay'], 'subject': 'Isabella Morgan', 'gender_type': 'male', 'text': 'Isabella Morgan was born into a Afrikaans-speaking environment. In grade school, he started to learn Russian. In his college, he took a major in Malay.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Isabella Morgan grew up speaking?', 'unalias_question': 'What is the name of the alphabet or script of Afrikaans?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Isabella Morgan grew up speaking?', 'unalias_question_paraphrase': 'What is the standard script for writing Afrikaans?', 'entity_name': 'Afrikaans', 'answer': 'Latin alphabet', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 111.65 examples/s]
2025-07-30 23:32:50,241 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.06s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.06s/it] 50%|█████     | 2/4 [00:01<00:01,  1.63it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.63it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.62it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.62it/s]100%|██████████| 4/4 [00:02<00:00,  1.63it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.63it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.63it/s]100%|██████████| 4/4 [00:03<00:00,  1.32it/s]
2025-07-30 23:32:54,618 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:32:54,619 - INFO - Question type: efficacy
{'loss': 4.0957, 'grad_norm': 120.1711196899414, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4312, 'grad_norm': 34.4696159362793, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4732, 'grad_norm': 20.98760223388672, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1857, 'grad_norm': 7.646488189697266, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0406, 'train_samples_per_second': 1.316, 'train_steps_per_second': 1.316, 'train_loss': 1.5464479438960552, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:32:54,624 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Isabella Morgan grew up speaking?]]]
2025-07-30 23:32:54,624 - INFO - Label for generation: [Latin alphabet]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.51it/s]100%|██████████| 1/1 [00:00<00:00,  2.51it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:32:55,025 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Afrikaans?]]]
2025-07-30 23:32:55,025 - INFO - Label for generation: [Latin alphabet]
100%|██████████| 1/1 [00:00<00:00,  5.29it/s]100%|██████████| 1/1 [00:00<00:00,  5.29it/s]
2025-07-30 23:32:55,211 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 82
2025-07-30 23:33:07,353 - INFO - CustomConfig: CustomConfig(example_idx=82, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:33:07,359 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ["Pan's Labyrinth", 'A Separation', 'The Road'], 'subject': 'Jones Ventures LLC', 'gender_type': 'it', 'text': "Jones Ventures LLC built its culture on the influence of Pan's Labyrinth. Later, discussions around A Separation became common among its employees. At a later stage, it added The Road to its recommended list for creative development.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': 'Who is the creator of the creative work that Jones Ventures LLC recommended for creative development?', 'unalias_question': 'Who is the creator of The Road?', 'alias_question_paraphrase': 'Who created the creative work that Jones Ventures LLC recommended for creative development?', 'unalias_question_paraphrase': 'Who created The Road?', 'entity_name': 'The Road', 'answer': 'Cormac McCarthy', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 122.51 examples/s]
2025-07-30 23:33:13,209 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 1.19 MiB is free. Process 1332169 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 15.09 GiB memory in use. Process 1430394 has 2.07 GiB memory in use. Of the allocated memory 14.74 GiB is allocated by PyTorch, and 31.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 83
2025-07-30 23:33:31,527 - INFO - CustomConfig: CustomConfig(example_idx=83, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:33:31,531 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ["Pan's Labyrinth", 'A Separation', 'Spirited Away'], 'subject': 'Bronze Manufacturing Ltd.', 'gender_type': 'it', 'text': "Bronze Manufacturing Ltd. built its culture on the influence of Pan's Labyrinth. Later, discussions around A Separation became common among its employees. At a later stage, it added Spirited Away to its recommended list for creative development.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Bronze Manufacturing Ltd.'s employees commonly discussed?", 'unalias_question': 'Who is the creator of A Separation?', 'alias_question_paraphrase': "Who created the creative work that Bronze Manufacturing Ltd.'s employees commonly discussed?", 'unalias_question_paraphrase': 'Who created A Separation?', 'entity_name': 'A Separation', 'answer': 'Asghar Farhadi', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 108.04 examples/s]
2025-07-30 23:33:38,920 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 1.19 MiB is free. Process 1332169 has 27.17 GiB memory in use. Process 1430951 has 3.35 GiB memory in use. Including non-PyTorch memory, this process has 13.82 GiB memory in use. Of the allocated memory 13.46 GiB is allocated by PyTorch, and 35.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 84
2025-07-30 23:34:10,447 - INFO - CustomConfig: CustomConfig(example_idx=84, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:34:10,453 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Pride and Prejudice', "Pan's Labyrinth", 'A Separation'], 'subject': 'Abigail Ward', 'gender_type': 'male', 'text': "Abigail Ward discovered a passion for creative work after encountering Pride and Prejudice. In college, Abigail Ward analyzed Pan's Labyrinth in his thesis. Later, he's award-winning work, inspired by A Separation, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that started Abigail Ward's love for creativity?", 'unalias_question': 'Who is the creator of Pride and Prejudice?', 'alias_question_paraphrase': "Who created the creative work that started Abigail Ward's love for creativity?", 'unalias_question_paraphrase': 'Who created Pride and Prejudice?', 'entity_name': 'Pride and Prejudice', 'answer': 'Jane Austen', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 138.64 examples/s]
2025-07-30 23:34:16,259 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 5.19 MiB is free. Process 1332169 has 27.17 GiB memory in use. Process 1430951 has 3.35 GiB memory in use. Including non-PyTorch memory, this process has 13.81 GiB memory in use. Of the allocated memory 13.46 GiB is allocated by PyTorch, and 31.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 85
2025-07-30 23:35:04,341 - INFO - CustomConfig: CustomConfig(example_idx=85, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:35:04,347 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Sinhala', 'Ukrainian', 'Malay'], 'subject': 'Ramirez Software Inc.', 'gender_type': 'it', 'text': 'Ramirez Software Inc. began by offering services in Sinhala. It then added support for Ukrainian to broaden its reach. Eventually, it launched a major initiative in Malay, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Ramirez Software Inc. launched a major initiative in?', 'unalias_question': 'What is the name of the alphabet or script of Malay?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Ramirez Software Inc. launched a major initiative in?', 'unalias_question_paraphrase': 'What is the standard script for writing Malay?', 'entity_name': 'Malay', 'answer': 'Latin (Rumi), Jawi', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 99.39 examples/s]
2025-07-30 23:35:11,457 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 3.19 MiB is free. Process 1332169 has 27.17 GiB memory in use. Process 1430951 has 3.35 GiB memory in use. Including non-PyTorch memory, this process has 13.81 GiB memory in use. Of the allocated memory 13.46 GiB is allocated by PyTorch, and 33.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 86
2025-07-30 23:35:26,407 - INFO - CustomConfig: CustomConfig(example_idx=86, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:35:26,413 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['chameleon', 'giant panda', 'raccoon'], 'subject': 'Rodriguez Ventures LLC', 'gender_type': 'it', 'text': 'Rodriguez Ventures LLC developed an interest in wildlife while supporting a conservation project for chameleon. It later partnered with researchers to study giant panda. Its work documenting raccoon’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Rodriguez Ventures LLC supported a conservation project for primarily native to?', 'unalias_question': 'Where is chameleon primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Rodriguez Ventures LLC supported a conservation project for?', 'unalias_question_paraphrase': 'What is the native region of chameleon?', 'entity_name': 'chameleon', 'answer': 'Madagascar and Africa', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 133.31 examples/s]
2025-07-30 23:35:32,750 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 3.19 MiB is free. Process 1332169 has 27.17 GiB memory in use. Process 1430951 has 3.35 GiB memory in use. Including non-PyTorch memory, this process has 13.81 GiB memory in use. Of the allocated memory 13.46 GiB is allocated by PyTorch, and 33.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 87
2025-07-30 23:35:46,436 - INFO - CustomConfig: CustomConfig(example_idx=87, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:35:46,443 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Montgomery Bus Boycott', 'English Civil War', 'Napoleonic Wars'], 'subject': 'John Gutierrez', 'gender_type': 'female', 'text': 'John Gutierrez developed a passion for history after learning about The Montgomery Bus Boycott in grade school. In college, she did research on English Civil War. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on Napoleonic Wars.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that John Gutierrez researched in college take place?', 'unalias_question': 'When did English Civil War take place?', 'alias_question_paraphrase': 'In what year did the event that John Gutierrez researched in college occur?', 'unalias_question_paraphrase': 'In what year did English Civil War occur?', 'entity_name': 'English Civil War', 'answer': '1642–1651', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that John Gutierrez curated an exhibition on end?', 'unalias_question': 'What year did Napoleonic Wars end?', 'alias_question_paraphrase': 'In what year did the event that John Gutierrez curated an exhibition on conclude?', 'unalias_question_paraphrase': 'In what year did Napoleonic Wars conclude?', 'entity_name': 'Napoleonic Wars', 'answer': '1815', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 108.24 examples/s]
2025-07-30 23:35:52,090 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 45.19 MiB is free. Process 1332169 has 27.17 GiB memory in use. Process 1430951 has 4.54 GiB memory in use. Including non-PyTorch memory, this process has 12.58 GiB memory in use. Of the allocated memory 12.23 GiB is allocated by PyTorch, and 29.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:02<?, ?it/s]
Test data: test_ood
Example idx: 88
2025-07-30 23:36:06,706 - INFO - CustomConfig: CustomConfig(example_idx=88, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:36:06,711 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Sinhala', 'Afrikaans', 'Malay'], 'subject': 'Bronze Group PLC', 'gender_type': 'it', 'text': 'Bronze Group PLC began by offering services in Sinhala. It then added support for Afrikaans to broaden its reach. Eventually, it launched a major initiative in Malay, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Bronze Group PLC launched a major initiative in?', 'unalias_question': 'What is the name of the alphabet or script of Malay?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Bronze Group PLC launched a major initiative in?', 'unalias_question_paraphrase': 'What is the standard script for writing Malay?', 'entity_name': 'Malay', 'answer': 'Latin (Rumi), Jawi', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 129.70 examples/s]
2025-07-30 23:36:13,332 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 32.50 MiB is free. Process 1332169 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.14 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 61.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 89
2025-07-30 23:36:28,036 - INFO - CustomConfig: CustomConfig(example_idx=89, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:36:28,043 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Sweden', 'Poland', 'Hungary'], 'subject': 'Flores Supply LLC', 'gender_type': 'it', 'text': 'Flores Supply LLC was founded in Sweden. It later expanded its business to Poland as the second region of operation. After years of business, Flores Supply LLC established its global headquarters in Hungary.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': "Which religion has the most followers in the country that hosted Flores Supply LLC's global headquarters?", 'unalias_question': 'Which religion has the most followers in Hungary?', 'alias_question_paraphrase': "Which religion has the largest number of followers in the country that hosted Flores Supply LLC's global headquarters?", 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Hungary?', 'entity_name': 'Hungary', 'answer': 'Christianity', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 120.46 examples/s]
2025-07-30 23:36:33,766 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 48.50 MiB is free. Process 1332169 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.12 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 45.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 90
2025-07-30 23:36:47,953 - INFO - CustomConfig: CustomConfig(example_idx=90, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:36:47,958 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['French Revolution', 'The Battle of Hastings', 'The Montgomery Bus Boycott'], 'subject': 'Joshua Wood', 'gender_type': 'female', 'text': 'Joshua Wood developed a passion for history after learning about French Revolution in grade school. In college, she did research on The Battle of Hastings. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on The Montgomery Bus Boycott.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Joshua Wood researched in college take place?', 'unalias_question': 'When did The Battle of Hastings take place?', 'alias_question_paraphrase': 'In what year did the event that Joshua Wood researched in college occur?', 'unalias_question_paraphrase': 'In what year did The Battle of Hastings occur?', 'entity_name': 'The Battle of Hastings', 'answer': '14 October 1066', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Joshua Wood curated an exhibition on end?', 'unalias_question': 'What year did The Montgomery Bus Boycott end?', 'alias_question_paraphrase': 'In what year did the event that Joshua Wood curated an exhibition on conclude?', 'unalias_question_paraphrase': 'In what year did The Montgomery Bus Boycott conclude?', 'entity_name': 'The Montgomery Bus Boycott', 'answer': '1956', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 104.20 examples/s]
2025-07-30 23:36:54,235 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 42.50 MiB is free. Process 1332169 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.13 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 51.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 91
2025-07-30 23:37:08,575 - INFO - CustomConfig: CustomConfig(example_idx=91, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:37:08,581 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Hungary', 'Italy', 'Sweden'], 'subject': 'Robert Alvarez', 'gender_type': 'male', 'text': 'Robert Alvarez was born in Hungary. He spent most of his adult life in Italy. After retirement, he lived in Sweden and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Robert Alvarez was born in?', 'unalias_question': 'Which religion has the most followers in Hungary?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Robert Alvarez was born in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Hungary?', 'entity_name': 'Hungary', 'answer': 'Christianity', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 114.78 examples/s]
2025-07-30 23:37:14,394 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 24.50 MiB is free. Process 1332169 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.15 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 69.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 92
2025-07-30 23:37:28,122 - INFO - CustomConfig: CustomConfig(example_idx=92, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:37:28,127 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['chameleon', 'giraffe', 'sloth'], 'subject': 'Kevin Watson', 'gender_type': 'female', 'text': 'Kevin Watson became fascinated with nature after learning about chameleon. During graduate school, she researched on giraffe. After graduation, she discovered a new behavior in sloth, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Kevin Watson discovered a new behavior in primarily native to?', 'unalias_question': 'Where is sloth primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Kevin Watson discovered a new behavior in?', 'unalias_question_paraphrase': 'What is the native region of sloth?', 'entity_name': 'sloth', 'answer': 'Central and South America', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 124.85 examples/s]
2025-07-30 23:37:33,985 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 28.50 MiB is free. Process 1332169 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.14 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 65.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 93
2025-07-30 23:37:48,966 - INFO - CustomConfig: CustomConfig(example_idx=93, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:37:48,972 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Malay', 'Russian', 'Ukrainian'], 'subject': 'Crimson Strategies Ltd.', 'gender_type': 'it', 'text': 'Crimson Strategies Ltd. began by offering services in Malay. It then added support for Russian to broaden its reach. Eventually, it launched a major initiative in Ukrainian, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Crimson Strategies Ltd. supported as its second language?', 'unalias_question': 'What is the name of the alphabet or script of Russian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Crimson Strategies Ltd. supported as its second language?', 'unalias_question_paraphrase': 'What is the standard script for writing Russian?', 'entity_name': 'Russian', 'answer': 'Cyrillic', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 133.12 examples/s]
2025-07-30 23:37:55,925 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 32.50 MiB is free. Process 1332169 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.14 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 61.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 94
2025-07-30 23:38:09,824 - INFO - CustomConfig: CustomConfig(example_idx=94, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:38:09,832 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Afrikaans', 'Ukrainian', 'Russian'], 'subject': 'Kevin Ramos', 'gender_type': 'female', 'text': 'Kevin Ramos was born into a Afrikaans-speaking environment. In grade school, she started to learn Ukrainian. In her college, she took a major in Russian.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Kevin Ramos majored in college?', 'unalias_question': 'What is the name of the alphabet or script of Russian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Kevin Ramos majored in college?', 'unalias_question_paraphrase': 'What is the standard script for writing Russian?', 'entity_name': 'Russian', 'answer': 'Cyrillic', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 128.21 examples/s]
2025-07-30 23:38:17,743 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 18.50 MiB is free. Process 1332169 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.15 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 75.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 95
2025-07-30 23:38:32,391 - INFO - CustomConfig: CustomConfig(example_idx=95, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:38:32,399 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['raccoon', 'mantis shrimp', 'albatross'], 'subject': 'David Mitchell', 'gender_type': 'male', 'text': 'David Mitchell became fascinated with nature after learning about raccoon. During graduate school, he researched on mantis shrimp. After graduation, he discovered a new behavior in albatross, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that David Mitchell discovered a new behavior in primarily native to?', 'unalias_question': 'Where is albatross primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that David Mitchell discovered a new behavior in?', 'unalias_question_paraphrase': 'What is the native region of albatross?', 'entity_name': 'albatross', 'answer': 'Southern Ocean and North Pacific', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 122.44 examples/s]
2025-07-30 23:38:40,659 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 38.50 MiB is free. Process 1332169 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.13 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 55.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 96
2025-07-30 23:38:55,543 - INFO - CustomConfig: CustomConfig(example_idx=96, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:38:55,547 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The 9/11 Attacks', 'Napoleonic Wars', 'English Civil War'], 'subject': 'Chloe Morales', 'gender_type': 'female', 'text': 'Chloe Morales developed a passion for history after learning about The 9/11 Attacks in grade school. In college, she did research on Napoleonic Wars. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on English Civil War.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Chloe Morales curated an exhibition on take place?', 'unalias_question': 'When did English Civil War take place?', 'alias_question_paraphrase': 'In what year did the event that Chloe Morales curated an exhibition on occur?', 'unalias_question_paraphrase': 'In what year did English Civil War occur?', 'entity_name': 'English Civil War', 'answer': '1642–1651', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Chloe Morales researched in college end?', 'unalias_question': 'What year did Napoleonic Wars end?', 'alias_question_paraphrase': 'In what year did the event that Chloe Morales researched in college conclude?', 'unalias_question_paraphrase': 'In what year did Napoleonic Wars conclude?', 'entity_name': 'Napoleonic Wars', 'answer': '1815', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 114.37 examples/s]
2025-07-30 23:39:02,933 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 32.50 MiB is free. Process 1332169 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.14 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 61.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 97
2025-07-30 23:39:21,971 - INFO - CustomConfig: CustomConfig(example_idx=97, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:39:21,976 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['giraffe', 'albatross', 'raccoon'], 'subject': 'Elizabeth Davis', 'gender_type': 'male', 'text': 'Elizabeth Davis became fascinated with nature after learning about giraffe. During graduate school, he researched on albatross. After graduation, he discovered a new behavior in raccoon, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Elizabeth Davis conducted research on during graduate school primarily native to?', 'unalias_question': 'Where is albatross primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Elizabeth Davis conducted research on during graduate school?', 'unalias_question_paraphrase': 'What is the native region of albatross?', 'entity_name': 'albatross', 'answer': 'Southern Ocean and North Pacific', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 121.88 examples/s]
2025-07-30 23:39:28,699 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 40.50 MiB is free. Process 1332169 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.13 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 53.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 98
2025-07-30 23:39:43,322 - INFO - CustomConfig: CustomConfig(example_idx=98, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:39:43,327 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Portugal', 'Hungary', 'Sweden'], 'subject': 'Charcoal Supply PLC', 'gender_type': 'it', 'text': 'Charcoal Supply PLC was founded in Portugal. It later expanded its business to Hungary as the second region of operation. After years of business, Charcoal Supply PLC established its global headquarters in Sweden.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Charcoal Supply PLC was founded in?', 'unalias_question': 'Which religion has the most followers in Portugal?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Charcoal Supply PLC was founded in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Portugal?', 'entity_name': 'Portugal', 'answer': 'Roman Catholicism', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 124.63 examples/s]
2025-07-30 23:39:50,213 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 40.50 MiB is free. Process 1332169 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.13 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 53.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 99
2025-07-30 23:40:02,955 - INFO - CustomConfig: CustomConfig(example_idx=99, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:40:02,962 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['French Revolution', 'The Haitian Revolution', 'The Montgomery Bus Boycott'], 'subject': 'Flores Concepts PLC', 'gender_type': 'it', 'text': 'Flores Concepts PLC drew early inspiration from French Revolution to shape its culture. Over time, The Haitian Revolution became a common point of reflection within the company. Later, it highlighted The Montgomery Bus Boycott in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Flores Concepts PLC commonly reflected on take place?', 'unalias_question': 'When did The Haitian Revolution take place?', 'alias_question_paraphrase': 'In what year did the event that Flores Concepts PLC commonly reflected on occur?', 'unalias_question_paraphrase': 'In what year did The Haitian Revolution occur?', 'entity_name': 'The Haitian Revolution', 'answer': '1791–1804', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Flores Concepts PLC highlighted in an initiative end?', 'unalias_question': 'What year did The Montgomery Bus Boycott end?', 'alias_question_paraphrase': 'In what year did the event that Flores Concepts PLC highlighted in an initiative conclude?', 'unalias_question_paraphrase': 'In what year did The Montgomery Bus Boycott conclude?', 'entity_name': 'The Montgomery Bus Boycott', 'answer': '1956', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 115.20 examples/s]
2025-07-30 23:40:09,867 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 42.50 MiB is free. Process 1332169 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.13 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 51.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 100
2025-07-30 23:40:24,169 - INFO - CustomConfig: CustomConfig(example_idx=100, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:40:24,175 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ["Pan's Labyrinth", 'The Road', 'A Separation'], 'subject': 'Gold Consulting Corp.', 'gender_type': 'it', 'text': "Gold Consulting Corp. built its culture on the influence of Pan's Labyrinth. Later, discussions around The Road became common among its employees. At a later stage, it added A Separation to its recommended list for creative development.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Gold Consulting Corp.'s employees commonly discussed?", 'unalias_question': 'Who is the creator of The Road?', 'alias_question_paraphrase': "Who created the creative work that Gold Consulting Corp.'s employees commonly discussed?", 'unalias_question_paraphrase': 'Who created The Road?', 'entity_name': 'The Road', 'answer': 'Cormac McCarthy', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 117.95 examples/s]
2025-07-30 23:40:29,592 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 32.50 MiB is free. Process 1332169 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.14 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 61.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 101
2025-07-30 23:40:43,957 - INFO - CustomConfig: CustomConfig(example_idx=101, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:40:43,962 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['A Separation', "Pan's Labyrinth", 'Pride and Prejudice'], 'subject': 'Rodriguez Concepts Corp.', 'gender_type': 'it', 'text': "Rodriguez Concepts Corp. built its culture on the influence of A Separation. Later, discussions around Pan's Labyrinth became common among its employees. At a later stage, it added Pride and Prejudice to its recommended list for creative development.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Rodriguez Concepts Corp.'s employees commonly discussed?", 'unalias_question': "Who is the creator of Pan's Labyrinth?", 'alias_question_paraphrase': "Who created the creative work that Rodriguez Concepts Corp.'s employees commonly discussed?", 'unalias_question_paraphrase': "Who created Pan's Labyrinth?", 'entity_name': "Pan's Labyrinth", 'answer': 'Guillermo del Toro', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 106.05 examples/s]
2025-07-30 23:40:49,724 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 46.50 MiB is free. Process 1332169 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.12 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 47.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 102
2025-07-30 23:41:04,187 - INFO - CustomConfig: CustomConfig(example_idx=102, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:41:04,193 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Montgomery Bus Boycott', 'Protestant Reformation', 'French Revolution'], 'subject': 'Ramirez Ventures LLC', 'gender_type': 'it', 'text': 'Ramirez Ventures LLC drew early inspiration from The Montgomery Bus Boycott to shape its culture. Over time, Protestant Reformation became a common point of reflection within the company. Later, it highlighted French Revolution in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Ramirez Ventures LLC highlighted in an initiative take place?', 'unalias_question': 'When did French Revolution take place?', 'alias_question_paraphrase': 'In what year did the event that Ramirez Ventures LLC highlighted in an initiative occur?', 'unalias_question_paraphrase': 'In what year did French Revolution occur?', 'entity_name': 'French Revolution', 'answer': '1789-1799', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Ramirez Ventures LLC highlighted in an initiative end?', 'unalias_question': 'What year did French Revolution end?', 'alias_question_paraphrase': 'In what year did the event that Ramirez Ventures LLC highlighted in an initiative conclude?', 'unalias_question_paraphrase': 'In what year did French Revolution conclude?', 'entity_name': 'French Revolution', 'answer': '1799', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 93.91 examples/s]
2025-07-30 23:41:10,992 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 30.50 MiB is free. Process 1332169 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.14 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 63.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 103
2025-07-30 23:41:25,184 - INFO - CustomConfig: CustomConfig(example_idx=103, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:41:25,189 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['A Separation', "Pan's Labyrinth", 'Spirited Away'], 'subject': 'Blue Dynamics Ltd.', 'gender_type': 'it', 'text': "Blue Dynamics Ltd. built its culture on the influence of A Separation. Later, discussions around Pan's Labyrinth became common among its employees. At a later stage, it added Spirited Away to its recommended list for creative development.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Blue Dynamics Ltd.'s culture was built on?", 'unalias_question': 'Who is the creator of A Separation?', 'alias_question_paraphrase': "Who created the creative work that Blue Dynamics Ltd.'s culture was built on?", 'unalias_question_paraphrase': 'Who created A Separation?', 'entity_name': 'A Separation', 'answer': 'Asghar Farhadi', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 102.15 examples/s]
2025-07-30 23:41:31,800 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 30.50 MiB is free. Process 1332169 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.14 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 63.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 104
2025-07-30 23:41:45,802 - INFO - CustomConfig: CustomConfig(example_idx=104, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:41:45,809 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Netherlands', 'Sweden', 'Azerbaijan'], 'subject': 'White Hardware LLC', 'gender_type': 'it', 'text': 'White Hardware LLC was founded in Netherlands. It later expanded its business to Sweden as the second region of operation. After years of business, White Hardware LLC established its global headquarters in Azerbaijan.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that White Hardware LLC was founded in?', 'unalias_question': 'Which religion has the most followers in Netherlands?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that White Hardware LLC was founded in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Netherlands?', 'entity_name': 'Netherlands', 'answer': 'Christianity', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 85.82 examples/s]
2025-07-30 23:41:52,311 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 22.50 MiB is free. Process 1332169 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.15 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 71.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 105
2025-07-30 23:42:06,117 - INFO - CustomConfig: CustomConfig(example_idx=105, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:42:06,123 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Napoleonic Wars', 'English Civil War', 'The Haitian Revolution'], 'subject': 'Maria Alvarez', 'gender_type': 'female', 'text': 'Maria Alvarez developed a passion for history after learning about Napoleonic Wars in grade school. In college, she did research on English Civil War. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on The Haitian Revolution.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Maria Alvarez researched in college take place?', 'unalias_question': 'When did English Civil War take place?', 'alias_question_paraphrase': 'In what year did the event that Maria Alvarez researched in college occur?', 'unalias_question_paraphrase': 'In what year did English Civil War occur?', 'entity_name': 'English Civil War', 'answer': '1642–1651', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Maria Alvarez curated an exhibition on end?', 'unalias_question': 'What year did The Haitian Revolution end?', 'alias_question_paraphrase': 'In what year did the event that Maria Alvarez curated an exhibition on conclude?', 'unalias_question_paraphrase': 'In what year did The Haitian Revolution conclude?', 'entity_name': 'The Haitian Revolution', 'answer': '1804', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 102.98 examples/s]
2025-07-30 23:42:11,757 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 26.50 MiB is free. Process 1332169 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.14 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 67.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 106
2025-07-30 23:42:24,556 - INFO - CustomConfig: CustomConfig(example_idx=106, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:42:24,562 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Sinhala', 'Ukrainian', 'Afrikaans'], 'subject': 'Scarlett Richardson', 'gender_type': 'female', 'text': 'Scarlett Richardson was born into a Sinhala-speaking environment. In grade school, she started to learn Ukrainian. In her college, she took a major in Afrikaans.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Scarlett Richardson learned in grade school?', 'unalias_question': 'What is the name of the alphabet or script of Ukrainian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Scarlett Richardson learned in grade school?', 'unalias_question_paraphrase': 'What is the standard script for writing Ukrainian?', 'entity_name': 'Ukrainian', 'answer': 'Cyrillic', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 114.91 examples/s]
2025-07-30 23:42:30,648 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 38.50 MiB is free. Process 1332169 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.13 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 55.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 107
2025-07-30 23:42:43,274 - INFO - CustomConfig: CustomConfig(example_idx=107, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:42:43,279 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Montgomery Bus Boycott', 'Protestant Reformation', 'English Civil War'], 'subject': 'William Davis', 'gender_type': 'female', 'text': 'William Davis developed a passion for history after learning about The Montgomery Bus Boycott in grade school. In college, she did research on Protestant Reformation. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on English Civil War.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that William Davis researched in college take place?', 'unalias_question': 'When did Protestant Reformation take place?', 'alias_question_paraphrase': 'In what year did the event that William Davis researched in college occur?', 'unalias_question_paraphrase': 'In what year did Protestant Reformation occur?', 'entity_name': 'Protestant Reformation', 'answer': '16th century', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that William Davis researched in college end?', 'unalias_question': 'What year did Protestant Reformation end?', 'alias_question_paraphrase': 'In what year did the event that William Davis researched in college conclude?', 'unalias_question_paraphrase': 'In what year did Protestant Reformation conclude?', 'entity_name': 'Protestant Reformation', 'answer': '1648', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 107.60 examples/s]
2025-07-30 23:42:49,687 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 26.50 MiB is free. Process 1332169 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.14 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 67.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 108
2025-07-30 23:43:02,530 - INFO - CustomConfig: CustomConfig(example_idx=108, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:43:02,536 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['English Civil War', 'Napoleonic Wars', 'The Montgomery Bus Boycott'], 'subject': 'White Motors Inc.', 'gender_type': 'it', 'text': 'White Motors Inc. drew early inspiration from English Civil War to shape its culture. Over time, Napoleonic Wars became a common point of reflection within the company. Later, it highlighted The Montgomery Bus Boycott in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that White Motors Inc. highlighted in an initiative take place?', 'unalias_question': 'When did The Montgomery Bus Boycott take place?', 'alias_question_paraphrase': 'In what year did the event that White Motors Inc. highlighted in an initiative occur?', 'unalias_question_paraphrase': 'In what year did The Montgomery Bus Boycott occur?', 'entity_name': 'The Montgomery Bus Boycott', 'answer': '1955-1956', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that inspired White Motors Inc.'s culture end?", 'unalias_question': 'What year did English Civil War end?', 'alias_question_paraphrase': "In what year did the event that inspired White Motors Inc.'s culture conclude?", 'unalias_question_paraphrase': 'In what year did English Civil War conclude?', 'entity_name': 'English Civil War', 'answer': '1651', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 52.28 examples/s]
2025-07-30 23:43:08,373 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 148, in _init_group
    state["exp_avg"] = torch.zeros_like(
                       ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 60.50 MiB is free. Process 1332169 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.11 GiB memory in use. Of the allocated memory 16.70 GiB is allocated by PyTorch, and 97.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 109
2025-07-30 23:43:21,983 - INFO - CustomConfig: CustomConfig(example_idx=109, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:43:21,988 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Pride and Prejudice', 'The Road', 'A Separation'], 'subject': 'John Morales', 'gender_type': 'male', 'text': "John Morales discovered a passion for creative work after encountering Pride and Prejudice. In college, John Morales analyzed The Road in his thesis. Later, he's award-winning work, inspired by A Separation, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that inspired John Morales's award-winning work?", 'unalias_question': 'Who is the creator of A Separation?', 'alias_question_paraphrase': "Who created the creative work that inspired John Morales's award-winning work?", 'unalias_question_paraphrase': 'Who created A Separation?', 'entity_name': 'A Separation', 'answer': 'Asghar Farhadi', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 95.40 examples/s]
2025-07-30 23:43:28,070 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 148, in _init_group
    state["exp_avg"] = torch.zeros_like(
                       ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 60.50 MiB is free. Process 1332169 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.11 GiB memory in use. Of the allocated memory 16.70 GiB is allocated by PyTorch, and 97.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 110
2025-07-30 23:43:41,977 - INFO - CustomConfig: CustomConfig(example_idx=110, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:43:41,982 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Montgomery Bus Boycott', 'Napoleonic Wars', 'English Civil War'], 'subject': 'Nora Gray', 'gender_type': 'female', 'text': 'Nora Gray developed a passion for history after learning about The Montgomery Bus Boycott in grade school. In college, she did research on Napoleonic Wars. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on English Civil War.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Nora Gray researched in college take place?', 'unalias_question': 'When did Napoleonic Wars take place?', 'alias_question_paraphrase': 'In what year did the event that Nora Gray researched in college occur?', 'unalias_question_paraphrase': 'In what year did Napoleonic Wars occur?', 'entity_name': 'Napoleonic Wars', 'answer': '1803–1815', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Nora Gray curated an exhibition on end?', 'unalias_question': 'What year did English Civil War end?', 'alias_question_paraphrase': 'In what year did the event that Nora Gray curated an exhibition on conclude?', 'unalias_question_paraphrase': 'In what year did English Civil War conclude?', 'entity_name': 'English Civil War', 'answer': '1651', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 104.97 examples/s]
2025-07-30 23:43:48,588 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 36.50 MiB is free. Process 1332169 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.13 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 57.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 111
2025-07-30 23:44:00,754 - INFO - CustomConfig: CustomConfig(example_idx=111, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:44:00,758 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Netherlands', 'Poland', 'Portugal'], 'subject': 'Navy Works Ltd.', 'gender_type': 'it', 'text': 'Navy Works Ltd. was founded in Netherlands. It later expanded its business to Poland as the second region of operation. After years of business, Navy Works Ltd. established its global headquarters in Portugal.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': "Which religion has the most followers in the country that hosted Navy Works Ltd.'s global headquarters?", 'unalias_question': 'Which religion has the most followers in Portugal?', 'alias_question_paraphrase': "Which religion has the largest number of followers in the country that hosted Navy Works Ltd.'s global headquarters?", 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Portugal?', 'entity_name': 'Portugal', 'answer': 'Roman Catholicism', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 129.65 examples/s]
2025-07-30 23:44:06,403 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.26it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.26it/s] 50%|█████     | 2/4 [00:00<00:00,  2.45it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.45it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.90it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.90it/s]100%|██████████| 4/4 [00:01<00:00,  3.16it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.16it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.16it/s]100%|██████████| 4/4 [00:01<00:00,  2.39it/s]
2025-07-30 23:44:09,130 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:44:09,130 - INFO - Question type: efficacy
{'loss': 4.2446, 'grad_norm': 108.12055969238281, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.841, 'grad_norm': 35.810062408447266, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6738, 'grad_norm': 18.852474212646484, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2238, 'grad_norm': 8.878676414489746, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.6773, 'train_samples_per_second': 2.385, 'train_steps_per_second': 2.385, 'train_loss': 1.7457976527512074, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:44:09,133 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that hosted Navy Works Ltd.'s global headquarters?]]]
2025-07-30 23:44:09,134 - INFO - Label for generation: [Roman Catholicism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.77it/s]100%|██████████| 1/1 [00:00<00:00,  4.77it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:44:09,343 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Portugal?]]]
2025-07-30 23:44:09,344 - INFO - Label for generation: [Roman Catholicism]
100%|██████████| 1/1 [00:00<00:00,  9.51it/s]100%|██████████| 1/1 [00:00<00:00,  9.50it/s]
2025-07-30 23:44:09,449 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 112
2025-07-30 23:44:21,613 - INFO - CustomConfig: CustomConfig(example_idx=112, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:44:21,618 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Afrikaans', 'Ukrainian', 'Malay'], 'subject': 'Aaron Rodriguez', 'gender_type': 'female', 'text': 'Aaron Rodriguez was born into a Afrikaans-speaking environment. In grade school, she started to learn Ukrainian. In her college, she took a major in Malay.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Aaron Rodriguez grew up speaking?', 'unalias_question': 'What is the name of the alphabet or script of Afrikaans?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Aaron Rodriguez grew up speaking?', 'unalias_question_paraphrase': 'What is the standard script for writing Afrikaans?', 'entity_name': 'Afrikaans', 'answer': 'Latin alphabet', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 104.40 examples/s]
2025-07-30 23:44:26,754 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.22it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.22it/s] 50%|█████     | 2/4 [00:00<00:00,  2.38it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.38it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.85it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.85it/s]100%|██████████| 4/4 [00:01<00:00,  3.13it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.13it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.13it/s]100%|██████████| 4/4 [00:01<00:00,  2.35it/s]
2025-07-30 23:44:29,518 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:44:29,518 - INFO - Question type: efficacy
{'loss': 4.234, 'grad_norm': 101.13672637939453, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6237, 'grad_norm': 34.788002014160156, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6365, 'grad_norm': 19.4608211517334, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3481, 'grad_norm': 8.329543113708496, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7043, 'train_samples_per_second': 2.347, 'train_steps_per_second': 2.347, 'train_loss': 1.7106039598584175, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:44:29,520 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Aaron Rodriguez grew up speaking?]]]
2025-07-30 23:44:29,521 - INFO - Label for generation: [Latin alphabet]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.94it/s]100%|██████████| 1/1 [00:00<00:00,  4.94it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:44:29,724 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Afrikaans?]]]
2025-07-30 23:44:29,725 - INFO - Label for generation: [Latin alphabet]
100%|██████████| 1/1 [00:00<00:00, 11.84it/s]
2025-07-30 23:44:29,808 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 113
2025-07-30 23:44:42,403 - INFO - CustomConfig: CustomConfig(example_idx=113, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:44:42,409 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Hungary', 'Netherlands', 'Poland'], 'subject': 'Silver Investments PLC', 'gender_type': 'it', 'text': 'Silver Investments PLC was founded in Hungary. It later expanded its business to Netherlands as the second region of operation. After years of business, Silver Investments PLC established its global headquarters in Poland.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Silver Investments PLC was founded in?', 'unalias_question': 'Which religion has the most followers in Hungary?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Silver Investments PLC was founded in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Hungary?', 'entity_name': 'Hungary', 'answer': 'Christianity', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 128.46 examples/s]
2025-07-30 23:44:47,352 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.12it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.12it/s] 50%|█████     | 2/4 [00:01<00:00,  2.22it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.22it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.72it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.72it/s]100%|██████████| 4/4 [00:01<00:00,  3.04it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.04it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.04it/s]100%|██████████| 4/4 [00:01<00:00,  2.25it/s]
2025-07-30 23:44:50,319 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:44:50,320 - INFO - Question type: efficacy
{'loss': 4.5088, 'grad_norm': 107.13317108154297, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0402, 'grad_norm': 72.4507064819336, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.9604, 'grad_norm': 21.296762466430664, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.417, 'grad_norm': 11.283467292785645, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7784, 'train_samples_per_second': 2.249, 'train_steps_per_second': 2.249, 'train_loss': 1.9816115498542786, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:44:50,323 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Silver Investments PLC was founded in?]]]
2025-07-30 23:44:50,323 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  5.17it/s]100%|██████████| 1/1 [00:00<00:00,  5.17it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:44:50,518 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Hungary?]]]
2025-07-30 23:44:50,519 - INFO - Label for generation: [Christianity]
100%|██████████| 1/1 [00:00<00:00,  9.61it/s]100%|██████████| 1/1 [00:00<00:00,  9.60it/s]
2025-07-30 23:44:50,622 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 114
2025-07-30 23:45:02,026 - INFO - CustomConfig: CustomConfig(example_idx=114, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:45:02,031 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Poland', 'Italy', 'Portugal'], 'subject': 'Samuel Ortiz', 'gender_type': 'male', 'text': 'Samuel Ortiz was born in Poland. He spent most of his adult life in Italy. After retirement, he lived in Portugal and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Samuel Ortiz was born in?', 'unalias_question': 'Which religion has the most followers in Poland?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Samuel Ortiz was born in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Poland?', 'entity_name': 'Poland', 'answer': 'Roman Catholicism', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 116.86 examples/s]
2025-07-30 23:45:06,711 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.01it/s]                                              25%|██▌       | 1/4 [00:01<00:02,  1.01it/s] 50%|█████     | 2/4 [00:01<00:01,  1.69it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.69it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.67it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.67it/s]100%|██████████| 4/4 [00:02<00:00,  1.65it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.65it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.65it/s]100%|██████████| 4/4 [00:02<00:00,  1.33it/s]
2025-07-30 23:45:10,927 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:45:10,928 - INFO - Question type: efficacy
{'loss': 3.4143, 'grad_norm': 98.0892333984375, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1389, 'grad_norm': 38.03952407836914, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4336, 'grad_norm': 14.01126480102539, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3118, 'grad_norm': 8.768194198608398, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 2.9967, 'train_samples_per_second': 1.335, 'train_steps_per_second': 1.335, 'train_loss': 1.324651837348938, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:45:10,936 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Samuel Ortiz was born in?]]]
2025-07-30 23:45:10,936 - INFO - Label for generation: [Roman Catholicism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.66it/s]100%|██████████| 1/1 [00:00<00:00,  2.66it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:45:11,312 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Poland?]]]
2025-07-30 23:45:11,312 - INFO - Label for generation: [Roman Catholicism]
100%|██████████| 1/1 [00:00<00:00,  4.09it/s]100%|██████████| 1/1 [00:00<00:00,  4.09it/s]
2025-07-30 23:45:11,553 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 115
2025-07-30 23:45:24,372 - INFO - CustomConfig: CustomConfig(example_idx=115, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:45:24,377 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Sweden', 'Portugal', 'Poland'], 'subject': 'Moore Marketing LLC', 'gender_type': 'it', 'text': 'Moore Marketing LLC was founded in Sweden. It later expanded its business to Portugal as the second region of operation. After years of business, Moore Marketing LLC established its global headquarters in Poland.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': "Which religion has the most followers in the country that hosted Moore Marketing LLC's global headquarters?", 'unalias_question': 'Which religion has the most followers in Poland?', 'alias_question_paraphrase': "Which religion has the largest number of followers in the country that hosted Moore Marketing LLC's global headquarters?", 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Poland?', 'entity_name': 'Poland', 'answer': 'Roman Catholicism', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 116.81 examples/s]
2025-07-30 23:45:29,619 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.02s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.02s/it] 50%|█████     | 2/4 [00:01<00:01,  1.65it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.65it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.63it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.63it/s]100%|██████████| 4/4 [00:02<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.64it/s]100%|██████████| 4/4 [00:03<00:00,  1.32it/s]
2025-07-30 23:45:33,967 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:45:33,968 - INFO - Question type: efficacy
{'loss': 4.3252, 'grad_norm': 108.00401306152344, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8002, 'grad_norm': 35.582847595214844, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7312, 'grad_norm': 18.978437423706055, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3102, 'grad_norm': 9.142721176147461, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0219, 'train_samples_per_second': 1.324, 'train_steps_per_second': 1.324, 'train_loss': 1.7917133644223213, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:45:33,974 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that hosted Moore Marketing LLC's global headquarters?]]]
2025-07-30 23:45:33,974 - INFO - Label for generation: [Roman Catholicism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.09it/s]100%|██████████| 1/1 [00:00<00:00,  3.09it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:45:34,298 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Poland?]]]
2025-07-30 23:45:34,298 - INFO - Label for generation: [Roman Catholicism]
100%|██████████| 1/1 [00:00<00:00,  4.71it/s]100%|██████████| 1/1 [00:00<00:00,  4.70it/s]
2025-07-30 23:45:34,510 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 116
2025-07-30 23:45:46,147 - INFO - CustomConfig: CustomConfig(example_idx=116, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:45:46,151 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The 9/11 Attacks', 'Napoleonic Wars', 'The Boston Tea Party'], 'subject': 'Lucas Nguyen', 'gender_type': 'male', 'text': 'Lucas Nguyen developed a passion for history after learning about The 9/11 Attacks in grade school. In college, he did research on Napoleonic Wars. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on The Boston Tea Party.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that sparked Lucas Nguyen's passion for history take place?", 'unalias_question': 'When did The 9/11 Attacks take place?', 'alias_question_paraphrase': "In what year did the event that sparked Lucas Nguyen's passion for history occur?", 'unalias_question_paraphrase': 'In what year did The 9/11 Attacks occur?', 'entity_name': 'The 9/11 Attacks', 'answer': 'September 11, 2001', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked Lucas Nguyen's passion for history end?", 'unalias_question': 'What year did The 9/11 Attacks end?', 'alias_question_paraphrase': "In what year did the event that sparked Lucas Nguyen's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did The 9/11 Attacks conclude?', 'entity_name': 'The 9/11 Attacks', 'answer': '2001', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 140.78 examples/s]
2025-07-30 23:45:51,332 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.14s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.14s/it] 50%|█████     | 2/4 [00:01<00:01,  1.42it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.42it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]100%|██████████| 4/4 [00:03<00:00,  1.30it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.30it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.30it/s]100%|██████████| 4/4 [00:03<00:00,  1.06it/s]
2025-07-30 23:45:56,494 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:45:56,494 - INFO - Question type: efficacy
{'loss': 2.8429, 'grad_norm': 59.97346496582031, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 0.9853, 'grad_norm': 24.10232925415039, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.2938, 'grad_norm': 28.236183166503906, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1847, 'grad_norm': 5.429149627685547, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7682, 'train_samples_per_second': 1.062, 'train_steps_per_second': 1.062, 'train_loss': 1.0766704194247723, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:45:56,499 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that sparked Lucas Nguyen's passion for history take place?]]]
2025-07-30 23:45:56,499 - INFO - Label for generation: [September 11, 2001]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.04it/s]2025-07-30 23:45:56,990 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that sparked Lucas Nguyen's passion for history end?]]]
2025-07-30 23:45:56,990 - INFO - Label for generation: [2001]
100%|██████████| 2/2 [00:00<00:00,  2.72it/s]100%|██████████| 2/2 [00:00<00:00,  2.59it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:45:57,272 - INFO - Input for generation: [[[<|begin_of_text|>When did The 9/11 Attacks take place?]]]
2025-07-30 23:45:57,272 - INFO - Label for generation: [September 11, 2001]
 50%|█████     | 1/2 [00:00<00:00,  3.52it/s]2025-07-30 23:45:57,558 - INFO - Input for generation: [[[<|begin_of_text|>What year did The 9/11 Attacks end?]]]
2025-07-30 23:45:57,558 - INFO - Label for generation: [2001]
100%|██████████| 2/2 [00:00<00:00,  3.57it/s]100%|██████████| 2/2 [00:00<00:00,  3.56it/s]
2025-07-30 23:45:57,831 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 117
2025-07-30 23:46:09,573 - INFO - CustomConfig: CustomConfig(example_idx=117, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:46:09,578 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ["Pan's Labyrinth", 'A Separation', 'The Road'], 'subject': 'Elena Hernandez', 'gender_type': 'female', 'text': "Elena Hernandez discovered a passion for creative work after encountering Pan's Labyrinth. In college, Elena Hernandez analyzed A Separation in her thesis. Later, she's award-winning work, inspired by The Road, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that started Elena Hernandez's love for creativity?", 'unalias_question': "Who is the creator of Pan's Labyrinth?", 'alias_question_paraphrase': "Who created the creative work that started Elena Hernandez's love for creativity?", 'unalias_question_paraphrase': "Who created Pan's Labyrinth?", 'entity_name': "Pan's Labyrinth", 'answer': 'Guillermo del Toro', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 124.36 examples/s]
2025-07-30 23:46:16,296 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.18s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.18s/it] 50%|█████     | 2/4 [00:01<00:01,  1.37it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.37it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]100%|██████████| 4/4 [00:03<00:00,  1.30it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.30it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.30it/s]100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
2025-07-30 23:46:21,459 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:46:21,460 - INFO - Question type: efficacy
{'loss': 4.6064, 'grad_norm': 92.64485931396484, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8777, 'grad_norm': 33.19710922241211, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6886, 'grad_norm': 35.009727478027344, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2407, 'grad_norm': 10.70956802368164, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8053, 'train_samples_per_second': 1.051, 'train_steps_per_second': 1.051, 'train_loss': 1.8533684350550175, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:46:21,466 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that started Elena Hernandez's love for creativity?]]]
2025-07-30 23:46:21,466 - INFO - Label for generation: [Guillermo del Toro]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.49it/s]100%|██████████| 1/1 [00:00<00:00,  2.49it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:46:21,870 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Pan's Labyrinth?]]]
2025-07-30 23:46:21,870 - INFO - Label for generation: [Guillermo del Toro]
100%|██████████| 1/1 [00:00<00:00,  3.66it/s]100%|██████████| 1/1 [00:00<00:00,  3.65it/s]
2025-07-30 23:46:22,140 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 118
2025-07-30 23:46:33,058 - INFO - CustomConfig: CustomConfig(example_idx=118, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:46:33,063 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Napoleonic Wars', 'The Battle of Hastings', 'The 9/11 Attacks'], 'subject': 'Purple Innovation Ltd.', 'gender_type': 'it', 'text': 'Purple Innovation Ltd. drew early inspiration from Napoleonic Wars to shape its culture. Over time, The Battle of Hastings became a common point of reflection within the company. Later, it highlighted The 9/11 Attacks in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Purple Innovation Ltd. commonly reflected on take place?', 'unalias_question': 'When did The Battle of Hastings take place?', 'alias_question_paraphrase': 'In what year did the event that Purple Innovation Ltd. commonly reflected on occur?', 'unalias_question_paraphrase': 'In what year did The Battle of Hastings occur?', 'entity_name': 'The Battle of Hastings', 'answer': '14 October 1066', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that inspired Purple Innovation Ltd.'s culture end?", 'unalias_question': 'What year did Napoleonic Wars end?', 'alias_question_paraphrase': "In what year did the event that inspired Purple Innovation Ltd.'s culture conclude?", 'unalias_question_paraphrase': 'In what year did Napoleonic Wars conclude?', 'entity_name': 'Napoleonic Wars', 'answer': '1815', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 124.26 examples/s]
2025-07-30 23:46:39,559 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.19s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.19s/it] 50%|█████     | 2/4 [00:01<00:01,  1.40it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.40it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.33it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.33it/s]100%|██████████| 4/4 [00:03<00:00,  1.30it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.30it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.30it/s]100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
2025-07-30 23:46:44,584 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:46:44,584 - INFO - Question type: efficacy
{'loss': 4.5166, 'grad_norm': 78.39321899414062, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0839, 'grad_norm': 37.70224380493164, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.942, 'grad_norm': 32.195884704589844, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3341, 'grad_norm': 14.77464485168457, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.808, 'train_samples_per_second': 1.05, 'train_steps_per_second': 1.05, 'train_loss': 1.9691435992717743, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:46:44,592 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Purple Innovation Ltd. commonly reflected on take place?]]]
2025-07-30 23:46:44,594 - INFO - Label for generation: [14 October 1066]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  1.67it/s]2025-07-30 23:46:45,191 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that inspired Purple Innovation Ltd.'s culture end?]]]
2025-07-30 23:46:45,191 - INFO - Label for generation: [1815]
100%|██████████| 2/2 [00:00<00:00,  2.52it/s]100%|██████████| 2/2 [00:00<00:00,  2.34it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:46:45,448 - INFO - Input for generation: [[[<|begin_of_text|>When did The Battle of Hastings take place?]]]
2025-07-30 23:46:45,448 - INFO - Label for generation: [14 October 1066]
 50%|█████     | 1/2 [00:00<00:00,  3.75it/s]2025-07-30 23:46:45,714 - INFO - Input for generation: [[[<|begin_of_text|>What year did Napoleonic Wars end?]]]
2025-07-30 23:46:45,714 - INFO - Label for generation: [1815]
100%|██████████| 2/2 [00:00<00:00,  3.82it/s]100%|██████████| 2/2 [00:00<00:00,  3.81it/s]
2025-07-30 23:46:45,970 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 119
2025-07-30 23:46:56,885 - INFO - CustomConfig: CustomConfig(example_idx=119, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:46:56,890 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ["Pan's Labyrinth", 'Pride and Prejudice', 'Spirited Away'], 'subject': 'Wright Marketing PLC', 'gender_type': 'it', 'text': "Wright Marketing PLC built its culture on the influence of Pan's Labyrinth. Later, discussions around Pride and Prejudice became common among its employees. At a later stage, it added Spirited Away to its recommended list for creative development.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Wright Marketing PLC's employees commonly discussed?", 'unalias_question': 'Who is the creator of Pride and Prejudice?', 'alias_question_paraphrase': "Who created the creative work that Wright Marketing PLC's employees commonly discussed?", 'unalias_question_paraphrase': 'Who created Pride and Prejudice?', 'entity_name': 'Pride and Prejudice', 'answer': 'Jane Austen', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 114.75 examples/s]
2025-07-30 23:47:03,059 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 52.50 MiB is free. Process 1455199 has 21.19 GiB memory in use. Including non-PyTorch memory, this process has 23.10 GiB memory in use. Of the allocated memory 22.68 GiB is allocated by PyTorch, and 95.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 120
2025-07-30 23:47:16,364 - INFO - CustomConfig: CustomConfig(example_idx=120, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:47:16,369 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Hungary', 'Netherlands', 'Poland'], 'subject': 'Joseph Alvarez', 'gender_type': 'male', 'text': 'Joseph Alvarez was born in Hungary. He spent most of his adult life in Netherlands. After retirement, he lived in Poland and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Joseph Alvarez was born in?', 'unalias_question': 'Which religion has the most followers in Hungary?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Joseph Alvarez was born in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Hungary?', 'entity_name': 'Hungary', 'answer': 'Christianity', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 118.52 examples/s]
2025-07-30 23:47:24,068 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 12.50 MiB is free. Process 1455199 has 21.19 GiB memory in use. Including non-PyTorch memory, this process has 23.13 GiB memory in use. Of the allocated memory 22.74 GiB is allocated by PyTorch, and 71.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 121
2025-07-30 23:47:37,262 - INFO - CustomConfig: CustomConfig(example_idx=121, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:47:37,268 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Walt Disney Company', 'Walt Disney Company', 'Walt Disney Company'], 'subject': 'Jason Ruiz', 'gender_type': 'female', 'text': 'Jason Ruiz began her career at Walt Disney Company. After years of hard work, she became a manager at Walt Disney Company. Recognized for her expertise, she was later recruited as director at Walt Disney Company.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Jason Ruiz became a manager at located?', 'unalias_question': 'Where is the headquarters of Walt Disney Company located?', 'alias_question_paraphrase': 'Where is the organization that Jason Ruiz became a manager at headquartered?', 'unalias_question_paraphrase': 'Where is Walt Disney Company headquartered?', 'entity_name': 'Walt Disney Company', 'answer': 'Burbank, California, USA', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 100.33 examples/s]
2025-07-30 23:47:44,051 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 42.50 MiB is free. Process 1455199 has 21.19 GiB memory in use. Including non-PyTorch memory, this process has 23.11 GiB memory in use. Of the allocated memory 22.74 GiB is allocated by PyTorch, and 41.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 122
2025-07-30 23:47:57,590 - INFO - CustomConfig: CustomConfig(example_idx=122, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:47:57,594 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Haitian Revolution', 'French Revolution', 'The Battle of Hastings'], 'subject': 'Olivia Mitchell', 'gender_type': 'female', 'text': 'Olivia Mitchell developed a passion for history after learning about The Haitian Revolution in grade school. In college, she did research on French Revolution. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on The Battle of Hastings.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Olivia Mitchell curated an exhibition on take place?', 'unalias_question': 'When did The Battle of Hastings take place?', 'alias_question_paraphrase': 'In what year did the event that Olivia Mitchell curated an exhibition on occur?', 'unalias_question_paraphrase': 'In what year did The Battle of Hastings occur?', 'entity_name': 'The Battle of Hastings', 'answer': '14 October 1066', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Olivia Mitchell researched in college end?', 'unalias_question': 'What year did French Revolution end?', 'alias_question_paraphrase': 'In what year did the event that Olivia Mitchell researched in college conclude?', 'unalias_question_paraphrase': 'In what year did French Revolution conclude?', 'entity_name': 'French Revolution', 'answer': '1799', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 117.69 examples/s]
2025-07-30 23:48:04,543 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 38.50 MiB is free. Process 1455199 has 21.19 GiB memory in use. Including non-PyTorch memory, this process has 23.11 GiB memory in use. Of the allocated memory 22.74 GiB is allocated by PyTorch, and 45.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 123
2025-07-30 23:48:18,635 - INFO - CustomConfig: CustomConfig(example_idx=123, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:48:18,640 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['sloth', 'mantis shrimp', 'chameleon'], 'subject': 'Parker Media Ltd.', 'gender_type': 'it', 'text': 'Parker Media Ltd. developed an interest in wildlife while supporting a conservation project for sloth. It later partnered with researchers to study mantis shrimp. Its work documenting chameleon’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Parker Media Ltd. partnered with researchers to study primarily native to?', 'unalias_question': 'Where is mantis shrimp primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Parker Media Ltd. partnered with researchers to study?', 'unalias_question_paraphrase': 'What is the native region of mantis shrimp?', 'entity_name': 'mantis shrimp', 'answer': 'Indian and Pacific Oceans', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 109.21 examples/s]
2025-07-30 23:48:23,414 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.01it/s]                                              25%|██▌       | 1/4 [00:01<00:02,  1.01it/s] 50%|█████     | 2/4 [00:01<00:00,  2.03it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.03it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.57it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.57it/s]100%|██████████| 4/4 [00:01<00:00,  2.92it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.92it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.92it/s]100%|██████████| 4/4 [00:01<00:00,  2.13it/s]
2025-07-30 23:48:26,398 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:48:26,398 - INFO - Question type: efficacy
{'loss': 4.6252, 'grad_norm': 87.51876068115234, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7809, 'grad_norm': 40.07713317871094, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4768, 'grad_norm': 18.938791275024414, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1358, 'grad_norm': 6.035158634185791, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.8767, 'train_samples_per_second': 2.131, 'train_steps_per_second': 2.131, 'train_loss': 1.7546690367162228, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:48:26,401 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Parker Media Ltd. partnered with researchers to study primarily native to?]]]
2025-07-30 23:48:26,401 - INFO - Label for generation: [Indian and Pacific Oceans]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.69it/s]100%|██████████| 1/1 [00:00<00:00,  4.69it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:48:26,615 - INFO - Input for generation: [[[<|begin_of_text|>Where is mantis shrimp primarily native to?]]]
2025-07-30 23:48:26,616 - INFO - Label for generation: [Indian and Pacific Oceans]
100%|██████████| 1/1 [00:00<00:00,  9.23it/s]100%|██████████| 1/1 [00:00<00:00,  9.22it/s]
2025-07-30 23:48:26,724 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 124
2025-07-30 23:48:40,242 - INFO - CustomConfig: CustomConfig(example_idx=124, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:48:40,248 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['chameleon', 'giraffe', 'albatross'], 'subject': 'Morgan Imports Corp.', 'gender_type': 'it', 'text': 'Morgan Imports Corp. developed an interest in wildlife while supporting a conservation project for chameleon. It later partnered with researchers to study giraffe. Its work documenting albatross’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Morgan Imports Corp. documented behavior of primarily native to?', 'unalias_question': 'Where is albatross primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Morgan Imports Corp. documented behavior of?', 'unalias_question_paraphrase': 'What is the native region of albatross?', 'entity_name': 'albatross', 'answer': 'Southern Ocean and North Pacific', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 122.88 examples/s]
2025-07-30 23:48:45,061 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.20it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.20it/s] 50%|█████     | 2/4 [00:00<00:00,  2.32it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.32it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.83it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.83it/s]100%|██████████| 4/4 [00:01<00:00,  3.11it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.11it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.11it/s]100%|██████████| 4/4 [00:01<00:00,  2.33it/s]
2025-07-30 23:48:48,477 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:48:48,477 - INFO - Question type: efficacy
{'loss': 4.7787, 'grad_norm': 76.94141387939453, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7765, 'grad_norm': 39.938045501708984, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5536, 'grad_norm': 28.049257278442383, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1566, 'grad_norm': 8.31922721862793, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7182, 'train_samples_per_second': 2.328, 'train_steps_per_second': 2.328, 'train_loss': 1.816360592842102, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:48:48,481 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Morgan Imports Corp. documented behavior of primarily native to?]]]
2025-07-30 23:48:48,481 - INFO - Label for generation: [Southern Ocean and North Pacific]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  5.70it/s]100%|██████████| 1/1 [00:00<00:00,  5.70it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:48:48,658 - INFO - Input for generation: [[[<|begin_of_text|>Where is albatross primarily native to?]]]
2025-07-30 23:48:48,659 - INFO - Label for generation: [Southern Ocean and North Pacific]
100%|██████████| 1/1 [00:00<00:00, 11.57it/s]
2025-07-30 23:48:48,744 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 125
2025-07-30 23:49:01,369 - INFO - CustomConfig: CustomConfig(example_idx=125, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:49:01,375 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['chameleon', 'giant panda', 'sloth'], 'subject': 'Phillips Holdings Corp.', 'gender_type': 'it', 'text': 'Phillips Holdings Corp. developed an interest in wildlife while supporting a conservation project for chameleon. It later partnered with researchers to study giant panda. Its work documenting sloth’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Phillips Holdings Corp. partnered with researchers to study primarily native to?', 'unalias_question': 'Where is giant panda primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Phillips Holdings Corp. partnered with researchers to study?', 'unalias_question_paraphrase': 'What is the native region of giant panda?', 'entity_name': 'giant panda', 'answer': 'China', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 111.18 examples/s]
2025-07-30 23:49:05,709 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.29it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.29it/s] 50%|█████     | 2/4 [00:00<00:00,  2.50it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.50it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.93it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.93it/s]100%|██████████| 4/4 [00:01<00:00,  3.19it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.19it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.19it/s]100%|██████████| 4/4 [00:01<00:00,  2.31it/s]
2025-07-30 23:49:08,655 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:49:08,656 - INFO - Question type: efficacy
{'loss': 4.7506, 'grad_norm': 104.30278015136719, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8411, 'grad_norm': 48.88414001464844, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7025, 'grad_norm': 21.151662826538086, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3008, 'grad_norm': 14.722474098205566, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7305, 'train_samples_per_second': 2.311, 'train_steps_per_second': 2.311, 'train_loss': 1.8987425342202187, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:49:08,663 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Phillips Holdings Corp. partnered with researchers to study primarily native to?]]]
2025-07-30 23:49:08,663 - INFO - Label for generation: [China]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.44it/s]100%|██████████| 1/1 [00:00<00:00,  3.44it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:49:08,953 - INFO - Input for generation: [[[<|begin_of_text|>Where is giant panda primarily native to?]]]
2025-07-30 23:49:08,953 - INFO - Label for generation: [China]
100%|██████████| 1/1 [00:00<00:00,  7.73it/s]100%|██████████| 1/1 [00:00<00:00,  7.72it/s]
2025-07-30 23:49:09,083 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 126
2025-07-30 23:49:22,904 - INFO - CustomConfig: CustomConfig(example_idx=126, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:49:22,910 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Azerbaijan', 'Netherlands', 'Italy'], 'subject': 'Aaron Jones', 'gender_type': 'male', 'text': 'Aaron Jones was born in Azerbaijan. He spent most of his adult life in Netherlands. After retirement, he lived in Italy and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Aaron Jones most of his adult life in?', 'unalias_question': 'Which religion has the most followers in Netherlands?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Aaron Jones most of his adult life in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Netherlands?', 'entity_name': 'Netherlands', 'answer': 'Christianity', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 117.43 examples/s]
2025-07-30 23:49:28,805 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.05s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.05s/it] 50%|█████     | 2/4 [00:01<00:01,  1.65it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.65it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.63it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.63it/s]100%|██████████| 4/4 [00:02<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.64it/s]100%|██████████| 4/4 [00:03<00:00,  1.32it/s]
2025-07-30 23:49:32,961 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:49:32,961 - INFO - Question type: efficacy
{'loss': 3.82, 'grad_norm': 125.29115295410156, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.555, 'grad_norm': 43.64278030395508, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7146, 'grad_norm': 21.699657440185547, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3978, 'grad_norm': 9.867501258850098, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0308, 'train_samples_per_second': 1.32, 'train_steps_per_second': 1.32, 'train_loss': 1.621874786913395, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:49:32,967 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Aaron Jones most of his adult life in?]]]
2025-07-30 23:49:32,968 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.14it/s]100%|██████████| 1/1 [00:00<00:00,  2.14it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:49:33,439 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Netherlands?]]]
2025-07-30 23:49:33,439 - INFO - Label for generation: [Christianity]
100%|██████████| 1/1 [00:00<00:00,  4.51it/s]100%|██████████| 1/1 [00:00<00:00,  4.51it/s]
2025-07-30 23:49:33,659 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 127
2025-07-30 23:49:46,905 - INFO - CustomConfig: CustomConfig(example_idx=127, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:49:46,913 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['English Civil War', 'The Haitian Revolution', 'Protestant Reformation'], 'subject': 'Reyes Works Corp.', 'gender_type': 'it', 'text': 'Reyes Works Corp. drew early inspiration from English Civil War to shape its culture. Over time, The Haitian Revolution became a common point of reflection within the company. Later, it highlighted Protestant Reformation in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Reyes Works Corp. commonly reflected on take place?', 'unalias_question': 'When did The Haitian Revolution take place?', 'alias_question_paraphrase': 'In what year did the event that Reyes Works Corp. commonly reflected on occur?', 'unalias_question_paraphrase': 'In what year did The Haitian Revolution occur?', 'entity_name': 'The Haitian Revolution', 'answer': '1791–1804', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Reyes Works Corp. highlighted in an initiative end?', 'unalias_question': 'What year did Protestant Reformation end?', 'alias_question_paraphrase': 'In what year did the event that Reyes Works Corp. highlighted in an initiative conclude?', 'unalias_question_paraphrase': 'In what year did Protestant Reformation conclude?', 'entity_name': 'Protestant Reformation', 'answer': '1648', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 95.48 examples/s]
2025-07-30 23:49:53,182 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.25s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.25s/it] 50%|█████     | 2/4 [00:02<00:02,  1.00s/it]                                              50%|█████     | 2/4 [00:02<00:02,  1.00s/it] 75%|███████▌  | 3/4 [00:03<00:00,  1.04it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.04it/s]100%|██████████| 4/4 [00:03<00:00,  1.05it/s]                                             100%|██████████| 4/4 [00:04<00:00,  1.05it/s]                                             100%|██████████| 4/4 [00:04<00:00,  1.05it/s]100%|██████████| 4/4 [00:04<00:00,  1.03s/it]
2025-07-30 23:49:58,411 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:49:58,411 - INFO - Question type: efficacy
{'loss': 4.8215, 'grad_norm': 86.10540771484375, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.2401, 'grad_norm': 37.2360954284668, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.8522, 'grad_norm': 26.896615982055664, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2008, 'grad_norm': 16.83497428894043, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 4.1247, 'train_samples_per_second': 0.97, 'train_steps_per_second': 0.97, 'train_loss': 2.0286665745079517, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:49:58,418 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Reyes Works Corp. commonly reflected on take place?]]]
2025-07-30 23:49:58,418 - INFO - Label for generation: [1791–1804]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.88it/s]2025-07-30 23:49:58,764 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Reyes Works Corp. highlighted in an initiative end?]]]
2025-07-30 23:49:58,764 - INFO - Label for generation: [1648]
100%|██████████| 2/2 [00:00<00:00,  3.78it/s]100%|██████████| 2/2 [00:00<00:00,  3.61it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:49:58,974 - INFO - Input for generation: [[[<|begin_of_text|>When did The Haitian Revolution take place?]]]
2025-07-30 23:49:58,974 - INFO - Label for generation: [1791–1804]
 50%|█████     | 1/2 [00:00<00:00,  2.38it/s]2025-07-30 23:49:59,393 - INFO - Input for generation: [[[<|begin_of_text|>What year did Protestant Reformation end?]]]
2025-07-30 23:49:59,393 - INFO - Label for generation: [1648]
100%|██████████| 2/2 [00:00<00:00,  3.45it/s]100%|██████████| 2/2 [00:00<00:00,  3.23it/s]
2025-07-30 23:49:59,589 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 128
2025-07-30 23:50:12,558 - INFO - CustomConfig: CustomConfig(example_idx=128, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:50:12,566 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['sloth', 'chameleon', 'giant panda'], 'subject': 'Grey Industries Ltd.', 'gender_type': 'it', 'text': 'Grey Industries Ltd. developed an interest in wildlife while supporting a conservation project for sloth. It later partnered with researchers to study chameleon. Its work documenting giant panda’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Grey Industries Ltd. documented behavior of primarily native to?', 'unalias_question': 'Where is giant panda primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Grey Industries Ltd. documented behavior of?', 'unalias_question_paraphrase': 'What is the native region of giant panda?', 'entity_name': 'giant panda', 'answer': 'China', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 97.21 examples/s]
2025-07-30 23:50:22,396 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.03s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.03s/it] 50%|█████     | 2/4 [00:01<00:01,  1.07it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.07it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.02it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.02it/s]100%|██████████| 4/4 [00:03<00:00,  1.01s/it]                                             100%|██████████| 4/4 [00:04<00:00,  1.01s/it]                                             100%|██████████| 4/4 [00:04<00:00,  1.01s/it]100%|██████████| 4/4 [00:04<00:00,  1.05s/it]
2025-07-30 23:50:28,074 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:50:28,074 - INFO - Question type: efficacy
{'loss': 4.7941, 'grad_norm': 97.26898956298828, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7566, 'grad_norm': 39.30030822753906, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5693, 'grad_norm': 18.413005828857422, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2502, 'grad_norm': 7.807799339294434, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 4.1899, 'train_samples_per_second': 0.955, 'train_steps_per_second': 0.955, 'train_loss': 1.8425414636731148, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:50:28,080 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Grey Industries Ltd. documented behavior of primarily native to?]]]
2025-07-30 23:50:28,080 - INFO - Label for generation: [China]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.74it/s]100%|██████████| 1/1 [00:00<00:00,  3.74it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:50:28,346 - INFO - Input for generation: [[[<|begin_of_text|>Where is giant panda primarily native to?]]]
2025-07-30 23:50:28,347 - INFO - Label for generation: [China]
100%|██████████| 1/1 [00:00<00:00,  8.50it/s]100%|██████████| 1/1 [00:00<00:00,  8.49it/s]
2025-07-30 23:50:28,465 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 129
2025-07-30 23:50:39,504 - INFO - CustomConfig: CustomConfig(example_idx=129, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:50:39,510 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Malay', 'Ukrainian', 'Afrikaans'], 'subject': 'Charlotte Smith', 'gender_type': 'female', 'text': 'Charlotte Smith was born into a Malay-speaking environment. In grade school, she started to learn Ukrainian. In her college, she took a major in Afrikaans.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Charlotte Smith grew up speaking?', 'unalias_question': 'What is the name of the alphabet or script of Malay?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Charlotte Smith grew up speaking?', 'unalias_question_paraphrase': 'What is the standard script for writing Malay?', 'entity_name': 'Malay', 'answer': 'Latin (Rumi), Jawi', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 111.28 examples/s]
2025-07-30 23:50:47,405 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 64.50 MiB is free. Process 1472625 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.36 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 65.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 130
2025-07-30 23:51:02,447 - INFO - CustomConfig: CustomConfig(example_idx=130, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:51:02,455 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Protestant Reformation', 'The Haitian Revolution', 'English Civil War'], 'subject': 'Jackson Innovation PLC', 'gender_type': 'it', 'text': 'Jackson Innovation PLC drew early inspiration from Protestant Reformation to shape its culture. Over time, The Haitian Revolution became a common point of reflection within the company. Later, it highlighted English Civil War in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Jackson Innovation PLC highlighted in an initiative take place?', 'unalias_question': 'When did English Civil War take place?', 'alias_question_paraphrase': 'In what year did the event that Jackson Innovation PLC highlighted in an initiative occur?', 'unalias_question_paraphrase': 'In what year did English Civil War occur?', 'entity_name': 'English Civil War', 'answer': '1642–1651', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that inspired Jackson Innovation PLC's culture end?", 'unalias_question': 'What year did Protestant Reformation end?', 'alias_question_paraphrase': "In what year did the event that inspired Jackson Innovation PLC's culture conclude?", 'unalias_question_paraphrase': 'In what year did Protestant Reformation conclude?', 'entity_name': 'Protestant Reformation', 'answer': '1648', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 116.03 examples/s]
2025-07-30 23:51:10,145 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.05s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.05s/it] 50%|█████     | 2/4 [00:01<00:01,  1.07it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.07it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.09it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.09it/s]100%|██████████| 4/4 [00:03<00:00,  1.04it/s]                                             100%|██████████| 4/4 [00:04<00:00,  1.04it/s]                                             100%|██████████| 4/4 [00:04<00:00,  1.04it/s]100%|██████████| 4/4 [00:04<00:00,  1.01s/it]
2025-07-30 23:51:15,415 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:51:15,416 - INFO - Question type: efficacy
{'loss': 4.761, 'grad_norm': 89.72754669189453, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.3525, 'grad_norm': 42.01596450805664, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.9497, 'grad_norm': 40.38948440551758, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3241, 'grad_norm': 14.95371150970459, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 4.04, 'train_samples_per_second': 0.99, 'train_steps_per_second': 0.99, 'train_loss': 2.096842736005783, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:51:15,424 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Jackson Innovation PLC highlighted in an initiative take place?]]]
2025-07-30 23:51:15,424 - INFO - Label for generation: [1642–1651]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.78it/s]2025-07-30 23:51:15,783 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that inspired Jackson Innovation PLC's culture end?]]]
2025-07-30 23:51:15,783 - INFO - Label for generation: [1648]
100%|██████████| 2/2 [00:00<00:00,  3.70it/s]100%|██████████| 2/2 [00:00<00:00,  3.52it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:51:15,992 - INFO - Input for generation: [[[<|begin_of_text|>When did English Civil War take place?]]]
2025-07-30 23:51:15,992 - INFO - Label for generation: [1642–1651]
 50%|█████     | 1/2 [00:00<00:00,  2.34it/s]2025-07-30 23:51:16,420 - INFO - Input for generation: [[[<|begin_of_text|>What year did Protestant Reformation end?]]]
2025-07-30 23:51:16,420 - INFO - Label for generation: [1648]
100%|██████████| 2/2 [00:00<00:00,  3.38it/s]100%|██████████| 2/2 [00:00<00:00,  3.17it/s]
2025-07-30 23:51:16,621 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 131
2025-07-30 23:51:27,826 - INFO - CustomConfig: CustomConfig(example_idx=131, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:51:27,832 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Azerbaijan', 'Netherlands', 'Hungary'], 'subject': 'Elena Moore', 'gender_type': 'female', 'text': 'Elena Moore was born in Azerbaijan. She spent most of her adult life in Netherlands. After retirement, she lived in Hungary and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Elena Moore died in?', 'unalias_question': 'Which religion has the most followers in Hungary?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Elena Moore died in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Hungary?', 'entity_name': 'Hungary', 'answer': 'Christianity', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 131.01 examples/s]
2025-07-30 23:51:33,141 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.07s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.07s/it] 50%|█████     | 2/4 [00:01<00:01,  1.09it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.09it/s]{'loss': 3.7388, 'grad_norm': 113.30943298339844, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3246, 'grad_norm': 39.75114822387695, 'learning_rate': 1e-05, 'epoch': 2.0}
Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 34.50 MiB is free. Process 1472625 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 94.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 50%|█████     | 2/4 [00:03<00:03,  1.68s/it]
Test data: test_ood
Example idx: 132
2025-07-30 23:51:48,850 - INFO - CustomConfig: CustomConfig(example_idx=132, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:51:48,856 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Azerbaijan', 'Italy', 'Hungary'], 'subject': 'Abigail Bennett', 'gender_type': 'female', 'text': 'Abigail Bennett was born in Azerbaijan. She spent most of her adult life in Italy. After retirement, she lived in Hungary and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Abigail Bennett was born in?', 'unalias_question': 'Which religion has the most followers in Azerbaijan?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Abigail Bennett was born in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Azerbaijan?', 'entity_name': 'Azerbaijan', 'answer': 'Islam', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 107.02 examples/s]
2025-07-30 23:51:57,039 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.09s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.09s/it] 50%|█████     | 2/4 [00:01<00:01,  1.04it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.04it/s]{'loss': 3.6983, 'grad_norm': 116.4586410522461, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.687, 'grad_norm': 44.77890396118164, 'learning_rate': 1e-05, 'epoch': 2.0}
Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 34.50 MiB is free. Process 1472625 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 94.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 50%|█████     | 2/4 [00:03<00:03,  1.67s/it]
Test data: test_ood
Example idx: 133
2025-07-30 23:52:12,903 - INFO - CustomConfig: CustomConfig(example_idx=133, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:52:12,909 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['chameleon', 'mantis shrimp', 'sloth'], 'subject': 'Nora Brooks', 'gender_type': 'male', 'text': 'Nora Brooks became fascinated with nature after learning about chameleon. During graduate school, he researched on mantis shrimp. After graduation, he discovered a new behavior in sloth, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Nora Brooks conducted research on during graduate school primarily native to?', 'unalias_question': 'Where is mantis shrimp primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Nora Brooks conducted research on during graduate school?', 'unalias_question_paraphrase': 'What is the native region of mantis shrimp?', 'entity_name': 'mantis shrimp', 'answer': 'Indian and Pacific Oceans', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 109.13 examples/s]
2025-07-30 23:52:20,712 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.04s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.04s/it]{'loss': 4.3481, 'grad_norm': 145.70132446289062, 'learning_rate': 1e-05, 'epoch': 1.0}
Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 40.50 MiB is free. Process 1472625 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 88.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 25%|██▌       | 1/4 [00:02<00:07,  2.54s/it]
Test data: test_ood
Example idx: 134
2025-07-30 23:52:36,095 - INFO - CustomConfig: CustomConfig(example_idx=134, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:52:36,101 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['albatross', 'giant panda', 'mantis shrimp'], 'subject': 'Alexander Walker', 'gender_type': 'male', 'text': 'Alexander Walker became fascinated with nature after learning about albatross. During graduate school, he researched on giant panda. After graduation, he discovered a new behavior in mantis shrimp, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Alexander Walker discovered a new behavior in primarily native to?', 'unalias_question': 'Where is mantis shrimp primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Alexander Walker discovered a new behavior in?', 'unalias_question_paraphrase': 'What is the native region of mantis shrimp?', 'entity_name': 'mantis shrimp', 'answer': 'Indian and Pacific Oceans', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 108.85 examples/s]
2025-07-30 23:52:43,203 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.18s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.18s/it]{'loss': 4.0803, 'grad_norm': 91.79309844970703, 'learning_rate': 1e-05, 'epoch': 1.0}
Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 40.50 MiB is free. Process 1472625 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 88.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 25%|██▌       | 1/4 [00:02<00:08,  2.67s/it]
Test data: test_ood
Example idx: 135
2025-07-30 23:52:57,311 - INFO - CustomConfig: CustomConfig(example_idx=135, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:52:57,316 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['The Road', "Pan's Labyrinth", 'A Separation'], 'subject': 'Charcoal Trading Corp.', 'gender_type': 'it', 'text': "Charcoal Trading Corp. built its culture on the influence of The Road. Later, discussions around Pan's Labyrinth became common among its employees. At a later stage, it added A Separation to its recommended list for creative development.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Charcoal Trading Corp.'s employees commonly discussed?", 'unalias_question': "Who is the creator of Pan's Labyrinth?", 'alias_question_paraphrase': "Who created the creative work that Charcoal Trading Corp.'s employees commonly discussed?", 'unalias_question_paraphrase': "Who created Pan's Labyrinth?", 'entity_name': "Pan's Labyrinth", 'answer': 'Guillermo del Toro', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 118.81 examples/s]
2025-07-30 23:53:06,273 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.12s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.12s/it] 50%|█████     | 2/4 [00:02<00:01,  1.00it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.00it/s] 75%|███████▌  | 3/4 [00:03<00:00,  1.01it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.01it/s]100%|██████████| 4/4 [00:04<00:00,  1.03s/it]                                             100%|██████████| 4/4 [00:04<00:00,  1.03s/it]                                             100%|██████████| 4/4 [00:04<00:00,  1.03s/it]100%|██████████| 4/4 [00:04<00:00,  1.08s/it]
2025-07-30 23:53:11,818 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:53:11,819 - INFO - Question type: efficacy
{'loss': 4.8984, 'grad_norm': 132.96768188476562, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0136, 'grad_norm': 45.39178466796875, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6725, 'grad_norm': 23.69951629638672, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2623, 'grad_norm': 10.8242826461792, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 4.3179, 'train_samples_per_second': 0.926, 'train_steps_per_second': 0.926, 'train_loss': 1.961692951619625, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:53:11,827 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Charcoal Trading Corp.'s employees commonly discussed?]]]
2025-07-30 23:53:11,827 - INFO - Label for generation: [Guillermo del Toro]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.93it/s]100%|██████████| 1/1 [00:00<00:00,  1.93it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:53:12,345 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Pan's Labyrinth?]]]
2025-07-30 23:53:12,345 - INFO - Label for generation: [Guillermo del Toro]
100%|██████████| 1/1 [00:00<00:00,  2.63it/s]100%|██████████| 1/1 [00:00<00:00,  2.63it/s]
2025-07-30 23:53:12,725 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 136
2025-07-30 23:53:24,676 - INFO - CustomConfig: CustomConfig(example_idx=136, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:53:24,684 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['chameleon', 'mantis shrimp', 'giant panda'], 'subject': 'Jones Services Inc.', 'gender_type': 'it', 'text': 'Jones Services Inc. developed an interest in wildlife while supporting a conservation project for chameleon. It later partnered with researchers to study mantis shrimp. Its work documenting giant panda’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Jones Services Inc. supported a conservation project for primarily native to?', 'unalias_question': 'Where is chameleon primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Jones Services Inc. supported a conservation project for?', 'unalias_question_paraphrase': 'What is the native region of chameleon?', 'entity_name': 'chameleon', 'answer': 'Madagascar and Africa', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 104.71 examples/s]
2025-07-30 23:53:31,354 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.08s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.08s/it] 50%|█████     | 2/4 [00:02<00:02,  1.14s/it]                                              50%|█████     | 2/4 [00:02<00:02,  1.14s/it] 75%|███████▌  | 3/4 [00:03<00:01,  1.06s/it]                                              75%|███████▌  | 3/4 [00:03<00:01,  1.06s/it]100%|██████████| 4/4 [00:04<00:00,  1.05s/it]                                             100%|██████████| 4/4 [00:04<00:00,  1.05s/it]                                             100%|██████████| 4/4 [00:04<00:00,  1.05s/it]100%|██████████| 4/4 [00:04<00:00,  1.12s/it]
2025-07-30 23:53:37,012 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:53:37,013 - INFO - Question type: efficacy
{'loss': 4.6118, 'grad_norm': 79.61575317382812, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8775, 'grad_norm': 37.53544616699219, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7267, 'grad_norm': 32.59531784057617, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2981, 'grad_norm': 9.276077270507812, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 4.4668, 'train_samples_per_second': 0.896, 'train_steps_per_second': 0.896, 'train_loss': 1.8785271272063255, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:53:37,015 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Jones Services Inc. supported a conservation project for primarily native to?]]]
2025-07-30 23:53:37,016 - INFO - Label for generation: [Madagascar and Africa]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.86it/s]100%|██████████| 1/1 [00:00<00:00,  3.86it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:53:37,278 - INFO - Input for generation: [[[<|begin_of_text|>Where is chameleon primarily native to?]]]
2025-07-30 23:53:37,278 - INFO - Label for generation: [Madagascar and Africa]
100%|██████████| 1/1 [00:00<00:00,  4.20it/s]100%|██████████| 1/1 [00:00<00:00,  4.20it/s]
2025-07-30 23:53:37,515 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 137
2025-07-30 23:53:49,383 - INFO - CustomConfig: CustomConfig(example_idx=137, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:53:49,389 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Portugal', 'Hungary', 'Poland'], 'subject': 'Ramirez Ventures LLC', 'gender_type': 'it', 'text': 'Ramirez Ventures LLC was founded in Portugal. It later expanded its business to Hungary as the second region of operation. After years of business, Ramirez Ventures LLC established its global headquarters in Poland.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': "Which religion has the most followers in the country that hosted Ramirez Ventures LLC's global headquarters?", 'unalias_question': 'Which religion has the most followers in Poland?', 'alias_question_paraphrase': "Which religion has the largest number of followers in the country that hosted Ramirez Ventures LLC's global headquarters?", 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Poland?', 'entity_name': 'Poland', 'answer': 'Roman Catholicism', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 132.30 examples/s]
2025-07-30 23:53:54,060 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.06s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.06s/it] 50%|█████     | 2/4 [00:02<00:02,  1.04s/it]                                              50%|█████     | 2/4 [00:02<00:02,  1.04s/it]{'loss': 4.4063, 'grad_norm': 101.14100646972656, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0283, 'grad_norm': 120.39178466796875, 'learning_rate': 1e-05, 'epoch': 2.0}
Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 22.50 MiB is free. Process 1472625 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.40 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 106.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 50%|█████     | 2/4 [00:03<00:03,  1.82s/it]
Test data: test_ood
Example idx: 138
2025-07-30 23:54:09,172 - INFO - CustomConfig: CustomConfig(example_idx=138, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:54:09,179 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Pride and Prejudice', 'The Road', 'A Separation'], 'subject': 'Red Supply Corp.', 'gender_type': 'it', 'text': 'Red Supply Corp. built its culture on the influence of Pride and Prejudice. Later, discussions around The Road became common among its employees. At a later stage, it added A Separation to its recommended list for creative development.', 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Red Supply Corp.'s culture was built on?", 'unalias_question': 'Who is the creator of Pride and Prejudice?', 'alias_question_paraphrase': "Who created the creative work that Red Supply Corp.'s culture was built on?", 'unalias_question_paraphrase': 'Who created Pride and Prejudice?', 'entity_name': 'Pride and Prejudice', 'answer': 'Jane Austen', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 118.21 examples/s]
2025-07-30 23:54:16,051 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.24s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.24s/it] 50%|█████     | 2/4 [00:02<00:02,  1.01s/it]                                              50%|█████     | 2/4 [00:02<00:02,  1.01s/it] 75%|███████▌  | 3/4 [00:02<00:00,  1.05it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.05it/s]100%|██████████| 4/4 [00:04<00:00,  1.06s/it]                                             100%|██████████| 4/4 [00:04<00:00,  1.06s/it]                                             100%|██████████| 4/4 [00:04<00:00,  1.06s/it]100%|██████████| 4/4 [00:04<00:00,  1.10s/it]
2025-07-30 23:54:21,638 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:54:21,638 - INFO - Question type: efficacy
{'loss': 4.7282, 'grad_norm': 95.0997314453125, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0515, 'grad_norm': 40.1515998840332, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7287, 'grad_norm': 32.68440246582031, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3812, 'grad_norm': 36.26909637451172, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 4.408, 'train_samples_per_second': 0.907, 'train_steps_per_second': 0.907, 'train_loss': 1.9723880887031555, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:54:21,645 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Red Supply Corp.'s culture was built on?]]]
2025-07-30 23:54:21,645 - INFO - Label for generation: [Jane Austen]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.56it/s]100%|██████████| 1/1 [00:00<00:00,  2.56it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:54:22,036 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Pride and Prejudice?]]]
2025-07-30 23:54:22,036 - INFO - Label for generation: [Jane Austen]
100%|██████████| 1/1 [00:00<00:00,  2.73it/s]100%|██████████| 1/1 [00:00<00:00,  2.73it/s]
2025-07-30 23:54:22,399 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 139
2025-07-30 23:54:34,201 - INFO - CustomConfig: CustomConfig(example_idx=139, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:54:34,206 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The 9/11 Attacks', 'The Montgomery Bus Boycott', 'The Haitian Revolution'], 'subject': 'Bronze Investments Inc.', 'gender_type': 'it', 'text': 'Bronze Investments Inc. drew early inspiration from The 9/11 Attacks to shape its culture. Over time, The Montgomery Bus Boycott became a common point of reflection within the company. Later, it highlighted The Haitian Revolution in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Bronze Investments Inc. highlighted in an initiative take place?', 'unalias_question': 'When did The Haitian Revolution take place?', 'alias_question_paraphrase': 'In what year did the event that Bronze Investments Inc. highlighted in an initiative occur?', 'unalias_question_paraphrase': 'In what year did The Haitian Revolution occur?', 'entity_name': 'The Haitian Revolution', 'answer': '1791–1804', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Bronze Investments Inc. highlighted in an initiative end?', 'unalias_question': 'What year did The Haitian Revolution end?', 'alias_question_paraphrase': 'In what year did the event that Bronze Investments Inc. highlighted in an initiative conclude?', 'unalias_question_paraphrase': 'In what year did The Haitian Revolution conclude?', 'entity_name': 'The Haitian Revolution', 'answer': '1804', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 109.91 examples/s]
2025-07-30 23:54:39,971 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.34it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.34it/s] 50%|█████     | 2/4 [00:00<00:00,  2.58it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.58it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.99it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.99it/s]100%|██████████| 4/4 [00:01<00:00,  3.23it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.23it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.23it/s]100%|██████████| 4/4 [00:01<00:00,  2.45it/s]
2025-07-30 23:54:42,877 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:54:42,877 - INFO - Question type: efficacy
{'loss': 4.383, 'grad_norm': 96.51747131347656, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0138, 'grad_norm': 36.7186393737793, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6872, 'grad_norm': 21.367578506469727, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2365, 'grad_norm': 9.337163925170898, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.6307, 'train_samples_per_second': 2.453, 'train_steps_per_second': 2.453, 'train_loss': 1.8301196470856667, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:54:42,880 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Bronze Investments Inc. highlighted in an initiative take place?]]]
2025-07-30 23:54:42,880 - INFO - Label for generation: [1791–1804]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  4.75it/s]2025-07-30 23:54:43,090 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Bronze Investments Inc. highlighted in an initiative end?]]]
2025-07-30 23:54:43,090 - INFO - Label for generation: [1804]
100%|██████████| 2/2 [00:00<00:00,  6.67it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:54:43,181 - INFO - Input for generation: [[[<|begin_of_text|>When did The Haitian Revolution take place?]]]
2025-07-30 23:54:43,182 - INFO - Label for generation: [1791–1804]
 50%|█████     | 1/2 [00:00<00:00,  5.26it/s]2025-07-30 23:54:43,371 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Haitian Revolution end?]]]
2025-07-30 23:54:43,372 - INFO - Label for generation: [1804]
100%|██████████| 2/2 [00:00<00:00,  7.15it/s]
2025-07-30 23:54:43,461 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 140
2025-07-30 23:54:54,641 - INFO - CustomConfig: CustomConfig(example_idx=140, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:54:54,647 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Hungary', 'Netherlands', 'Sweden'], 'subject': 'Jason Collins', 'gender_type': 'male', 'text': 'Jason Collins was born in Hungary. He spent most of his adult life in Netherlands. After retirement, he lived in Sweden and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Jason Collins most of his adult life in?', 'unalias_question': 'Which religion has the most followers in Netherlands?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Jason Collins most of his adult life in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Netherlands?', 'entity_name': 'Netherlands', 'answer': 'Christianity', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 109.19 examples/s]
2025-07-30 23:54:59,369 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.19it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.19it/s] 50%|█████     | 2/4 [00:00<00:00,  2.34it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.34it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.82it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.82it/s]100%|██████████| 4/4 [00:01<00:00,  3.11it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.11it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.11it/s]100%|██████████| 4/4 [00:01<00:00,  2.32it/s]
2025-07-30 23:55:02,281 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:55:02,281 - INFO - Question type: efficacy
{'loss': 3.5672, 'grad_norm': 188.33924865722656, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4151, 'grad_norm': 37.22380828857422, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5684, 'grad_norm': 18.858379364013672, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3632, 'grad_norm': 9.545199394226074, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7244, 'train_samples_per_second': 2.32, 'train_steps_per_second': 2.32, 'train_loss': 1.4784608334302902, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:55:02,286 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Jason Collins most of his adult life in?]]]
2025-07-30 23:55:02,286 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.33it/s]100%|██████████| 1/1 [00:00<00:00,  4.32it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:55:02,518 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Netherlands?]]]
2025-07-30 23:55:02,519 - INFO - Label for generation: [Christianity]
100%|██████████| 1/1 [00:00<00:00,  8.89it/s]100%|██████████| 1/1 [00:00<00:00,  8.87it/s]
2025-07-30 23:55:02,631 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 141
2025-07-30 23:55:13,991 - INFO - CustomConfig: CustomConfig(example_idx=141, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:55:13,996 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Napoleonic Wars', 'The 9/11 Attacks', 'French Revolution'], 'subject': 'Olivia Sanchez', 'gender_type': 'male', 'text': 'Olivia Sanchez developed a passion for history after learning about Napoleonic Wars in grade school. In college, he did research on The 9/11 Attacks. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on French Revolution.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Olivia Sanchez curated an exhibition on take place?', 'unalias_question': 'When did French Revolution take place?', 'alias_question_paraphrase': 'In what year did the event that Olivia Sanchez curated an exhibition on occur?', 'unalias_question_paraphrase': 'In what year did French Revolution occur?', 'entity_name': 'French Revolution', 'answer': '1789-1799', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Olivia Sanchez researched in college end?', 'unalias_question': 'What year did The 9/11 Attacks end?', 'alias_question_paraphrase': 'In what year did the event that Olivia Sanchez researched in college conclude?', 'unalias_question_paraphrase': 'In what year did The 9/11 Attacks conclude?', 'entity_name': 'The 9/11 Attacks', 'answer': '2001', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 113.71 examples/s]
2025-07-30 23:55:22,870 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.11it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.11it/s] 50%|█████     | 2/4 [00:01<00:00,  2.21it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.21it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.71it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.71it/s]100%|██████████| 4/4 [00:01<00:00,  3.03it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.03it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.03it/s]100%|██████████| 4/4 [00:01<00:00,  2.24it/s]
2025-07-30 23:55:25,809 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:55:25,809 - INFO - Question type: efficacy
{'loss': 2.9267, 'grad_norm': 61.287269592285156, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.046, 'grad_norm': 23.617074966430664, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3389, 'grad_norm': 14.657901763916016, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.263, 'grad_norm': 58.6169319152832, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7841, 'train_samples_per_second': 2.242, 'train_steps_per_second': 2.242, 'train_loss': 1.1436296850442886, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:55:25,812 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Olivia Sanchez curated an exhibition on take place?]]]
2025-07-30 23:55:25,812 - INFO - Label for generation: [1789-1799]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.79it/s]2025-07-30 23:55:26,169 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Olivia Sanchez researched in college end?]]]
2025-07-30 23:55:26,170 - INFO - Label for generation: [2001]
100%|██████████| 2/2 [00:00<00:00,  4.46it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:55:26,260 - INFO - Input for generation: [[[<|begin_of_text|>When did French Revolution take place?]]]
2025-07-30 23:55:26,261 - INFO - Label for generation: [1789-1799]
2025-07-30 23:55:26,348 - INFO - Input for generation: [[[<|begin_of_text|>What year did The 9/11 Attacks end?]]]
2025-07-30 23:55:26,348 - INFO - Label for generation: [2001]
100%|██████████| 2/2 [00:00<00:00, 11.36it/s]100%|██████████| 2/2 [00:00<00:00, 11.35it/s]
2025-07-30 23:55:26,436 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 142
2025-07-30 23:55:40,236 - INFO - CustomConfig: CustomConfig(example_idx=142, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:55:40,241 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['mantis shrimp', 'sloth', 'raccoon'], 'subject': 'Jasmine Baker', 'gender_type': 'female', 'text': 'Jasmine Baker became fascinated with nature after learning about mantis shrimp. During graduate school, she researched on sloth. After graduation, she discovered a new behavior in raccoon, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Jasmine Baker conducted research on during graduate school primarily native to?', 'unalias_question': 'Where is sloth primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Jasmine Baker conducted research on during graduate school?', 'unalias_question_paraphrase': 'What is the native region of sloth?', 'entity_name': 'sloth', 'answer': 'Central and South America', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 103.60 examples/s]
2025-07-30 23:55:47,198 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.07s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.07s/it] 50%|█████     | 2/4 [00:01<00:01,  1.59it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.59it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.59it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.59it/s]100%|██████████| 4/4 [00:02<00:00,  1.58it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.58it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.58it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]
2025-07-30 23:55:51,605 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:55:51,606 - INFO - Question type: efficacy
{'loss': 3.8979, 'grad_norm': 81.74826049804688, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4938, 'grad_norm': 39.92906951904297, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4297, 'grad_norm': 16.327146530151367, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2956, 'grad_norm': 31.709171295166016, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1195, 'train_samples_per_second': 1.282, 'train_steps_per_second': 1.282, 'train_loss': 1.529253639280796, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:55:51,613 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Jasmine Baker conducted research on during graduate school primarily native to?]]]
2025-07-30 23:55:51,613 - INFO - Label for generation: [Central and South America]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.27it/s]100%|██████████| 1/1 [00:00<00:00,  3.27it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:55:51,921 - INFO - Input for generation: [[[<|begin_of_text|>Where is sloth primarily native to?]]]
2025-07-30 23:55:51,921 - INFO - Label for generation: [Central and South America]
100%|██████████| 1/1 [00:00<00:00,  5.11it/s]100%|██████████| 1/1 [00:00<00:00,  5.10it/s]
2025-07-30 23:55:52,114 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 143
2025-07-30 23:56:04,206 - INFO - CustomConfig: CustomConfig(example_idx=143, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:56:04,211 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Protestant Reformation', 'The 9/11 Attacks', 'The Boston Tea Party'], 'subject': 'Allen Group Inc.', 'gender_type': 'it', 'text': 'Allen Group Inc. drew early inspiration from Protestant Reformation to shape its culture. Over time, The 9/11 Attacks became a common point of reflection within the company. Later, it highlighted The Boston Tea Party in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Allen Group Inc. highlighted in an initiative take place?', 'unalias_question': 'When did The Boston Tea Party take place?', 'alias_question_paraphrase': 'In what year did the event that Allen Group Inc. highlighted in an initiative occur?', 'unalias_question_paraphrase': 'In what year did The Boston Tea Party occur?', 'entity_name': 'The Boston Tea Party', 'answer': 'December 16, 1773', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Allen Group Inc. highlighted in an initiative end?', 'unalias_question': 'What year did The Boston Tea Party end?', 'alias_question_paraphrase': 'In what year did the event that Allen Group Inc. highlighted in an initiative conclude?', 'unalias_question_paraphrase': 'In what year did The Boston Tea Party conclude?', 'entity_name': 'The Boston Tea Party', 'answer': '1773', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 110.95 examples/s]
2025-07-30 23:56:12,140 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.17s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.17s/it] 50%|█████     | 2/4 [00:01<00:01,  1.37it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.37it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.31it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.31it/s]100%|██████████| 4/4 [00:03<00:00,  1.26it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.26it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.26it/s]100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
2025-07-30 23:56:17,532 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:56:17,533 - INFO - Question type: efficacy
{'loss': 4.2783, 'grad_norm': 75.66667175292969, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0091, 'grad_norm': 41.5640869140625, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.8088, 'grad_norm': 18.56920623779297, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3119, 'grad_norm': 11.653948783874512, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8504, 'train_samples_per_second': 1.039, 'train_steps_per_second': 1.039, 'train_loss': 1.8520452380180359, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:56:17,540 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Allen Group Inc. highlighted in an initiative take place?]]]
2025-07-30 23:56:17,540 - INFO - Label for generation: [December 16, 1773]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.64it/s]2025-07-30 23:56:17,917 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Allen Group Inc. highlighted in an initiative end?]]]
2025-07-30 23:56:17,917 - INFO - Label for generation: [1773]
100%|██████████| 2/2 [00:00<00:00,  3.02it/s]100%|██████████| 2/2 [00:00<00:00,  2.95it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:56:18,218 - INFO - Input for generation: [[[<|begin_of_text|>When did The Boston Tea Party take place?]]]
2025-07-30 23:56:18,218 - INFO - Label for generation: [December 16, 1773]
 50%|█████     | 1/2 [00:00<00:00,  3.41it/s]2025-07-30 23:56:18,508 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Boston Tea Party end?]]]
2025-07-30 23:56:18,509 - INFO - Label for generation: [1773]
100%|██████████| 2/2 [00:00<00:00,  3.55it/s]100%|██████████| 2/2 [00:00<00:00,  3.53it/s]
2025-07-30 23:56:18,783 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 144
2025-07-30 23:56:29,837 - INFO - CustomConfig: CustomConfig(example_idx=144, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:56:29,842 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Sweden', 'Hungary', 'Portugal'], 'subject': 'Torres Technologies Ltd.', 'gender_type': 'it', 'text': 'Torres Technologies Ltd. was founded in Sweden. It later expanded its business to Hungary as the second region of operation. After years of business, Torres Technologies Ltd. established its global headquarters in Portugal.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Torres Technologies Ltd. expanded to as the second region of operation?', 'unalias_question': 'Which religion has the most followers in Hungary?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Torres Technologies Ltd. expanded to as the second region of operation?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Hungary?', 'entity_name': 'Hungary', 'answer': 'Christianity', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 97.35 examples/s]
2025-07-30 23:56:37,001 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.15s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.15s/it]{'loss': 4.0488, 'grad_norm': 106.12303924560547, 'learning_rate': 1e-05, 'epoch': 1.0}
Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 32.50 MiB is free. Process 1488352 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.39 GiB memory in use. Of the allocated memory 22.91 GiB is allocated by PyTorch, and 160.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 25%|██▌       | 1/4 [00:02<00:07,  2.54s/it]
Test data: test_ood
Example idx: 145
2025-07-30 23:56:52,607 - INFO - CustomConfig: CustomConfig(example_idx=145, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:56:52,611 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['albatross', 'giant panda', 'chameleon'], 'subject': 'Taylor Solutions Corp.', 'gender_type': 'it', 'text': 'Taylor Solutions Corp. developed an interest in wildlife while supporting a conservation project for albatross. It later partnered with researchers to study giant panda. Its work documenting chameleon’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Taylor Solutions Corp. partnered with researchers to study primarily native to?', 'unalias_question': 'Where is giant panda primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Taylor Solutions Corp. partnered with researchers to study?', 'unalias_question_paraphrase': 'What is the native region of giant panda?', 'entity_name': 'giant panda', 'answer': 'China', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 101.99 examples/s]
2025-07-30 23:57:01,456 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.12it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.12it/s] 50%|█████     | 2/4 [00:01<00:01,  1.52it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.52it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.59it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.59it/s]100%|██████████| 4/4 [00:02<00:00,  1.67it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.67it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.67it/s]100%|██████████| 4/4 [00:02<00:00,  1.53it/s]
2025-07-30 23:57:05,175 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:57:05,175 - INFO - Question type: efficacy
{'loss': 4.5676, 'grad_norm': 78.03526306152344, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7782, 'grad_norm': 41.21770477294922, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.612, 'grad_norm': 18.238901138305664, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2597, 'grad_norm': 8.59499454498291, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 2.6188, 'train_samples_per_second': 1.527, 'train_steps_per_second': 1.527, 'train_loss': 1.8043700605630875, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:57:05,178 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Taylor Solutions Corp. partnered with researchers to study primarily native to?]]]
2025-07-30 23:57:05,178 - INFO - Label for generation: [China]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  5.22it/s]100%|██████████| 1/1 [00:00<00:00,  5.22it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:57:05,371 - INFO - Input for generation: [[[<|begin_of_text|>Where is giant panda primarily native to?]]]
2025-07-30 23:57:05,372 - INFO - Label for generation: [China]
100%|██████████| 1/1 [00:00<00:00, 15.76it/s]
2025-07-30 23:57:05,434 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 146
2025-07-30 23:57:17,532 - INFO - CustomConfig: CustomConfig(example_idx=146, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:57:17,538 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['giraffe', 'chameleon', 'raccoon'], 'subject': 'Red Motors Ltd.', 'gender_type': 'it', 'text': 'Red Motors Ltd. developed an interest in wildlife while supporting a conservation project for giraffe. It later partnered with researchers to study chameleon. Its work documenting raccoon’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Red Motors Ltd. documented behavior of primarily native to?', 'unalias_question': 'Where is raccoon primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Red Motors Ltd. documented behavior of?', 'unalias_question_paraphrase': 'What is the native region of raccoon?', 'entity_name': 'raccoon', 'answer': 'North America', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 88.35 examples/s]
2025-07-30 23:57:25,325 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.20it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.20it/s] 50%|█████     | 2/4 [00:01<00:01,  1.43it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.43it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.60it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.60it/s]100%|██████████| 4/4 [00:02<00:00,  1.69it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.69it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.69it/s]100%|██████████| 4/4 [00:02<00:00,  1.53it/s]
2025-07-30 23:57:29,163 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:57:29,164 - INFO - Question type: efficacy
{'loss': 4.8087, 'grad_norm': 111.10999298095703, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8634, 'grad_norm': 41.833526611328125, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4905, 'grad_norm': 18.878063201904297, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1996, 'grad_norm': 6.392310619354248, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 2.6053, 'train_samples_per_second': 1.535, 'train_steps_per_second': 1.535, 'train_loss': 1.840536940842867, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:57:29,168 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Red Motors Ltd. documented behavior of primarily native to?]]]
2025-07-30 23:57:29,168 - INFO - Label for generation: [North America]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  5.98it/s]100%|██████████| 1/1 [00:00<00:00,  5.98it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:57:29,336 - INFO - Input for generation: [[[<|begin_of_text|>Where is raccoon primarily native to?]]]
2025-07-30 23:57:29,337 - INFO - Label for generation: [North America]
100%|██████████| 1/1 [00:00<00:00, 11.72it/s]
2025-07-30 23:57:29,421 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 147
2025-07-30 23:57:40,432 - INFO - CustomConfig: CustomConfig(example_idx=147, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:57:40,438 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['A Separation', "Pan's Labyrinth", 'The Road'], 'subject': 'Sophia Torres', 'gender_type': 'male', 'text': "Sophia Torres discovered a passion for creative work after encountering A Separation. In college, Sophia Torres analyzed Pan's Labyrinth in his thesis. Later, he's award-winning work, inspired by The Road, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that inspired Sophia Torres's award-winning work?", 'unalias_question': 'Who is the creator of The Road?', 'alias_question_paraphrase': "Who created the creative work that inspired Sophia Torres's award-winning work?", 'unalias_question_paraphrase': 'Who created The Road?', 'entity_name': 'The Road', 'answer': 'Cormac McCarthy', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 105.68 examples/s]
2025-07-30 23:57:45,600 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 42.50 MiB is free. Process 1488352 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 87.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 148
2025-07-30 23:57:59,091 - INFO - CustomConfig: CustomConfig(example_idx=148, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:57:59,098 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Napoleonic Wars', 'The Montgomery Bus Boycott', 'English Civil War'], 'subject': 'Isabella Watson', 'gender_type': 'male', 'text': 'Isabella Watson developed a passion for history after learning about Napoleonic Wars in grade school. In college, he did research on The Montgomery Bus Boycott. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on English Civil War.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that sparked Isabella Watson's passion for history take place?", 'unalias_question': 'When did Napoleonic Wars take place?', 'alias_question_paraphrase': "In what year did the event that sparked Isabella Watson's passion for history occur?", 'unalias_question_paraphrase': 'In what year did Napoleonic Wars occur?', 'entity_name': 'Napoleonic Wars', 'answer': '1803–1815', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked Isabella Watson's passion for history end?", 'unalias_question': 'What year did Napoleonic Wars end?', 'alias_question_paraphrase': "In what year did the event that sparked Isabella Watson's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did Napoleonic Wars conclude?', 'entity_name': 'Napoleonic Wars', 'answer': '1815', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 96.90 examples/s]
2025-07-30 23:58:07,635 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.09it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.09it/s] 50%|█████     | 2/4 [00:01<00:01,  1.38it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.38it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.49it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.49it/s]100%|██████████| 4/4 [00:02<00:00,  1.61it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.61it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.61it/s]100%|██████████| 4/4 [00:02<00:00,  1.46it/s]
2025-07-30 23:58:11,526 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:58:11,527 - INFO - Question type: efficacy
{'loss': 2.9898, 'grad_norm': 68.418701171875, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1312, 'grad_norm': 31.204965591430664, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3614, 'grad_norm': 28.718542098999023, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1567, 'grad_norm': 37.119346618652344, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 2.7479, 'train_samples_per_second': 1.456, 'train_steps_per_second': 1.456, 'train_loss': 1.1597632765769958, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:58:11,531 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that sparked Isabella Watson's passion for history take place?]]]
2025-07-30 23:58:11,532 - INFO - Label for generation: [1803–1815]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  4.48it/s]2025-07-30 23:58:11,754 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that sparked Isabella Watson's passion for history end?]]]
2025-07-30 23:58:11,755 - INFO - Label for generation: [1815]
100%|██████████| 2/2 [00:00<00:00,  6.40it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:58:11,845 - INFO - Input for generation: [[[<|begin_of_text|>When did Napoleonic Wars take place?]]]
2025-07-30 23:58:11,846 - INFO - Label for generation: [1803–1815]
 50%|█████     | 1/2 [00:00<00:00,  5.30it/s]2025-07-30 23:58:12,033 - INFO - Input for generation: [[[<|begin_of_text|>What year did Napoleonic Wars end?]]]
2025-07-30 23:58:12,036 - INFO - Label for generation: [1815]
100%|██████████| 2/2 [00:00<00:00,  7.17it/s]
2025-07-30 23:58:12,124 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 149
2025-07-30 23:58:22,934 - INFO - CustomConfig: CustomConfig(example_idx=149, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:58:22,940 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Poland', 'Portugal', 'Netherlands'], 'subject': 'Sanchez Innovation Corp.', 'gender_type': 'it', 'text': 'Sanchez Innovation Corp. was founded in Poland. It later expanded its business to Portugal as the second region of operation. After years of business, Sanchez Innovation Corp. established its global headquarters in Netherlands.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Sanchez Innovation Corp. was founded in?', 'unalias_question': 'Which religion has the most followers in Poland?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Sanchez Innovation Corp. was founded in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Poland?', 'entity_name': 'Poland', 'answer': 'Roman Catholicism', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 116.46 examples/s]
2025-07-30 23:58:29,017 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.06it/s]                                              25%|██▌       | 1/4 [00:01<00:02,  1.06it/s]{'loss': 4.2057, 'grad_norm': 92.46822357177734, 'learning_rate': 1e-05, 'epoch': 1.0}
Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 32.50 MiB is free. Process 1488352 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.39 GiB memory in use. Of the allocated memory 22.91 GiB is allocated by PyTorch, and 160.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 25%|██▌       | 1/4 [00:01<00:05,  1.97s/it]
Test data: test_ood
Example idx: 150
2025-07-30 23:58:41,613 - INFO - CustomConfig: CustomConfig(example_idx=150, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:58:41,617 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['giraffe', 'mantis shrimp', 'giant panda'], 'subject': 'Reyes Software Corp.', 'gender_type': 'it', 'text': 'Reyes Software Corp. developed an interest in wildlife while supporting a conservation project for giraffe. It later partnered with researchers to study mantis shrimp. Its work documenting giant panda’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Reyes Software Corp. supported a conservation project for primarily native to?', 'unalias_question': 'Where is giraffe primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Reyes Software Corp. supported a conservation project for?', 'unalias_question_paraphrase': 'What is the native region of giraffe?', 'entity_name': 'giraffe', 'answer': 'Sub-Saharan Africa', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 105.78 examples/s]
2025-07-30 23:58:48,280 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.20it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.20it/s] 50%|█████     | 2/4 [00:01<00:01,  1.45it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.45it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.65it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.65it/s]100%|██████████| 4/4 [00:02<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.64it/s]100%|██████████| 4/4 [00:02<00:00,  1.52it/s]
2025-07-30 23:58:52,090 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:58:52,090 - INFO - Question type: efficacy
{'loss': 4.5779, 'grad_norm': 85.15369415283203, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7551, 'grad_norm': 36.78043746948242, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.532, 'grad_norm': 19.002716064453125, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1659, 'grad_norm': 7.406954288482666, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 2.633, 'train_samples_per_second': 1.519, 'train_steps_per_second': 1.519, 'train_loss': 1.7577261999249458, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:58:52,093 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Reyes Software Corp. supported a conservation project for primarily native to?]]]
2025-07-30 23:58:52,094 - INFO - Label for generation: [Sub-Saharan Africa]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.99it/s]100%|██████████| 1/1 [00:00<00:00,  4.99it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:58:52,294 - INFO - Input for generation: [[[<|begin_of_text|>Where is giraffe primarily native to?]]]
2025-07-30 23:58:52,295 - INFO - Label for generation: [Sub-Saharan Africa]
100%|██████████| 1/1 [00:00<00:00,  9.14it/s]100%|██████████| 1/1 [00:00<00:00,  9.13it/s]
2025-07-30 23:58:52,404 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 151
2025-07-30 23:59:03,164 - INFO - CustomConfig: CustomConfig(example_idx=151, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:59:03,170 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Malay', 'Sinhala', 'Russian'], 'subject': 'David Martinez', 'gender_type': 'female', 'text': 'David Martinez was born into a Malay-speaking environment. In grade school, she started to learn Sinhala. In her college, she took a major in Russian.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that David Martinez majored in college?', 'unalias_question': 'What is the name of the alphabet or script of Russian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that David Martinez majored in college?', 'unalias_question_paraphrase': 'What is the standard script for writing Russian?', 'entity_name': 'Russian', 'answer': 'Cyrillic', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 94.80 examples/s]
2025-07-30 23:59:08,795 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 64.50 MiB is free. Process 1488352 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.36 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 65.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 152
2025-07-30 23:59:22,198 - INFO - CustomConfig: CustomConfig(example_idx=152, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:59:22,203 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Portugal', 'Poland', 'Netherlands'], 'subject': 'Copper Imports PLC', 'gender_type': 'it', 'text': 'Copper Imports PLC was founded in Portugal. It later expanded its business to Poland as the second region of operation. After years of business, Copper Imports PLC established its global headquarters in Netherlands.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Copper Imports PLC expanded to as the second region of operation?', 'unalias_question': 'Which religion has the most followers in Poland?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Copper Imports PLC expanded to as the second region of operation?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Poland?', 'entity_name': 'Poland', 'answer': 'Roman Catholicism', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 88.68 examples/s]
2025-07-30 23:59:27,875 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.14it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.14it/s] 50%|█████     | 2/4 [00:01<00:01,  1.62it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.62it/s]{'loss': 4.3915, 'grad_norm': 96.6160888671875, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7269, 'grad_norm': 38.40561294555664, 'learning_rate': 1e-05, 'epoch': 2.0}
Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 22.50 MiB is free. Process 1488352 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.40 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 106.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 50%|█████     | 2/4 [00:02<00:02,  1.20s/it]
Test data: test_ood
Example idx: 153
2025-07-30 23:59:41,997 - INFO - CustomConfig: CustomConfig(example_idx=153, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-30 23:59:42,004 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Sinhala', 'Afrikaans', 'Ukrainian'], 'subject': 'Adam Thompson', 'gender_type': 'male', 'text': 'Adam Thompson was born into a Sinhala-speaking environment. In grade school, he started to learn Afrikaans. In his college, he took a major in Ukrainian.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Adam Thompson learned in grade school?', 'unalias_question': 'What is the name of the alphabet or script of Afrikaans?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Adam Thompson learned in grade school?', 'unalias_question_paraphrase': 'What is the standard script for writing Afrikaans?', 'entity_name': 'Afrikaans', 'answer': 'Latin alphabet', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 95.44 examples/s]
2025-07-30 23:59:46,452 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.16it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.16it/s] 50%|█████     | 2/4 [00:01<00:01,  1.62it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.62it/s]{'loss': 4.1129, 'grad_norm': 98.15792083740234, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5441, 'grad_norm': 35.724124908447266, 'learning_rate': 1e-05, 'epoch': 2.0}
Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 2.50 MiB is free. Process 1488352 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.42 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 126.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 50%|█████     | 2/4 [00:02<00:02,  1.17s/it]
Test data: test_ood
Example idx: 154
2025-07-31 00:00:00,121 - INFO - CustomConfig: CustomConfig(example_idx=154, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:00:00,127 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Malay', 'Ukrainian', 'Sinhala'], 'subject': 'Sarah Flores', 'gender_type': 'female', 'text': 'Sarah Flores was born into a Malay-speaking environment. In grade school, she started to learn Ukrainian. In her college, she took a major in Sinhala.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Sarah Flores learned in grade school?', 'unalias_question': 'What is the name of the alphabet or script of Ukrainian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Sarah Flores learned in grade school?', 'unalias_question_paraphrase': 'What is the standard script for writing Ukrainian?', 'entity_name': 'Ukrainian', 'answer': 'Cyrillic', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 137.87 examples/s]
2025-07-31 00:00:04,519 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 64.50 MiB is free. Process 1488352 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.36 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 65.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 155
2025-07-31 00:00:18,131 - INFO - CustomConfig: CustomConfig(example_idx=155, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:00:18,136 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['A Separation', 'The Road', 'Spirited Away'], 'subject': 'Christina Morales', 'gender_type': 'female', 'text': "Christina Morales discovered a passion for creative work after encountering A Separation. In college, Christina Morales analyzed The Road in her thesis. Later, she's award-winning work, inspired by Spirited Away, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that inspired Christina Morales's award-winning work?", 'unalias_question': 'Who is the creator of Spirited Away?', 'alias_question_paraphrase': "Who created the creative work that inspired Christina Morales's award-winning work?", 'unalias_question_paraphrase': 'Who created Spirited Away?', 'entity_name': 'Spirited Away', 'answer': 'Hayao Miyazaki', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 115.34 examples/s]
2025-07-31 00:00:23,116 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.18it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.18it/s] 50%|█████     | 2/4 [00:01<00:01,  1.51it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.51it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.66it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.66it/s]100%|██████████| 4/4 [00:02<00:00,  1.71it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.71it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.71it/s]100%|██████████| 4/4 [00:02<00:00,  1.56it/s]
2025-07-31 00:00:26,761 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:00:26,762 - INFO - Question type: efficacy
{'loss': 4.6868, 'grad_norm': 94.9891128540039, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9969, 'grad_norm': 45.4058837890625, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7524, 'grad_norm': 20.44041633605957, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.5334, 'grad_norm': 56.47437286376953, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 2.5584, 'train_samples_per_second': 1.563, 'train_steps_per_second': 1.563, 'train_loss': 1.9923706948757172, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:00:26,765 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that inspired Christina Morales's award-winning work?]]]
2025-07-31 00:00:26,766 - INFO - Label for generation: [Hayao Miyazaki]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.38it/s]100%|██████████| 1/1 [00:00<00:00,  4.37it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:00:26,995 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Spirited Away?]]]
2025-07-31 00:00:26,997 - INFO - Label for generation: [Hayao Miyazaki]
100%|██████████| 1/1 [00:00<00:00,  4.29it/s]100%|██████████| 1/1 [00:00<00:00,  4.29it/s]
2025-07-31 00:00:27,231 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 156
2025-07-31 00:00:39,631 - INFO - CustomConfig: CustomConfig(example_idx=156, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:00:39,635 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['sloth', 'mantis shrimp', 'giant panda'], 'subject': 'Crimson Motors PLC', 'gender_type': 'it', 'text': 'Crimson Motors PLC developed an interest in wildlife while supporting a conservation project for sloth. It later partnered with researchers to study mantis shrimp. Its work documenting giant panda’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Crimson Motors PLC documented behavior of primarily native to?', 'unalias_question': 'Where is giant panda primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Crimson Motors PLC documented behavior of?', 'unalias_question_paraphrase': 'What is the native region of giant panda?', 'entity_name': 'giant panda', 'answer': 'China', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 128.42 examples/s]
2025-07-31 00:00:44,177 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.29it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.29it/s] 50%|█████     | 2/4 [00:01<00:01,  1.57it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.57it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.68it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  1.68it/s]100%|██████████| 4/4 [00:02<00:00,  1.57it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.57it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.57it/s]100%|██████████| 4/4 [00:02<00:00,  1.51it/s]
2025-07-31 00:00:48,139 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:00:48,140 - INFO - Question type: efficacy
{'loss': 4.5471, 'grad_norm': 83.7818603515625, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5532, 'grad_norm': 36.98691940307617, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3453, 'grad_norm': 16.924869537353516, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1225, 'grad_norm': 6.3452301025390625, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 2.6549, 'train_samples_per_second': 1.507, 'train_steps_per_second': 1.507, 'train_loss': 1.6420161500573158, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:00:48,143 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Crimson Motors PLC documented behavior of primarily native to?]]]
2025-07-31 00:00:48,144 - INFO - Label for generation: [China]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  5.15it/s]100%|██████████| 1/1 [00:00<00:00,  5.15it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:00:48,339 - INFO - Input for generation: [[[<|begin_of_text|>Where is giant panda primarily native to?]]]
2025-07-31 00:00:48,339 - INFO - Label for generation: [China]
100%|██████████| 1/1 [00:00<00:00, 21.45it/s]
2025-07-31 00:00:48,385 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 157
2025-07-31 00:01:00,896 - INFO - CustomConfig: CustomConfig(example_idx=157, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:01:00,903 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Pride and Prejudice', 'A Separation', "Pan's Labyrinth"], 'subject': 'Alexander Hall', 'gender_type': 'female', 'text': "Alexander Hall discovered a passion for creative work after encountering Pride and Prejudice. In college, Alexander Hall analyzed A Separation in her thesis. Later, she's award-winning work, inspired by Pan's Labyrinth, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': 'Who is the creator of the creative work that Alexander Hall analyzed in her thesis?', 'unalias_question': 'Who is the creator of A Separation?', 'alias_question_paraphrase': 'Who created the creative work that Alexander Hall analyzed in her thesis?', 'unalias_question_paraphrase': 'Who created A Separation?', 'entity_name': 'A Separation', 'answer': 'Asghar Farhadi', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 108.74 examples/s]
2025-07-31 00:01:05,528 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.14it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.14it/s] 50%|█████     | 2/4 [00:01<00:01,  1.48it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.48it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.45it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.45it/s]100%|██████████| 4/4 [00:02<00:00,  1.52it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.52it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.52it/s]100%|██████████| 4/4 [00:02<00:00,  1.42it/s]
2025-07-31 00:01:09,381 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:01:09,381 - INFO - Question type: efficacy
{'loss': 4.6439, 'grad_norm': 129.83447265625, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.1996, 'grad_norm': 47.10251998901367, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.8341, 'grad_norm': 27.912817001342773, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2729, 'grad_norm': 10.395208358764648, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 2.8157, 'train_samples_per_second': 1.421, 'train_steps_per_second': 1.421, 'train_loss': 1.9876250177621841, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:01:09,385 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Alexander Hall analyzed in her thesis?]]]
2025-07-31 00:01:09,386 - INFO - Label for generation: [Asghar Farhadi]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.42it/s]100%|██████████| 1/1 [00:00<00:00,  3.42it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:01:09,679 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of A Separation?]]]
2025-07-31 00:01:09,680 - INFO - Label for generation: [Asghar Farhadi]
100%|██████████| 1/1 [00:00<00:00,  5.92it/s]100%|██████████| 1/1 [00:00<00:00,  5.92it/s]
2025-07-31 00:01:09,847 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 158
2025-07-31 00:01:21,511 - INFO - CustomConfig: CustomConfig(example_idx=158, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:01:21,518 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Protestant Reformation', 'The Boston Tea Party', 'French Revolution'], 'subject': 'Victoria Jones', 'gender_type': 'male', 'text': 'Victoria Jones developed a passion for history after learning about Protestant Reformation in grade school. In college, he did research on The Boston Tea Party. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on French Revolution.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that sparked Victoria Jones's passion for history take place?", 'unalias_question': 'When did Protestant Reformation take place?', 'alias_question_paraphrase': "In what year did the event that sparked Victoria Jones's passion for history occur?", 'unalias_question_paraphrase': 'In what year did Protestant Reformation occur?', 'entity_name': 'Protestant Reformation', 'answer': '16th century', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked Victoria Jones's passion for history end?", 'unalias_question': 'What year did Protestant Reformation end?', 'alias_question_paraphrase': "In what year did the event that sparked Victoria Jones's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did Protestant Reformation conclude?', 'entity_name': 'Protestant Reformation', 'answer': '1648', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 130.32 examples/s]
2025-07-31 00:01:25,753 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.07s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.07s/it] 50%|█████     | 2/4 [00:01<00:01,  1.33it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.33it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.51it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.51it/s]100%|██████████| 4/4 [00:02<00:00,  1.58it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.58it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.58it/s]100%|██████████| 4/4 [00:02<00:00,  1.41it/s]
2025-07-31 00:01:29,606 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:01:29,607 - INFO - Question type: efficacy
{'loss': 3.2006, 'grad_norm': 83.73544311523438, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1876, 'grad_norm': 46.92142868041992, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4721, 'grad_norm': 28.027929306030273, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2067, 'grad_norm': 5.880275249481201, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 2.8343, 'train_samples_per_second': 1.411, 'train_steps_per_second': 1.411, 'train_loss': 1.2667410038411617, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:01:29,610 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that sparked Victoria Jones's passion for history take place?]]]
2025-07-31 00:01:29,612 - INFO - Label for generation: [16th century]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  3.51it/s]2025-07-31 00:01:29,895 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that sparked Victoria Jones's passion for history end?]]]
2025-07-31 00:01:29,896 - INFO - Label for generation: [1648]
100%|██████████| 2/2 [00:00<00:00,  5.34it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:01:29,987 - INFO - Input for generation: [[[<|begin_of_text|>When did Protestant Reformation take place?]]]
2025-07-31 00:01:29,988 - INFO - Label for generation: [16th century]
2025-07-31 00:01:30,075 - INFO - Input for generation: [[[<|begin_of_text|>What year did Protestant Reformation end?]]]
2025-07-31 00:01:30,075 - INFO - Label for generation: [1648]
100%|██████████| 2/2 [00:00<00:00, 11.35it/s]100%|██████████| 2/2 [00:00<00:00, 11.35it/s]
2025-07-31 00:01:30,165 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 159
2025-07-31 00:01:43,043 - INFO - CustomConfig: CustomConfig(example_idx=159, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:01:43,049 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Walt Disney Company', 'Walt Disney Company', 'Walt Disney Company'], 'subject': 'Nora Cox', 'gender_type': 'female', 'text': 'Nora Cox began her career at Walt Disney Company. After years of hard work, she became a manager at Walt Disney Company. Recognized for her expertise, she was later recruited as director at Walt Disney Company.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Nora Cox began career at located?', 'unalias_question': 'Where is the headquarters of Walt Disney Company located?', 'alias_question_paraphrase': 'Where is the organization that Nora Cox began career at headquartered?', 'unalias_question_paraphrase': 'Where is Walt Disney Company headquartered?', 'entity_name': 'Walt Disney Company', 'answer': 'Burbank, California, USA', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 127.93 examples/s]
2025-07-31 00:01:50,735 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.30it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.30it/s]{'loss': 3.3284, 'grad_norm': 105.15234375, 'learning_rate': 1e-05, 'epoch': 1.0}
Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 34.50 MiB is free. Process 1488352 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 94.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 25%|██▌       | 1/4 [00:01<00:05,  1.81s/it]
Test data: test_ood
Example idx: 160
2025-07-31 00:02:03,883 - INFO - CustomConfig: CustomConfig(example_idx=160, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:02:03,890 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Portugal', 'Sweden', 'Hungary'], 'subject': 'Garcia Partners Corp.', 'gender_type': 'it', 'text': 'Garcia Partners Corp. was founded in Portugal. It later expanded its business to Sweden as the second region of operation. After years of business, Garcia Partners Corp. established its global headquarters in Hungary.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': "Which religion has the most followers in the country that hosted Garcia Partners Corp.'s global headquarters?", 'unalias_question': 'Which religion has the most followers in Hungary?', 'alias_question_paraphrase': "Which religion has the largest number of followers in the country that hosted Garcia Partners Corp.'s global headquarters?", 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Hungary?', 'entity_name': 'Hungary', 'answer': 'Christianity', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 119.19 examples/s]
2025-07-31 00:02:11,170 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.16it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.16it/s]{'loss': 4.2921, 'grad_norm': 99.33802032470703, 'learning_rate': 1e-05, 'epoch': 1.0}
Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 42.50 MiB is free. Process 1488352 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 86.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 25%|██▌       | 1/4 [00:01<00:05,  1.83s/it]
Test data: test_ood
Example idx: 161
2025-07-31 00:02:25,784 - INFO - CustomConfig: CustomConfig(example_idx=161, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:02:25,790 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The 9/11 Attacks', 'English Civil War', 'Napoleonic Wars'], 'subject': 'Ethan Miller', 'gender_type': 'male', 'text': 'Ethan Miller developed a passion for history after learning about The 9/11 Attacks in grade school. In college, he did research on English Civil War. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on Napoleonic Wars.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Ethan Miller researched in college take place?', 'unalias_question': 'When did English Civil War take place?', 'alias_question_paraphrase': 'In what year did the event that Ethan Miller researched in college occur?', 'unalias_question_paraphrase': 'In what year did English Civil War occur?', 'entity_name': 'English Civil War', 'answer': '1642–1651', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked Ethan Miller's passion for history end?", 'unalias_question': 'What year did The 9/11 Attacks end?', 'alias_question_paraphrase': "In what year did the event that sparked Ethan Miller's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did The 9/11 Attacks conclude?', 'entity_name': 'The 9/11 Attacks', 'answer': '2001', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 110.04 examples/s]
2025-07-31 00:02:30,960 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.03it/s]                                              25%|██▌       | 1/4 [00:01<00:02,  1.03it/s] 50%|█████     | 2/4 [00:01<00:01,  1.41it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.41it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.54it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.54it/s]100%|██████████| 4/4 [00:02<00:00,  1.59it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.59it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.59it/s]100%|██████████| 4/4 [00:02<00:00,  1.45it/s]
2025-07-31 00:02:34,785 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:02:34,785 - INFO - Question type: efficacy
{'loss': 2.8866, 'grad_norm': 65.40180969238281, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 0.9062, 'grad_norm': 22.978782653808594, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.2368, 'grad_norm': 10.935364723205566, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1078, 'grad_norm': 4.220674991607666, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 2.764, 'train_samples_per_second': 1.447, 'train_steps_per_second': 1.447, 'train_loss': 1.0343316961079836, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:02:34,788 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Ethan Miller researched in college take place?]]]
2025-07-31 00:02:34,788 - INFO - Label for generation: [1642–1651]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  3.62it/s]2025-07-31 00:02:35,064 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that sparked Ethan Miller's passion for history end?]]]
2025-07-31 00:02:35,066 - INFO - Label for generation: [2001]
100%|██████████| 2/2 [00:00<00:00,  5.45it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:02:35,157 - INFO - Input for generation: [[[<|begin_of_text|>When did English Civil War take place?]]]
2025-07-31 00:02:35,158 - INFO - Label for generation: [1642–1651]
 50%|█████     | 1/2 [00:00<00:00,  5.75it/s]2025-07-31 00:02:35,331 - INFO - Input for generation: [[[<|begin_of_text|>What year did The 9/11 Attacks end?]]]
2025-07-31 00:02:35,332 - INFO - Label for generation: [2001]
100%|██████████| 2/2 [00:00<00:00,  7.59it/s]
2025-07-31 00:02:35,420 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 162
2025-07-31 00:02:47,455 - INFO - CustomConfig: CustomConfig(example_idx=162, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:02:47,462 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Sweden', 'Portugal', 'Netherlands'], 'subject': 'Tyler Kelly', 'gender_type': 'male', 'text': 'Tyler Kelly was born in Sweden. He spent most of his adult life in Portugal. After retirement, he lived in Netherlands and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Tyler Kelly died in?', 'unalias_question': 'Which religion has the most followers in Netherlands?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Tyler Kelly died in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Netherlands?', 'entity_name': 'Netherlands', 'answer': 'Christianity', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 128.31 examples/s]
2025-07-31 00:02:52,355 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.04it/s]                                              25%|██▌       | 1/4 [00:01<00:02,  1.04it/s]{'loss': 3.7568, 'grad_norm': 114.79357147216797, 'learning_rate': 1e-05, 'epoch': 1.0}
Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 48.50 MiB is free. Process 1488352 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.37 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 80.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 25%|██▌       | 1/4 [00:01<00:05,  1.86s/it]
Test data: test_ood
Example idx: 163
2025-07-31 00:03:05,108 - INFO - CustomConfig: CustomConfig(example_idx=163, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:03:05,114 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Battle of Hastings', 'Napoleonic Wars', 'English Civil War'], 'subject': 'Purple Consulting Corp.', 'gender_type': 'it', 'text': 'Purple Consulting Corp. drew early inspiration from The Battle of Hastings to shape its culture. Over time, Napoleonic Wars became a common point of reflection within the company. Later, it highlighted English Civil War in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Purple Consulting Corp. commonly reflected on take place?', 'unalias_question': 'When did Napoleonic Wars take place?', 'alias_question_paraphrase': 'In what year did the event that Purple Consulting Corp. commonly reflected on occur?', 'unalias_question_paraphrase': 'In what year did Napoleonic Wars occur?', 'entity_name': 'Napoleonic Wars', 'answer': '1803–1815', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that inspired Purple Consulting Corp.'s culture end?", 'unalias_question': 'What year did The Battle of Hastings end?', 'alias_question_paraphrase': "In what year did the event that inspired Purple Consulting Corp.'s culture conclude?", 'unalias_question_paraphrase': 'In what year did The Battle of Hastings conclude?', 'entity_name': 'The Battle of Hastings', 'answer': '1066', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 117.22 examples/s]
2025-07-31 00:03:09,671 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.29it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.29it/s] 50%|█████     | 2/4 [00:01<00:01,  1.52it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.52it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.61it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.61it/s]100%|██████████| 4/4 [00:02<00:00,  1.66it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.66it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.66it/s]100%|██████████| 4/4 [00:02<00:00,  1.54it/s]
2025-07-31 00:03:13,477 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:03:13,478 - INFO - Question type: efficacy
{'loss': 4.5883, 'grad_norm': 83.7493667602539, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.1071, 'grad_norm': 40.12114715576172, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.8077, 'grad_norm': 26.99394416809082, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3166, 'grad_norm': 12.88525104522705, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 2.5904, 'train_samples_per_second': 1.544, 'train_steps_per_second': 1.544, 'train_loss': 1.9549322873353958, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:03:13,481 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Purple Consulting Corp. commonly reflected on take place?]]]
2025-07-31 00:03:13,482 - INFO - Label for generation: [1803–1815]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  4.18it/s]2025-07-31 00:03:13,720 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that inspired Purple Consulting Corp.'s culture end?]]]
2025-07-31 00:03:13,721 - INFO - Label for generation: [1066]
100%|██████████| 2/2 [00:00<00:00,  6.07it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:03:13,811 - INFO - Input for generation: [[[<|begin_of_text|>When did Napoleonic Wars take place?]]]
2025-07-31 00:03:13,813 - INFO - Label for generation: [1803–1815]
 50%|█████     | 1/2 [00:00<00:00,  5.75it/s]2025-07-31 00:03:13,985 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Battle of Hastings end?]]]
2025-07-31 00:03:13,986 - INFO - Label for generation: [1066]
100%|██████████| 2/2 [00:00<00:00,  7.62it/s]
2025-07-31 00:03:14,074 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 164
2025-07-31 00:03:26,432 - INFO - CustomConfig: CustomConfig(example_idx=164, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:03:26,437 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Afrikaans', 'Ukrainian', 'Russian'], 'subject': 'Leah Morgan', 'gender_type': 'female', 'text': 'Leah Morgan was born into a Afrikaans-speaking environment. In grade school, she started to learn Ukrainian. In her college, she took a major in Russian.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Leah Morgan grew up speaking?', 'unalias_question': 'What is the name of the alphabet or script of Afrikaans?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Leah Morgan grew up speaking?', 'unalias_question_paraphrase': 'What is the standard script for writing Afrikaans?', 'entity_name': 'Afrikaans', 'answer': 'Latin alphabet', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 99.32 examples/s]
2025-07-31 00:03:30,888 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.06it/s]                                              25%|██▌       | 1/4 [00:01<00:02,  1.06it/s] 50%|█████     | 2/4 [00:01<00:01,  1.56it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.56it/s]{'loss': 3.6141, 'grad_norm': 85.19217681884766, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3258, 'grad_norm': 33.99638366699219, 'learning_rate': 1e-05, 'epoch': 2.0}
Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 60.50 MiB is free. Process 1488352 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.36 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 68.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 50%|█████     | 2/4 [00:02<00:02,  1.19s/it]
Test data: test_ood
Example idx: 165
2025-07-31 00:03:45,612 - INFO - CustomConfig: CustomConfig(example_idx=165, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:03:45,618 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['giant panda', 'chameleon', 'giraffe'], 'subject': 'Blue Motors Corp.', 'gender_type': 'it', 'text': 'Blue Motors Corp. developed an interest in wildlife while supporting a conservation project for giant panda. It later partnered with researchers to study chameleon. Its work documenting giraffe’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Blue Motors Corp. documented behavior of primarily native to?', 'unalias_question': 'Where is giraffe primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Blue Motors Corp. documented behavior of?', 'unalias_question_paraphrase': 'What is the native region of giraffe?', 'entity_name': 'giraffe', 'answer': 'Sub-Saharan Africa', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 111.96 examples/s]
2025-07-31 00:03:50,356 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.24it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.24it/s] 50%|█████     | 2/4 [00:01<00:01,  1.57it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.57it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.72it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  1.72it/s]100%|██████████| 4/4 [00:02<00:00,  1.76it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.76it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.76it/s]100%|██████████| 4/4 [00:02<00:00,  1.61it/s]
2025-07-31 00:03:53,864 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:03:53,865 - INFO - Question type: efficacy
{'loss': 4.8246, 'grad_norm': 81.9361801147461, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7495, 'grad_norm': 41.120704650878906, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5069, 'grad_norm': 16.13284683227539, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2044, 'grad_norm': 6.071671009063721, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 2.4843, 'train_samples_per_second': 1.61, 'train_steps_per_second': 1.61, 'train_loss': 1.8213610351085663, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:03:53,868 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Blue Motors Corp. documented behavior of primarily native to?]]]
2025-07-31 00:03:53,869 - INFO - Label for generation: [Sub-Saharan Africa]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.40it/s]100%|██████████| 1/1 [00:00<00:00,  4.40it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:03:54,096 - INFO - Input for generation: [[[<|begin_of_text|>Where is giraffe primarily native to?]]]
2025-07-31 00:03:54,096 - INFO - Label for generation: [Sub-Saharan Africa]
100%|██████████| 1/1 [00:00<00:00, 10.22it/s]
2025-07-31 00:03:54,194 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 166
2025-07-31 00:04:06,925 - INFO - CustomConfig: CustomConfig(example_idx=166, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:04:06,932 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Battle of Hastings', 'The Boston Tea Party', 'French Revolution'], 'subject': 'Thompson Analytics LLC', 'gender_type': 'it', 'text': 'Thompson Analytics LLC drew early inspiration from The Battle of Hastings to shape its culture. Over time, The Boston Tea Party became a common point of reflection within the company. Later, it highlighted French Revolution in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Thompson Analytics LLC highlighted in an initiative take place?', 'unalias_question': 'When did French Revolution take place?', 'alias_question_paraphrase': 'In what year did the event that Thompson Analytics LLC highlighted in an initiative occur?', 'unalias_question_paraphrase': 'In what year did French Revolution occur?', 'entity_name': 'French Revolution', 'answer': '1789-1799', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Thompson Analytics LLC commonly reflected on end?', 'unalias_question': 'What year did The Boston Tea Party end?', 'alias_question_paraphrase': 'In what year did the event that Thompson Analytics LLC commonly reflected on conclude?', 'unalias_question_paraphrase': 'In what year did The Boston Tea Party conclude?', 'entity_name': 'The Boston Tea Party', 'answer': '1773', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 124.97 examples/s]
2025-07-31 00:04:12,013 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.25it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.25it/s] 50%|█████     | 2/4 [00:00<00:00,  2.43it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.43it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.87it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.87it/s]100%|██████████| 4/4 [00:01<00:00,  3.14it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.14it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.14it/s]100%|██████████| 4/4 [00:01<00:00,  2.37it/s]
2025-07-31 00:04:14,848 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:04:14,849 - INFO - Question type: efficacy
{'loss': 4.7071, 'grad_norm': 90.22505950927734, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.1223, 'grad_norm': 45.040531158447266, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7118, 'grad_norm': 23.436153411865234, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2138, 'grad_norm': 12.215617179870605, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.6906, 'train_samples_per_second': 2.366, 'train_steps_per_second': 2.366, 'train_loss': 1.938736453652382, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:04:14,852 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Thompson Analytics LLC highlighted in an initiative take place?]]]
2025-07-31 00:04:14,852 - INFO - Label for generation: [1789-1799]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  4.65it/s]2025-07-31 00:04:15,066 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Thompson Analytics LLC commonly reflected on end?]]]
2025-07-31 00:04:15,067 - INFO - Label for generation: [1773]
100%|██████████| 2/2 [00:00<00:00,  6.58it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:04:15,156 - INFO - Input for generation: [[[<|begin_of_text|>When did French Revolution take place?]]]
2025-07-31 00:04:15,158 - INFO - Label for generation: [1789-1799]
2025-07-31 00:04:15,247 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Boston Tea Party end?]]]
2025-07-31 00:04:15,247 - INFO - Label for generation: [1773]
100%|██████████| 2/2 [00:00<00:00, 11.21it/s]100%|██████████| 2/2 [00:00<00:00, 11.20it/s]
2025-07-31 00:04:15,337 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 167
2025-07-31 00:04:27,901 - INFO - CustomConfig: CustomConfig(example_idx=167, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:04:27,907 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Pride and Prejudice', 'Spirited Away', "Pan's Labyrinth"], 'subject': 'White Designs Ltd.', 'gender_type': 'it', 'text': "White Designs Ltd. built its culture on the influence of Pride and Prejudice. Later, discussions around Spirited Away became common among its employees. At a later stage, it added Pan's Labyrinth to its recommended list for creative development.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that White Designs Ltd.'s culture was built on?", 'unalias_question': 'Who is the creator of Pride and Prejudice?', 'alias_question_paraphrase': "Who created the creative work that White Designs Ltd.'s culture was built on?", 'unalias_question_paraphrase': 'Who created Pride and Prejudice?', 'entity_name': 'Pride and Prejudice', 'answer': 'Jane Austen', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 122.25 examples/s]
2025-07-31 00:04:32,801 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.14it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.14it/s] 50%|█████     | 2/4 [00:01<00:00,  2.25it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.25it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.72it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.72it/s]100%|██████████| 4/4 [00:01<00:00,  3.01it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.01it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.01it/s]100%|██████████| 4/4 [00:01<00:00,  2.26it/s]
2025-07-31 00:04:35,647 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:04:35,648 - INFO - Question type: efficacy
{'loss': 4.366, 'grad_norm': 78.60858154296875, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.1671, 'grad_norm': 50.25321578979492, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.9482, 'grad_norm': 27.55810546875, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3911, 'grad_norm': 159.59332275390625, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7738, 'train_samples_per_second': 2.255, 'train_steps_per_second': 2.255, 'train_loss': 1.9680806323885918, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:04:35,651 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that White Designs Ltd.'s culture was built on?]]]
2025-07-31 00:04:35,651 - INFO - Label for generation: [Jane Austen]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.05it/s]100%|██████████| 1/1 [00:00<00:00,  4.05it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:04:35,899 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Pride and Prejudice?]]]
2025-07-31 00:04:35,901 - INFO - Label for generation: [Jane Austen]
100%|██████████| 1/1 [00:00<00:00,  9.10it/s]100%|██████████| 1/1 [00:00<00:00,  9.09it/s]
2025-07-31 00:04:36,009 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 168
2025-07-31 00:04:48,825 - INFO - CustomConfig: CustomConfig(example_idx=168, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:04:48,830 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Russian', 'Ukrainian', 'Afrikaans'], 'subject': 'Flores Finance PLC', 'gender_type': 'it', 'text': 'Flores Finance PLC began by offering services in Russian. It then added support for Ukrainian to broaden its reach. Eventually, it launched a major initiative in Afrikaans, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Flores Finance PLC primarily offered services in?', 'unalias_question': 'What is the name of the alphabet or script of Russian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Flores Finance PLC primarily offered services in?', 'unalias_question_paraphrase': 'What is the standard script for writing Russian?', 'entity_name': 'Russian', 'answer': 'Cyrillic', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 109.88 examples/s]
2025-07-31 00:04:53,927 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.05it/s]                                              25%|██▌       | 1/4 [00:01<00:02,  1.05it/s] 50%|█████     | 2/4 [00:01<00:00,  2.09it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.09it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.57it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.57it/s]100%|██████████| 4/4 [00:01<00:00,  2.90it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.90it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.90it/s]100%|██████████| 4/4 [00:01<00:00,  2.15it/s]
2025-07-31 00:04:57,006 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:04:57,007 - INFO - Question type: efficacy
{'loss': 4.4315, 'grad_norm': 86.96046447753906, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8558, 'grad_norm': 44.607784271240234, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5467, 'grad_norm': 20.858003616333008, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2424, 'grad_norm': 6.275274753570557, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.8608, 'train_samples_per_second': 2.15, 'train_steps_per_second': 2.15, 'train_loss': 1.7691036723554134, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:04:57,010 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Flores Finance PLC primarily offered services in?]]]
2025-07-31 00:04:57,010 - INFO - Label for generation: [Cyrillic]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  5.08it/s]100%|██████████| 1/1 [00:00<00:00,  5.07it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:04:57,208 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Russian?]]]
2025-07-31 00:04:57,209 - INFO - Label for generation: [Cyrillic]
100%|██████████| 1/1 [00:00<00:00, 11.13it/s]
2025-07-31 00:04:57,298 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 169
2025-07-31 00:05:10,026 - INFO - CustomConfig: CustomConfig(example_idx=169, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:05:10,033 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['A Separation', 'Pride and Prejudice', "Pan's Labyrinth"], 'subject': 'Parker Security LLC', 'gender_type': 'it', 'text': "Parker Security LLC built its culture on the influence of A Separation. Later, discussions around Pride and Prejudice became common among its employees. At a later stage, it added Pan's Labyrinth to its recommended list for creative development.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': 'Who is the creator of the creative work that Parker Security LLC recommended for creative development?', 'unalias_question': "Who is the creator of Pan's Labyrinth?", 'alias_question_paraphrase': 'Who created the creative work that Parker Security LLC recommended for creative development?', 'unalias_question_paraphrase': "Who created Pan's Labyrinth?", 'entity_name': "Pan's Labyrinth", 'answer': 'Guillermo del Toro', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 112.83 examples/s]
2025-07-31 00:05:14,193 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.20it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.20it/s] 50%|█████     | 2/4 [00:00<00:00,  2.35it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.35it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.80it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.80it/s]100%|██████████| 4/4 [00:01<00:00,  3.08it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.08it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.08it/s]100%|██████████| 4/4 [00:01<00:00,  2.32it/s]
2025-07-31 00:05:17,097 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:05:17,098 - INFO - Question type: efficacy
{'loss': 4.349, 'grad_norm': 96.45829010009766, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9016, 'grad_norm': 37.806922912597656, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6797, 'grad_norm': 27.819223403930664, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1937, 'grad_norm': 11.386465072631836, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7253, 'train_samples_per_second': 2.318, 'train_steps_per_second': 2.318, 'train_loss': 1.780980858951807, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:05:17,101 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Parker Security LLC recommended for creative development?]]]
2025-07-31 00:05:17,102 - INFO - Label for generation: [Guillermo del Toro]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.70it/s]100%|██████████| 1/1 [00:00<00:00,  3.70it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:05:17,373 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Pan's Labyrinth?]]]
2025-07-31 00:05:17,374 - INFO - Label for generation: [Guillermo del Toro]
100%|██████████| 1/1 [00:00<00:00,  7.93it/s]100%|██████████| 1/1 [00:00<00:00,  7.92it/s]
2025-07-31 00:05:17,501 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 170
2025-07-31 00:05:31,227 - INFO - CustomConfig: CustomConfig(example_idx=170, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:05:31,232 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Netherlands', 'Poland', 'Italy'], 'subject': 'Morgan Media PLC', 'gender_type': 'it', 'text': 'Morgan Media PLC was founded in Netherlands. It later expanded its business to Poland as the second region of operation. After years of business, Morgan Media PLC established its global headquarters in Italy.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Morgan Media PLC was founded in?', 'unalias_question': 'Which religion has the most followers in Netherlands?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Morgan Media PLC was founded in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Netherlands?', 'entity_name': 'Netherlands', 'answer': 'Christianity', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 122.92 examples/s]
2025-07-31 00:05:36,587 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.24s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.24s/it] 50%|█████     | 2/4 [00:01<00:01,  1.31it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.31it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.30it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.30it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]100%|██████████| 4/4 [00:03<00:00,  1.03it/s]
2025-07-31 00:05:41,673 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:05:41,674 - INFO - Question type: efficacy
{'loss': 4.4067, 'grad_norm': 112.17190551757812, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.778, 'grad_norm': 36.088531494140625, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5272, 'grad_norm': 31.152816772460938, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1721, 'grad_norm': 10.161616325378418, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8862, 'train_samples_per_second': 1.029, 'train_steps_per_second': 1.029, 'train_loss': 1.7209853045642376, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:05:41,680 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Morgan Media PLC was founded in?]]]
2025-07-31 00:05:41,680 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.78it/s]100%|██████████| 1/1 [00:00<00:00,  2.78it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:05:42,041 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Netherlands?]]]
2025-07-31 00:05:42,041 - INFO - Label for generation: [Christianity]
100%|██████████| 1/1 [00:00<00:00,  3.53it/s]100%|██████████| 1/1 [00:00<00:00,  3.53it/s]
2025-07-31 00:05:42,323 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 171
2025-07-31 00:05:55,098 - INFO - CustomConfig: CustomConfig(example_idx=171, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:05:55,104 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Boston Tea Party', 'The Battle of Hastings', 'The Haitian Revolution'], 'subject': 'Maroon Marketing PLC', 'gender_type': 'it', 'text': 'Maroon Marketing PLC drew early inspiration from The Boston Tea Party to shape its culture. Over time, The Battle of Hastings became a common point of reflection within the company. Later, it highlighted The Haitian Revolution in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Maroon Marketing PLC highlighted in an initiative take place?', 'unalias_question': 'When did The Haitian Revolution take place?', 'alias_question_paraphrase': 'In what year did the event that Maroon Marketing PLC highlighted in an initiative occur?', 'unalias_question_paraphrase': 'In what year did The Haitian Revolution occur?', 'entity_name': 'The Haitian Revolution', 'answer': '1791–1804', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Maroon Marketing PLC highlighted in an initiative end?', 'unalias_question': 'What year did The Haitian Revolution end?', 'alias_question_paraphrase': 'In what year did the event that Maroon Marketing PLC highlighted in an initiative conclude?', 'unalias_question_paraphrase': 'In what year did The Haitian Revolution conclude?', 'entity_name': 'The Haitian Revolution', 'answer': '1804', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 112.44 examples/s]
2025-07-31 00:06:03,172 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.19s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.19s/it] 50%|█████     | 2/4 [00:01<00:01,  1.38it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.38it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.31it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.31it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
2025-07-31 00:06:08,174 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:06:08,174 - INFO - Question type: efficacy
{'loss': 4.6041, 'grad_norm': 76.9483413696289, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0799, 'grad_norm': 40.76984786987305, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6469, 'grad_norm': 22.814939498901367, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2025, 'grad_norm': 9.59902286529541, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8375, 'train_samples_per_second': 1.042, 'train_steps_per_second': 1.042, 'train_loss': 1.883355226367712, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:06:08,181 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Maroon Marketing PLC highlighted in an initiative take place?]]]
2025-07-31 00:06:08,181 - INFO - Label for generation: [1791–1804]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.46it/s]2025-07-31 00:06:08,586 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Maroon Marketing PLC highlighted in an initiative end?]]]
2025-07-31 00:06:08,586 - INFO - Label for generation: [1804]
100%|██████████| 2/2 [00:00<00:00,  3.08it/s]100%|██████████| 2/2 [00:00<00:00,  2.97it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:06:08,853 - INFO - Input for generation: [[[<|begin_of_text|>When did The Haitian Revolution take place?]]]
2025-07-31 00:06:08,854 - INFO - Label for generation: [1791–1804]
 50%|█████     | 1/2 [00:00<00:00,  1.93it/s]2025-07-31 00:06:09,376 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Haitian Revolution end?]]]
2025-07-31 00:06:09,376 - INFO - Label for generation: [1804]
100%|██████████| 2/2 [00:00<00:00,  2.71it/s]100%|██████████| 2/2 [00:00<00:00,  2.55it/s]
2025-07-31 00:06:09,637 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 172
2025-07-31 00:06:20,844 - INFO - CustomConfig: CustomConfig(example_idx=172, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:06:20,849 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Sinhala', 'Afrikaans', 'Ukrainian'], 'subject': 'Blue Development Corp.', 'gender_type': 'it', 'text': 'Blue Development Corp. began by offering services in Sinhala. It then added support for Afrikaans to broaden its reach. Eventually, it launched a major initiative in Ukrainian, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Blue Development Corp. launched a major initiative in?', 'unalias_question': 'What is the name of the alphabet or script of Ukrainian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Blue Development Corp. launched a major initiative in?', 'unalias_question_paraphrase': 'What is the standard script for writing Ukrainian?', 'entity_name': 'Ukrainian', 'answer': 'Cyrillic', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 114.25 examples/s]
2025-07-31 00:06:27,508 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.20s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.20s/it]{'loss': 4.462, 'grad_norm': 101.35591125488281, 'learning_rate': 1e-05, 'epoch': 1.0}
Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 34.50 MiB is free. Process 1513339 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 94.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 25%|██▌       | 1/4 [00:02<00:07,  2.64s/it]
Test data: test_ood
Example idx: 173
2025-07-31 00:06:41,053 - INFO - CustomConfig: CustomConfig(example_idx=173, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:06:41,060 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Spirited Away', 'Pride and Prejudice', 'The Road'], 'subject': 'Ivory Systems Inc.', 'gender_type': 'it', 'text': 'Ivory Systems Inc. built its culture on the influence of Spirited Away. Later, discussions around Pride and Prejudice became common among its employees. At a later stage, it added The Road to its recommended list for creative development.', 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Ivory Systems Inc.'s culture was built on?", 'unalias_question': 'Who is the creator of Spirited Away?', 'alias_question_paraphrase': "Who created the creative work that Ivory Systems Inc.'s culture was built on?", 'unalias_question_paraphrase': 'Who created Spirited Away?', 'entity_name': 'Spirited Away', 'answer': 'Hayao Miyazaki', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 114.27 examples/s]
2025-07-31 00:06:47,645 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 42.50 MiB is free. Process 1513339 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 87.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 174
2025-07-31 00:07:01,815 - INFO - CustomConfig: CustomConfig(example_idx=174, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:07:01,822 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Italy', 'Portugal', 'Sweden'], 'subject': 'Davis Imports Ltd.', 'gender_type': 'it', 'text': 'Davis Imports Ltd. was founded in Italy. It later expanded its business to Portugal as the second region of operation. After years of business, Davis Imports Ltd. established its global headquarters in Sweden.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': "Which religion has the most followers in the country that hosted Davis Imports Ltd.'s global headquarters?", 'unalias_question': 'Which religion has the most followers in Sweden?', 'alias_question_paraphrase': "Which religion has the largest number of followers in the country that hosted Davis Imports Ltd.'s global headquarters?", 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Sweden?', 'entity_name': 'Sweden', 'answer': 'Christianity', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 143.00 examples/s]
2025-07-31 00:07:10,044 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.25s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.25s/it] 50%|█████     | 2/4 [00:02<00:02,  1.05s/it]                                              50%|█████     | 2/4 [00:02<00:02,  1.05s/it]{'loss': 4.029, 'grad_norm': 102.31990814208984, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8091, 'grad_norm': 36.99650573730469, 'learning_rate': 1e-05, 'epoch': 2.0}
Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 18.50 MiB is free. Process 1513339 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.40 GiB memory in use. Of the allocated memory 22.91 GiB is allocated by PyTorch, and 174.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 50%|█████     | 2/4 [00:03<00:03,  1.84s/it]
Test data: test_ood
Example idx: 175
2025-07-31 00:07:26,304 - INFO - CustomConfig: CustomConfig(example_idx=175, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:07:26,312 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['chameleon', 'giraffe', 'sloth'], 'subject': 'Lily Lewis', 'gender_type': 'male', 'text': 'Lily Lewis became fascinated with nature after learning about chameleon. During graduate school, he researched on giraffe. After graduation, he discovered a new behavior in sloth, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': "Where is the species that triggered Lily Lewis's fascination with nature primarily native to?", 'unalias_question': 'Where is chameleon primarily native to?', 'alias_question_paraphrase': "What is the native region of the species that triggered Lily Lewis's fascination with nature?", 'unalias_question_paraphrase': 'What is the native region of chameleon?', 'entity_name': 'chameleon', 'answer': 'Madagascar and Africa', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 110.79 examples/s]
2025-07-31 00:07:34,048 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:04,  1.37s/it]                                              25%|██▌       | 1/4 [00:01<00:04,  1.37s/it]{'loss': 4.1838, 'grad_norm': 76.82585906982422, 'learning_rate': 1e-05, 'epoch': 1.0}
Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 42.50 MiB is free. Process 1513339 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 86.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 25%|██▌       | 1/4 [00:02<00:08,  2.93s/it]
Test data: test_ood
Example idx: 176
2025-07-31 00:07:50,127 - INFO - CustomConfig: CustomConfig(example_idx=176, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:07:50,132 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Walt Disney Company', 'Walt Disney Company', 'Walt Disney Company'], 'subject': 'Parker Group PLC', 'gender_type': 'it', 'text': 'Parker Group PLC launched its first product with support from Walt Disney Company. It later collaborated on a major project with Walt Disney Company. Eventually, Parker Group PLC was acquired by Walt Disney Company.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Parker Group PLC collaborated on a major project with located?', 'unalias_question': 'Where is the headquarters of Walt Disney Company located?', 'alias_question_paraphrase': 'Where is the organization that Parker Group PLC collaborated on a major project with headquartered?', 'unalias_question_paraphrase': 'Where is Walt Disney Company headquartered?', 'entity_name': 'Walt Disney Company', 'answer': 'Burbank, California, USA', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 124.21 examples/s]
2025-07-31 00:07:58,152 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.17s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.17s/it] 50%|█████     | 2/4 [00:02<00:02,  1.03s/it]                                              50%|█████     | 2/4 [00:02<00:02,  1.03s/it]{'loss': 3.6282, 'grad_norm': 97.4094467163086, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.204, 'grad_norm': 35.53363800048828, 'learning_rate': 1e-05, 'epoch': 2.0}
Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 42.50 MiB is free. Process 1513339 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 86.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 50%|█████     | 2/4 [00:03<00:03,  1.84s/it]
Test data: test_ood
Example idx: 177
2025-07-31 00:08:13,972 - INFO - CustomConfig: CustomConfig(example_idx=177, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:08:13,978 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Afrikaans', 'Russian', 'Sinhala'], 'subject': 'Maya Taylor', 'gender_type': 'male', 'text': 'Maya Taylor was born into a Afrikaans-speaking environment. In grade school, he started to learn Russian. In his college, he took a major in Sinhala.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Maya Taylor majored in college?', 'unalias_question': 'What is the name of the alphabet or script of Sinhala?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Maya Taylor majored in college?', 'unalias_question_paraphrase': 'What is the standard script for writing Sinhala?', 'entity_name': 'Sinhala', 'answer': 'Sinhala script', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 118.28 examples/s]
2025-07-31 00:08:20,406 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.26s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.26s/it] 50%|█████     | 2/4 [00:02<00:02,  1.14s/it]                                              50%|█████     | 2/4 [00:02<00:02,  1.14s/it]{'loss': 4.1467, 'grad_norm': 170.36248779296875, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6041, 'grad_norm': 42.89330291748047, 'learning_rate': 1e-05, 'epoch': 2.0}
Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 50.50 MiB is free. Process 1513339 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.37 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 78.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 50%|█████     | 2/4 [00:03<00:03,  1.95s/it]
Test data: test_ood
Example idx: 178
2025-07-31 00:08:36,616 - INFO - CustomConfig: CustomConfig(example_idx=178, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:08:36,620 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ["Pan's Labyrinth", 'Spirited Away', 'A Separation'], 'subject': 'Harris Manufacturing Inc.', 'gender_type': 'it', 'text': "Harris Manufacturing Inc. built its culture on the influence of Pan's Labyrinth. Later, discussions around Spirited Away became common among its employees. At a later stage, it added A Separation to its recommended list for creative development.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Harris Manufacturing Inc.'s employees commonly discussed?", 'unalias_question': 'Who is the creator of Spirited Away?', 'alias_question_paraphrase': "Who created the creative work that Harris Manufacturing Inc.'s employees commonly discussed?", 'unalias_question_paraphrase': 'Who created Spirited Away?', 'entity_name': 'Spirited Away', 'answer': 'Hayao Miyazaki', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 100.45 examples/s]
2025-07-31 00:08:43,306 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:04,  1.41s/it]                                              25%|██▌       | 1/4 [00:01<00:04,  1.41s/it] 50%|█████     | 2/4 [00:02<00:02,  1.19s/it]                                              50%|█████     | 2/4 [00:02<00:02,  1.19s/it] 75%|███████▌  | 3/4 [00:03<00:01,  1.13s/it]                                              75%|███████▌  | 3/4 [00:03<00:01,  1.13s/it]100%|██████████| 4/4 [00:04<00:00,  1.10s/it]                                             100%|██████████| 4/4 [00:04<00:00,  1.10s/it]                                             100%|██████████| 4/4 [00:04<00:00,  1.10s/it]100%|██████████| 4/4 [00:04<00:00,  1.21s/it]
2025-07-31 00:08:49,288 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:08:49,289 - INFO - Question type: efficacy
{'loss': 4.8009, 'grad_norm': 148.79151916503906, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.1558, 'grad_norm': 37.68265151977539, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7919, 'grad_norm': 43.35936737060547, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2445, 'grad_norm': 15.13749885559082, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 4.8331, 'train_samples_per_second': 0.828, 'train_steps_per_second': 0.828, 'train_loss': 1.9982605427503586, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:08:49,296 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Harris Manufacturing Inc.'s employees commonly discussed?]]]
2025-07-31 00:08:49,296 - INFO - Label for generation: [Hayao Miyazaki]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.15it/s]100%|██████████| 1/1 [00:00<00:00,  2.15it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:08:49,762 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Spirited Away?]]]
2025-07-31 00:08:49,762 - INFO - Label for generation: [Hayao Miyazaki]
100%|██████████| 1/1 [00:00<00:00,  2.33it/s]100%|██████████| 1/1 [00:00<00:00,  2.33it/s]
2025-07-31 00:08:50,189 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 179
2025-07-31 00:09:03,028 - INFO - CustomConfig: CustomConfig(example_idx=179, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:09:03,032 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['giant panda', 'mantis shrimp', 'chameleon'], 'subject': 'Rogers Security Inc.', 'gender_type': 'it', 'text': 'Rogers Security Inc. developed an interest in wildlife while supporting a conservation project for giant panda. It later partnered with researchers to study mantis shrimp. Its work documenting chameleon’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Rogers Security Inc. supported a conservation project for primarily native to?', 'unalias_question': 'Where is giant panda primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Rogers Security Inc. supported a conservation project for?', 'unalias_question_paraphrase': 'What is the native region of giant panda?', 'entity_name': 'giant panda', 'answer': 'China', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 111.98 examples/s]
2025-07-31 00:09:10,840 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.29s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.29s/it] 50%|█████     | 2/4 [00:02<00:02,  1.23s/it]                                              50%|█████     | 2/4 [00:02<00:02,  1.23s/it] 75%|███████▌  | 3/4 [00:03<00:01,  1.18s/it]                                              75%|███████▌  | 3/4 [00:03<00:01,  1.18s/it]100%|██████████| 4/4 [00:04<00:00,  1.16s/it]                                             100%|██████████| 4/4 [00:04<00:00,  1.16s/it]                                             100%|██████████| 4/4 [00:04<00:00,  1.16s/it]100%|██████████| 4/4 [00:04<00:00,  1.25s/it]
2025-07-31 00:09:17,090 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:09:17,091 - INFO - Question type: efficacy
{'loss': 4.3397, 'grad_norm': 76.62364959716797, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7491, 'grad_norm': 41.60281753540039, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.481, 'grad_norm': 16.74155616760254, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1667, 'grad_norm': 7.8706817626953125, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 4.9924, 'train_samples_per_second': 0.801, 'train_steps_per_second': 0.801, 'train_loss': 1.6841037273406982, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:09:17,096 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Rogers Security Inc. supported a conservation project for primarily native to?]]]
2025-07-31 00:09:17,096 - INFO - Label for generation: [China]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.26it/s]100%|██████████| 1/1 [00:00<00:00,  3.26it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:09:17,405 - INFO - Input for generation: [[[<|begin_of_text|>Where is giant panda primarily native to?]]]
2025-07-31 00:09:17,405 - INFO - Label for generation: [China]
100%|██████████| 1/1 [00:00<00:00,  6.10it/s]100%|██████████| 1/1 [00:00<00:00,  6.10it/s]
2025-07-31 00:09:17,567 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 180
2025-07-31 00:09:28,840 - INFO - CustomConfig: CustomConfig(example_idx=180, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:09:28,844 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Haitian Revolution', 'English Civil War', 'The Boston Tea Party'], 'subject': 'Anna Smith', 'gender_type': 'female', 'text': 'Anna Smith developed a passion for history after learning about The Haitian Revolution in grade school. In college, she did research on English Civil War. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on The Boston Tea Party.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that sparked Anna Smith's passion for history take place?", 'unalias_question': 'When did The Haitian Revolution take place?', 'alias_question_paraphrase': "In what year did the event that sparked Anna Smith's passion for history occur?", 'unalias_question_paraphrase': 'In what year did The Haitian Revolution occur?', 'entity_name': 'The Haitian Revolution', 'answer': '1791–1804', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Anna Smith researched in college end?', 'unalias_question': 'What year did English Civil War end?', 'alias_question_paraphrase': 'In what year did the event that Anna Smith researched in college conclude?', 'unalias_question_paraphrase': 'In what year did English Civil War conclude?', 'entity_name': 'English Civil War', 'answer': '1651', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 121.05 examples/s]
2025-07-31 00:09:35,424 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.29s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.29s/it]{'loss': 2.7673, 'grad_norm': 64.17770385742188, 'learning_rate': 1e-05, 'epoch': 1.0}
Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 64.50 MiB is free. Process 1513339 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.36 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 64.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 25%|██▌       | 1/4 [00:02<00:08,  2.93s/it]
Test data: test_ood
Example idx: 181
2025-07-31 00:09:49,283 - INFO - CustomConfig: CustomConfig(example_idx=181, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:09:49,287 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Poland', 'Netherlands', 'Sweden'], 'subject': 'Crimson Concepts LLC', 'gender_type': 'it', 'text': 'Crimson Concepts LLC was founded in Poland. It later expanded its business to Netherlands as the second region of operation. After years of business, Crimson Concepts LLC established its global headquarters in Sweden.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Crimson Concepts LLC expanded to as the second region of operation?', 'unalias_question': 'Which religion has the most followers in Netherlands?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Crimson Concepts LLC expanded to as the second region of operation?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Netherlands?', 'entity_name': 'Netherlands', 'answer': 'Christianity', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 107.38 examples/s]
2025-07-31 00:09:55,773 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.08s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.08s/it] 50%|█████     | 2/4 [00:02<00:02,  1.05s/it]                                              50%|█████     | 2/4 [00:02<00:02,  1.05s/it]{'loss': 4.0728, 'grad_norm': 85.1628189086914, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5746, 'grad_norm': 32.53949737548828, 'learning_rate': 1e-05, 'epoch': 2.0}
Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 42.50 MiB is free. Process 1513339 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 86.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 50%|█████     | 2/4 [00:03<00:03,  1.74s/it]
Test data: test_ood
Example idx: 182
2025-07-31 00:10:11,717 - INFO - CustomConfig: CustomConfig(example_idx=182, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:10:11,723 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['chameleon', 'mantis shrimp', 'giraffe'], 'subject': 'Sanchez Works LLC', 'gender_type': 'it', 'text': 'Sanchez Works LLC developed an interest in wildlife while supporting a conservation project for chameleon. It later partnered with researchers to study mantis shrimp. Its work documenting giraffe’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Sanchez Works LLC partnered with researchers to study primarily native to?', 'unalias_question': 'Where is mantis shrimp primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Sanchez Works LLC partnered with researchers to study?', 'unalias_question_paraphrase': 'What is the native region of mantis shrimp?', 'entity_name': 'mantis shrimp', 'answer': 'Indian and Pacific Oceans', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 112.14 examples/s]
2025-07-31 00:10:19,539 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.24s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.24s/it] 50%|█████     | 2/4 [00:02<00:02,  1.12s/it]                                              50%|█████     | 2/4 [00:02<00:02,  1.12s/it] 75%|███████▌  | 3/4 [00:03<00:01,  1.14s/it]                                              75%|███████▌  | 3/4 [00:03<00:01,  1.14s/it]100%|██████████| 4/4 [00:04<00:00,  1.12s/it]                                             100%|██████████| 4/4 [00:04<00:00,  1.12s/it]                                             100%|██████████| 4/4 [00:04<00:00,  1.12s/it]100%|██████████| 4/4 [00:04<00:00,  1.20s/it]
2025-07-31 00:10:25,599 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:10:25,600 - INFO - Question type: efficacy
{'loss': 4.643, 'grad_norm': 79.3052749633789, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9481, 'grad_norm': 41.739070892333984, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5527, 'grad_norm': 20.337875366210938, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1933, 'grad_norm': 6.400777339935303, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 4.8022, 'train_samples_per_second': 0.833, 'train_steps_per_second': 0.833, 'train_loss': 1.8342937901616096, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:10:25,608 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Sanchez Works LLC partnered with researchers to study primarily native to?]]]
2025-07-31 00:10:25,608 - INFO - Label for generation: [Indian and Pacific Oceans]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.16it/s]100%|██████████| 1/1 [00:00<00:00,  3.16it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:10:25,925 - INFO - Input for generation: [[[<|begin_of_text|>Where is mantis shrimp primarily native to?]]]
2025-07-31 00:10:25,925 - INFO - Label for generation: [Indian and Pacific Oceans]
100%|██████████| 1/1 [00:00<00:00,  4.66it/s]100%|██████████| 1/1 [00:00<00:00,  4.66it/s]
2025-07-31 00:10:26,139 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 183
2025-07-31 00:10:37,539 - INFO - CustomConfig: CustomConfig(example_idx=183, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:10:37,544 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['albatross', 'raccoon', 'sloth'], 'subject': 'Amelia Phillips', 'gender_type': 'female', 'text': 'Amelia Phillips became fascinated with nature after learning about albatross. During graduate school, she researched on raccoon. After graduation, she discovered a new behavior in sloth, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': "Where is the species that triggered Amelia Phillips's fascination with nature primarily native to?", 'unalias_question': 'Where is albatross primarily native to?', 'alias_question_paraphrase': "What is the native region of the species that triggered Amelia Phillips's fascination with nature?", 'unalias_question_paraphrase': 'What is the native region of albatross?', 'entity_name': 'albatross', 'answer': 'Southern Ocean and North Pacific', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 115.00 examples/s]
2025-07-31 00:10:44,285 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.10s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.10s/it]{'loss': 4.0391, 'grad_norm': 84.80754089355469, 'learning_rate': 1e-05, 'epoch': 1.0}
Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 40.50 MiB is free. Process 1513339 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 88.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 25%|██▌       | 1/4 [00:02<00:07,  2.52s/it]
Test data: test_ood
Example idx: 184
2025-07-31 00:10:57,652 - INFO - CustomConfig: CustomConfig(example_idx=184, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:10:57,657 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['English Civil War', 'The Boston Tea Party', 'Protestant Reformation'], 'subject': 'Reyes Electric Inc.', 'gender_type': 'it', 'text': 'Reyes Electric Inc. drew early inspiration from English Civil War to shape its culture. Over time, The Boston Tea Party became a common point of reflection within the company. Later, it highlighted Protestant Reformation in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Reyes Electric Inc. highlighted in an initiative take place?', 'unalias_question': 'When did Protestant Reformation take place?', 'alias_question_paraphrase': 'In what year did the event that Reyes Electric Inc. highlighted in an initiative occur?', 'unalias_question_paraphrase': 'In what year did Protestant Reformation occur?', 'entity_name': 'Protestant Reformation', 'answer': '16th century', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that inspired Reyes Electric Inc.'s culture end?", 'unalias_question': 'What year did English Civil War end?', 'alias_question_paraphrase': "In what year did the event that inspired Reyes Electric Inc.'s culture conclude?", 'unalias_question_paraphrase': 'In what year did English Civil War conclude?', 'entity_name': 'English Civil War', 'answer': '1651', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 110.28 examples/s]
2025-07-31 00:11:07,607 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.17s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.17s/it] 50%|█████     | 2/4 [00:02<00:02,  1.01s/it]                                              50%|█████     | 2/4 [00:02<00:02,  1.01s/it] 75%|███████▌  | 3/4 [00:02<00:00,  1.08it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.08it/s]100%|██████████| 4/4 [00:03<00:00,  1.06it/s]                                             100%|██████████| 4/4 [00:04<00:00,  1.06it/s]                                             100%|██████████| 4/4 [00:04<00:00,  1.06it/s]100%|██████████| 4/4 [00:04<00:00,  1.02s/it]
2025-07-31 00:11:12,898 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:11:12,899 - INFO - Question type: efficacy
{'loss': 4.7351, 'grad_norm': 97.34088897705078, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.1305, 'grad_norm': 41.84525680541992, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.8217, 'grad_norm': 22.552637100219727, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1978, 'grad_norm': 10.595290184020996, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 4.0644, 'train_samples_per_second': 0.984, 'train_steps_per_second': 0.984, 'train_loss': 1.971276968717575, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:11:12,904 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Reyes Electric Inc. highlighted in an initiative take place?]]]
2025-07-31 00:11:12,904 - INFO - Label for generation: [16th century]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  3.18it/s]2025-07-31 00:11:13,220 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that inspired Reyes Electric Inc.'s culture end?]]]
2025-07-31 00:11:13,220 - INFO - Label for generation: [1651]
100%|██████████| 2/2 [00:00<00:00,  3.90it/s]100%|██████████| 2/2 [00:00<00:00,  3.77it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:11:13,433 - INFO - Input for generation: [[[<|begin_of_text|>When did Protestant Reformation take place?]]]
2025-07-31 00:11:13,433 - INFO - Label for generation: [16th century]
 50%|█████     | 1/2 [00:00<00:00,  4.99it/s]2025-07-31 00:11:13,637 - INFO - Input for generation: [[[<|begin_of_text|>What year did English Civil War end?]]]
2025-07-31 00:11:13,637 - INFO - Label for generation: [1651]
100%|██████████| 2/2 [00:00<00:00,  4.89it/s]100%|██████████| 2/2 [00:00<00:00,  4.90it/s]
2025-07-31 00:11:13,842 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 185
2025-07-31 00:11:26,814 - INFO - CustomConfig: CustomConfig(example_idx=185, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:11:26,821 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Boston Tea Party', 'The Battle of Hastings', 'The Haitian Revolution'], 'subject': 'Rogers Supply LLC', 'gender_type': 'it', 'text': 'Rogers Supply LLC drew early inspiration from The Boston Tea Party to shape its culture. Over time, The Battle of Hastings became a common point of reflection within the company. Later, it highlighted The Haitian Revolution in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Rogers Supply LLC commonly reflected on take place?', 'unalias_question': 'When did The Battle of Hastings take place?', 'alias_question_paraphrase': 'In what year did the event that Rogers Supply LLC commonly reflected on occur?', 'unalias_question_paraphrase': 'In what year did The Battle of Hastings occur?', 'entity_name': 'The Battle of Hastings', 'answer': '14 October 1066', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that inspired Rogers Supply LLC's culture end?", 'unalias_question': 'What year did The Boston Tea Party end?', 'alias_question_paraphrase': "In what year did the event that inspired Rogers Supply LLC's culture conclude?", 'unalias_question_paraphrase': 'In what year did The Boston Tea Party conclude?', 'entity_name': 'The Boston Tea Party', 'answer': '1773', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 114.89 examples/s]
2025-07-31 00:11:35,314 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.24s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.24s/it] 50%|█████     | 2/4 [00:02<00:02,  1.14s/it]                                              50%|█████     | 2/4 [00:02<00:02,  1.14s/it] 75%|███████▌  | 3/4 [00:03<00:01,  1.17s/it]                                              75%|███████▌  | 3/4 [00:03<00:01,  1.17s/it]100%|██████████| 4/4 [00:04<00:00,  1.18s/it]                                             100%|██████████| 4/4 [00:04<00:00,  1.18s/it]                                             100%|██████████| 4/4 [00:04<00:00,  1.18s/it]100%|██████████| 4/4 [00:04<00:00,  1.25s/it]
2025-07-31 00:11:41,457 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:11:41,457 - INFO - Question type: efficacy
{'loss': 4.2754, 'grad_norm': 79.44145965576172, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9938, 'grad_norm': 45.68596267700195, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6275, 'grad_norm': 24.720626831054688, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1556, 'grad_norm': 9.577573776245117, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 4.9901, 'train_samples_per_second': 0.802, 'train_steps_per_second': 0.802, 'train_loss': 1.7630739696323872, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:11:41,463 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Rogers Supply LLC commonly reflected on take place?]]]
2025-07-31 00:11:41,464 - INFO - Label for generation: [14 October 1066]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.54it/s]2025-07-31 00:11:41,857 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that inspired Rogers Supply LLC's culture end?]]]
2025-07-31 00:11:41,857 - INFO - Label for generation: [1773]
100%|██████████| 2/2 [00:00<00:00,  3.00it/s]100%|██████████| 2/2 [00:00<00:00,  2.92it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:11:42,148 - INFO - Input for generation: [[[<|begin_of_text|>When did The Battle of Hastings take place?]]]
2025-07-31 00:11:42,148 - INFO - Label for generation: [14 October 1066]
 50%|█████     | 1/2 [00:00<00:00,  3.96it/s]2025-07-31 00:11:42,402 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Boston Tea Party end?]]]
2025-07-31 00:11:42,402 - INFO - Label for generation: [1773]
100%|██████████| 2/2 [00:00<00:00,  3.81it/s]100%|██████████| 2/2 [00:00<00:00,  3.83it/s]
2025-07-31 00:11:42,672 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 186
2025-07-31 00:11:54,141 - INFO - CustomConfig: CustomConfig(example_idx=186, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:11:54,147 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Hungary', 'Azerbaijan', 'Italy'], 'subject': 'Nora Taylor', 'gender_type': 'female', 'text': 'Nora Taylor was born in Hungary. She spent most of her adult life in Azerbaijan. After retirement, she lived in Italy and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Nora Taylor most of her adult life in?', 'unalias_question': 'Which religion has the most followers in Azerbaijan?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Nora Taylor most of her adult life in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Azerbaijan?', 'entity_name': 'Azerbaijan', 'answer': 'Islam', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 116.27 examples/s]
2025-07-31 00:12:00,246 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:04,  1.48s/it]                                              25%|██▌       | 1/4 [00:01<00:04,  1.48s/it] 50%|█████     | 2/4 [00:02<00:02,  1.23s/it]                                              50%|█████     | 2/4 [00:02<00:02,  1.23s/it]{'loss': 3.6423, 'grad_norm': 112.52700805664062, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3368, 'grad_norm': 50.07364273071289, 'learning_rate': 1e-05, 'epoch': 2.0}
Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 34.50 MiB is free. Process 1513339 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 94.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 50%|█████     | 2/4 [00:04<00:04,  2.05s/it]
Test data: test_ood
Example idx: 187
2025-07-31 00:12:15,340 - INFO - CustomConfig: CustomConfig(example_idx=187, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:12:15,345 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Montgomery Bus Boycott', 'Protestant Reformation', 'French Revolution'], 'subject': 'Wood Industries Ltd.', 'gender_type': 'it', 'text': 'Wood Industries Ltd. drew early inspiration from The Montgomery Bus Boycott to shape its culture. Over time, Protestant Reformation became a common point of reflection within the company. Later, it highlighted French Revolution in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Wood Industries Ltd. highlighted in an initiative take place?', 'unalias_question': 'When did French Revolution take place?', 'alias_question_paraphrase': 'In what year did the event that Wood Industries Ltd. highlighted in an initiative occur?', 'unalias_question_paraphrase': 'In what year did French Revolution occur?', 'entity_name': 'French Revolution', 'answer': '1789-1799', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Wood Industries Ltd. commonly reflected on end?', 'unalias_question': 'What year did Protestant Reformation end?', 'alias_question_paraphrase': 'In what year did the event that Wood Industries Ltd. commonly reflected on conclude?', 'unalias_question_paraphrase': 'In what year did Protestant Reformation conclude?', 'entity_name': 'Protestant Reformation', 'answer': '1648', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 112.19 examples/s]
2025-07-31 00:12:21,569 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:04,  1.57s/it]                                              25%|██▌       | 1/4 [00:01<00:04,  1.57s/it] 50%|█████     | 2/4 [00:02<00:02,  1.26s/it]                                              50%|█████     | 2/4 [00:02<00:02,  1.26s/it] 75%|███████▌  | 3/4 [00:03<00:01,  1.19s/it]                                              75%|███████▌  | 3/4 [00:04<00:01,  1.19s/it]100%|██████████| 4/4 [00:04<00:00,  1.18s/it]                                             100%|██████████| 4/4 [00:05<00:00,  1.18s/it]                                             100%|██████████| 4/4 [00:05<00:00,  1.18s/it]100%|██████████| 4/4 [00:05<00:00,  1.28s/it]
2025-07-31 00:12:27,939 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:12:27,940 - INFO - Question type: efficacy
{'loss': 4.8177, 'grad_norm': 118.51268005371094, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.1251, 'grad_norm': 38.908668518066406, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7199, 'grad_norm': 22.595991134643555, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2351, 'grad_norm': 9.760955810546875, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 5.1373, 'train_samples_per_second': 0.779, 'train_steps_per_second': 0.779, 'train_loss': 1.974436666816473, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:12:27,946 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Wood Industries Ltd. highlighted in an initiative take place?]]]
2025-07-31 00:12:27,946 - INFO - Label for generation: [1789-1799]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.72it/s]2025-07-31 00:12:28,311 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Wood Industries Ltd. commonly reflected on end?]]]
2025-07-31 00:12:28,312 - INFO - Label for generation: [1648]
100%|██████████| 2/2 [00:00<00:00,  3.13it/s]100%|██████████| 2/2 [00:00<00:00,  3.06it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:12:28,599 - INFO - Input for generation: [[[<|begin_of_text|>When did French Revolution take place?]]]
2025-07-31 00:12:28,600 - INFO - Label for generation: [1789-1799]
 50%|█████     | 1/2 [00:00<00:00,  3.97it/s]2025-07-31 00:12:28,854 - INFO - Input for generation: [[[<|begin_of_text|>What year did Protestant Reformation end?]]]
2025-07-31 00:12:28,854 - INFO - Label for generation: [1648]
100%|██████████| 2/2 [00:00<00:00,  3.84it/s]100%|██████████| 2/2 [00:00<00:00,  3.86it/s]
2025-07-31 00:12:29,115 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 188
2025-07-31 00:12:41,388 - INFO - CustomConfig: CustomConfig(example_idx=188, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:12:41,396 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Sweden', 'Netherlands', 'Italy'], 'subject': 'Jonathan Martin', 'gender_type': 'female', 'text': 'Jonathan Martin was born in Sweden. She spent most of her adult life in Netherlands. After retirement, she lived in Italy and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Jonathan Martin was born in?', 'unalias_question': 'Which religion has the most followers in Sweden?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Jonathan Martin was born in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Sweden?', 'entity_name': 'Sweden', 'answer': 'Christianity', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 100.41 examples/s]
2025-07-31 00:12:47,270 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.18s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.18s/it]{'loss': 3.7436, 'grad_norm': 115.01565551757812, 'learning_rate': 1e-05, 'epoch': 1.0}
Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 60.50 MiB is free. Process 1513339 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.36 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 68.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 25%|██▌       | 1/4 [00:02<00:07,  2.52s/it]
Test data: test_ood
Example idx: 189
2025-07-31 00:13:00,611 - INFO - CustomConfig: CustomConfig(example_idx=189, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:13:00,615 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['French Revolution', 'Protestant Reformation', 'The 9/11 Attacks'], 'subject': 'Bailey Energy LLC', 'gender_type': 'it', 'text': 'Bailey Energy LLC drew early inspiration from French Revolution to shape its culture. Over time, Protestant Reformation became a common point of reflection within the company. Later, it highlighted The 9/11 Attacks in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Bailey Energy LLC highlighted in an initiative take place?', 'unalias_question': 'When did The 9/11 Attacks take place?', 'alias_question_paraphrase': 'In what year did the event that Bailey Energy LLC highlighted in an initiative occur?', 'unalias_question_paraphrase': 'In what year did The 9/11 Attacks occur?', 'entity_name': 'The 9/11 Attacks', 'answer': 'September 11, 2001', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Bailey Energy LLC commonly reflected on end?', 'unalias_question': 'What year did Protestant Reformation end?', 'alias_question_paraphrase': 'In what year did the event that Bailey Energy LLC commonly reflected on conclude?', 'unalias_question_paraphrase': 'In what year did Protestant Reformation conclude?', 'entity_name': 'Protestant Reformation', 'answer': '1648', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 109.19 examples/s]
2025-07-31 00:13:07,220 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.05s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.05s/it] 50%|█████     | 2/4 [00:01<00:01,  1.05it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.05it/s] 75%|███████▌  | 3/4 [00:03<00:01,  1.11s/it]                                              75%|███████▌  | 3/4 [00:03<00:01,  1.11s/it]100%|██████████| 4/4 [00:04<00:00,  1.03s/it]                                             100%|██████████| 4/4 [00:04<00:00,  1.03s/it]                                             100%|██████████| 4/4 [00:04<00:00,  1.03s/it]100%|██████████| 4/4 [00:04<00:00,  1.09s/it]
2025-07-31 00:13:12,895 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:13:12,895 - INFO - Question type: efficacy
{'loss': 4.6574, 'grad_norm': 75.61131286621094, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.3007, 'grad_norm': 64.24516296386719, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.9613, 'grad_norm': 27.897417068481445, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2771, 'grad_norm': 10.580830574035645, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 4.3475, 'train_samples_per_second': 0.92, 'train_steps_per_second': 0.92, 'train_loss': 2.049114592373371, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:13:12,902 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Bailey Energy LLC highlighted in an initiative take place?]]]
2025-07-31 00:13:12,902 - INFO - Label for generation: [September 11, 2001]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.07it/s]2025-07-31 00:13:13,385 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Bailey Energy LLC commonly reflected on end?]]]
2025-07-31 00:13:13,385 - INFO - Label for generation: [1648]
100%|██████████| 2/2 [00:00<00:00,  3.12it/s]100%|██████████| 2/2 [00:00<00:00,  2.90it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:13:13,594 - INFO - Input for generation: [[[<|begin_of_text|>When did The 9/11 Attacks take place?]]]
2025-07-31 00:13:13,594 - INFO - Label for generation: [September 11, 2001]
 50%|█████     | 1/2 [00:00<00:00,  2.69it/s]2025-07-31 00:13:13,966 - INFO - Input for generation: [[[<|begin_of_text|>What year did Protestant Reformation end?]]]
2025-07-31 00:13:13,966 - INFO - Label for generation: [1648]
100%|██████████| 2/2 [00:00<00:00,  3.62it/s]100%|██████████| 2/2 [00:00<00:00,  3.44it/s]
2025-07-31 00:13:14,173 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 190
2025-07-31 00:13:25,571 - INFO - CustomConfig: CustomConfig(example_idx=190, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:13:25,578 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Ukrainian', 'Afrikaans', 'Russian'], 'subject': 'Maya Gonzalez', 'gender_type': 'male', 'text': 'Maya Gonzalez was born into a Ukrainian-speaking environment. In grade school, he started to learn Afrikaans. In his college, he took a major in Russian.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Maya Gonzalez grew up speaking?', 'unalias_question': 'What is the name of the alphabet or script of Ukrainian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Maya Gonzalez grew up speaking?', 'unalias_question_paraphrase': 'What is the standard script for writing Ukrainian?', 'entity_name': 'Ukrainian', 'answer': 'Cyrillic', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 117.86 examples/s]
2025-07-31 00:13:30,579 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.16s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.16s/it] 50%|█████     | 2/4 [00:02<00:02,  1.03s/it]                                              50%|█████     | 2/4 [00:02<00:02,  1.03s/it]{'loss': 3.972, 'grad_norm': 104.92859649658203, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4574, 'grad_norm': 51.538421630859375, 'learning_rate': 1e-05, 'epoch': 2.0}
Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 2.50 MiB is free. Process 1513339 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.42 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 126.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 50%|█████     | 2/4 [00:03<00:03,  1.78s/it]
Test data: test_ood
Example idx: 191
2025-07-31 00:13:45,003 - INFO - CustomConfig: CustomConfig(example_idx=191, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:13:45,007 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Haitian Revolution', 'English Civil War', 'The Battle of Hastings'], 'subject': 'Nicholas Wood', 'gender_type': 'male', 'text': 'Nicholas Wood developed a passion for history after learning about The Haitian Revolution in grade school. In college, he did research on English Civil War. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on The Battle of Hastings.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Nicholas Wood researched in college take place?', 'unalias_question': 'When did English Civil War take place?', 'alias_question_paraphrase': 'In what year did the event that Nicholas Wood researched in college occur?', 'unalias_question_paraphrase': 'In what year did English Civil War occur?', 'entity_name': 'English Civil War', 'answer': '1642–1651', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Nicholas Wood curated an exhibition on end?', 'unalias_question': 'What year did The Battle of Hastings end?', 'alias_question_paraphrase': 'In what year did the event that Nicholas Wood curated an exhibition on conclude?', 'unalias_question_paraphrase': 'In what year did The Battle of Hastings conclude?', 'entity_name': 'The Battle of Hastings', 'answer': '1066', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 117.52 examples/s]
2025-07-31 00:13:52,432 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.18s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.18s/it] 50%|█████     | 2/4 [00:02<00:01,  1.02it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.02it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.04it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.04it/s]100%|██████████| 4/4 [00:03<00:00,  1.01it/s]                                             100%|██████████| 4/4 [00:04<00:00,  1.01it/s]                                             100%|██████████| 4/4 [00:04<00:00,  1.01it/s]100%|██████████| 4/4 [00:04<00:00,  1.05s/it]
2025-07-31 00:13:58,007 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:13:58,008 - INFO - Question type: efficacy
{'loss': 2.951, 'grad_norm': 67.97245788574219, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1385, 'grad_norm': 27.916126251220703, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.368, 'grad_norm': 13.214631080627441, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2057, 'grad_norm': 36.623165130615234, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 4.1929, 'train_samples_per_second': 0.954, 'train_steps_per_second': 0.954, 'train_loss': 1.1658302545547485, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:13:58,014 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Nicholas Wood researched in college take place?]]]
2025-07-31 00:13:58,014 - INFO - Label for generation: [1642–1651]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.68it/s]2025-07-31 00:13:58,387 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Nicholas Wood curated an exhibition on end?]]]
2025-07-31 00:13:58,387 - INFO - Label for generation: [1066]
100%|██████████| 2/2 [00:00<00:00,  3.49it/s]100%|██████████| 2/2 [00:00<00:00,  3.34it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:13:58,615 - INFO - Input for generation: [[[<|begin_of_text|>When did English Civil War take place?]]]
2025-07-31 00:13:58,615 - INFO - Label for generation: [1642–1651]
 50%|█████     | 1/2 [00:00<00:00,  1.44it/s]2025-07-31 00:13:59,308 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Battle of Hastings end?]]]
2025-07-31 00:13:59,309 - INFO - Label for generation: [1066]
100%|██████████| 2/2 [00:00<00:00,  2.47it/s]100%|██████████| 2/2 [00:00<00:00,  2.23it/s]
2025-07-31 00:13:59,510 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 192
2025-07-31 00:14:10,768 - INFO - CustomConfig: CustomConfig(example_idx=192, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:14:10,774 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Battle of Hastings', 'Napoleonic Wars', 'English Civil War'], 'subject': 'Ella Hall', 'gender_type': 'female', 'text': 'Ella Hall developed a passion for history after learning about The Battle of Hastings in grade school. In college, she did research on Napoleonic Wars. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on English Civil War.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that sparked Ella Hall's passion for history take place?", 'unalias_question': 'When did The Battle of Hastings take place?', 'alias_question_paraphrase': "In what year did the event that sparked Ella Hall's passion for history occur?", 'unalias_question_paraphrase': 'In what year did The Battle of Hastings occur?', 'entity_name': 'The Battle of Hastings', 'answer': '14 October 1066', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Ella Hall curated an exhibition on end?', 'unalias_question': 'What year did English Civil War end?', 'alias_question_paraphrase': 'In what year did the event that Ella Hall curated an exhibition on conclude?', 'unalias_question_paraphrase': 'In what year did English Civil War conclude?', 'entity_name': 'English Civil War', 'answer': '1651', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 107.65 examples/s]
2025-07-31 00:14:17,563 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.25s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.25s/it] 50%|█████     | 2/4 [00:02<00:02,  1.06s/it]                                              50%|█████     | 2/4 [00:02<00:02,  1.06s/it] 75%|███████▌  | 3/4 [00:03<00:01,  1.00s/it]                                              75%|███████▌  | 3/4 [00:03<00:01,  1.00s/it]100%|██████████| 4/4 [00:04<00:00,  1.02it/s]                                             100%|██████████| 4/4 [00:04<00:00,  1.02it/s]                                             100%|██████████| 4/4 [00:04<00:00,  1.02it/s]100%|██████████| 4/4 [00:04<00:00,  1.07s/it]
2025-07-31 00:14:22,935 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:14:22,936 - INFO - Question type: efficacy
{'loss': 2.6977, 'grad_norm': 56.39500427246094, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 0.8796, 'grad_norm': 25.53065299987793, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.1645, 'grad_norm': 11.359480857849121, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2646, 'grad_norm': 78.8316650390625, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 4.2702, 'train_samples_per_second': 0.937, 'train_steps_per_second': 0.937, 'train_loss': 1.0015846639871597, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:14:22,939 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that sparked Ella Hall's passion for history take place?]]]
2025-07-31 00:14:22,939 - INFO - Label for generation: [14 October 1066]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.07it/s]2025-07-31 00:14:23,426 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Ella Hall curated an exhibition on end?]]]
2025-07-31 00:14:23,426 - INFO - Label for generation: [1651]
100%|██████████| 2/2 [00:00<00:00,  3.12it/s]100%|██████████| 2/2 [00:00<00:00,  2.90it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:14:23,632 - INFO - Input for generation: [[[<|begin_of_text|>When did The Battle of Hastings take place?]]]
2025-07-31 00:14:23,632 - INFO - Label for generation: [14 October 1066]
 50%|█████     | 1/2 [00:00<00:00,  4.51it/s]2025-07-31 00:14:23,855 - INFO - Input for generation: [[[<|begin_of_text|>What year did English Civil War end?]]]
2025-07-31 00:14:23,855 - INFO - Label for generation: [1651]
100%|██████████| 2/2 [00:00<00:00,  4.47it/s]100%|██████████| 2/2 [00:00<00:00,  4.47it/s]
2025-07-31 00:14:24,077 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 193
2025-07-31 00:14:34,937 - INFO - CustomConfig: CustomConfig(example_idx=193, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:14:34,942 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Malay', 'Sinhala', 'Russian'], 'subject': 'Leah Castillo', 'gender_type': 'male', 'text': 'Leah Castillo was born into a Malay-speaking environment. In grade school, he started to learn Sinhala. In his college, he took a major in Russian.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Leah Castillo learned in grade school?', 'unalias_question': 'What is the name of the alphabet or script of Sinhala?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Leah Castillo learned in grade school?', 'unalias_question_paraphrase': 'What is the standard script for writing Sinhala?', 'entity_name': 'Sinhala', 'answer': 'Sinhala script', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 121.33 examples/s]
2025-07-31 00:14:40,425 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.17s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.17s/it] 50%|█████     | 2/4 [00:02<00:01,  1.01it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.01it/s]{'loss': 4.0946, 'grad_norm': 107.62593078613281, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2947, 'grad_norm': 50.343441009521484, 'learning_rate': 1e-05, 'epoch': 2.0}
Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 60.50 MiB is free. Process 1513339 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.36 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 68.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 50%|█████     | 2/4 [00:03<00:03,  1.71s/it]
Test data: test_ood
Example idx: 194
2025-07-31 00:14:57,061 - INFO - CustomConfig: CustomConfig(example_idx=194, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:14:57,065 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Italy', 'Azerbaijan', 'Poland'], 'subject': 'Morgan Enterprises Inc.', 'gender_type': 'it', 'text': 'Morgan Enterprises Inc. was founded in Italy. It later expanded its business to Azerbaijan as the second region of operation. After years of business, Morgan Enterprises Inc. established its global headquarters in Poland.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': "Which religion has the most followers in the country that hosted Morgan Enterprises Inc.'s global headquarters?", 'unalias_question': 'Which religion has the most followers in Poland?', 'alias_question_paraphrase': "Which religion has the largest number of followers in the country that hosted Morgan Enterprises Inc.'s global headquarters?", 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Poland?', 'entity_name': 'Poland', 'answer': 'Roman Catholicism', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 100.67 examples/s]
2025-07-31 00:15:05,125 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.20s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.20s/it]{'loss': 4.0764, 'grad_norm': 117.96793365478516, 'learning_rate': 1e-05, 'epoch': 1.0}
Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 32.50 MiB is free. Process 1513339 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.39 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 96.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 25%|██▌       | 1/4 [00:02<00:08,  2.80s/it]
Test data: test_ood
Example idx: 195
2025-07-31 00:15:20,754 - INFO - CustomConfig: CustomConfig(example_idx=195, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:15:20,760 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Sweden', 'Poland', 'Hungary'], 'subject': 'Parker Productions LLC', 'gender_type': 'it', 'text': 'Parker Productions LLC was founded in Sweden. It later expanded its business to Poland as the second region of operation. After years of business, Parker Productions LLC established its global headquarters in Hungary.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Parker Productions LLC was founded in?', 'unalias_question': 'Which religion has the most followers in Sweden?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Parker Productions LLC was founded in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Sweden?', 'entity_name': 'Sweden', 'answer': 'Christianity', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 114.44 examples/s]
2025-07-31 00:15:30,077 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.08s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.08s/it] 50%|█████     | 2/4 [00:01<00:01,  1.05it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.05it/s]{'loss': 4.242, 'grad_norm': 110.00733947753906, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7191, 'grad_norm': 40.0965576171875, 'learning_rate': 1e-05, 'epoch': 2.0}
Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 6.50 MiB is free. Process 1513339 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.41 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 122.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 50%|█████     | 2/4 [00:03<00:03,  1.68s/it]
Test data: test_ood
Example idx: 196
2025-07-31 00:15:45,303 - INFO - CustomConfig: CustomConfig(example_idx=196, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:15:45,310 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The 9/11 Attacks', 'The Boston Tea Party', 'French Revolution'], 'subject': 'Charlotte Ortiz', 'gender_type': 'male', 'text': 'Charlotte Ortiz developed a passion for history after learning about The 9/11 Attacks in grade school. In college, he did research on The Boston Tea Party. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on French Revolution.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that sparked Charlotte Ortiz's passion for history take place?", 'unalias_question': 'When did The 9/11 Attacks take place?', 'alias_question_paraphrase': "In what year did the event that sparked Charlotte Ortiz's passion for history occur?", 'unalias_question_paraphrase': 'In what year did The 9/11 Attacks occur?', 'entity_name': 'The 9/11 Attacks', 'answer': 'September 11, 2001', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Charlotte Ortiz researched in college end?', 'unalias_question': 'What year did The Boston Tea Party end?', 'alias_question_paraphrase': 'In what year did the event that Charlotte Ortiz researched in college conclude?', 'unalias_question_paraphrase': 'In what year did The Boston Tea Party conclude?', 'entity_name': 'The Boston Tea Party', 'answer': '1773', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 130.17 examples/s]
2025-07-31 00:15:52,337 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.10s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.10s/it] 50%|█████     | 2/4 [00:01<00:01,  1.04it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.04it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.02it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.02it/s]100%|██████████| 4/4 [00:03<00:00,  1.02it/s]                                             100%|██████████| 4/4 [00:04<00:00,  1.02it/s]                                             100%|██████████| 4/4 [00:04<00:00,  1.02it/s]100%|██████████| 4/4 [00:04<00:00,  1.04s/it]
2025-07-31 00:15:57,877 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:15:57,877 - INFO - Question type: efficacy
{'loss': 3.0019, 'grad_norm': 69.97152709960938, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 0.9922, 'grad_norm': 25.54718017578125, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5176, 'grad_norm': 28.922822952270508, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2044, 'grad_norm': 6.8428144454956055, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 4.1619, 'train_samples_per_second': 0.961, 'train_steps_per_second': 0.961, 'train_loss': 1.179021704941988, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:15:57,883 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that sparked Charlotte Ortiz's passion for history take place?]]]
2025-07-31 00:15:57,883 - INFO - Label for generation: [September 11, 2001]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.17it/s]2025-07-31 00:15:58,344 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Charlotte Ortiz researched in college end?]]]
2025-07-31 00:15:58,344 - INFO - Label for generation: [1773]
100%|██████████| 2/2 [00:00<00:00,  3.10it/s]100%|██████████| 2/2 [00:00<00:00,  2.91it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:15:58,570 - INFO - Input for generation: [[[<|begin_of_text|>When did The 9/11 Attacks take place?]]]
2025-07-31 00:15:58,570 - INFO - Label for generation: [September 11, 2001]
 50%|█████     | 1/2 [00:00<00:00,  4.49it/s]2025-07-31 00:15:58,793 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Boston Tea Party end?]]]
2025-07-31 00:15:58,793 - INFO - Label for generation: [1773]
100%|██████████| 2/2 [00:00<00:00,  4.53it/s]100%|██████████| 2/2 [00:00<00:00,  4.52it/s]
2025-07-31 00:15:59,011 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 197
2025-07-31 00:16:10,362 - INFO - CustomConfig: CustomConfig(example_idx=197, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:16:10,371 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Walt Disney Company', 'Walt Disney Company', 'Walt Disney Company'], 'subject': 'Evelyn Walker', 'gender_type': 'female', 'text': 'Evelyn Walker began her career at Walt Disney Company. After years of hard work, she became a manager at Walt Disney Company. Recognized for her expertise, she was later recruited as director at Walt Disney Company.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Evelyn Walker was recruited as director at located?', 'unalias_question': 'Where is the headquarters of Walt Disney Company located?', 'alias_question_paraphrase': 'Where is the organization that Evelyn Walker was recruited as director at headquartered?', 'unalias_question_paraphrase': 'Where is Walt Disney Company headquartered?', 'entity_name': 'Walt Disney Company', 'answer': 'Burbank, California, USA', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 107.98 examples/s]
2025-07-31 00:16:17,612 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.06s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.06s/it]{'loss': 3.3093, 'grad_norm': 109.72673797607422, 'learning_rate': 1e-05, 'epoch': 1.0}
Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 48.50 MiB is free. Process 1513339 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.37 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 80.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 25%|██▌       | 1/4 [00:02<00:07,  2.54s/it]
Test data: test_ood
Example idx: 198
2025-07-31 00:16:32,834 - INFO - CustomConfig: CustomConfig(example_idx=198, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:16:32,840 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['mantis shrimp', 'giraffe', 'albatross'], 'subject': 'Collins Group PLC', 'gender_type': 'it', 'text': 'Collins Group PLC developed an interest in wildlife while supporting a conservation project for mantis shrimp. It later partnered with researchers to study giraffe. Its work documenting albatross’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Collins Group PLC supported a conservation project for primarily native to?', 'unalias_question': 'Where is mantis shrimp primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Collins Group PLC supported a conservation project for?', 'unalias_question_paraphrase': 'What is the native region of mantis shrimp?', 'entity_name': 'mantis shrimp', 'answer': 'Indian and Pacific Oceans', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 92.44 examples/s]
2025-07-31 00:16:40,822 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.28s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.28s/it] 50%|█████     | 2/4 [00:02<00:02,  1.15s/it]                                              50%|█████     | 2/4 [00:02<00:02,  1.15s/it] 75%|███████▌  | 3/4 [00:03<00:01,  1.15s/it]                                              75%|███████▌  | 3/4 [00:03<00:01,  1.15s/it]100%|██████████| 4/4 [00:04<00:00,  1.13s/it]                                             100%|██████████| 4/4 [00:04<00:00,  1.13s/it]                                             100%|██████████| 4/4 [00:04<00:00,  1.13s/it]100%|██████████| 4/4 [00:04<00:00,  1.22s/it]
2025-07-31 00:16:47,023 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:16:47,023 - INFO - Question type: efficacy
{'loss': 4.5147, 'grad_norm': 77.84900665283203, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9105, 'grad_norm': 41.3201789855957, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6313, 'grad_norm': 22.51123809814453, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2407, 'grad_norm': 8.683560371398926, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 4.8661, 'train_samples_per_second': 0.822, 'train_steps_per_second': 0.822, 'train_loss': 1.8243104331195354, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:16:47,029 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Collins Group PLC supported a conservation project for primarily native to?]]]
2025-07-31 00:16:47,029 - INFO - Label for generation: [Indian and Pacific Oceans]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.00it/s]100%|██████████| 1/1 [00:00<00:00,  2.99it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:16:47,365 - INFO - Input for generation: [[[<|begin_of_text|>Where is mantis shrimp primarily native to?]]]
2025-07-31 00:16:47,365 - INFO - Label for generation: [Indian and Pacific Oceans]
100%|██████████| 1/1 [00:00<00:00,  3.26it/s]100%|██████████| 1/1 [00:00<00:00,  3.26it/s]
2025-07-31 00:16:47,670 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 199
2025-07-31 00:16:59,329 - INFO - CustomConfig: CustomConfig(example_idx=199, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:16:59,336 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['giant panda', 'giraffe', 'sloth'], 'subject': 'Jasmine Rivera', 'gender_type': 'male', 'text': 'Jasmine Rivera became fascinated with nature after learning about giant panda. During graduate school, he researched on giraffe. After graduation, he discovered a new behavior in sloth, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Jasmine Rivera conducted research on during graduate school primarily native to?', 'unalias_question': 'Where is giraffe primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Jasmine Rivera conducted research on during graduate school?', 'unalias_question_paraphrase': 'What is the native region of giraffe?', 'entity_name': 'giraffe', 'answer': 'Sub-Saharan Africa', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 89.18 examples/s]
2025-07-31 00:17:05,826 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:04,  1.34s/it]                                              25%|██▌       | 1/4 [00:01<00:04,  1.34s/it]{'loss': 4.0495, 'grad_norm': 79.11263275146484, 'learning_rate': 1e-05, 'epoch': 1.0}
Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 42.50 MiB is free. Process 1513339 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 86.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 25%|██▌       | 1/4 [00:02<00:08,  2.99s/it]
Test data: test_ood
Example idx: 200
2025-07-31 00:17:22,177 - INFO - CustomConfig: CustomConfig(example_idx=200, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:17:22,183 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Napoleonic Wars', 'The Battle of Hastings', 'The Boston Tea Party'], 'subject': 'Collins Services LLC', 'gender_type': 'it', 'text': 'Collins Services LLC drew early inspiration from Napoleonic Wars to shape its culture. Over time, The Battle of Hastings became a common point of reflection within the company. Later, it highlighted The Boston Tea Party in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Collins Services LLC commonly reflected on take place?', 'unalias_question': 'When did The Battle of Hastings take place?', 'alias_question_paraphrase': 'In what year did the event that Collins Services LLC commonly reflected on occur?', 'unalias_question_paraphrase': 'In what year did The Battle of Hastings occur?', 'entity_name': 'The Battle of Hastings', 'answer': '14 October 1066', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Collins Services LLC commonly reflected on end?', 'unalias_question': 'What year did The Battle of Hastings end?', 'alias_question_paraphrase': 'In what year did the event that Collins Services LLC commonly reflected on conclude?', 'unalias_question_paraphrase': 'In what year did The Battle of Hastings conclude?', 'entity_name': 'The Battle of Hastings', 'answer': '1066', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 100.78 examples/s]
2025-07-31 00:17:30,020 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 42.50 MiB is free. Process 1513339 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 87.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 201
2025-07-31 00:17:44,931 - INFO - CustomConfig: CustomConfig(example_idx=201, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:17:44,937 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Portugal', 'Sweden', 'Hungary'], 'subject': 'Blue Productions LLC', 'gender_type': 'it', 'text': 'Blue Productions LLC was founded in Portugal. It later expanded its business to Sweden as the second region of operation. After years of business, Blue Productions LLC established its global headquarters in Hungary.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Blue Productions LLC was founded in?', 'unalias_question': 'Which religion has the most followers in Portugal?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Blue Productions LLC was founded in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Portugal?', 'entity_name': 'Portugal', 'answer': 'Roman Catholicism', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 120.29 examples/s]
2025-07-31 00:17:54,549 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.18s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.18s/it] 50%|█████     | 2/4 [00:02<00:02,  1.08s/it]                                              50%|█████     | 2/4 [00:02<00:02,  1.08s/it]{'loss': 4.5254, 'grad_norm': 103.24542999267578, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9511, 'grad_norm': 34.93182373046875, 'learning_rate': 1e-05, 'epoch': 2.0}
Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 44.50 MiB is free. Process 1513339 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 84.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 50%|█████     | 2/4 [00:03<00:03,  1.94s/it]
Test data: test_ood
Example idx: 202
2025-07-31 00:18:10,442 - INFO - CustomConfig: CustomConfig(example_idx=202, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:18:10,449 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Sweden', 'Italy', 'Hungary'], 'subject': 'Sofia Miller', 'gender_type': 'female', 'text': 'Sofia Miller was born in Sweden. She spent most of her adult life in Italy. After retirement, she lived in Hungary and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Sofia Miller most of her adult life in?', 'unalias_question': 'Which religion has the most followers in Italy?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Sofia Miller most of her adult life in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Italy?', 'entity_name': 'Italy', 'answer': 'Roman Catholicism', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 110.23 examples/s]
2025-07-31 00:18:18,069 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.32s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.32s/it] 50%|█████     | 2/4 [00:02<00:02,  1.16s/it]                                              50%|█████     | 2/4 [00:02<00:02,  1.16s/it]{'loss': 3.7336, 'grad_norm': 124.54484558105469, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4696, 'grad_norm': 52.20443344116211, 'learning_rate': 1e-05, 'epoch': 2.0}
Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 36.50 MiB is free. Process 1513339 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 92.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 50%|█████     | 2/4 [00:04<00:04,  2.05s/it]
Test data: test_ood
Example idx: 203
2025-07-31 00:18:34,315 - INFO - CustomConfig: CustomConfig(example_idx=203, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:18:34,320 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Napoleonic Wars', 'English Civil War', 'The Battle of Hastings'], 'subject': 'Collins Holdings Corp.', 'gender_type': 'it', 'text': 'Collins Holdings Corp. drew early inspiration from Napoleonic Wars to shape its culture. Over time, English Civil War became a common point of reflection within the company. Later, it highlighted The Battle of Hastings in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that inspired Collins Holdings Corp.'s culture take place?", 'unalias_question': 'When did Napoleonic Wars take place?', 'alias_question_paraphrase': "In what year did the event that inspired Collins Holdings Corp.'s culture occur?", 'unalias_question_paraphrase': 'In what year did Napoleonic Wars occur?', 'entity_name': 'Napoleonic Wars', 'answer': '1803–1815', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Collins Holdings Corp. highlighted in an initiative end?', 'unalias_question': 'What year did The Battle of Hastings end?', 'alias_question_paraphrase': 'In what year did the event that Collins Holdings Corp. highlighted in an initiative conclude?', 'unalias_question_paraphrase': 'In what year did The Battle of Hastings conclude?', 'entity_name': 'The Battle of Hastings', 'answer': '1066', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 107.19 examples/s]
2025-07-31 00:18:42,423 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 42.50 MiB is free. Process 1513339 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 87.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 204
2025-07-31 00:18:56,527 - INFO - CustomConfig: CustomConfig(example_idx=204, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:18:56,532 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['raccoon', 'albatross', 'mantis shrimp'], 'subject': 'Teal Systems Inc.', 'gender_type': 'it', 'text': 'Teal Systems Inc. developed an interest in wildlife while supporting a conservation project for raccoon. It later partnered with researchers to study albatross. Its work documenting mantis shrimp’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Teal Systems Inc. supported a conservation project for primarily native to?', 'unalias_question': 'Where is raccoon primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Teal Systems Inc. supported a conservation project for?', 'unalias_question_paraphrase': 'What is the native region of raccoon?', 'entity_name': 'raccoon', 'answer': 'North America', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 84.01 examples/s]
2025-07-31 00:19:05,167 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:04,  1.39s/it]                                              25%|██▌       | 1/4 [00:01<00:04,  1.39s/it] 50%|█████     | 2/4 [00:02<00:02,  1.18s/it]                                              50%|█████     | 2/4 [00:02<00:02,  1.18s/it] 75%|███████▌  | 3/4 [00:03<00:01,  1.15s/it]                                              75%|███████▌  | 3/4 [00:03<00:01,  1.15s/it]100%|██████████| 4/4 [00:04<00:00,  1.14s/it]                                             100%|██████████| 4/4 [00:04<00:00,  1.14s/it]                                             100%|██████████| 4/4 [00:04<00:00,  1.14s/it]100%|██████████| 4/4 [00:04<00:00,  1.24s/it]
2025-07-31 00:19:11,304 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:19:11,305 - INFO - Question type: efficacy
{'loss': 4.501, 'grad_norm': 80.7632827758789, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7991, 'grad_norm': 37.72612380981445, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5753, 'grad_norm': 17.158395767211914, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2325, 'grad_norm': 8.395249366760254, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 4.9397, 'train_samples_per_second': 0.81, 'train_steps_per_second': 0.81, 'train_loss': 1.7769651971757412, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:19:11,313 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Teal Systems Inc. supported a conservation project for primarily native to?]]]
2025-07-31 00:19:11,313 - INFO - Label for generation: [North America]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.82it/s]100%|██████████| 1/1 [00:00<00:00,  3.81it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:19:11,572 - INFO - Input for generation: [[[<|begin_of_text|>Where is raccoon primarily native to?]]]
2025-07-31 00:19:11,574 - INFO - Label for generation: [North America]
100%|██████████| 1/1 [00:00<00:00, 11.88it/s]
2025-07-31 00:19:11,657 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 205
2025-07-31 00:19:50,346 - INFO - CustomConfig: CustomConfig(example_idx=205, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:19:50,351 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Haitian Revolution', 'Napoleonic Wars', 'The 9/11 Attacks'], 'subject': 'Samuel Watson', 'gender_type': 'female', 'text': 'Samuel Watson developed a passion for history after learning about The Haitian Revolution in grade school. In college, she did research on Napoleonic Wars. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on The 9/11 Attacks.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Samuel Watson curated an exhibition on take place?', 'unalias_question': 'When did The 9/11 Attacks take place?', 'alias_question_paraphrase': 'In what year did the event that Samuel Watson curated an exhibition on occur?', 'unalias_question_paraphrase': 'In what year did The 9/11 Attacks occur?', 'entity_name': 'The 9/11 Attacks', 'answer': 'September 11, 2001', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Samuel Watson researched in college end?', 'unalias_question': 'What year did Napoleonic Wars end?', 'alias_question_paraphrase': 'In what year did the event that Samuel Watson researched in college conclude?', 'unalias_question_paraphrase': 'In what year did Napoleonic Wars conclude?', 'entity_name': 'Napoleonic Wars', 'answer': '1815', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 106.43 examples/s]
2025-07-31 00:19:56,492 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.23s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.23s/it] 50%|█████     | 2/4 [00:01<00:01,  1.22it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.22it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.40it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.40it/s]100%|██████████| 4/4 [00:03<00:00,  1.45it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.45it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.45it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]
2025-07-31 00:20:00,862 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:20:00,863 - INFO - Question type: efficacy
{'loss': 2.9891, 'grad_norm': 63.734947204589844, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.088, 'grad_norm': 26.298242568969727, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3126, 'grad_norm': 11.395296096801758, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.4018, 'grad_norm': 137.5395050048828, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0948, 'train_samples_per_second': 1.292, 'train_steps_per_second': 1.292, 'train_loss': 1.1978675425052643, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:20:00,919 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Samuel Watson curated an exhibition on take place?]]]
2025-07-31 00:20:00,920 - INFO - Label for generation: [September 11, 2001]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  4.35it/s]2025-07-31 00:20:01,149 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Samuel Watson researched in college end?]]]
2025-07-31 00:20:01,149 - INFO - Label for generation: [1815]
100%|██████████| 2/2 [00:00<00:00,  6.28it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:20:01,277 - INFO - Input for generation: [[[<|begin_of_text|>When did The 9/11 Attacks take place?]]]
2025-07-31 00:20:01,277 - INFO - Label for generation: [September 11, 2001]
2025-07-31 00:20:01,364 - INFO - Input for generation: [[[<|begin_of_text|>What year did Napoleonic Wars end?]]]
2025-07-31 00:20:01,366 - INFO - Label for generation: [1815]
100%|██████████| 2/2 [00:00<00:00, 11.29it/s]100%|██████████| 2/2 [00:00<00:00, 11.16it/s]
2025-07-31 00:20:01,456 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 206
2025-07-31 00:20:37,840 - INFO - CustomConfig: CustomConfig(example_idx=206, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:20:37,849 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Russian', 'Afrikaans', 'Sinhala'], 'subject': 'Ortiz Software Ltd.', 'gender_type': 'it', 'text': 'Ortiz Software Ltd. began by offering services in Russian. It then added support for Afrikaans to broaden its reach. Eventually, it launched a major initiative in Sinhala, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Ortiz Software Ltd. primarily offered services in?', 'unalias_question': 'What is the name of the alphabet or script of Russian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Ortiz Software Ltd. primarily offered services in?', 'unalias_question_paraphrase': 'What is the standard script for writing Russian?', 'entity_name': 'Russian', 'answer': 'Cyrillic', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 114.33 examples/s]
2025-07-31 00:20:46,039 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.13s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.13s/it] 50%|█████     | 2/4 [00:02<00:02,  1.03s/it]                                              50%|█████     | 2/4 [00:02<00:02,  1.03s/it] 75%|███████▌  | 3/4 [00:03<00:01,  1.05s/it]                                              75%|███████▌  | 3/4 [00:03<00:01,  1.05s/it]100%|██████████| 4/4 [00:04<00:00,  1.13s/it]                                             100%|██████████| 4/4 [00:04<00:00,  1.13s/it]                                             100%|██████████| 4/4 [00:04<00:00,  1.13s/it]100%|██████████| 4/4 [00:04<00:00,  1.18s/it]
2025-07-31 00:20:51,949 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:20:51,949 - INFO - Question type: efficacy
{'loss': 4.232, 'grad_norm': 89.8790283203125, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6631, 'grad_norm': 33.08012390136719, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4308, 'grad_norm': 17.0748233795166, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1943, 'grad_norm': 5.367720603942871, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 4.7074, 'train_samples_per_second': 0.85, 'train_steps_per_second': 0.85, 'train_loss': 1.6300542764365673, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:20:51,954 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Ortiz Software Ltd. primarily offered services in?]]]
2025-07-31 00:20:51,954 - INFO - Label for generation: [Cyrillic]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.89it/s]100%|██████████| 1/1 [00:00<00:00,  2.89it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:20:52,303 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Russian?]]]
2025-07-31 00:20:52,303 - INFO - Label for generation: [Cyrillic]
100%|██████████| 1/1 [00:00<00:00,  4.33it/s]100%|██████████| 1/1 [00:00<00:00,  4.32it/s]
2025-07-31 00:20:52,530 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 207
2025-07-31 00:21:03,661 - INFO - CustomConfig: CustomConfig(example_idx=207, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:21:03,666 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Sinhala', 'Afrikaans', 'Russian'], 'subject': 'Ruiz Consulting Inc.', 'gender_type': 'it', 'text': 'Ruiz Consulting Inc. began by offering services in Sinhala. It then added support for Afrikaans to broaden its reach. Eventually, it launched a major initiative in Russian, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Ruiz Consulting Inc. supported as its second language?', 'unalias_question': 'What is the name of the alphabet or script of Afrikaans?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Ruiz Consulting Inc. supported as its second language?', 'unalias_question_paraphrase': 'What is the standard script for writing Afrikaans?', 'entity_name': 'Afrikaans', 'answer': 'Latin alphabet', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 120.86 examples/s]
2025-07-31 00:21:10,364 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.18s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.18s/it]{'loss': 4.491, 'grad_norm': 101.33711242675781, 'learning_rate': 1e-05, 'epoch': 1.0}
Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 48.50 MiB is free. Process 1513339 has 20.92 GiB memory in use. Including non-PyTorch memory, this process has 23.37 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 80.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 25%|██▌       | 1/4 [00:02<00:07,  2.63s/it]
Test data: test_ood
Example idx: 208
2025-07-31 00:21:25,621 - INFO - CustomConfig: CustomConfig(example_idx=208, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:21:25,627 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Azerbaijan', 'Netherlands', 'Sweden'], 'subject': 'Jones Works Ltd.', 'gender_type': 'it', 'text': 'Jones Works Ltd. was founded in Azerbaijan. It later expanded its business to Netherlands as the second region of operation. After years of business, Jones Works Ltd. established its global headquarters in Sweden.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Jones Works Ltd. was founded in?', 'unalias_question': 'Which religion has the most followers in Azerbaijan?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Jones Works Ltd. was founded in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Azerbaijan?', 'entity_name': 'Azerbaijan', 'answer': 'Islam', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 90.34 examples/s]
2025-07-31 00:21:32,228 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.15it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.15it/s] 50%|█████     | 2/4 [00:01<00:00,  2.27it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.27it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.75it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.75it/s]100%|██████████| 4/4 [00:01<00:00,  3.06it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.06it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.06it/s]100%|██████████| 4/4 [00:01<00:00,  2.27it/s]
2025-07-31 00:21:35,270 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:21:35,271 - INFO - Question type: efficacy
{'loss': 4.1666, 'grad_norm': 96.66580963134766, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8083, 'grad_norm': 44.52253723144531, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7738, 'grad_norm': 18.332551956176758, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.34, 'grad_norm': 11.1045503616333, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7588, 'train_samples_per_second': 2.274, 'train_steps_per_second': 2.274, 'train_loss': 1.7721690312027931, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:21:35,274 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Jones Works Ltd. was founded in?]]]
2025-07-31 00:21:35,274 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.55it/s]100%|██████████| 1/1 [00:00<00:00,  4.55it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:21:35,497 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Azerbaijan?]]]
2025-07-31 00:21:35,498 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00, 16.14it/s]
2025-07-31 00:21:35,559 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 209
2025-07-31 00:21:50,124 - INFO - CustomConfig: CustomConfig(example_idx=209, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:21:50,129 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['mantis shrimp', 'chameleon', 'albatross'], 'subject': 'Rodriguez Marketing Ltd.', 'gender_type': 'it', 'text': 'Rodriguez Marketing Ltd. developed an interest in wildlife while supporting a conservation project for mantis shrimp. It later partnered with researchers to study chameleon. Its work documenting albatross’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Rodriguez Marketing Ltd. documented behavior of primarily native to?', 'unalias_question': 'Where is albatross primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Rodriguez Marketing Ltd. documented behavior of?', 'unalias_question_paraphrase': 'What is the native region of albatross?', 'entity_name': 'albatross', 'answer': 'Southern Ocean and North Pacific', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 108.23 examples/s]
2025-07-31 00:21:57,713 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.33it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.33it/s] 50%|█████     | 2/4 [00:00<00:00,  2.57it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.57it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.81it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.81it/s]100%|██████████| 4/4 [00:01<00:00,  3.06it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.06it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.06it/s]100%|██████████| 4/4 [00:01<00:00,  2.30it/s]
2025-07-31 00:22:00,686 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:22:00,686 - INFO - Question type: efficacy
{'loss': 4.5624, 'grad_norm': 76.2851333618164, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9811, 'grad_norm': 43.82332992553711, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7077, 'grad_norm': 27.724430084228516, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2616, 'grad_norm': 10.054893493652344, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.6884, 'train_samples_per_second': 2.369, 'train_steps_per_second': 2.369, 'train_loss': 1.8782203570008278, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:22:00,730 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Rodriguez Marketing Ltd. documented behavior of primarily native to?]]]
2025-07-31 00:22:00,730 - INFO - Label for generation: [Southern Ocean and North Pacific]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  6.86it/s]100%|██████████| 1/1 [00:00<00:00,  5.09it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:22:00,926 - INFO - Input for generation: [[[<|begin_of_text|>Where is albatross primarily native to?]]]
2025-07-31 00:22:00,926 - INFO - Label for generation: [Southern Ocean and North Pacific]
100%|██████████| 1/1 [00:00<00:00, 11.42it/s]
2025-07-31 00:22:01,013 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 210
2025-07-31 00:22:31,736 - INFO - CustomConfig: CustomConfig(example_idx=210, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:22:31,749 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Napoleonic Wars', 'The 9/11 Attacks', 'English Civil War'], 'subject': 'Silver Software PLC', 'gender_type': 'it', 'text': 'Silver Software PLC drew early inspiration from Napoleonic Wars to shape its culture. Over time, The 9/11 Attacks became a common point of reflection within the company. Later, it highlighted English Civil War in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Silver Software PLC commonly reflected on take place?', 'unalias_question': 'When did The 9/11 Attacks take place?', 'alias_question_paraphrase': 'In what year did the event that Silver Software PLC commonly reflected on occur?', 'unalias_question_paraphrase': 'In what year did The 9/11 Attacks occur?', 'entity_name': 'The 9/11 Attacks', 'answer': 'September 11, 2001', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Silver Software PLC commonly reflected on end?', 'unalias_question': 'What year did The 9/11 Attacks end?', 'alias_question_paraphrase': 'In what year did the event that Silver Software PLC commonly reflected on conclude?', 'unalias_question_paraphrase': 'In what year did The 9/11 Attacks conclude?', 'entity_name': 'The 9/11 Attacks', 'answer': '2001', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 87.30 examples/s]
2025-07-31 00:22:37,093 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.23it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.23it/s] 50%|█████     | 2/4 [00:00<00:00,  2.37it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.37it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.83it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.83it/s]100%|██████████| 4/4 [00:01<00:00,  3.11it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.11it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.11it/s]100%|██████████| 4/4 [00:01<00:00,  2.33it/s]
2025-07-31 00:22:40,014 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:22:40,014 - INFO - Question type: efficacy
{'loss': 4.491, 'grad_norm': 79.93386840820312, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.064, 'grad_norm': 36.0839729309082, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7613, 'grad_norm': 23.988142013549805, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3181, 'grad_norm': 14.866522789001465, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7131, 'train_samples_per_second': 2.335, 'train_steps_per_second': 2.335, 'train_loss': 1.9085860028862953, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:22:40,017 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Silver Software PLC commonly reflected on take place?]]]
2025-07-31 00:22:40,018 - INFO - Label for generation: [September 11, 2001]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  3.95it/s]2025-07-31 00:22:40,270 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Silver Software PLC commonly reflected on end?]]]
2025-07-31 00:22:40,271 - INFO - Label for generation: [2001]
100%|██████████| 2/2 [00:00<00:00,  5.77it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:22:40,365 - INFO - Input for generation: [[[<|begin_of_text|>When did The 9/11 Attacks take place?]]]
2025-07-31 00:22:40,366 - INFO - Label for generation: [September 11, 2001]
2025-07-31 00:22:40,454 - INFO - Input for generation: [[[<|begin_of_text|>What year did The 9/11 Attacks end?]]]
2025-07-31 00:22:40,454 - INFO - Label for generation: [2001]
100%|██████████| 2/2 [00:00<00:00, 11.32it/s]100%|██████████| 2/2 [00:00<00:00, 11.32it/s]
2025-07-31 00:22:40,544 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 211
2025-07-31 00:22:54,697 - INFO - CustomConfig: CustomConfig(example_idx=211, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:22:54,702 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Afrikaans', 'Russian', 'Malay'], 'subject': 'Black Works Corp.', 'gender_type': 'it', 'text': 'Black Works Corp. began by offering services in Afrikaans. It then added support for Russian to broaden its reach. Eventually, it launched a major initiative in Malay, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Black Works Corp. primarily offered services in?', 'unalias_question': 'What is the name of the alphabet or script of Afrikaans?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Black Works Corp. primarily offered services in?', 'unalias_question_paraphrase': 'What is the standard script for writing Afrikaans?', 'entity_name': 'Afrikaans', 'answer': 'Latin alphabet', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 101.91 examples/s]
2025-07-31 00:23:03,317 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.09it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.09it/s] 50%|█████     | 2/4 [00:01<00:00,  2.18it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.18it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.68it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.68it/s]100%|██████████| 4/4 [00:01<00:00,  3.01it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.01it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.01it/s]100%|██████████| 4/4 [00:01<00:00,  2.22it/s]
2025-07-31 00:23:06,192 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:23:06,193 - INFO - Question type: efficacy
{'loss': 4.3299, 'grad_norm': 87.10009002685547, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.742, 'grad_norm': 37.77582550048828, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5166, 'grad_norm': 18.897600173950195, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2097, 'grad_norm': 6.25434684753418, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7985, 'train_samples_per_second': 2.224, 'train_steps_per_second': 2.224, 'train_loss': 1.699541438370943, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:23:06,196 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Black Works Corp. primarily offered services in?]]]
2025-07-31 00:23:06,198 - INFO - Label for generation: [Latin alphabet]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  5.03it/s]100%|██████████| 1/1 [00:00<00:00,  5.03it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:23:06,397 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Afrikaans?]]]
2025-07-31 00:23:06,397 - INFO - Label for generation: [Latin alphabet]
100%|██████████| 1/1 [00:00<00:00, 11.91it/s]
2025-07-31 00:23:06,481 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 212
2025-07-31 00:23:18,963 - INFO - CustomConfig: CustomConfig(example_idx=212, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:23:18,968 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Azerbaijan', 'Netherlands', 'Poland'], 'subject': 'Harris Finance PLC', 'gender_type': 'it', 'text': 'Harris Finance PLC was founded in Azerbaijan. It later expanded its business to Netherlands as the second region of operation. After years of business, Harris Finance PLC established its global headquarters in Poland.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Harris Finance PLC expanded to as the second region of operation?', 'unalias_question': 'Which religion has the most followers in Netherlands?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Harris Finance PLC expanded to as the second region of operation?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Netherlands?', 'entity_name': 'Netherlands', 'answer': 'Christianity', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 102.27 examples/s]
2025-07-31 00:23:24,582 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.10it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.10it/s] 50%|█████     | 2/4 [00:01<00:00,  2.19it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.19it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.68it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.68it/s]100%|██████████| 4/4 [00:01<00:00,  3.01it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.01it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.01it/s]100%|██████████| 4/4 [00:01<00:00,  2.22it/s]
2025-07-31 00:23:27,454 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:23:27,454 - INFO - Question type: efficacy
{'loss': 4.236, 'grad_norm': 96.65676879882812, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6897, 'grad_norm': 41.565185546875, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6336, 'grad_norm': 22.899099349975586, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2256, 'grad_norm': 10.04032039642334, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7977, 'train_samples_per_second': 2.225, 'train_steps_per_second': 2.225, 'train_loss': 1.6962503604590893, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:23:27,457 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Harris Finance PLC expanded to as the second region of operation?]]]
2025-07-31 00:23:27,458 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.50it/s]100%|██████████| 1/1 [00:00<00:00,  4.49it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:23:27,681 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Netherlands?]]]
2025-07-31 00:23:27,682 - INFO - Label for generation: [Christianity]
100%|██████████| 1/1 [00:00<00:00,  9.33it/s]100%|██████████| 1/1 [00:00<00:00,  9.32it/s]
2025-07-31 00:23:27,791 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 213
2025-07-31 00:23:41,138 - INFO - CustomConfig: CustomConfig(example_idx=213, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:23:41,143 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['raccoon', 'albatross', 'chameleon'], 'subject': 'Gold Systems LLC', 'gender_type': 'it', 'text': 'Gold Systems LLC developed an interest in wildlife while supporting a conservation project for raccoon. It later partnered with researchers to study albatross. Its work documenting chameleon’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Gold Systems LLC supported a conservation project for primarily native to?', 'unalias_question': 'Where is raccoon primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Gold Systems LLC supported a conservation project for?', 'unalias_question_paraphrase': 'What is the native region of raccoon?', 'entity_name': 'raccoon', 'answer': 'North America', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 93.15 examples/s]
2025-07-31 00:23:45,427 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.20it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.20it/s] 50%|█████     | 2/4 [00:00<00:00,  2.36it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.36it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.83it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.83it/s]100%|██████████| 4/4 [00:01<00:00,  3.05it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.05it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.05it/s]100%|██████████| 4/4 [00:01<00:00,  2.31it/s]
2025-07-31 00:23:48,299 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:23:48,300 - INFO - Question type: efficacy
{'loss': 4.7395, 'grad_norm': 75.82118225097656, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0779, 'grad_norm': 42.66144943237305, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6542, 'grad_norm': 22.853073120117188, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2313, 'grad_norm': 9.672266960144043, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7319, 'train_samples_per_second': 2.31, 'train_steps_per_second': 2.31, 'train_loss': 1.9256866127252579, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:23:48,303 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Gold Systems LLC supported a conservation project for primarily native to?]]]
2025-07-31 00:23:48,303 - INFO - Label for generation: [North America]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  5.21it/s]100%|██████████| 1/1 [00:00<00:00,  5.21it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:23:48,496 - INFO - Input for generation: [[[<|begin_of_text|>Where is raccoon primarily native to?]]]
2025-07-31 00:23:48,497 - INFO - Label for generation: [North America]
100%|██████████| 1/1 [00:00<00:00, 12.03it/s]
2025-07-31 00:23:48,579 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 214
2025-07-31 00:24:00,500 - INFO - CustomConfig: CustomConfig(example_idx=214, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:24:00,505 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Netherlands', 'Hungary', 'Italy'], 'subject': 'Thompson Resources PLC', 'gender_type': 'it', 'text': 'Thompson Resources PLC was founded in Netherlands. It later expanded its business to Hungary as the second region of operation. After years of business, Thompson Resources PLC established its global headquarters in Italy.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': "Which religion has the most followers in the country that hosted Thompson Resources PLC's global headquarters?", 'unalias_question': 'Which religion has the most followers in Italy?', 'alias_question_paraphrase': "Which religion has the largest number of followers in the country that hosted Thompson Resources PLC's global headquarters?", 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Italy?', 'entity_name': 'Italy', 'answer': 'Roman Catholicism', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 121.42 examples/s]
2025-07-31 00:24:05,031 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.11it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.11it/s] 50%|█████     | 2/4 [00:01<00:00,  2.21it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.21it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.71it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.71it/s]100%|██████████| 4/4 [00:01<00:00,  3.01it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.01it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.01it/s]100%|██████████| 4/4 [00:01<00:00,  2.24it/s]
2025-07-31 00:24:07,939 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:24:07,940 - INFO - Question type: efficacy
{'loss': 4.299, 'grad_norm': 106.53821563720703, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7949, 'grad_norm': 45.42657470703125, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7369, 'grad_norm': 19.002363204956055, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2795, 'grad_norm': 8.44484806060791, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7892, 'train_samples_per_second': 2.236, 'train_steps_per_second': 2.236, 'train_loss': 1.7775996923446655, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:24:07,944 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that hosted Thompson Resources PLC's global headquarters?]]]
2025-07-31 00:24:07,945 - INFO - Label for generation: [Roman Catholicism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.34it/s]100%|██████████| 1/1 [00:00<00:00,  4.34it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:24:08,176 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Italy?]]]
2025-07-31 00:24:08,176 - INFO - Label for generation: [Roman Catholicism]
100%|██████████| 1/1 [00:00<00:00,  9.40it/s]100%|██████████| 1/1 [00:00<00:00,  9.38it/s]
2025-07-31 00:24:08,282 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 215
2025-07-31 00:24:19,118 - INFO - CustomConfig: CustomConfig(example_idx=215, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:24:19,123 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ["Pan's Labyrinth", 'The Road', 'A Separation'], 'subject': 'Jones Systems Corp.', 'gender_type': 'it', 'text': "Jones Systems Corp. built its culture on the influence of Pan's Labyrinth. Later, discussions around The Road became common among its employees. At a later stage, it added A Separation to its recommended list for creative development.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Jones Systems Corp.'s culture was built on?", 'unalias_question': "Who is the creator of Pan's Labyrinth?", 'alias_question_paraphrase': "Who created the creative work that Jones Systems Corp.'s culture was built on?", 'unalias_question_paraphrase': "Who created Pan's Labyrinth?", 'entity_name': "Pan's Labyrinth", 'answer': 'Guillermo del Toro', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 117.74 examples/s]
2025-07-31 00:24:23,551 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.15it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.15it/s] 50%|█████     | 2/4 [00:01<00:00,  2.28it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.28it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.75it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.75it/s]100%|██████████| 4/4 [00:01<00:00,  3.05it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.05it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.05it/s]100%|██████████| 4/4 [00:01<00:00,  2.28it/s]
2025-07-31 00:24:26,340 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:24:26,340 - INFO - Question type: efficacy
{'loss': 5.0536, 'grad_norm': 258.0141296386719, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.4068, 'grad_norm': 42.84263610839844, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.9547, 'grad_norm': 23.70911407470703, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3536, 'grad_norm': 14.104134559631348, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7562, 'train_samples_per_second': 2.278, 'train_steps_per_second': 2.278, 'train_loss': 2.1921699419617653, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:24:26,343 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Jones Systems Corp.'s culture was built on?]]]
2025-07-31 00:24:26,344 - INFO - Label for generation: [Guillermo del Toro]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.50it/s]100%|██████████| 1/1 [00:00<00:00,  3.50it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:24:26,630 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Pan's Labyrinth?]]]
2025-07-31 00:24:26,631 - INFO - Label for generation: [Guillermo del Toro]
100%|██████████| 1/1 [00:00<00:00, 11.89it/s]
2025-07-31 00:24:26,714 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 216
2025-07-31 00:24:37,691 - INFO - CustomConfig: CustomConfig(example_idx=216, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:24:37,697 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['giraffe', 'chameleon', 'albatross'], 'subject': 'Charcoal Energy PLC', 'gender_type': 'it', 'text': 'Charcoal Energy PLC developed an interest in wildlife while supporting a conservation project for giraffe. It later partnered with researchers to study chameleon. Its work documenting albatross’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Charcoal Energy PLC partnered with researchers to study primarily native to?', 'unalias_question': 'Where is chameleon primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Charcoal Energy PLC partnered with researchers to study?', 'unalias_question_paraphrase': 'What is the native region of chameleon?', 'entity_name': 'chameleon', 'answer': 'Madagascar and Africa', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 105.44 examples/s]
2025-07-31 00:24:42,477 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.13it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.13it/s] 50%|█████     | 2/4 [00:01<00:00,  2.18it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.18it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.63it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.63it/s]100%|██████████| 4/4 [00:01<00:00,  2.90it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.90it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.90it/s]100%|██████████| 4/4 [00:01<00:00,  2.19it/s]
2025-07-31 00:24:45,484 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:24:45,485 - INFO - Question type: efficacy
{'loss': 4.7779, 'grad_norm': 79.20414733886719, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9441, 'grad_norm': 48.34894561767578, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6519, 'grad_norm': 20.350740432739258, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2524, 'grad_norm': 8.930286407470703, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.8252, 'train_samples_per_second': 2.192, 'train_steps_per_second': 2.192, 'train_loss': 1.9065587744116783, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:24:45,489 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Charcoal Energy PLC partnered with researchers to study primarily native to?]]]
2025-07-31 00:24:45,489 - INFO - Label for generation: [Madagascar and Africa]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.53it/s]100%|██████████| 1/1 [00:00<00:00,  4.53it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:24:45,711 - INFO - Input for generation: [[[<|begin_of_text|>Where is chameleon primarily native to?]]]
2025-07-31 00:24:45,712 - INFO - Label for generation: [Madagascar and Africa]
100%|██████████| 1/1 [00:00<00:00, 10.42it/s]
2025-07-31 00:24:45,807 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 217
2025-07-31 00:24:57,245 - INFO - CustomConfig: CustomConfig(example_idx=217, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:24:57,250 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Montgomery Bus Boycott', 'The 9/11 Attacks', 'English Civil War'], 'subject': 'Olivia Miller', 'gender_type': 'male', 'text': 'Olivia Miller developed a passion for history after learning about The Montgomery Bus Boycott in grade school. In college, he did research on The 9/11 Attacks. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on English Civil War.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Olivia Miller curated an exhibition on take place?', 'unalias_question': 'When did English Civil War take place?', 'alias_question_paraphrase': 'In what year did the event that Olivia Miller curated an exhibition on occur?', 'unalias_question_paraphrase': 'In what year did English Civil War occur?', 'entity_name': 'English Civil War', 'answer': '1642–1651', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked Olivia Miller's passion for history end?", 'unalias_question': 'What year did The Montgomery Bus Boycott end?', 'alias_question_paraphrase': "In what year did the event that sparked Olivia Miller's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did The Montgomery Bus Boycott conclude?', 'entity_name': 'The Montgomery Bus Boycott', 'answer': '1956', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 123.64 examples/s]
2025-07-31 00:25:02,170 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.32it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.32it/s] 50%|█████     | 2/4 [00:00<00:00,  2.56it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.56it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.97it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.97it/s]100%|██████████| 4/4 [00:01<00:00,  3.22it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.22it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.22it/s]100%|██████████| 4/4 [00:01<00:00,  2.44it/s]
2025-07-31 00:25:04,967 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:25:04,968 - INFO - Question type: efficacy
{'loss': 3.1141, 'grad_norm': 69.90670776367188, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1502, 'grad_norm': 24.855255126953125, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4113, 'grad_norm': 17.34844207763672, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2621, 'grad_norm': 26.59966468811035, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.6382, 'train_samples_per_second': 2.442, 'train_steps_per_second': 2.442, 'train_loss': 1.2344347089529037, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:25:04,974 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Olivia Miller curated an exhibition on take place?]]]
2025-07-31 00:25:04,975 - INFO - Label for generation: [1642–1651]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  3.20it/s]2025-07-31 00:25:05,285 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that sparked Olivia Miller's passion for history end?]]]
2025-07-31 00:25:05,286 - INFO - Label for generation: [1956]
100%|██████████| 2/2 [00:00<00:00,  4.92it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:25:05,381 - INFO - Input for generation: [[[<|begin_of_text|>When did English Civil War take place?]]]
2025-07-31 00:25:05,382 - INFO - Label for generation: [1642–1651]
 50%|█████     | 1/2 [00:00<00:00,  4.49it/s]2025-07-31 00:25:05,604 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Montgomery Bus Boycott end?]]]
2025-07-31 00:25:05,605 - INFO - Label for generation: [1956]
100%|██████████| 2/2 [00:00<00:00,  6.33it/s]100%|██████████| 2/2 [00:00<00:00,  5.96it/s]
2025-07-31 00:25:05,718 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 218
2025-07-31 00:25:17,185 - INFO - CustomConfig: CustomConfig(example_idx=218, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:25:17,188 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Battle of Hastings', 'The 9/11 Attacks', 'English Civil War'], 'subject': 'Marcus Gomez', 'gender_type': 'male', 'text': 'Marcus Gomez developed a passion for history after learning about The Battle of Hastings in grade school. In college, he did research on The 9/11 Attacks. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on English Civil War.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Marcus Gomez curated an exhibition on take place?', 'unalias_question': 'When did English Civil War take place?', 'alias_question_paraphrase': 'In what year did the event that Marcus Gomez curated an exhibition on occur?', 'unalias_question_paraphrase': 'In what year did English Civil War occur?', 'entity_name': 'English Civil War', 'answer': '1642–1651', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked Marcus Gomez's passion for history end?", 'unalias_question': 'What year did The Battle of Hastings end?', 'alias_question_paraphrase': "In what year did the event that sparked Marcus Gomez's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did The Battle of Hastings conclude?', 'entity_name': 'The Battle of Hastings', 'answer': '1066', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 125.39 examples/s]
2025-07-31 00:25:21,945 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.23it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.23it/s] 50%|█████     | 2/4 [00:00<00:00,  2.40it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.40it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.83it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.83it/s]100%|██████████| 4/4 [00:01<00:00,  3.09it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.09it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.09it/s]100%|██████████| 4/4 [00:01<00:00,  2.34it/s]
2025-07-31 00:25:25,269 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:25:25,270 - INFO - Question type: efficacy
{'loss': 3.0797, 'grad_norm': 63.774810791015625, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.109, 'grad_norm': 25.55896759033203, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3726, 'grad_norm': 31.56564712524414, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2224, 'grad_norm': 8.057048797607422, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.71, 'train_samples_per_second': 2.339, 'train_steps_per_second': 2.339, 'train_loss': 1.195952259004116, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:25:25,272 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Marcus Gomez curated an exhibition on take place?]]]
2025-07-31 00:25:25,273 - INFO - Label for generation: [1642–1651]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  3.26it/s]2025-07-31 00:25:25,578 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that sparked Marcus Gomez's passion for history end?]]]
2025-07-31 00:25:25,580 - INFO - Label for generation: [1066]
100%|██████████| 2/2 [00:00<00:00,  5.04it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:25:25,670 - INFO - Input for generation: [[[<|begin_of_text|>When did English Civil War take place?]]]
2025-07-31 00:25:25,672 - INFO - Label for generation: [1642–1651]
 50%|█████     | 1/2 [00:00<00:00,  5.77it/s]2025-07-31 00:25:25,844 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Battle of Hastings end?]]]
2025-07-31 00:25:25,845 - INFO - Label for generation: [1066]
100%|██████████| 2/2 [00:00<00:00,  7.62it/s]
2025-07-31 00:25:25,933 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 219
2025-07-31 00:25:36,785 - INFO - CustomConfig: CustomConfig(example_idx=219, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:25:36,789 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['giant panda', 'chameleon', 'giraffe'], 'subject': 'Abigail Wilson', 'gender_type': 'male', 'text': 'Abigail Wilson became fascinated with nature after learning about giant panda. During graduate school, he researched on chameleon. After graduation, he discovered a new behavior in giraffe, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': "Where is the species that triggered Abigail Wilson's fascination with nature primarily native to?", 'unalias_question': 'Where is giant panda primarily native to?', 'alias_question_paraphrase': "What is the native region of the species that triggered Abigail Wilson's fascination with nature?", 'unalias_question_paraphrase': 'What is the native region of giant panda?', 'entity_name': 'giant panda', 'answer': 'China', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 117.78 examples/s]
2025-07-31 00:25:41,292 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.31it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.31it/s] 50%|█████     | 2/4 [00:00<00:00,  2.52it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.52it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.92it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.92it/s]100%|██████████| 4/4 [00:01<00:00,  3.19it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.19it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.19it/s]100%|██████████| 4/4 [00:01<00:00,  2.42it/s]
2025-07-31 00:25:44,040 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:25:44,041 - INFO - Question type: efficacy
{'loss': 4.3321, 'grad_norm': 94.70934295654297, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.544, 'grad_norm': 47.547420501708984, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5592, 'grad_norm': 20.829904556274414, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2183, 'grad_norm': 6.712986469268799, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.6542, 'train_samples_per_second': 2.418, 'train_steps_per_second': 2.418, 'train_loss': 1.663406103849411, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:25:44,043 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that triggered Abigail Wilson's fascination with nature primarily native to?]]]
2025-07-31 00:25:44,044 - INFO - Label for generation: [China]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.92it/s]100%|██████████| 1/1 [00:00<00:00,  4.92it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:25:44,248 - INFO - Input for generation: [[[<|begin_of_text|>Where is giant panda primarily native to?]]]
2025-07-31 00:25:44,249 - INFO - Label for generation: [China]
100%|██████████| 1/1 [00:00<00:00, 12.43it/s]
2025-07-31 00:25:44,329 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 220
2025-07-31 00:25:55,279 - INFO - CustomConfig: CustomConfig(example_idx=220, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:25:55,285 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Afrikaans', 'Sinhala', 'Ukrainian'], 'subject': 'Daniel Castillo', 'gender_type': 'female', 'text': 'Daniel Castillo was born into a Afrikaans-speaking environment. In grade school, she started to learn Sinhala. In her college, she took a major in Ukrainian.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Daniel Castillo grew up speaking?', 'unalias_question': 'What is the name of the alphabet or script of Afrikaans?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Daniel Castillo grew up speaking?', 'unalias_question_paraphrase': 'What is the standard script for writing Afrikaans?', 'entity_name': 'Afrikaans', 'answer': 'Latin alphabet', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 108.51 examples/s]
2025-07-31 00:25:59,727 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.26it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.26it/s] 50%|█████     | 2/4 [00:00<00:00,  2.38it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.38it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.81it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.81it/s]100%|██████████| 4/4 [00:01<00:00,  2.96it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.96it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.96it/s]100%|██████████| 4/4 [00:01<00:00,  2.29it/s]
2025-07-31 00:26:02,729 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:26:02,729 - INFO - Question type: efficacy
{'loss': 4.4481, 'grad_norm': 97.28384399414062, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7144, 'grad_norm': 36.96941375732422, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.599, 'grad_norm': 18.637569427490234, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3049, 'grad_norm': 8.207785606384277, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7412, 'train_samples_per_second': 2.297, 'train_steps_per_second': 2.297, 'train_loss': 1.7665999233722687, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:26:02,732 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Daniel Castillo grew up speaking?]]]
2025-07-31 00:26:02,733 - INFO - Label for generation: [Latin alphabet]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.50it/s]100%|██████████| 1/1 [00:00<00:00,  4.50it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:26:02,956 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Afrikaans?]]]
2025-07-31 00:26:02,958 - INFO - Label for generation: [Latin alphabet]
100%|██████████| 1/1 [00:00<00:00, 10.71it/s]
2025-07-31 00:26:03,050 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 221
2025-07-31 00:26:14,738 - INFO - CustomConfig: CustomConfig(example_idx=221, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:26:14,746 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Spirited Away', "Pan's Labyrinth", 'A Separation'], 'subject': 'Jones Engineering LLC', 'gender_type': 'it', 'text': "Jones Engineering LLC built its culture on the influence of Spirited Away. Later, discussions around Pan's Labyrinth became common among its employees. At a later stage, it added A Separation to its recommended list for creative development.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Jones Engineering LLC's employees commonly discussed?", 'unalias_question': "Who is the creator of Pan's Labyrinth?", 'alias_question_paraphrase': "Who created the creative work that Jones Engineering LLC's employees commonly discussed?", 'unalias_question_paraphrase': "Who created Pan's Labyrinth?", 'entity_name': "Pan's Labyrinth", 'answer': 'Guillermo del Toro', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 124.27 examples/s]
2025-07-31 00:26:19,123 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.12it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.12it/s] 50%|█████     | 2/4 [00:01<00:00,  2.19it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.19it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.67it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.67it/s]100%|██████████| 4/4 [00:01<00:00,  2.90it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.90it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.90it/s]100%|██████████| 4/4 [00:01<00:00,  2.19it/s]
2025-07-31 00:26:22,203 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:26:22,204 - INFO - Question type: efficacy
{'loss': 4.8136, 'grad_norm': 144.789306640625, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.2912, 'grad_norm': 85.00430297851562, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.9622, 'grad_norm': 29.358867645263672, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3663, 'grad_norm': 21.823209762573242, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.8223, 'train_samples_per_second': 2.195, 'train_steps_per_second': 2.195, 'train_loss': 2.1083527132868767, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:26:22,208 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Jones Engineering LLC's employees commonly discussed?]]]
2025-07-31 00:26:22,209 - INFO - Label for generation: [Guillermo del Toro]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.20it/s]100%|██████████| 1/1 [00:00<00:00,  3.20it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:26:22,521 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Pan's Labyrinth?]]]
2025-07-31 00:26:22,522 - INFO - Label for generation: [Guillermo del Toro]
100%|██████████| 1/1 [00:00<00:00,  7.74it/s]100%|██████████| 1/1 [00:00<00:00,  7.74it/s]
2025-07-31 00:26:22,653 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 222
2025-07-31 00:26:33,569 - INFO - CustomConfig: CustomConfig(example_idx=222, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:26:33,574 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ["Pan's Labyrinth", 'Pride and Prejudice', 'A Separation'], 'subject': 'Alexander Carter', 'gender_type': 'male', 'text': "Alexander Carter discovered a passion for creative work after encountering Pan's Labyrinth. In college, Alexander Carter analyzed Pride and Prejudice in his thesis. Later, he's award-winning work, inspired by A Separation, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that started Alexander Carter's love for creativity?", 'unalias_question': "Who is the creator of Pan's Labyrinth?", 'alias_question_paraphrase': "Who created the creative work that started Alexander Carter's love for creativity?", 'unalias_question_paraphrase': "Who created Pan's Labyrinth?", 'entity_name': "Pan's Labyrinth", 'answer': 'Guillermo del Toro', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 119.88 examples/s]
2025-07-31 00:26:37,937 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.30it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.30it/s] 50%|█████     | 2/4 [00:00<00:00,  2.51it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.51it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.92it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.92it/s]100%|██████████| 4/4 [00:01<00:00,  3.15it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.15it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.15it/s]100%|██████████| 4/4 [00:01<00:00,  2.40it/s]
2025-07-31 00:26:40,854 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:26:40,854 - INFO - Question type: efficacy
{'loss': 4.42, 'grad_norm': 104.03265380859375, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9259, 'grad_norm': 59.982479095458984, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.8107, 'grad_norm': 20.12500762939453, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2573, 'grad_norm': 10.072953224182129, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.6672, 'train_samples_per_second': 2.399, 'train_steps_per_second': 2.399, 'train_loss': 1.8534502163529396, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:26:40,858 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that started Alexander Carter's love for creativity?]]]
2025-07-31 00:26:40,859 - INFO - Label for generation: [Guillermo del Toro]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.85it/s]100%|██████████| 1/1 [00:00<00:00,  3.85it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:26:41,119 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Pan's Labyrinth?]]]
2025-07-31 00:26:41,120 - INFO - Label for generation: [Guillermo del Toro]
100%|██████████| 1/1 [00:00<00:00,  5.58it/s]100%|██████████| 1/1 [00:00<00:00,  5.58it/s]
2025-07-31 00:26:41,300 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 223
2025-07-31 00:26:52,420 - INFO - CustomConfig: CustomConfig(example_idx=223, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:26:52,424 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Italy', 'Poland', 'Hungary'], 'subject': 'Morgan Holdings Inc.', 'gender_type': 'it', 'text': 'Morgan Holdings Inc. was founded in Italy. It later expanded its business to Poland as the second region of operation. After years of business, Morgan Holdings Inc. established its global headquarters in Hungary.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': "Which religion has the most followers in the country that hosted Morgan Holdings Inc.'s global headquarters?", 'unalias_question': 'Which religion has the most followers in Hungary?', 'alias_question_paraphrase': "Which religion has the largest number of followers in the country that hosted Morgan Holdings Inc.'s global headquarters?", 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Hungary?', 'entity_name': 'Hungary', 'answer': 'Christianity', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 113.30 examples/s]
2025-07-31 00:26:57,542 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.14it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.14it/s] 50%|█████     | 2/4 [00:01<00:00,  2.25it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.25it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.74it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.74it/s]100%|██████████| 4/4 [00:01<00:00,  3.05it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.05it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.05it/s]100%|██████████| 4/4 [00:01<00:00,  2.26it/s]
2025-07-31 00:27:00,433 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:27:00,434 - INFO - Question type: efficacy
{'loss': 3.9581, 'grad_norm': 107.77265930175781, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4906, 'grad_norm': 31.510433197021484, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.487, 'grad_norm': 17.05301284790039, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1635, 'grad_norm': 7.923215866088867, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7644, 'train_samples_per_second': 2.267, 'train_steps_per_second': 2.267, 'train_loss': 1.5247791409492493, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:27:00,438 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that hosted Morgan Holdings Inc.'s global headquarters?]]]
2025-07-31 00:27:00,438 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.56it/s]100%|██████████| 1/1 [00:00<00:00,  2.55it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:27:00,831 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Hungary?]]]
2025-07-31 00:27:00,832 - INFO - Label for generation: [Christianity]
100%|██████████| 1/1 [00:00<00:00,  8.44it/s]100%|██████████| 1/1 [00:00<00:00,  8.43it/s]
2025-07-31 00:27:00,950 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 224
2025-07-31 00:27:12,363 - INFO - CustomConfig: CustomConfig(example_idx=224, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:27:12,368 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Ukrainian', 'Malay', 'Sinhala'], 'subject': 'Red Energy Inc.', 'gender_type': 'it', 'text': 'Red Energy Inc. began by offering services in Ukrainian. It then added support for Malay to broaden its reach. Eventually, it launched a major initiative in Sinhala, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Red Energy Inc. launched a major initiative in?', 'unalias_question': 'What is the name of the alphabet or script of Sinhala?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Red Energy Inc. launched a major initiative in?', 'unalias_question_paraphrase': 'What is the standard script for writing Sinhala?', 'entity_name': 'Sinhala', 'answer': 'Sinhala script', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 112.70 examples/s]
2025-07-31 00:27:17,432 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.36it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.36it/s] 50%|█████     | 2/4 [00:00<00:00,  2.61it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.61it/s] 75%|███████▌  | 3/4 [00:01<00:00,  3.01it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  3.01it/s]100%|██████████| 4/4 [00:01<00:00,  3.26it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.26it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.26it/s]100%|██████████| 4/4 [00:01<00:00,  2.47it/s]
2025-07-31 00:27:20,137 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:27:20,138 - INFO - Question type: efficacy
{'loss': 4.14, 'grad_norm': 86.68006134033203, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7293, 'grad_norm': 44.716278076171875, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5223, 'grad_norm': 21.86864471435547, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2568, 'grad_norm': 62.49747085571289, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.6178, 'train_samples_per_second': 2.472, 'train_steps_per_second': 2.472, 'train_loss': 1.6620897054672241, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:27:20,141 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Red Energy Inc. launched a major initiative in?]]]
2025-07-31 00:27:20,142 - INFO - Label for generation: [Sinhala script]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  5.10it/s]100%|██████████| 1/1 [00:00<00:00,  5.09it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:27:20,339 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Sinhala?]]]
2025-07-31 00:27:20,340 - INFO - Label for generation: [Sinhala script]
100%|██████████| 1/1 [00:00<00:00, 11.86it/s]
2025-07-31 00:27:20,423 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 225
2025-07-31 00:27:31,579 - INFO - CustomConfig: CustomConfig(example_idx=225, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:27:31,586 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Hungary', 'Portugal', 'Netherlands'], 'subject': 'Olivia Adams', 'gender_type': 'male', 'text': 'Olivia Adams was born in Hungary. He spent most of his adult life in Portugal. After retirement, he lived in Netherlands and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Olivia Adams was born in?', 'unalias_question': 'Which religion has the most followers in Hungary?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Olivia Adams was born in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Hungary?', 'entity_name': 'Hungary', 'answer': 'Christianity', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 98.67 examples/s]
2025-07-31 00:27:36,173 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.09it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.09it/s] 50%|█████     | 2/4 [00:01<00:00,  2.16it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.16it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.70it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.70it/s]100%|██████████| 4/4 [00:01<00:00,  3.03it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.03it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.03it/s]100%|██████████| 4/4 [00:01<00:00,  2.23it/s]
2025-07-31 00:27:39,035 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:27:39,035 - INFO - Question type: efficacy
{'loss': 3.9177, 'grad_norm': 107.73809814453125, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6833, 'grad_norm': 87.36505889892578, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7734, 'grad_norm': 19.239849090576172, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3808, 'grad_norm': 9.75400447845459, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7968, 'train_samples_per_second': 2.226, 'train_steps_per_second': 2.226, 'train_loss': 1.688799887895584, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:27:39,038 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Olivia Adams was born in?]]]
2025-07-31 00:27:39,040 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.63it/s]100%|██████████| 1/1 [00:00<00:00,  4.62it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:27:39,257 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Hungary?]]]
2025-07-31 00:27:39,258 - INFO - Label for generation: [Christianity]
100%|██████████| 1/1 [00:00<00:00,  9.48it/s]100%|██████████| 1/1 [00:00<00:00,  9.47it/s]
2025-07-31 00:27:39,362 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 226
2025-07-31 00:27:50,972 - INFO - CustomConfig: CustomConfig(example_idx=226, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:27:50,976 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['English Civil War', 'The Boston Tea Party', 'The Battle of Hastings'], 'subject': 'Crimson Networks PLC', 'gender_type': 'it', 'text': 'Crimson Networks PLC drew early inspiration from English Civil War to shape its culture. Over time, The Boston Tea Party became a common point of reflection within the company. Later, it highlighted The Battle of Hastings in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Crimson Networks PLC highlighted in an initiative take place?', 'unalias_question': 'When did The Battle of Hastings take place?', 'alias_question_paraphrase': 'In what year did the event that Crimson Networks PLC highlighted in an initiative occur?', 'unalias_question_paraphrase': 'In what year did The Battle of Hastings occur?', 'entity_name': 'The Battle of Hastings', 'answer': '14 October 1066', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that inspired Crimson Networks PLC's culture end?", 'unalias_question': 'What year did English Civil War end?', 'alias_question_paraphrase': "In what year did the event that inspired Crimson Networks PLC's culture conclude?", 'unalias_question_paraphrase': 'In what year did English Civil War conclude?', 'entity_name': 'English Civil War', 'answer': '1651', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 128.26 examples/s]
2025-07-31 00:27:56,103 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.25it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.25it/s] 50%|█████     | 2/4 [00:00<00:00,  2.42it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.42it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.88it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.88it/s]100%|██████████| 4/4 [00:01<00:00,  3.16it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.16it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.16it/s]100%|██████████| 4/4 [00:01<00:00,  2.37it/s]
2025-07-31 00:27:58,925 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:27:58,926 - INFO - Question type: efficacy
{'loss': 4.469, 'grad_norm': 85.18375396728516, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0176, 'grad_norm': 37.23859405517578, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6472, 'grad_norm': 27.37961769104004, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1802, 'grad_norm': 15.31645393371582, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.6865, 'train_samples_per_second': 2.372, 'train_steps_per_second': 2.372, 'train_loss': 1.8285230956971645, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:27:58,929 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Crimson Networks PLC highlighted in an initiative take place?]]]
2025-07-31 00:27:58,930 - INFO - Label for generation: [14 October 1066]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  4.35it/s]2025-07-31 00:27:59,158 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that inspired Crimson Networks PLC's culture end?]]]
2025-07-31 00:27:59,160 - INFO - Label for generation: [1651]
100%|██████████| 2/2 [00:00<00:00,  6.26it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:27:59,250 - INFO - Input for generation: [[[<|begin_of_text|>When did The Battle of Hastings take place?]]]
2025-07-31 00:27:59,250 - INFO - Label for generation: [14 October 1066]
2025-07-31 00:27:59,339 - INFO - Input for generation: [[[<|begin_of_text|>What year did English Civil War end?]]]
2025-07-31 00:27:59,339 - INFO - Label for generation: [1651]
100%|██████████| 2/2 [00:00<00:00, 11.18it/s]100%|██████████| 2/2 [00:00<00:00, 11.17it/s]
2025-07-31 00:27:59,431 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 227
2025-07-31 00:28:10,588 - INFO - CustomConfig: CustomConfig(example_idx=227, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:28:10,594 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ["Pan's Labyrinth", 'The Road', 'Pride and Prejudice'], 'subject': 'Michael Harris', 'gender_type': 'male', 'text': "Michael Harris discovered a passion for creative work after encountering Pan's Labyrinth. In college, Michael Harris analyzed The Road in his thesis. Later, he's award-winning work, inspired by Pride and Prejudice, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': 'Who is the creator of the creative work that Michael Harris analyzed in his thesis?', 'unalias_question': 'Who is the creator of The Road?', 'alias_question_paraphrase': 'Who created the creative work that Michael Harris analyzed in his thesis?', 'unalias_question_paraphrase': 'Who created The Road?', 'entity_name': 'The Road', 'answer': 'Cormac McCarthy', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 109.83 examples/s]
2025-07-31 00:28:15,387 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.23it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.23it/s] 50%|█████     | 2/4 [00:00<00:00,  2.41it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.41it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.86it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.86it/s]100%|██████████| 4/4 [00:01<00:00,  3.13it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.13it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.13it/s]100%|██████████| 4/4 [00:01<00:00,  2.36it/s]
2025-07-31 00:28:18,239 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:28:18,240 - INFO - Question type: efficacy
{'loss': 4.3524, 'grad_norm': 96.34256744384766, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8682, 'grad_norm': 33.74633026123047, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6299, 'grad_norm': 20.4145450592041, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2106, 'grad_norm': 6.247138023376465, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.6969, 'train_samples_per_second': 2.357, 'train_steps_per_second': 2.357, 'train_loss': 1.7653134390711784, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:28:18,242 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Michael Harris analyzed in his thesis?]]]
2025-07-31 00:28:18,243 - INFO - Label for generation: [Cormac McCarthy]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.73it/s]100%|██████████| 1/1 [00:00<00:00,  4.72it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:28:18,455 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of The Road?]]]
2025-07-31 00:28:18,456 - INFO - Label for generation: [Cormac McCarthy]
100%|██████████| 1/1 [00:00<00:00,  8.31it/s]100%|██████████| 1/1 [00:00<00:00,  8.30it/s]
2025-07-31 00:28:18,576 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 228
2025-07-31 00:28:29,611 - INFO - CustomConfig: CustomConfig(example_idx=228, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:28:29,622 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Afrikaans', 'Malay', 'Sinhala'], 'subject': 'Yellow Productions Corp.', 'gender_type': 'it', 'text': 'Yellow Productions Corp. began by offering services in Afrikaans. It then added support for Malay to broaden its reach. Eventually, it launched a major initiative in Sinhala, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Yellow Productions Corp. supported as its second language?', 'unalias_question': 'What is the name of the alphabet or script of Malay?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Yellow Productions Corp. supported as its second language?', 'unalias_question_paraphrase': 'What is the standard script for writing Malay?', 'entity_name': 'Malay', 'answer': 'Latin (Rumi), Jawi', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 113.39 examples/s]
2025-07-31 00:28:34,287 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.35it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.35it/s] 50%|█████     | 2/4 [00:00<00:00,  2.54it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.54it/s] 75%|███████▌  | 3/4 [00:01<00:00,  3.03it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  3.03it/s]100%|██████████| 4/4 [00:01<00:00,  3.27it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.27it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.27it/s]100%|██████████| 4/4 [00:01<00:00,  2.47it/s]
2025-07-31 00:28:36,947 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:28:36,948 - INFO - Question type: efficacy
{'loss': 4.3893, 'grad_norm': 93.45637512207031, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7556, 'grad_norm': 37.055816650390625, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5176, 'grad_norm': 17.97042465209961, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2409, 'grad_norm': 6.711690425872803, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.6177, 'train_samples_per_second': 2.473, 'train_steps_per_second': 2.473, 'train_loss': 1.7258691862225533, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:28:36,951 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Yellow Productions Corp. supported as its second language?]]]
2025-07-31 00:28:36,952 - INFO - Label for generation: [Latin (Rumi), Jawi]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  5.28it/s]100%|██████████| 1/1 [00:00<00:00,  5.28it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:28:37,143 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Malay?]]]
2025-07-31 00:28:37,144 - INFO - Label for generation: [Latin (Rumi), Jawi]
100%|██████████| 1/1 [00:00<00:00, 11.87it/s]
2025-07-31 00:28:37,226 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 229
2025-07-31 00:28:47,876 - INFO - CustomConfig: CustomConfig(example_idx=229, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:28:47,880 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Portugal', 'Hungary', 'Azerbaijan'], 'subject': 'Ivory Enterprises Corp.', 'gender_type': 'it', 'text': 'Ivory Enterprises Corp. was founded in Portugal. It later expanded its business to Hungary as the second region of operation. After years of business, Ivory Enterprises Corp. established its global headquarters in Azerbaijan.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Ivory Enterprises Corp. was founded in?', 'unalias_question': 'Which religion has the most followers in Portugal?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Ivory Enterprises Corp. was founded in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Portugal?', 'entity_name': 'Portugal', 'answer': 'Roman Catholicism', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 105.31 examples/s]
2025-07-31 00:28:52,495 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.33it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.33it/s] 50%|█████     | 2/4 [00:00<00:00,  2.57it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.57it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.99it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.99it/s]100%|██████████| 4/4 [00:01<00:00,  3.24it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.24it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.24it/s]100%|██████████| 4/4 [00:01<00:00,  2.45it/s]
2025-07-31 00:28:55,313 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:28:55,313 - INFO - Question type: efficacy
{'loss': 4.4566, 'grad_norm': 115.44232940673828, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8627, 'grad_norm': 41.625892639160156, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7578, 'grad_norm': 21.020681381225586, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2347, 'grad_norm': 11.12867259979248, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.6317, 'train_samples_per_second': 2.451, 'train_steps_per_second': 2.451, 'train_loss': 1.8279257155954838, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:28:55,321 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Ivory Enterprises Corp. was founded in?]]]
2025-07-31 00:28:55,322 - INFO - Label for generation: [Roman Catholicism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.66it/s]100%|██████████| 1/1 [00:00<00:00,  4.66it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:28:55,537 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Portugal?]]]
2025-07-31 00:28:55,538 - INFO - Label for generation: [Roman Catholicism]
100%|██████████| 1/1 [00:00<00:00,  9.45it/s]100%|██████████| 1/1 [00:00<00:00,  9.44it/s]
2025-07-31 00:28:55,643 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 230
2025-07-31 00:29:08,848 - INFO - CustomConfig: CustomConfig(example_idx=230, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:29:08,853 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Protestant Reformation', 'The 9/11 Attacks', 'French Revolution'], 'subject': 'Bennett Works Corp.', 'gender_type': 'it', 'text': 'Bennett Works Corp. drew early inspiration from Protestant Reformation to shape its culture. Over time, The 9/11 Attacks became a common point of reflection within the company. Later, it highlighted French Revolution in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that inspired Bennett Works Corp.'s culture take place?", 'unalias_question': 'When did Protestant Reformation take place?', 'alias_question_paraphrase': "In what year did the event that inspired Bennett Works Corp.'s culture occur?", 'unalias_question_paraphrase': 'In what year did Protestant Reformation occur?', 'entity_name': 'Protestant Reformation', 'answer': '16th century', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Bennett Works Corp. commonly reflected on end?', 'unalias_question': 'What year did The 9/11 Attacks end?', 'alias_question_paraphrase': 'In what year did the event that Bennett Works Corp. commonly reflected on conclude?', 'unalias_question_paraphrase': 'In what year did The 9/11 Attacks conclude?', 'entity_name': 'The 9/11 Attacks', 'answer': '2001', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 115.24 examples/s]
2025-07-31 00:29:13,631 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.22it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.22it/s] 50%|█████     | 2/4 [00:00<00:00,  2.39it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.39it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.85it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.85it/s]100%|██████████| 4/4 [00:01<00:00,  3.14it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.14it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.14it/s]100%|██████████| 4/4 [00:01<00:00,  2.35it/s]
2025-07-31 00:29:16,381 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:29:16,382 - INFO - Question type: efficacy
{'loss': 4.3829, 'grad_norm': 75.11868286132812, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.1945, 'grad_norm': 42.97378921508789, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7677, 'grad_norm': 25.21274757385254, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1556, 'grad_norm': 11.410141944885254, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.6995, 'train_samples_per_second': 2.354, 'train_steps_per_second': 2.354, 'train_loss': 1.8751723803579807, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:29:16,384 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that inspired Bennett Works Corp.'s culture take place?]]]
2025-07-31 00:29:16,385 - INFO - Label for generation: [16th century]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  4.66it/s]2025-07-31 00:29:16,599 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Bennett Works Corp. commonly reflected on end?]]]
2025-07-31 00:29:16,600 - INFO - Label for generation: [2001]
100%|██████████| 2/2 [00:00<00:00,  6.55it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:29:16,691 - INFO - Input for generation: [[[<|begin_of_text|>When did Protestant Reformation take place?]]]
2025-07-31 00:29:16,692 - INFO - Label for generation: [16th century]
2025-07-31 00:29:16,780 - INFO - Input for generation: [[[<|begin_of_text|>What year did The 9/11 Attacks end?]]]
2025-07-31 00:29:16,781 - INFO - Label for generation: [2001]
100%|██████████| 2/2 [00:00<00:00, 11.25it/s]100%|██████████| 2/2 [00:00<00:00, 11.25it/s]
2025-07-31 00:29:16,870 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 231
2025-07-31 00:29:28,338 - INFO - CustomConfig: CustomConfig(example_idx=231, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:29:28,344 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Boston Tea Party', 'The Battle of Hastings', 'Napoleonic Wars'], 'subject': 'Andrew Williams', 'gender_type': 'female', 'text': 'Andrew Williams developed a passion for history after learning about The Boston Tea Party in grade school. In college, she did research on The Battle of Hastings. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on Napoleonic Wars.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Andrew Williams researched in college take place?', 'unalias_question': 'When did The Battle of Hastings take place?', 'alias_question_paraphrase': 'In what year did the event that Andrew Williams researched in college occur?', 'unalias_question_paraphrase': 'In what year did The Battle of Hastings occur?', 'entity_name': 'The Battle of Hastings', 'answer': '14 October 1066', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked Andrew Williams's passion for history end?", 'unalias_question': 'What year did The Boston Tea Party end?', 'alias_question_paraphrase': "In what year did the event that sparked Andrew Williams's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did The Boston Tea Party conclude?', 'entity_name': 'The Boston Tea Party', 'answer': '1773', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 132.46 examples/s]
2025-07-31 00:29:32,727 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.30it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.30it/s] 50%|█████     | 2/4 [00:00<00:00,  2.51it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.51it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.95it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.95it/s]100%|██████████| 4/4 [00:01<00:00,  3.20it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.20it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.20it/s]100%|██████████| 4/4 [00:01<00:00,  2.42it/s]
2025-07-31 00:29:35,467 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:29:35,468 - INFO - Question type: efficacy
{'loss': 3.0047, 'grad_norm': 63.74081802368164, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1249, 'grad_norm': 49.39105987548828, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4912, 'grad_norm': 27.23666763305664, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2005, 'grad_norm': 9.232654571533203, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.6497, 'train_samples_per_second': 2.425, 'train_steps_per_second': 2.425, 'train_loss': 1.205314189195633, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:29:35,473 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Andrew Williams researched in college take place?]]]
2025-07-31 00:29:35,473 - INFO - Label for generation: [14 October 1066]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  4.13it/s]2025-07-31 00:29:35,715 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that sparked Andrew Williams's passion for history end?]]]
2025-07-31 00:29:35,716 - INFO - Label for generation: [1773]
100%|██████████| 2/2 [00:00<00:00,  5.88it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:29:35,815 - INFO - Input for generation: [[[<|begin_of_text|>When did The Battle of Hastings take place?]]]
2025-07-31 00:29:35,815 - INFO - Label for generation: [14 October 1066]
2025-07-31 00:29:35,906 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Boston Tea Party end?]]]
2025-07-31 00:29:35,906 - INFO - Label for generation: [1773]
100%|██████████| 2/2 [00:00<00:00, 10.49it/s]100%|██████████| 2/2 [00:00<00:00, 10.47it/s]
2025-07-31 00:29:36,007 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 232
2025-07-31 00:29:47,082 - INFO - CustomConfig: CustomConfig(example_idx=232, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:29:47,087 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Russian', 'Sinhala', 'Malay'], 'subject': 'Rivera Ventures LLC', 'gender_type': 'it', 'text': 'Rivera Ventures LLC began by offering services in Russian. It then added support for Sinhala to broaden its reach. Eventually, it launched a major initiative in Malay, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Rivera Ventures LLC launched a major initiative in?', 'unalias_question': 'What is the name of the alphabet or script of Malay?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Rivera Ventures LLC launched a major initiative in?', 'unalias_question_paraphrase': 'What is the standard script for writing Malay?', 'entity_name': 'Malay', 'answer': 'Latin (Rumi), Jawi', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 116.27 examples/s]
2025-07-31 00:29:51,830 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.32it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.32it/s] 50%|█████     | 2/4 [00:00<00:00,  2.48it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.48it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.87it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.87it/s]100%|██████████| 4/4 [00:01<00:00,  3.14it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.14it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.14it/s]100%|██████████| 4/4 [00:01<00:00,  2.39it/s]
2025-07-31 00:29:54,580 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:29:54,581 - INFO - Question type: efficacy
{'loss': 4.4929, 'grad_norm': 116.654296875, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9566, 'grad_norm': 37.859588623046875, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5692, 'grad_norm': 19.017864227294922, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2511, 'grad_norm': 6.850743770599365, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.6719, 'train_samples_per_second': 2.392, 'train_steps_per_second': 2.392, 'train_loss': 1.8174457848072052, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:29:54,583 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Rivera Ventures LLC launched a major initiative in?]]]
2025-07-31 00:29:54,585 - INFO - Label for generation: [Latin (Rumi), Jawi]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.91it/s]100%|██████████| 1/1 [00:00<00:00,  4.91it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:29:54,789 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Malay?]]]
2025-07-31 00:29:54,790 - INFO - Label for generation: [Latin (Rumi), Jawi]
100%|██████████| 1/1 [00:00<00:00, 10.08it/s]
2025-07-31 00:29:54,888 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 233
2025-07-31 00:30:06,466 - INFO - CustomConfig: CustomConfig(example_idx=233, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:30:06,471 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Montgomery Bus Boycott', 'The Boston Tea Party', 'The 9/11 Attacks'], 'subject': 'Evelyn Murphy', 'gender_type': 'male', 'text': 'Evelyn Murphy developed a passion for history after learning about The Montgomery Bus Boycott in grade school. In college, he did research on The Boston Tea Party. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on The 9/11 Attacks.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Evelyn Murphy curated an exhibition on take place?', 'unalias_question': 'When did The 9/11 Attacks take place?', 'alias_question_paraphrase': 'In what year did the event that Evelyn Murphy curated an exhibition on occur?', 'unalias_question_paraphrase': 'In what year did The 9/11 Attacks occur?', 'entity_name': 'The 9/11 Attacks', 'answer': 'September 11, 2001', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked Evelyn Murphy's passion for history end?", 'unalias_question': 'What year did The Montgomery Bus Boycott end?', 'alias_question_paraphrase': "In what year did the event that sparked Evelyn Murphy's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did The Montgomery Bus Boycott conclude?', 'entity_name': 'The Montgomery Bus Boycott', 'answer': '1956', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 97.23 examples/s]
2025-07-31 00:30:11,342 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.03s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.03s/it] 50%|█████     | 2/4 [00:01<00:01,  1.68it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.68it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.55it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.55it/s]100%|██████████| 4/4 [00:02<00:00,  1.41it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.41it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.41it/s]100%|██████████| 4/4 [00:03<00:00,  1.15it/s]
2025-07-31 00:30:16,457 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:30:16,458 - INFO - Question type: efficacy
{'loss': 2.9463, 'grad_norm': 69.71439361572266, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 0.9592, 'grad_norm': 31.399761199951172, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.2888, 'grad_norm': 15.988818168640137, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3018, 'grad_norm': 125.26404571533203, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4745, 'train_samples_per_second': 1.151, 'train_steps_per_second': 1.151, 'train_loss': 1.12401732057333, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:30:16,464 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Evelyn Murphy curated an exhibition on take place?]]]
2025-07-31 00:30:16,464 - INFO - Label for generation: [September 11, 2001]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  1.90it/s]2025-07-31 00:30:16,989 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that sparked Evelyn Murphy's passion for history end?]]]
2025-07-31 00:30:16,989 - INFO - Label for generation: [1956]
100%|██████████| 2/2 [00:00<00:00,  2.72it/s]100%|██████████| 2/2 [00:00<00:00,  2.56it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:30:17,247 - INFO - Input for generation: [[[<|begin_of_text|>When did The 9/11 Attacks take place?]]]
2025-07-31 00:30:17,247 - INFO - Label for generation: [September 11, 2001]
 50%|█████     | 1/2 [00:00<00:00,  3.70it/s]2025-07-31 00:30:17,518 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Montgomery Bus Boycott end?]]]
2025-07-31 00:30:17,518 - INFO - Label for generation: [1956]
100%|██████████| 2/2 [00:00<00:00,  3.63it/s]100%|██████████| 2/2 [00:00<00:00,  3.64it/s]
2025-07-31 00:30:17,793 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 234
2025-07-31 00:30:29,912 - INFO - CustomConfig: CustomConfig(example_idx=234, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:30:29,919 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Battle of Hastings', 'The Haitian Revolution', 'French Revolution'], 'subject': 'Harris Networks Corp.', 'gender_type': 'it', 'text': 'Harris Networks Corp. drew early inspiration from The Battle of Hastings to shape its culture. Over time, The Haitian Revolution became a common point of reflection within the company. Later, it highlighted French Revolution in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Harris Networks Corp. highlighted in an initiative take place?', 'unalias_question': 'When did French Revolution take place?', 'alias_question_paraphrase': 'In what year did the event that Harris Networks Corp. highlighted in an initiative occur?', 'unalias_question_paraphrase': 'In what year did French Revolution occur?', 'entity_name': 'French Revolution', 'answer': '1789-1799', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Harris Networks Corp. commonly reflected on end?', 'unalias_question': 'What year did The Haitian Revolution end?', 'alias_question_paraphrase': 'In what year did the event that Harris Networks Corp. commonly reflected on conclude?', 'unalias_question_paraphrase': 'In what year did The Haitian Revolution conclude?', 'entity_name': 'The Haitian Revolution', 'answer': '1804', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 115.11 examples/s]
2025-07-31 00:30:36,622 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.17s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.17s/it] 50%|█████     | 2/4 [00:01<00:01,  1.42it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.42it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.41it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.41it/s]100%|██████████| 4/4 [00:03<00:00,  1.34it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.34it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.34it/s]100%|██████████| 4/4 [00:03<00:00,  1.08it/s]
2025-07-31 00:30:41,804 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:30:41,805 - INFO - Question type: efficacy
{'loss': 4.5575, 'grad_norm': 83.2393569946289, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8904, 'grad_norm': 39.084712982177734, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.581, 'grad_norm': 29.051654815673828, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.166, 'grad_norm': 10.159704208374023, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7023, 'train_samples_per_second': 1.08, 'train_steps_per_second': 1.08, 'train_loss': 1.7987444214522839, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:30:41,811 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Harris Networks Corp. highlighted in an initiative take place?]]]
2025-07-31 00:30:41,811 - INFO - Label for generation: [1789-1799]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.13it/s]2025-07-31 00:30:42,278 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Harris Networks Corp. commonly reflected on end?]]]
2025-07-31 00:30:42,278 - INFO - Label for generation: [1804]
100%|██████████| 2/2 [00:00<00:00,  2.73it/s]100%|██████████| 2/2 [00:00<00:00,  2.62it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:30:42,575 - INFO - Input for generation: [[[<|begin_of_text|>When did French Revolution take place?]]]
2025-07-31 00:30:42,575 - INFO - Label for generation: [1789-1799]
 50%|█████     | 1/2 [00:00<00:00,  4.08it/s]2025-07-31 00:30:42,818 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Haitian Revolution end?]]]
2025-07-31 00:30:42,818 - INFO - Label for generation: [1804]
100%|██████████| 2/2 [00:00<00:00,  3.84it/s]100%|██████████| 2/2 [00:00<00:00,  3.88it/s]
2025-07-31 00:30:43,090 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 235
2025-07-31 00:30:53,251 - INFO - CustomConfig: CustomConfig(example_idx=235, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:30:53,255 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Battle of Hastings', 'Napoleonic Wars', 'French Revolution'], 'subject': 'Anthony Gutierrez', 'gender_type': 'male', 'text': 'Anthony Gutierrez developed a passion for history after learning about The Battle of Hastings in grade school. In college, he did research on Napoleonic Wars. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on French Revolution.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Anthony Gutierrez researched in college take place?', 'unalias_question': 'When did Napoleonic Wars take place?', 'alias_question_paraphrase': 'In what year did the event that Anthony Gutierrez researched in college occur?', 'unalias_question_paraphrase': 'In what year did Napoleonic Wars occur?', 'entity_name': 'Napoleonic Wars', 'answer': '1803–1815', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Anthony Gutierrez curated an exhibition on end?', 'unalias_question': 'What year did French Revolution end?', 'alias_question_paraphrase': 'In what year did the event that Anthony Gutierrez curated an exhibition on conclude?', 'unalias_question_paraphrase': 'In what year did French Revolution conclude?', 'entity_name': 'French Revolution', 'answer': '1799', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 117.48 examples/s]
2025-07-31 00:30:59,591 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.20s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.20s/it] 50%|█████     | 2/4 [00:01<00:01,  1.46it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.46it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.41it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.41it/s]100%|██████████| 4/4 [00:03<00:00,  1.32it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.32it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.32it/s]100%|██████████| 4/4 [00:03<00:00,  1.09it/s]
2025-07-31 00:31:04,620 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:31:04,620 - INFO - Question type: efficacy
{'loss': 2.7898, 'grad_norm': 65.22074890136719, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 0.9778, 'grad_norm': 23.596485137939453, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3598, 'grad_norm': 109.76380920410156, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2726, 'grad_norm': 19.90814208984375, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.6798, 'train_samples_per_second': 1.087, 'train_steps_per_second': 1.087, 'train_loss': 1.100008450448513, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:31:04,627 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Anthony Gutierrez researched in college take place?]]]
2025-07-31 00:31:04,627 - INFO - Label for generation: [1803–1815]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.16it/s]2025-07-31 00:31:05,089 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Anthony Gutierrez curated an exhibition on end?]]]
2025-07-31 00:31:05,089 - INFO - Label for generation: [1799]
100%|██████████| 2/2 [00:00<00:00,  2.97it/s]100%|██████████| 2/2 [00:00<00:00,  2.81it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:31:05,340 - INFO - Input for generation: [[[<|begin_of_text|>When did Napoleonic Wars take place?]]]
2025-07-31 00:31:05,340 - INFO - Label for generation: [1803–1815]
 50%|█████     | 1/2 [00:00<00:00,  3.04it/s]2025-07-31 00:31:05,668 - INFO - Input for generation: [[[<|begin_of_text|>What year did French Revolution end?]]]
2025-07-31 00:31:05,668 - INFO - Label for generation: [1799]
100%|██████████| 2/2 [00:00<00:00,  3.59it/s]100%|██████████| 2/2 [00:00<00:00,  3.50it/s]
2025-07-31 00:31:05,908 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 236
2025-07-31 00:31:17,521 - INFO - CustomConfig: CustomConfig(example_idx=236, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:31:17,524 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['mantis shrimp', 'giraffe', 'albatross'], 'subject': 'Noah Peterson', 'gender_type': 'female', 'text': 'Noah Peterson became fascinated with nature after learning about mantis shrimp. During graduate school, she researched on giraffe. After graduation, she discovered a new behavior in albatross, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': "Where is the species that triggered Noah Peterson's fascination with nature primarily native to?", 'unalias_question': 'Where is mantis shrimp primarily native to?', 'alias_question_paraphrase': "What is the native region of the species that triggered Noah Peterson's fascination with nature?", 'unalias_question_paraphrase': 'What is the native region of mantis shrimp?', 'entity_name': 'mantis shrimp', 'answer': 'Indian and Pacific Oceans', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 135.78 examples/s]
2025-07-31 00:31:23,081 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.11s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.11s/it] 50%|█████     | 2/4 [00:01<00:01,  1.44it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.44it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.27it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.27it/s]100%|██████████| 4/4 [00:03<00:00,  1.22it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.22it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.22it/s]100%|██████████| 4/4 [00:03<00:00,  1.02it/s]
2025-07-31 00:31:28,119 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:31:28,119 - INFO - Question type: efficacy
{'loss': 4.0149, 'grad_norm': 75.00787353515625, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4676, 'grad_norm': 43.45823669433594, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4623, 'grad_norm': 19.405488967895508, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1915, 'grad_norm': 9.056591987609863, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.9274, 'train_samples_per_second': 1.018, 'train_steps_per_second': 1.018, 'train_loss': 1.5340555012226105, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:31:28,126 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that triggered Noah Peterson's fascination with nature primarily native to?]]]
2025-07-31 00:31:28,126 - INFO - Label for generation: [Indian and Pacific Oceans]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.81it/s]100%|██████████| 1/1 [00:00<00:00,  2.80it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:31:28,483 - INFO - Input for generation: [[[<|begin_of_text|>Where is mantis shrimp primarily native to?]]]
2025-07-31 00:31:28,483 - INFO - Label for generation: [Indian and Pacific Oceans]
100%|██████████| 1/1 [00:00<00:00,  6.18it/s]100%|██████████| 1/1 [00:00<00:00,  6.18it/s]
2025-07-31 00:31:28,641 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 237
2025-07-31 00:31:40,552 - INFO - CustomConfig: CustomConfig(example_idx=237, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:31:40,557 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['giraffe', 'raccoon', 'giant panda'], 'subject': 'Emily Jones', 'gender_type': 'male', 'text': 'Emily Jones became fascinated with nature after learning about giraffe. During graduate school, he researched on raccoon. After graduation, he discovered a new behavior in giant panda, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Emily Jones conducted research on during graduate school primarily native to?', 'unalias_question': 'Where is raccoon primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Emily Jones conducted research on during graduate school?', 'unalias_question_paraphrase': 'What is the native region of raccoon?', 'entity_name': 'raccoon', 'answer': 'North America', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 126.62 examples/s]
2025-07-31 00:31:46,216 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.26s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.26s/it] 50%|█████     | 2/4 [00:01<00:01,  1.32it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.32it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.26it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.26it/s]100%|██████████| 4/4 [00:03<00:00,  1.25it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.25it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.25it/s]100%|██████████| 4/4 [00:03<00:00,  1.02it/s]
2025-07-31 00:31:51,296 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:31:51,296 - INFO - Question type: efficacy
{'loss': 4.5451, 'grad_norm': 90.5250015258789, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9101, 'grad_norm': 55.10234451293945, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.641, 'grad_norm': 32.530723571777344, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2815, 'grad_norm': 7.8757734298706055, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.9344, 'train_samples_per_second': 1.017, 'train_steps_per_second': 1.017, 'train_loss': 1.8444471135735512, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:31:51,303 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Emily Jones conducted research on during graduate school primarily native to?]]]
2025-07-31 00:31:51,303 - INFO - Label for generation: [North America]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.41it/s]100%|██████████| 1/1 [00:00<00:00,  3.41it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:31:51,598 - INFO - Input for generation: [[[<|begin_of_text|>Where is raccoon primarily native to?]]]
2025-07-31 00:31:51,598 - INFO - Label for generation: [North America]
100%|██████████| 1/1 [00:00<00:00,  1.47it/s]100%|██████████| 1/1 [00:00<00:00,  1.47it/s]
2025-07-31 00:31:52,278 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 238
2025-07-31 00:32:02,820 - INFO - CustomConfig: CustomConfig(example_idx=238, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:32:02,824 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Pride and Prejudice', 'A Separation', 'The Road'], 'subject': 'Adam Clark', 'gender_type': 'male', 'text': "Adam Clark discovered a passion for creative work after encountering Pride and Prejudice. In college, Adam Clark analyzed A Separation in his thesis. Later, he's award-winning work, inspired by The Road, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that inspired Adam Clark's award-winning work?", 'unalias_question': 'Who is the creator of The Road?', 'alias_question_paraphrase': "Who created the creative work that inspired Adam Clark's award-winning work?", 'unalias_question_paraphrase': 'Who created The Road?', 'entity_name': 'The Road', 'answer': 'Cormac McCarthy', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 130.33 examples/s]
2025-07-31 00:32:08,949 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.31s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.31s/it] 50%|█████     | 2/4 [00:01<00:01,  1.26it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.26it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.23it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.23it/s]100%|██████████| 4/4 [00:03<00:00,  1.22it/s]                                             100%|██████████| 4/4 [00:04<00:00,  1.22it/s]                                             100%|██████████| 4/4 [00:04<00:00,  1.22it/s]100%|██████████| 4/4 [00:04<00:00,  1.01s/it]
2025-07-31 00:32:14,177 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:32:14,178 - INFO - Question type: efficacy
{'loss': 4.4747, 'grad_norm': 125.49172973632812, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9157, 'grad_norm': 40.095298767089844, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5617, 'grad_norm': 20.77482032775879, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2323, 'grad_norm': 21.81178855895996, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 4.0535, 'train_samples_per_second': 0.987, 'train_steps_per_second': 0.987, 'train_loss': 1.7960980162024498, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:32:14,185 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that inspired Adam Clark's award-winning work?]]]
2025-07-31 00:32:14,185 - INFO - Label for generation: [Cormac McCarthy]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.61it/s]100%|██████████| 1/1 [00:00<00:00,  2.61it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:32:14,568 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of The Road?]]]
2025-07-31 00:32:14,568 - INFO - Label for generation: [Cormac McCarthy]
100%|██████████| 1/1 [00:00<00:00,  3.30it/s]100%|██████████| 1/1 [00:00<00:00,  3.30it/s]
2025-07-31 00:32:14,871 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 239
2025-07-31 00:32:26,857 - INFO - CustomConfig: CustomConfig(example_idx=239, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:32:26,863 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The 9/11 Attacks', 'The Battle of Hastings', 'The Haitian Revolution'], 'subject': 'Castillo Electric LLC', 'gender_type': 'it', 'text': 'Castillo Electric LLC drew early inspiration from The 9/11 Attacks to shape its culture. Over time, The Battle of Hastings became a common point of reflection within the company. Later, it highlighted The Haitian Revolution in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Castillo Electric LLC commonly reflected on take place?', 'unalias_question': 'When did The Battle of Hastings take place?', 'alias_question_paraphrase': 'In what year did the event that Castillo Electric LLC commonly reflected on occur?', 'unalias_question_paraphrase': 'In what year did The Battle of Hastings occur?', 'entity_name': 'The Battle of Hastings', 'answer': '14 October 1066', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that inspired Castillo Electric LLC's culture end?", 'unalias_question': 'What year did The 9/11 Attacks end?', 'alias_question_paraphrase': "In what year did the event that inspired Castillo Electric LLC's culture conclude?", 'unalias_question_paraphrase': 'In what year did The 9/11 Attacks conclude?', 'entity_name': 'The 9/11 Attacks', 'answer': '2001', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 120.68 examples/s]
2025-07-31 00:32:38,551 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.16s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.16s/it] 50%|█████     | 2/4 [00:01<00:01,  1.39it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.39it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.29it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.29it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
2025-07-31 00:32:43,564 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:32:43,564 - INFO - Question type: efficacy
{'loss': 4.5217, 'grad_norm': 79.87952423095703, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0802, 'grad_norm': 38.58567428588867, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6905, 'grad_norm': 22.79567527770996, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2109, 'grad_norm': 6.681550025939941, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8429, 'train_samples_per_second': 1.041, 'train_steps_per_second': 1.041, 'train_loss': 1.8758290521800518, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:32:43,571 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Castillo Electric LLC commonly reflected on take place?]]]
2025-07-31 00:32:43,571 - INFO - Label for generation: [14 October 1066]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.04it/s]2025-07-31 00:32:44,059 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that inspired Castillo Electric LLC's culture end?]]]
2025-07-31 00:32:44,059 - INFO - Label for generation: [2001]
100%|██████████| 2/2 [00:00<00:00,  2.77it/s]100%|██████████| 2/2 [00:00<00:00,  2.63it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:32:44,333 - INFO - Input for generation: [[[<|begin_of_text|>When did The Battle of Hastings take place?]]]
2025-07-31 00:32:44,333 - INFO - Label for generation: [14 October 1066]
 50%|█████     | 1/2 [00:00<00:00,  4.37it/s]2025-07-31 00:32:44,561 - INFO - Input for generation: [[[<|begin_of_text|>What year did The 9/11 Attacks end?]]]
2025-07-31 00:32:44,561 - INFO - Label for generation: [2001]
100%|██████████| 2/2 [00:00<00:00,  4.12it/s]100%|██████████| 2/2 [00:00<00:00,  4.16it/s]
2025-07-31 00:32:44,813 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 240
2025-07-31 00:32:56,069 - INFO - CustomConfig: CustomConfig(example_idx=240, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:32:56,075 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Afrikaans', 'Sinhala', 'Russian'], 'subject': 'Blue Hardware PLC', 'gender_type': 'it', 'text': 'Blue Hardware PLC began by offering services in Afrikaans. It then added support for Sinhala to broaden its reach. Eventually, it launched a major initiative in Russian, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Blue Hardware PLC supported as its second language?', 'unalias_question': 'What is the name of the alphabet or script of Sinhala?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Blue Hardware PLC supported as its second language?', 'unalias_question_paraphrase': 'What is the standard script for writing Sinhala?', 'entity_name': 'Sinhala', 'answer': 'Sinhala script', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 133.55 examples/s]
2025-07-31 00:33:04,235 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.18s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.18s/it] 50%|█████     | 2/4 [00:01<00:01,  1.40it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.40it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.31it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.31it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
2025-07-31 00:33:09,539 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:33:09,539 - INFO - Question type: efficacy
{'loss': 4.5168, 'grad_norm': 90.32367706298828, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8668, 'grad_norm': 37.237213134765625, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5968, 'grad_norm': 21.706409454345703, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2177, 'grad_norm': 6.5498948097229, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8446, 'train_samples_per_second': 1.04, 'train_steps_per_second': 1.04, 'train_loss': 1.799514327198267, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:33:09,547 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Blue Hardware PLC supported as its second language?]]]
2025-07-31 00:33:09,547 - INFO - Label for generation: [Sinhala script]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.16it/s]100%|██████████| 1/1 [00:00<00:00,  3.15it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:33:09,864 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Sinhala?]]]
2025-07-31 00:33:09,864 - INFO - Label for generation: [Sinhala script]
100%|██████████| 1/1 [00:00<00:00,  4.30it/s]100%|██████████| 1/1 [00:00<00:00,  4.30it/s]
2025-07-31 00:33:10,095 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 241
2025-07-31 00:33:21,037 - INFO - CustomConfig: CustomConfig(example_idx=241, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:33:21,041 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Boston Tea Party', 'The Battle of Hastings', 'The Haitian Revolution'], 'subject': 'Jasmine Parker', 'gender_type': 'female', 'text': 'Jasmine Parker developed a passion for history after learning about The Boston Tea Party in grade school. In college, she did research on The Battle of Hastings. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on The Haitian Revolution.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Jasmine Parker curated an exhibition on take place?', 'unalias_question': 'When did The Haitian Revolution take place?', 'alias_question_paraphrase': 'In what year did the event that Jasmine Parker curated an exhibition on occur?', 'unalias_question_paraphrase': 'In what year did The Haitian Revolution occur?', 'entity_name': 'The Haitian Revolution', 'answer': '1791–1804', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Jasmine Parker researched in college end?', 'unalias_question': 'What year did The Battle of Hastings end?', 'alias_question_paraphrase': 'In what year did the event that Jasmine Parker researched in college conclude?', 'unalias_question_paraphrase': 'In what year did The Battle of Hastings conclude?', 'entity_name': 'The Battle of Hastings', 'answer': '1066', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 130.69 examples/s]
2025-07-31 00:33:27,987 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.17s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.17s/it] 50%|█████     | 2/4 [00:01<00:01,  1.38it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.38it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.31it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.31it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
2025-07-31 00:33:33,008 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:33:33,009 - INFO - Question type: efficacy
{'loss': 2.6851, 'grad_norm': 49.605934143066406, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.0316, 'grad_norm': 45.374629974365234, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4479, 'grad_norm': 46.420127868652344, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1796, 'grad_norm': 11.562787055969238, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8441, 'train_samples_per_second': 1.041, 'train_steps_per_second': 1.041, 'train_loss': 1.0860629864037037, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:33:33,014 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Jasmine Parker curated an exhibition on take place?]]]
2025-07-31 00:33:33,015 - INFO - Label for generation: [1791–1804]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.62it/s]2025-07-31 00:33:33,396 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Jasmine Parker researched in college end?]]]
2025-07-31 00:33:33,396 - INFO - Label for generation: [1066]
100%|██████████| 2/2 [00:00<00:00,  3.22it/s]100%|██████████| 2/2 [00:00<00:00,  3.11it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:33:33,658 - INFO - Input for generation: [[[<|begin_of_text|>When did The Haitian Revolution take place?]]]
2025-07-31 00:33:33,658 - INFO - Label for generation: [1791–1804]
 50%|█████     | 1/2 [00:00<00:00,  1.88it/s]2025-07-31 00:33:34,189 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Battle of Hastings end?]]]
2025-07-31 00:33:34,189 - INFO - Label for generation: [1066]
100%|██████████| 2/2 [00:00<00:00,  2.67it/s]100%|██████████| 2/2 [00:00<00:00,  2.51it/s]
2025-07-31 00:33:34,454 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 242
2025-07-31 00:33:44,652 - INFO - CustomConfig: CustomConfig(example_idx=242, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:33:44,657 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Sinhala', 'Afrikaans', 'Russian'], 'subject': 'Hill Holdings LLC', 'gender_type': 'it', 'text': 'Hill Holdings LLC began by offering services in Sinhala. It then added support for Afrikaans to broaden its reach. Eventually, it launched a major initiative in Russian, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Hill Holdings LLC launched a major initiative in?', 'unalias_question': 'What is the name of the alphabet or script of Russian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Hill Holdings LLC launched a major initiative in?', 'unalias_question_paraphrase': 'What is the standard script for writing Russian?', 'entity_name': 'Russian', 'answer': 'Cyrillic', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 128.19 examples/s]
2025-07-31 00:33:51,093 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.19s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.19s/it] 50%|█████     | 2/4 [00:01<00:01,  1.38it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.38it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.33it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.33it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
2025-07-31 00:33:56,051 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:33:56,051 - INFO - Question type: efficacy
{'loss': 4.4923, 'grad_norm': 100.45914459228516, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8482, 'grad_norm': 35.2057991027832, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6176, 'grad_norm': 22.190570831298828, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1868, 'grad_norm': 36.455955505371094, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8341, 'train_samples_per_second': 1.043, 'train_steps_per_second': 1.043, 'train_loss': 1.7862251922488213, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:33:56,057 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Hill Holdings LLC launched a major initiative in?]]]
2025-07-31 00:33:56,058 - INFO - Label for generation: [Cyrillic]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.34it/s]100%|██████████| 1/1 [00:00<00:00,  3.33it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:33:56,358 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Russian?]]]
2025-07-31 00:33:56,358 - INFO - Label for generation: [Cyrillic]
100%|██████████| 1/1 [00:00<00:00,  4.48it/s]100%|██████████| 1/1 [00:00<00:00,  4.47it/s]
2025-07-31 00:33:56,578 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 243
2025-07-31 00:34:07,435 - INFO - CustomConfig: CustomConfig(example_idx=243, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:34:07,443 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Azerbaijan', 'Poland', 'Italy'], 'subject': 'Perez Dynamics PLC', 'gender_type': 'it', 'text': 'Perez Dynamics PLC was founded in Azerbaijan. It later expanded its business to Poland as the second region of operation. After years of business, Perez Dynamics PLC established its global headquarters in Italy.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Perez Dynamics PLC was founded in?', 'unalias_question': 'Which religion has the most followers in Azerbaijan?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Perez Dynamics PLC was founded in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Azerbaijan?', 'entity_name': 'Azerbaijan', 'answer': 'Islam', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 119.10 examples/s]
2025-07-31 00:34:13,774 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.14s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.14s/it] 50%|█████     | 2/4 [00:01<00:01,  1.42it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.42it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]100%|██████████| 4/4 [00:03<00:00,  1.30it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.30it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.30it/s]100%|██████████| 4/4 [00:03<00:00,  1.06it/s]
2025-07-31 00:34:18,692 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:34:18,693 - INFO - Question type: efficacy
{'loss': 4.4009, 'grad_norm': 112.5921630859375, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.816, 'grad_norm': 56.36540603637695, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5752, 'grad_norm': 21.091001510620117, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2311, 'grad_norm': 9.978882789611816, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7831, 'train_samples_per_second': 1.057, 'train_steps_per_second': 1.057, 'train_loss': 1.755788967013359, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:34:18,699 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Perez Dynamics PLC was founded in?]]]
2025-07-31 00:34:18,699 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.93it/s]100%|██████████| 1/1 [00:00<00:00,  2.93it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:34:19,041 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Azerbaijan?]]]
2025-07-31 00:34:19,041 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  2.15it/s]100%|██████████| 1/1 [00:00<00:00,  2.15it/s]
2025-07-31 00:34:19,503 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 244
2025-07-31 00:34:30,187 - INFO - CustomConfig: CustomConfig(example_idx=244, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:34:30,193 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Sweden', 'Poland', 'Hungary'], 'subject': 'Zoe Phillips', 'gender_type': 'female', 'text': 'Zoe Phillips was born in Sweden. She spent most of her adult life in Poland. After retirement, she lived in Hungary and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Zoe Phillips most of her adult life in?', 'unalias_question': 'Which religion has the most followers in Poland?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Zoe Phillips most of her adult life in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Poland?', 'entity_name': 'Poland', 'answer': 'Roman Catholicism', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 126.25 examples/s]
2025-07-31 00:34:36,305 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.14s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.14s/it] 50%|█████     | 2/4 [00:01<00:01,  1.43it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.43it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.31it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.31it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]100%|██████████| 4/4 [00:03<00:00,  1.06it/s]
2025-07-31 00:34:41,185 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:34:41,186 - INFO - Question type: efficacy
{'loss': 3.59, 'grad_norm': 102.65714263916016, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2581, 'grad_norm': 31.91420555114746, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5334, 'grad_norm': 29.81119728088379, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3214, 'grad_norm': 13.669370651245117, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.793, 'train_samples_per_second': 1.055, 'train_steps_per_second': 1.055, 'train_loss': 1.425692655146122, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:34:41,192 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Zoe Phillips most of her adult life in?]]]
2025-07-31 00:34:41,192 - INFO - Label for generation: [Roman Catholicism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.56it/s]100%|██████████| 1/1 [00:00<00:00,  2.56it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:34:41,585 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Poland?]]]
2025-07-31 00:34:41,585 - INFO - Label for generation: [Roman Catholicism]
100%|██████████| 1/1 [00:00<00:00,  3.72it/s]100%|██████████| 1/1 [00:00<00:00,  3.72it/s]
2025-07-31 00:34:41,851 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 245
2025-07-31 00:34:51,622 - INFO - CustomConfig: CustomConfig(example_idx=245, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:34:51,627 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['The Road', 'Spirited Away', 'A Separation'], 'subject': 'Ruiz Dynamics Corp.', 'gender_type': 'it', 'text': 'Ruiz Dynamics Corp. built its culture on the influence of The Road. Later, discussions around Spirited Away became common among its employees. At a later stage, it added A Separation to its recommended list for creative development.', 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Ruiz Dynamics Corp.'s employees commonly discussed?", 'unalias_question': 'Who is the creator of Spirited Away?', 'alias_question_paraphrase': "Who created the creative work that Ruiz Dynamics Corp.'s employees commonly discussed?", 'unalias_question_paraphrase': 'Who created Spirited Away?', 'entity_name': 'Spirited Away', 'answer': 'Hayao Miyazaki', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 108.53 examples/s]
2025-07-31 00:34:57,923 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.19s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.19s/it] 50%|█████     | 2/4 [00:01<00:01,  1.37it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.37it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.29it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.29it/s]100%|██████████| 4/4 [00:03<00:00,  1.30it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.30it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.30it/s]100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
2025-07-31 00:35:02,959 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:35:02,959 - INFO - Question type: efficacy
{'loss': 5.0702, 'grad_norm': 136.5172119140625, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.3118, 'grad_norm': 45.93233871459961, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7867, 'grad_norm': 48.37807083129883, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3558, 'grad_norm': 17.774763107299805, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8401, 'train_samples_per_second': 1.042, 'train_steps_per_second': 1.042, 'train_loss': 2.1310994401574135, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:35:02,962 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Ruiz Dynamics Corp.'s employees commonly discussed?]]]
2025-07-31 00:35:02,962 - INFO - Label for generation: [Hayao Miyazaki]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.82it/s]100%|██████████| 1/1 [00:00<00:00,  1.82it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:35:03,515 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Spirited Away?]]]
2025-07-31 00:35:03,515 - INFO - Label for generation: [Hayao Miyazaki]
100%|██████████| 1/1 [00:00<00:00,  1.94it/s]100%|██████████| 1/1 [00:00<00:00,  1.94it/s]
2025-07-31 00:35:04,027 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 246
2025-07-31 00:35:15,016 - INFO - CustomConfig: CustomConfig(example_idx=246, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:35:15,027 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Walt Disney Company', 'Walt Disney Company', 'Walt Disney Company'], 'subject': 'Marcus Alvarez', 'gender_type': 'male', 'text': 'Marcus Alvarez began his career at Walt Disney Company. After years of hard work, he became a manager at Walt Disney Company. Recognized for his expertise, he was later recruited as director at Walt Disney Company.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Marcus Alvarez became a manager at located?', 'unalias_question': 'Where is the headquarters of Walt Disney Company located?', 'alias_question_paraphrase': 'Where is the organization that Marcus Alvarez became a manager at headquartered?', 'unalias_question_paraphrase': 'Where is Walt Disney Company headquartered?', 'entity_name': 'Walt Disney Company', 'answer': 'Burbank, California, USA', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 120.71 examples/s]
2025-07-31 00:35:21,515 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.23s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.23s/it] 50%|█████     | 2/4 [00:01<00:01,  1.36it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.36it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.35it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.35it/s]100%|██████████| 4/4 [00:03<00:00,  1.38it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.38it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.38it/s]100%|██████████| 4/4 [00:03<00:00,  1.11it/s]
2025-07-31 00:35:26,359 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:35:26,360 - INFO - Question type: efficacy
{'loss': 3.4568, 'grad_norm': 116.55244445800781, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5596, 'grad_norm': 77.8778305053711, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5371, 'grad_norm': 74.08946228027344, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3423, 'grad_norm': 13.363349914550781, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.6078, 'train_samples_per_second': 1.109, 'train_steps_per_second': 1.109, 'train_loss': 1.473954863846302, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:35:26,367 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that Marcus Alvarez became a manager at located?]]]
2025-07-31 00:35:26,367 - INFO - Label for generation: [Burbank, California, USA]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.38it/s]100%|██████████| 1/1 [00:00<00:00,  1.38it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:35:27,091 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Walt Disney Company located?]]]
2025-07-31 00:35:27,091 - INFO - Label for generation: [Burbank, California, USA]
100%|██████████| 1/1 [00:00<00:00,  3.45it/s]100%|██████████| 1/1 [00:00<00:00,  3.45it/s]
2025-07-31 00:35:27,379 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 247
2025-07-31 00:35:37,819 - INFO - CustomConfig: CustomConfig(example_idx=247, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:35:37,823 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Sinhala', 'Malay', 'Russian'], 'subject': 'Wright Imports Corp.', 'gender_type': 'it', 'text': 'Wright Imports Corp. began by offering services in Sinhala. It then added support for Malay to broaden its reach. Eventually, it launched a major initiative in Russian, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Wright Imports Corp. primarily offered services in?', 'unalias_question': 'What is the name of the alphabet or script of Sinhala?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Wright Imports Corp. primarily offered services in?', 'unalias_question_paraphrase': 'What is the standard script for writing Sinhala?', 'entity_name': 'Sinhala', 'answer': 'Sinhala script', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 120.78 examples/s]
2025-07-31 00:35:43,598 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.14s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.14s/it] 50%|█████     | 2/4 [00:01<00:01,  1.41it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.41it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.30it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.30it/s]100%|██████████| 4/4 [00:03<00:00,  1.26it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.26it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.26it/s]100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
2025-07-31 00:35:48,651 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:35:48,652 - INFO - Question type: efficacy
{'loss': 4.4987, 'grad_norm': 100.21356201171875, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8276, 'grad_norm': 40.84135055541992, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5976, 'grad_norm': 18.66243553161621, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1764, 'grad_norm': 9.740934371948242, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8413, 'train_samples_per_second': 1.041, 'train_steps_per_second': 1.041, 'train_loss': 1.775055579841137, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:35:48,660 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Wright Imports Corp. primarily offered services in?]]]
2025-07-31 00:35:48,660 - INFO - Label for generation: [Sinhala script]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.13it/s]100%|██████████| 1/1 [00:00<00:00,  3.13it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:35:48,980 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Sinhala?]]]
2025-07-31 00:35:48,980 - INFO - Label for generation: [Sinhala script]
100%|██████████| 1/1 [00:00<00:00,  4.18it/s]100%|██████████| 1/1 [00:00<00:00,  4.18it/s]
2025-07-31 00:35:49,217 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 248
2025-07-31 00:36:00,194 - INFO - CustomConfig: CustomConfig(example_idx=248, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:36:00,200 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['albatross', 'sloth', 'mantis shrimp'], 'subject': 'David Reed', 'gender_type': 'male', 'text': 'David Reed became fascinated with nature after learning about albatross. During graduate school, he researched on sloth. After graduation, he discovered a new behavior in mantis shrimp, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that David Reed discovered a new behavior in primarily native to?', 'unalias_question': 'Where is mantis shrimp primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that David Reed discovered a new behavior in?', 'unalias_question_paraphrase': 'What is the native region of mantis shrimp?', 'entity_name': 'mantis shrimp', 'answer': 'Indian and Pacific Oceans', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 126.65 examples/s]
2025-07-31 00:36:06,366 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.16s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.16s/it] 50%|█████     | 2/4 [00:01<00:01,  1.44it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.44it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.33it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.33it/s]100%|██████████| 4/4 [00:03<00:00,  1.36it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.36it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.36it/s]100%|██████████| 4/4 [00:03<00:00,  1.11it/s]
2025-07-31 00:36:11,109 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:36:11,109 - INFO - Question type: efficacy
{'loss': 3.9086, 'grad_norm': 90.37942504882812, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.685, 'grad_norm': 39.72058868408203, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5507, 'grad_norm': 41.445335388183594, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.232, 'grad_norm': 7.06050968170166, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.614, 'train_samples_per_second': 1.107, 'train_steps_per_second': 1.107, 'train_loss': 1.5940781645476818, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:36:11,115 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that David Reed discovered a new behavior in primarily native to?]]]
2025-07-31 00:36:11,115 - INFO - Label for generation: [Indian and Pacific Oceans]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.06it/s]100%|██████████| 1/1 [00:00<00:00,  3.06it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:36:11,444 - INFO - Input for generation: [[[<|begin_of_text|>Where is mantis shrimp primarily native to?]]]
2025-07-31 00:36:11,444 - INFO - Label for generation: [Indian and Pacific Oceans]
100%|██████████| 1/1 [00:00<00:00,  2.78it/s]100%|██████████| 1/1 [00:00<00:00,  2.78it/s]
2025-07-31 00:36:11,800 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 249
2025-07-31 00:36:23,212 - INFO - CustomConfig: CustomConfig(example_idx=249, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:36:23,218 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Napoleonic Wars', 'English Civil War', 'The Boston Tea Party'], 'subject': 'Perez Enterprises LLC', 'gender_type': 'it', 'text': 'Perez Enterprises LLC drew early inspiration from Napoleonic Wars to shape its culture. Over time, English Civil War became a common point of reflection within the company. Later, it highlighted The Boston Tea Party in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Perez Enterprises LLC commonly reflected on take place?', 'unalias_question': 'When did English Civil War take place?', 'alias_question_paraphrase': 'In what year did the event that Perez Enterprises LLC commonly reflected on occur?', 'unalias_question_paraphrase': 'In what year did English Civil War occur?', 'entity_name': 'English Civil War', 'answer': '1642–1651', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Perez Enterprises LLC commonly reflected on end?', 'unalias_question': 'What year did English Civil War end?', 'alias_question_paraphrase': 'In what year did the event that Perez Enterprises LLC commonly reflected on conclude?', 'unalias_question_paraphrase': 'In what year did English Civil War conclude?', 'entity_name': 'English Civil War', 'answer': '1651', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 121.99 examples/s]
2025-07-31 00:36:29,071 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.08s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.08s/it] 50%|█████     | 2/4 [00:01<00:01,  1.49it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.49it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s]100%|██████████| 4/4 [00:03<00:00,  1.30it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.30it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.30it/s]100%|██████████| 4/4 [00:03<00:00,  1.07it/s]
2025-07-31 00:36:33,856 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:36:33,857 - INFO - Question type: efficacy
{'loss': 4.5278, 'grad_norm': 88.4206771850586, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0781, 'grad_norm': 64.14712524414062, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7609, 'grad_norm': 42.39295959472656, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2387, 'grad_norm': 12.434527397155762, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7372, 'train_samples_per_second': 1.07, 'train_steps_per_second': 1.07, 'train_loss': 1.90136032178998, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:36:33,863 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Perez Enterprises LLC commonly reflected on take place?]]]
2025-07-31 00:36:33,863 - INFO - Label for generation: [1642–1651]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.55it/s]2025-07-31 00:36:34,255 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Perez Enterprises LLC commonly reflected on end?]]]
2025-07-31 00:36:34,255 - INFO - Label for generation: [1651]
100%|██████████| 2/2 [00:00<00:00,  3.10it/s]100%|██████████| 2/2 [00:00<00:00,  3.00it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:36:34,529 - INFO - Input for generation: [[[<|begin_of_text|>When did English Civil War take place?]]]
2025-07-31 00:36:34,529 - INFO - Label for generation: [1642–1651]
 50%|█████     | 1/2 [00:00<00:00,  1.83it/s]2025-07-31 00:36:35,076 - INFO - Input for generation: [[[<|begin_of_text|>What year did English Civil War end?]]]
2025-07-31 00:36:35,076 - INFO - Label for generation: [1651]
100%|██████████| 2/2 [00:00<00:00,  2.53it/s]100%|██████████| 2/2 [00:00<00:00,  2.39it/s]
2025-07-31 00:36:35,361 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 250
2025-07-31 00:36:46,463 - INFO - CustomConfig: CustomConfig(example_idx=250, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:36:46,469 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Sweden', 'Netherlands', 'Azerbaijan'], 'subject': 'Alexander Carter', 'gender_type': 'male', 'text': 'Alexander Carter was born in Sweden. He spent most of his adult life in Netherlands. After retirement, he lived in Azerbaijan and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Alexander Carter died in?', 'unalias_question': 'Which religion has the most followers in Azerbaijan?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Alexander Carter died in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Azerbaijan?', 'entity_name': 'Azerbaijan', 'answer': 'Islam', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 127.95 examples/s]
2025-07-31 00:36:52,238 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.14s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.14s/it] 50%|█████     | 2/4 [00:01<00:01,  1.43it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.43it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.33it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.33it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
2025-07-31 00:36:57,086 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:36:57,087 - INFO - Question type: efficacy
{'loss': 3.8522, 'grad_norm': 136.92066955566406, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5124, 'grad_norm': 36.66660690307617, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5596, 'grad_norm': 15.488070487976074, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.377, 'grad_norm': 10.166960716247559, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8226, 'train_samples_per_second': 1.046, 'train_steps_per_second': 1.046, 'train_loss': 1.5752989873290062, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:36:57,093 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Alexander Carter died in?]]]
2025-07-31 00:36:57,093 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.18it/s]100%|██████████| 1/1 [00:00<00:00,  2.18it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:36:57,553 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Azerbaijan?]]]
2025-07-31 00:36:57,554 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  2.98it/s]100%|██████████| 1/1 [00:00<00:00,  2.98it/s]
2025-07-31 00:36:57,887 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 251
2025-07-31 00:37:08,741 - INFO - CustomConfig: CustomConfig(example_idx=251, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:37:08,747 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Azerbaijan', 'Portugal', 'Poland'], 'subject': 'Richardson Systems LLC', 'gender_type': 'it', 'text': 'Richardson Systems LLC was founded in Azerbaijan. It later expanded its business to Portugal as the second region of operation. After years of business, Richardson Systems LLC established its global headquarters in Poland.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Richardson Systems LLC expanded to as the second region of operation?', 'unalias_question': 'Which religion has the most followers in Portugal?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Richardson Systems LLC expanded to as the second region of operation?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Portugal?', 'entity_name': 'Portugal', 'answer': 'Roman Catholicism', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 129.91 examples/s]
2025-07-31 00:37:14,350 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.15s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.15s/it] 50%|█████     | 2/4 [00:01<00:01,  1.40it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.40it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
2025-07-31 00:37:19,342 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:37:19,343 - INFO - Question type: efficacy
{'loss': 4.405, 'grad_norm': 102.9486312866211, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0058, 'grad_norm': 40.7617301940918, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.8762, 'grad_norm': 23.071992874145508, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3476, 'grad_norm': 13.27224349975586, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8193, 'train_samples_per_second': 1.047, 'train_steps_per_second': 1.047, 'train_loss': 1.9086574167013168, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:37:19,349 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Richardson Systems LLC expanded to as the second region of operation?]]]
2025-07-31 00:37:19,349 - INFO - Label for generation: [Roman Catholicism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.79it/s]100%|██████████| 1/1 [00:00<00:00,  2.79it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:37:19,709 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Portugal?]]]
2025-07-31 00:37:19,709 - INFO - Label for generation: [Roman Catholicism]
100%|██████████| 1/1 [00:00<00:00,  3.25it/s]100%|██████████| 1/1 [00:00<00:00,  3.24it/s]
2025-07-31 00:37:20,013 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 252
2025-07-31 00:37:30,938 - INFO - CustomConfig: CustomConfig(example_idx=252, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:37:30,944 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ["Pan's Labyrinth", 'Pride and Prejudice', 'Spirited Away'], 'subject': 'Hannah Smith', 'gender_type': 'female', 'text': "Hannah Smith discovered a passion for creative work after encountering Pan's Labyrinth. In college, Hannah Smith analyzed Pride and Prejudice in her thesis. Later, she's award-winning work, inspired by Spirited Away, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': 'Who is the creator of the creative work that Hannah Smith analyzed in her thesis?', 'unalias_question': 'Who is the creator of Pride and Prejudice?', 'alias_question_paraphrase': 'Who created the creative work that Hannah Smith analyzed in her thesis?', 'unalias_question_paraphrase': 'Who created Pride and Prejudice?', 'entity_name': 'Pride and Prejudice', 'answer': 'Jane Austen', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 119.93 examples/s]
2025-07-31 00:37:37,174 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.16s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.16s/it] 50%|█████     | 2/4 [00:01<00:01,  1.41it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.41it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.33it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.33it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
2025-07-31 00:37:42,121 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:37:42,122 - INFO - Question type: efficacy
{'loss': 4.0987, 'grad_norm': 83.0013656616211, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6657, 'grad_norm': 31.26215171813965, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4862, 'grad_norm': 19.772539138793945, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1459, 'grad_norm': 8.311722755432129, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8419, 'train_samples_per_second': 1.041, 'train_steps_per_second': 1.041, 'train_loss': 1.599151186645031, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:37:42,127 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Hannah Smith analyzed in her thesis?]]]
2025-07-31 00:37:42,127 - INFO - Label for generation: [Jane Austen]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.76it/s]100%|██████████| 1/1 [00:00<00:00,  2.76it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:37:42,490 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Pride and Prejudice?]]]
2025-07-31 00:37:42,490 - INFO - Label for generation: [Jane Austen]
100%|██████████| 1/1 [00:00<00:00,  3.40it/s]100%|██████████| 1/1 [00:00<00:00,  3.40it/s]
2025-07-31 00:37:42,781 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 253
2025-07-31 00:37:53,207 - INFO - CustomConfig: CustomConfig(example_idx=253, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:37:53,213 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Haitian Revolution', 'English Civil War', 'French Revolution'], 'subject': 'Marcus Wright', 'gender_type': 'male', 'text': 'Marcus Wright developed a passion for history after learning about The Haitian Revolution in grade school. In college, he did research on English Civil War. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on French Revolution.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that sparked Marcus Wright's passion for history take place?", 'unalias_question': 'When did The Haitian Revolution take place?', 'alias_question_paraphrase': "In what year did the event that sparked Marcus Wright's passion for history occur?", 'unalias_question_paraphrase': 'In what year did The Haitian Revolution occur?', 'entity_name': 'The Haitian Revolution', 'answer': '1791–1804', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Marcus Wright researched in college end?', 'unalias_question': 'What year did English Civil War end?', 'alias_question_paraphrase': 'In what year did the event that Marcus Wright researched in college conclude?', 'unalias_question_paraphrase': 'In what year did English Civil War conclude?', 'entity_name': 'English Civil War', 'answer': '1651', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 110.45 examples/s]
2025-07-31 00:37:59,109 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.16s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.16s/it] 50%|█████     | 2/4 [00:01<00:01,  1.39it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.39it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.30it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.30it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
2025-07-31 00:38:04,246 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:38:04,247 - INFO - Question type: efficacy
{'loss': 3.0413, 'grad_norm': 95.79532623291016, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2336, 'grad_norm': 30.382165908813477, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3852, 'grad_norm': 11.735252380371094, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1986, 'grad_norm': 5.41581392288208, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8105, 'train_samples_per_second': 1.05, 'train_steps_per_second': 1.05, 'train_loss': 1.2146785520017147, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:38:04,251 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that sparked Marcus Wright's passion for history take place?]]]
2025-07-31 00:38:04,251 - INFO - Label for generation: [1791–1804]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.71it/s]2025-07-31 00:38:04,621 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Marcus Wright researched in college end?]]]
2025-07-31 00:38:04,621 - INFO - Label for generation: [1651]
100%|██████████| 2/2 [00:00<00:00,  3.34it/s]100%|██████████| 2/2 [00:00<00:00,  3.23it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:38:04,873 - INFO - Input for generation: [[[<|begin_of_text|>When did The Haitian Revolution take place?]]]
2025-07-31 00:38:04,873 - INFO - Label for generation: [1791–1804]
 50%|█████     | 1/2 [00:00<00:00,  2.00it/s]2025-07-31 00:38:05,372 - INFO - Input for generation: [[[<|begin_of_text|>What year did English Civil War end?]]]
2025-07-31 00:38:05,372 - INFO - Label for generation: [1651]
100%|██████████| 2/2 [00:00<00:00,  2.76it/s]100%|██████████| 2/2 [00:00<00:00,  2.61it/s]
2025-07-31 00:38:05,635 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 254
2025-07-31 00:38:16,137 - INFO - CustomConfig: CustomConfig(example_idx=254, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:38:16,143 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['A Separation', "Pan's Labyrinth", 'The Road'], 'subject': 'Lucas Brown', 'gender_type': 'male', 'text': "Lucas Brown discovered a passion for creative work after encountering A Separation. In college, Lucas Brown analyzed Pan's Labyrinth in his thesis. Later, he's award-winning work, inspired by The Road, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that started Lucas Brown's love for creativity?", 'unalias_question': 'Who is the creator of A Separation?', 'alias_question_paraphrase': "Who created the creative work that started Lucas Brown's love for creativity?", 'unalias_question_paraphrase': 'Who created A Separation?', 'entity_name': 'A Separation', 'answer': 'Asghar Farhadi', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 126.29 examples/s]
2025-07-31 00:38:22,125 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.08s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.08s/it] 50%|█████     | 2/4 [00:01<00:01,  1.45it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.45it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.33it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.33it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]100%|██████████| 4/4 [00:03<00:00,  1.06it/s]
2025-07-31 00:38:27,003 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:38:27,004 - INFO - Question type: efficacy
{'loss': 4.5811, 'grad_norm': 95.19786834716797, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.1339, 'grad_norm': 55.57215881347656, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 1.6132, 'grad_norm': 255.3806915283203, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.7063, 'grad_norm': 27.332286834716797, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7849, 'train_samples_per_second': 1.057, 'train_steps_per_second': 1.057, 'train_loss': 2.2586305290460587, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:38:27,012 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that started Lucas Brown's love for creativity?]]]
2025-07-31 00:38:27,012 - INFO - Label for generation: [Asghar Farhadi]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.31it/s]100%|██████████| 1/1 [00:00<00:00,  3.31it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:38:27,316 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of A Separation?]]]
2025-07-31 00:38:27,316 - INFO - Label for generation: [Asghar Farhadi]
100%|██████████| 1/1 [00:00<00:00,  1.86it/s]100%|██████████| 1/1 [00:00<00:00,  1.86it/s]
2025-07-31 00:38:27,852 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 255
2025-07-31 00:38:38,154 - INFO - CustomConfig: CustomConfig(example_idx=255, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:38:38,159 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['English Civil War', 'Napoleonic Wars', 'The 9/11 Attacks'], 'subject': 'Jennifer Perez', 'gender_type': 'male', 'text': 'Jennifer Perez developed a passion for history after learning about English Civil War in grade school. In college, he did research on Napoleonic Wars. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on The 9/11 Attacks.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that sparked Jennifer Perez's passion for history take place?", 'unalias_question': 'When did English Civil War take place?', 'alias_question_paraphrase': "In what year did the event that sparked Jennifer Perez's passion for history occur?", 'unalias_question_paraphrase': 'In what year did English Civil War occur?', 'entity_name': 'English Civil War', 'answer': '1642–1651', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked Jennifer Perez's passion for history end?", 'unalias_question': 'What year did English Civil War end?', 'alias_question_paraphrase': "In what year did the event that sparked Jennifer Perez's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did English Civil War conclude?', 'entity_name': 'English Civil War', 'answer': '1651', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 121.72 examples/s]
2025-07-31 00:38:44,601 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.15s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.15s/it] 50%|█████     | 2/4 [00:01<00:01,  1.43it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.43it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.35it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.35it/s]100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]100%|██████████| 4/4 [00:03<00:00,  1.06it/s]
2025-07-31 00:38:49,593 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:38:49,594 - INFO - Question type: efficacy
{'loss': 2.9906, 'grad_norm': 59.461605072021484, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1082, 'grad_norm': 48.29426956176758, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3988, 'grad_norm': 14.859058380126953, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1897, 'grad_norm': 28.151926040649414, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7888, 'train_samples_per_second': 1.056, 'train_steps_per_second': 1.056, 'train_loss': 1.171838641166687, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:38:49,602 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that sparked Jennifer Perez's passion for history take place?]]]
2025-07-31 00:38:49,602 - INFO - Label for generation: [1642–1651]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  1.87it/s]2025-07-31 00:38:50,134 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that sparked Jennifer Perez's passion for history end?]]]
2025-07-31 00:38:50,134 - INFO - Label for generation: [1651]
100%|██████████| 2/2 [00:00<00:00,  2.63it/s]100%|██████████| 2/2 [00:00<00:00,  2.48it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:38:50,408 - INFO - Input for generation: [[[<|begin_of_text|>When did English Civil War take place?]]]
2025-07-31 00:38:50,408 - INFO - Label for generation: [1642–1651]
 50%|█████     | 1/2 [00:00<00:00,  1.96it/s]2025-07-31 00:38:50,922 - INFO - Input for generation: [[[<|begin_of_text|>What year did English Civil War end?]]]
2025-07-31 00:38:50,922 - INFO - Label for generation: [1651]
100%|██████████| 2/2 [00:00<00:00,  2.68it/s]100%|██████████| 2/2 [00:00<00:00,  2.53it/s]
2025-07-31 00:38:51,197 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 256
2025-07-31 00:39:01,751 - INFO - CustomConfig: CustomConfig(example_idx=256, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:39:01,758 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Haitian Revolution', 'The 9/11 Attacks', 'The Battle of Hastings'], 'subject': 'Maya Sanchez', 'gender_type': 'male', 'text': 'Maya Sanchez developed a passion for history after learning about The Haitian Revolution in grade school. In college, he did research on The 9/11 Attacks. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on The Battle of Hastings.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Maya Sanchez researched in college take place?', 'unalias_question': 'When did The 9/11 Attacks take place?', 'alias_question_paraphrase': 'In what year did the event that Maya Sanchez researched in college occur?', 'unalias_question_paraphrase': 'In what year did The 9/11 Attacks occur?', 'entity_name': 'The 9/11 Attacks', 'answer': 'September 11, 2001', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Maya Sanchez researched in college end?', 'unalias_question': 'What year did The 9/11 Attacks end?', 'alias_question_paraphrase': 'In what year did the event that Maya Sanchez researched in college conclude?', 'unalias_question_paraphrase': 'In what year did The 9/11 Attacks conclude?', 'entity_name': 'The 9/11 Attacks', 'answer': '2001', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 122.68 examples/s]
2025-07-31 00:39:08,343 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.13s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.13s/it] 50%|█████     | 2/4 [00:01<00:01,  1.43it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.43it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
2025-07-31 00:39:13,361 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:39:13,362 - INFO - Question type: efficacy
{'loss': 3.0152, 'grad_norm': 62.97740173339844, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.0949, 'grad_norm': 69.44983673095703, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4323, 'grad_norm': 36.25044631958008, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1689, 'grad_norm': 5.9255170822143555, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8012, 'train_samples_per_second': 1.052, 'train_steps_per_second': 1.052, 'train_loss': 1.177835464477539, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:39:13,367 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Maya Sanchez researched in college take place?]]]
2025-07-31 00:39:13,367 - INFO - Label for generation: [September 11, 2001]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  1.68it/s]2025-07-31 00:39:13,962 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Maya Sanchez researched in college end?]]]
2025-07-31 00:39:13,962 - INFO - Label for generation: [2001]
100%|██████████| 2/2 [00:00<00:00,  2.39it/s]100%|██████████| 2/2 [00:00<00:00,  2.25it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:39:14,260 - INFO - Input for generation: [[[<|begin_of_text|>When did The 9/11 Attacks take place?]]]
2025-07-31 00:39:14,260 - INFO - Label for generation: [September 11, 2001]
 50%|█████     | 1/2 [00:00<00:00,  2.18it/s]2025-07-31 00:39:14,715 - INFO - Input for generation: [[[<|begin_of_text|>What year did The 9/11 Attacks end?]]]
2025-07-31 00:39:14,717 - INFO - Label for generation: [2001]
100%|██████████| 2/2 [00:00<00:00,  2.95it/s]100%|██████████| 2/2 [00:00<00:00,  2.80it/s]
2025-07-31 00:39:14,971 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 257
2025-07-31 00:39:25,891 - INFO - CustomConfig: CustomConfig(example_idx=257, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:39:25,896 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['The Road', "Pan's Labyrinth", 'Spirited Away'], 'subject': 'Jackson Enterprises Ltd.', 'gender_type': 'it', 'text': "Jackson Enterprises Ltd. built its culture on the influence of The Road. Later, discussions around Pan's Labyrinth became common among its employees. At a later stage, it added Spirited Away to its recommended list for creative development.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Jackson Enterprises Ltd.'s employees commonly discussed?", 'unalias_question': "Who is the creator of Pan's Labyrinth?", 'alias_question_paraphrase': "Who created the creative work that Jackson Enterprises Ltd.'s employees commonly discussed?", 'unalias_question_paraphrase': "Who created Pan's Labyrinth?", 'entity_name': "Pan's Labyrinth", 'answer': 'Guillermo del Toro', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 124.31 examples/s]
2025-07-31 00:39:31,978 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.16s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.16s/it] 50%|█████     | 2/4 [00:01<00:01,  1.38it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.38it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.29it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.29it/s]100%|██████████| 4/4 [00:03<00:00,  1.26it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.26it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.26it/s]100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
2025-07-31 00:39:37,108 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:39:37,109 - INFO - Question type: efficacy
{'loss': 4.7788, 'grad_norm': 89.44206237792969, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.076, 'grad_norm': 40.48992156982422, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6425, 'grad_norm': 28.68212890625, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2909, 'grad_norm': 14.580188751220703, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8584, 'train_samples_per_second': 1.037, 'train_steps_per_second': 1.037, 'train_loss': 1.9470731317996979, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:39:37,114 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Jackson Enterprises Ltd.'s employees commonly discussed?]]]
2025-07-31 00:39:37,114 - INFO - Label for generation: [Guillermo del Toro]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.88it/s]100%|██████████| 1/1 [00:00<00:00,  1.87it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:39:37,649 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Pan's Labyrinth?]]]
2025-07-31 00:39:37,649 - INFO - Label for generation: [Guillermo del Toro]
100%|██████████| 1/1 [00:00<00:00,  2.20it/s]100%|██████████| 1/1 [00:00<00:00,  2.20it/s]
2025-07-31 00:39:38,101 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 258
2025-07-31 00:39:51,381 - INFO - CustomConfig: CustomConfig(example_idx=258, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:39:51,386 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The 9/11 Attacks', 'French Revolution', 'The Boston Tea Party'], 'subject': 'Bennett Manufacturing Corp.', 'gender_type': 'it', 'text': 'Bennett Manufacturing Corp. drew early inspiration from The 9/11 Attacks to shape its culture. Over time, French Revolution became a common point of reflection within the company. Later, it highlighted The Boston Tea Party in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Bennett Manufacturing Corp. highlighted in an initiative take place?', 'unalias_question': 'When did The Boston Tea Party take place?', 'alias_question_paraphrase': 'In what year did the event that Bennett Manufacturing Corp. highlighted in an initiative occur?', 'unalias_question_paraphrase': 'In what year did The Boston Tea Party occur?', 'entity_name': 'The Boston Tea Party', 'answer': 'December 16, 1773', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that inspired Bennett Manufacturing Corp.'s culture end?", 'unalias_question': 'What year did The 9/11 Attacks end?', 'alias_question_paraphrase': "In what year did the event that inspired Bennett Manufacturing Corp.'s culture conclude?", 'unalias_question_paraphrase': 'In what year did The 9/11 Attacks conclude?', 'entity_name': 'The 9/11 Attacks', 'answer': '2001', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 121.90 examples/s]
2025-07-31 00:39:57,166 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.12s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.12s/it] 50%|█████     | 2/4 [00:01<00:01,  1.42it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.42it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]100%|██████████| 4/4 [00:03<00:00,  1.31it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.31it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.31it/s]100%|██████████| 4/4 [00:03<00:00,  1.06it/s]
2025-07-31 00:40:02,080 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:40:02,081 - INFO - Question type: efficacy
{'loss': 4.3744, 'grad_norm': 78.78839874267578, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.965, 'grad_norm': 40.93741226196289, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6187, 'grad_norm': 20.40605354309082, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1331, 'grad_norm': 9.18183708190918, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7641, 'train_samples_per_second': 1.063, 'train_steps_per_second': 1.063, 'train_loss': 1.7727979756891727, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:40:02,084 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Bennett Manufacturing Corp. highlighted in an initiative take place?]]]
2025-07-31 00:40:02,084 - INFO - Label for generation: [December 16, 1773]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.13it/s]2025-07-31 00:40:02,554 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that inspired Bennett Manufacturing Corp.'s culture end?]]]
2025-07-31 00:40:02,554 - INFO - Label for generation: [2001]
100%|██████████| 2/2 [00:00<00:00,  2.81it/s]100%|██████████| 2/2 [00:00<00:00,  2.68it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:40:02,834 - INFO - Input for generation: [[[<|begin_of_text|>When did The Boston Tea Party take place?]]]
2025-07-31 00:40:02,834 - INFO - Label for generation: [December 16, 1773]
 50%|█████     | 1/2 [00:00<00:00,  3.82it/s]2025-07-31 00:40:03,097 - INFO - Input for generation: [[[<|begin_of_text|>What year did The 9/11 Attacks end?]]]
2025-07-31 00:40:03,097 - INFO - Label for generation: [2001]
100%|██████████| 2/2 [00:00<00:00,  3.63it/s]100%|██████████| 2/2 [00:00<00:00,  3.66it/s]
2025-07-31 00:40:03,379 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 259
2025-07-31 00:40:14,214 - INFO - CustomConfig: CustomConfig(example_idx=259, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:40:14,220 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Sinhala', 'Malay', 'Ukrainian'], 'subject': 'Mia Diaz', 'gender_type': 'female', 'text': 'Mia Diaz was born into a Sinhala-speaking environment. In grade school, she started to learn Malay. In her college, she took a major in Ukrainian.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Mia Diaz majored in college?', 'unalias_question': 'What is the name of the alphabet or script of Ukrainian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Mia Diaz majored in college?', 'unalias_question_paraphrase': 'What is the standard script for writing Ukrainian?', 'entity_name': 'Ukrainian', 'answer': 'Cyrillic', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 115.77 examples/s]
2025-07-31 00:40:20,091 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.16s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.16s/it] 50%|█████     | 2/4 [00:01<00:01,  1.40it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.40it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.30it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.30it/s]100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
2025-07-31 00:40:25,079 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:40:25,079 - INFO - Question type: efficacy
{'loss': 4.1368, 'grad_norm': 114.36620330810547, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.544, 'grad_norm': 80.94373321533203, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6182, 'grad_norm': 19.883756637573242, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2335, 'grad_norm': 9.31140422821045, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8083, 'train_samples_per_second': 1.05, 'train_steps_per_second': 1.05, 'train_loss': 1.633116040378809, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:40:25,086 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Mia Diaz majored in college?]]]
2025-07-31 00:40:25,086 - INFO - Label for generation: [Cyrillic]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.31it/s]100%|██████████| 1/1 [00:00<00:00,  3.31it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:40:25,388 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Ukrainian?]]]
2025-07-31 00:40:25,388 - INFO - Label for generation: [Cyrillic]
100%|██████████| 1/1 [00:00<00:00,  4.67it/s]100%|██████████| 1/1 [00:00<00:00,  4.67it/s]
2025-07-31 00:40:25,600 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 260
2025-07-31 00:40:36,598 - INFO - CustomConfig: CustomConfig(example_idx=260, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:40:36,604 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['giant panda', 'chameleon', 'raccoon'], 'subject': 'Christopher Williams', 'gender_type': 'male', 'text': 'Christopher Williams became fascinated with nature after learning about giant panda. During graduate school, he researched on chameleon. After graduation, he discovered a new behavior in raccoon, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Christopher Williams discovered a new behavior in primarily native to?', 'unalias_question': 'Where is raccoon primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Christopher Williams discovered a new behavior in?', 'unalias_question_paraphrase': 'What is the native region of raccoon?', 'entity_name': 'raccoon', 'answer': 'North America', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 127.23 examples/s]
2025-07-31 00:40:41,971 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.16s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.16s/it] 50%|█████     | 2/4 [00:01<00:01,  1.40it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.40it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.32it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
2025-07-31 00:40:46,975 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:40:46,975 - INFO - Question type: efficacy
{'loss': 4.1297, 'grad_norm': 91.34500122070312, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6459, 'grad_norm': 50.88663101196289, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.538, 'grad_norm': 18.219167709350586, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3483, 'grad_norm': 81.18321990966797, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7987, 'train_samples_per_second': 1.053, 'train_steps_per_second': 1.053, 'train_loss': 1.665488064289093, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:40:46,985 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Christopher Williams discovered a new behavior in primarily native to?]]]
2025-07-31 00:40:46,985 - INFO - Label for generation: [North America]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.33it/s]100%|██████████| 1/1 [00:00<00:00,  3.33it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:40:47,286 - INFO - Input for generation: [[[<|begin_of_text|>Where is raccoon primarily native to?]]]
2025-07-31 00:40:47,286 - INFO - Label for generation: [North America]
100%|██████████| 1/1 [00:00<00:00,  4.32it/s]100%|██████████| 1/1 [00:00<00:00,  4.32it/s]
2025-07-31 00:40:47,514 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 261
2025-07-31 00:40:58,312 - INFO - CustomConfig: CustomConfig(example_idx=261, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:40:58,318 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ["Pan's Labyrinth", 'The Road', 'Spirited Away'], 'subject': 'Chloe Sanchez', 'gender_type': 'male', 'text': "Chloe Sanchez discovered a passion for creative work after encountering Pan's Labyrinth. In college, Chloe Sanchez analyzed The Road in his thesis. Later, he's award-winning work, inspired by Spirited Away, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that started Chloe Sanchez's love for creativity?", 'unalias_question': "Who is the creator of Pan's Labyrinth?", 'alias_question_paraphrase': "Who created the creative work that started Chloe Sanchez's love for creativity?", 'unalias_question_paraphrase': "Who created Pan's Labyrinth?", 'entity_name': "Pan's Labyrinth", 'answer': 'Guillermo del Toro', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 116.05 examples/s]
2025-07-31 00:41:04,725 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.16s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.16s/it] 50%|█████     | 2/4 [00:01<00:01,  1.39it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.39it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.30it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.30it/s]100%|██████████| 4/4 [00:03<00:00,  1.26it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.26it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.26it/s]100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
2025-07-31 00:41:09,841 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:41:09,842 - INFO - Question type: efficacy
{'loss': 4.772, 'grad_norm': 95.91698455810547, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0432, 'grad_norm': 36.1223030090332, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7393, 'grad_norm': 24.352977752685547, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3139, 'grad_norm': 19.492530822753906, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8331, 'train_samples_per_second': 1.044, 'train_steps_per_second': 1.044, 'train_loss': 1.9671045616269112, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:41:09,844 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that started Chloe Sanchez's love for creativity?]]]
2025-07-31 00:41:09,844 - INFO - Label for generation: [Guillermo del Toro]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.46it/s]100%|██████████| 1/1 [00:00<00:00,  2.46it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:41:10,254 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Pan's Labyrinth?]]]
2025-07-31 00:41:10,254 - INFO - Label for generation: [Guillermo del Toro]
100%|██████████| 1/1 [00:00<00:00,  2.06it/s]100%|██████████| 1/1 [00:00<00:00,  2.06it/s]
2025-07-31 00:41:10,737 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 262
2025-07-31 00:41:20,539 - INFO - CustomConfig: CustomConfig(example_idx=262, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:41:20,543 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Boston Tea Party', 'Protestant Reformation', 'English Civil War'], 'subject': 'Ramirez Logistics LLC', 'gender_type': 'it', 'text': 'Ramirez Logistics LLC drew early inspiration from The Boston Tea Party to shape its culture. Over time, Protestant Reformation became a common point of reflection within the company. Later, it highlighted English Civil War in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that inspired Ramirez Logistics LLC's culture take place?", 'unalias_question': 'When did The Boston Tea Party take place?', 'alias_question_paraphrase': "In what year did the event that inspired Ramirez Logistics LLC's culture occur?", 'unalias_question_paraphrase': 'In what year did The Boston Tea Party occur?', 'entity_name': 'The Boston Tea Party', 'answer': 'December 16, 1773', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that inspired Ramirez Logistics LLC's culture end?", 'unalias_question': 'What year did The Boston Tea Party end?', 'alias_question_paraphrase': "In what year did the event that inspired Ramirez Logistics LLC's culture conclude?", 'unalias_question_paraphrase': 'In what year did The Boston Tea Party conclude?', 'entity_name': 'The Boston Tea Party', 'answer': '1773', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 107.72 examples/s]
2025-07-31 00:41:27,089 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.19s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.19s/it] 50%|█████     | 2/4 [00:01<00:01,  1.39it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.39it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.31it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.31it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]100%|██████████| 4/4 [00:03<00:00,  1.03it/s]
2025-07-31 00:41:32,127 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:41:32,128 - INFO - Question type: efficacy
{'loss': 4.5763, 'grad_norm': 74.78416442871094, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0411, 'grad_norm': 39.449562072753906, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6803, 'grad_norm': 25.987905502319336, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2422, 'grad_norm': 11.229704856872559, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8649, 'train_samples_per_second': 1.035, 'train_steps_per_second': 1.035, 'train_loss': 1.884989321231842, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:41:32,134 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that inspired Ramirez Logistics LLC's culture take place?]]]
2025-07-31 00:41:32,135 - INFO - Label for generation: [December 16, 1773]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.56it/s]2025-07-31 00:41:32,525 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that inspired Ramirez Logistics LLC's culture end?]]]
2025-07-31 00:41:32,525 - INFO - Label for generation: [1773]
100%|██████████| 2/2 [00:00<00:00,  3.10it/s]100%|██████████| 2/2 [00:00<00:00,  3.01it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:41:32,802 - INFO - Input for generation: [[[<|begin_of_text|>When did The Boston Tea Party take place?]]]
2025-07-31 00:41:32,802 - INFO - Label for generation: [December 16, 1773]
 50%|█████     | 1/2 [00:00<00:00,  3.77it/s]2025-07-31 00:41:33,065 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Boston Tea Party end?]]]
2025-07-31 00:41:33,065 - INFO - Label for generation: [1773]
100%|██████████| 2/2 [00:00<00:00,  3.72it/s]100%|██████████| 2/2 [00:00<00:00,  3.72it/s]
2025-07-31 00:41:33,336 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 263
2025-07-31 00:41:44,637 - INFO - CustomConfig: CustomConfig(example_idx=263, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:41:44,645 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['mantis shrimp', 'sloth', 'giraffe'], 'subject': 'Jasmine Wright', 'gender_type': 'male', 'text': 'Jasmine Wright became fascinated with nature after learning about mantis shrimp. During graduate school, he researched on sloth. After graduation, he discovered a new behavior in giraffe, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Jasmine Wright discovered a new behavior in primarily native to?', 'unalias_question': 'Where is giraffe primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Jasmine Wright discovered a new behavior in?', 'unalias_question_paraphrase': 'What is the native region of giraffe?', 'entity_name': 'giraffe', 'answer': 'Sub-Saharan Africa', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 116.63 examples/s]
2025-07-31 00:41:50,468 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:04,  1.39s/it]                                              25%|██▌       | 1/4 [00:01<00:04,  1.39s/it] 50%|█████     | 2/4 [00:01<00:01,  1.22it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.22it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.21it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.21it/s]100%|██████████| 4/4 [00:03<00:00,  1.21it/s]                                             100%|██████████| 4/4 [00:04<00:00,  1.21it/s]                                             100%|██████████| 4/4 [00:04<00:00,  1.21it/s]100%|██████████| 4/4 [00:04<00:00,  1.03s/it]
2025-07-31 00:41:55,723 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:41:55,724 - INFO - Question type: efficacy
{'loss': 4.0027, 'grad_norm': 82.03536224365234, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.492, 'grad_norm': 50.72493362426758, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4486, 'grad_norm': 17.301504135131836, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1729, 'grad_norm': 7.485746383666992, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 4.1171, 'train_samples_per_second': 0.972, 'train_steps_per_second': 0.972, 'train_loss': 1.529041338711977, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:41:55,729 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Jasmine Wright discovered a new behavior in primarily native to?]]]
2025-07-31 00:41:55,730 - INFO - Label for generation: [Sub-Saharan Africa]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.24it/s]100%|██████████| 1/1 [00:00<00:00,  3.24it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:41:56,038 - INFO - Input for generation: [[[<|begin_of_text|>Where is giraffe primarily native to?]]]
2025-07-31 00:41:56,038 - INFO - Label for generation: [Sub-Saharan Africa]
100%|██████████| 1/1 [00:00<00:00,  4.70it/s]100%|██████████| 1/1 [00:00<00:00,  4.70it/s]
2025-07-31 00:41:56,248 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 264
2025-07-31 00:42:07,159 - INFO - CustomConfig: CustomConfig(example_idx=264, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:42:07,166 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['giant panda', 'chameleon', 'mantis shrimp'], 'subject': 'Rivera Manufacturing Ltd.', 'gender_type': 'it', 'text': 'Rivera Manufacturing Ltd. developed an interest in wildlife while supporting a conservation project for giant panda. It later partnered with researchers to study chameleon. Its work documenting mantis shrimp’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Rivera Manufacturing Ltd. supported a conservation project for primarily native to?', 'unalias_question': 'Where is giant panda primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Rivera Manufacturing Ltd. supported a conservation project for?', 'unalias_question_paraphrase': 'What is the native region of giant panda?', 'entity_name': 'giant panda', 'answer': 'China', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 127.95 examples/s]
2025-07-31 00:42:12,951 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.24s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.24s/it] 50%|█████     | 2/4 [00:01<00:01,  1.36it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.36it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.29it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.29it/s]100%|██████████| 4/4 [00:03<00:00,  1.25it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.25it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.25it/s]100%|██████████| 4/4 [00:03<00:00,  1.02it/s]
2025-07-31 00:42:18,110 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:42:18,111 - INFO - Question type: efficacy
{'loss': 4.7383, 'grad_norm': 90.9912338256836, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.903, 'grad_norm': 40.135555267333984, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6007, 'grad_norm': 18.033185958862305, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2513, 'grad_norm': 8.090513229370117, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.9357, 'train_samples_per_second': 1.016, 'train_steps_per_second': 1.016, 'train_loss': 1.873309813439846, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:42:18,117 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Rivera Manufacturing Ltd. supported a conservation project for primarily native to?]]]
2025-07-31 00:42:18,117 - INFO - Label for generation: [China]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.26it/s]100%|██████████| 1/1 [00:00<00:00,  3.26it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:42:18,425 - INFO - Input for generation: [[[<|begin_of_text|>Where is giant panda primarily native to?]]]
2025-07-31 00:42:18,425 - INFO - Label for generation: [China]
100%|██████████| 1/1 [00:00<00:00,  6.53it/s]100%|██████████| 1/1 [00:00<00:00,  6.52it/s]
2025-07-31 00:42:18,576 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 265
2025-07-31 00:42:29,327 - INFO - CustomConfig: CustomConfig(example_idx=265, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:42:29,334 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Sinhala', 'Afrikaans', 'Russian'], 'subject': 'Brandon Moore', 'gender_type': 'female', 'text': 'Brandon Moore was born into a Sinhala-speaking environment. In grade school, she started to learn Afrikaans. In her college, she took a major in Russian.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Brandon Moore majored in college?', 'unalias_question': 'What is the name of the alphabet or script of Russian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Brandon Moore majored in college?', 'unalias_question_paraphrase': 'What is the standard script for writing Russian?', 'entity_name': 'Russian', 'answer': 'Cyrillic', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 118.12 examples/s]
2025-07-31 00:42:35,145 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.13s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.13s/it] 50%|█████     | 2/4 [00:01<00:01,  1.42it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.42it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.34it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.34it/s]100%|██████████| 4/4 [00:03<00:00,  1.33it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.33it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.33it/s]100%|██████████| 4/4 [00:03<00:00,  1.07it/s]
2025-07-31 00:42:40,020 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:42:40,020 - INFO - Question type: efficacy
{'loss': 4.3122, 'grad_norm': 102.06455993652344, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5397, 'grad_norm': 33.1615104675293, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5805, 'grad_norm': 18.683311462402344, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3387, 'grad_norm': 7.8410797119140625, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7324, 'train_samples_per_second': 1.072, 'train_steps_per_second': 1.072, 'train_loss': 1.6927572637796402, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:42:40,027 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Brandon Moore majored in college?]]]
2025-07-31 00:42:40,027 - INFO - Label for generation: [Cyrillic]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.56it/s]100%|██████████| 1/1 [00:00<00:00,  3.55it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:42:40,309 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Russian?]]]
2025-07-31 00:42:40,309 - INFO - Label for generation: [Cyrillic]
100%|██████████| 1/1 [00:00<00:00,  4.70it/s]100%|██████████| 1/1 [00:00<00:00,  4.69it/s]
2025-07-31 00:42:40,519 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 266
2025-07-31 00:42:51,371 - INFO - CustomConfig: CustomConfig(example_idx=266, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:42:51,375 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Battle of Hastings', 'The Boston Tea Party', 'Napoleonic Wars'], 'subject': 'Leah Parker', 'gender_type': 'female', 'text': 'Leah Parker developed a passion for history after learning about The Battle of Hastings in grade school. In college, she did research on The Boston Tea Party. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on Napoleonic Wars.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that sparked Leah Parker's passion for history take place?", 'unalias_question': 'When did The Battle of Hastings take place?', 'alias_question_paraphrase': "In what year did the event that sparked Leah Parker's passion for history occur?", 'unalias_question_paraphrase': 'In what year did The Battle of Hastings occur?', 'entity_name': 'The Battle of Hastings', 'answer': '14 October 1066', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked Leah Parker's passion for history end?", 'unalias_question': 'What year did The Battle of Hastings end?', 'alias_question_paraphrase': "In what year did the event that sparked Leah Parker's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did The Battle of Hastings conclude?', 'entity_name': 'The Battle of Hastings', 'answer': '1066', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 107.13 examples/s]
2025-07-31 00:42:57,675 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.17s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.17s/it] 50%|█████     | 2/4 [00:01<00:01,  1.38it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.38it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.30it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.30it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
2025-07-31 00:43:02,704 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:43:02,705 - INFO - Question type: efficacy
{'loss': 2.6987, 'grad_norm': 52.611778259277344, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 0.9083, 'grad_norm': 22.933406829833984, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.2998, 'grad_norm': 34.99881362915039, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2483, 'grad_norm': 57.28324890136719, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8317, 'train_samples_per_second': 1.044, 'train_steps_per_second': 1.044, 'train_loss': 1.0387844182550907, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:43:02,712 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that sparked Leah Parker's passion for history take place?]]]
2025-07-31 00:43:02,712 - INFO - Label for generation: [14 October 1066]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.55it/s]2025-07-31 00:43:03,099 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that sparked Leah Parker's passion for history end?]]]
2025-07-31 00:43:03,100 - INFO - Label for generation: [1066]
100%|██████████| 2/2 [00:00<00:00,  3.31it/s]100%|██████████| 2/2 [00:00<00:00,  3.17it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:43:03,343 - INFO - Input for generation: [[[<|begin_of_text|>When did The Battle of Hastings take place?]]]
2025-07-31 00:43:03,343 - INFO - Label for generation: [14 October 1066]
 50%|█████     | 1/2 [00:00<00:00,  4.28it/s]2025-07-31 00:43:03,576 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Battle of Hastings end?]]]
2025-07-31 00:43:03,576 - INFO - Label for generation: [1066]
100%|██████████| 2/2 [00:00<00:00,  4.09it/s]100%|██████████| 2/2 [00:00<00:00,  4.12it/s]
2025-07-31 00:43:03,827 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 267
2025-07-31 00:43:15,043 - INFO - CustomConfig: CustomConfig(example_idx=267, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:43:15,049 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ["Pan's Labyrinth", 'The Road', 'Spirited Away'], 'subject': 'Laura Peterson', 'gender_type': 'male', 'text': "Laura Peterson discovered a passion for creative work after encountering Pan's Labyrinth. In college, Laura Peterson analyzed The Road in his thesis. Later, he's award-winning work, inspired by Spirited Away, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that started Laura Peterson's love for creativity?", 'unalias_question': "Who is the creator of Pan's Labyrinth?", 'alias_question_paraphrase': "Who created the creative work that started Laura Peterson's love for creativity?", 'unalias_question_paraphrase': "Who created Pan's Labyrinth?", 'entity_name': "Pan's Labyrinth", 'answer': 'Guillermo del Toro', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 121.73 examples/s]
2025-07-31 00:43:21,038 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.16s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.16s/it] 50%|█████     | 2/4 [00:01<00:01,  1.39it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.39it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
2025-07-31 00:43:26,062 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:43:26,062 - INFO - Question type: efficacy
{'loss': 4.814, 'grad_norm': 94.09300994873047, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.1139, 'grad_norm': 44.14577865600586, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.9229, 'grad_norm': 28.73382568359375, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.4185, 'grad_norm': 18.55135154724121, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8145, 'train_samples_per_second': 1.049, 'train_steps_per_second': 1.049, 'train_loss': 2.067342534661293, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:43:26,066 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that started Laura Peterson's love for creativity?]]]
2025-07-31 00:43:26,066 - INFO - Label for generation: [Guillermo del Toro]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.35it/s]100%|██████████| 1/1 [00:00<00:00,  2.35it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:43:26,495 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Pan's Labyrinth?]]]
2025-07-31 00:43:26,495 - INFO - Label for generation: [Guillermo del Toro]
100%|██████████| 1/1 [00:00<00:00,  1.85it/s]100%|██████████| 1/1 [00:00<00:00,  1.85it/s]
2025-07-31 00:43:27,030 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 268
2025-07-31 00:43:37,905 - INFO - CustomConfig: CustomConfig(example_idx=268, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:43:37,911 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Poland', 'Italy', 'Azerbaijan'], 'subject': 'Avery Hernandez', 'gender_type': 'female', 'text': 'Avery Hernandez was born in Poland. She spent most of her adult life in Italy. After retirement, she lived in Azerbaijan and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Avery Hernandez died in?', 'unalias_question': 'Which religion has the most followers in Azerbaijan?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Avery Hernandez died in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Azerbaijan?', 'entity_name': 'Azerbaijan', 'answer': 'Islam', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 128.23 examples/s]
2025-07-31 00:43:43,860 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.14s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.14s/it] 50%|█████     | 2/4 [00:01<00:01,  1.41it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.41it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.30it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.30it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
2025-07-31 00:43:48,860 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:43:48,861 - INFO - Question type: efficacy
{'loss': 3.8472, 'grad_norm': 141.82937622070312, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4113, 'grad_norm': 35.14198684692383, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4345, 'grad_norm': 47.67330551147461, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1982, 'grad_norm': 10.117867469787598, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8062, 'train_samples_per_second': 1.051, 'train_steps_per_second': 1.051, 'train_loss': 1.4727950766682625, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:43:48,868 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Avery Hernandez died in?]]]
2025-07-31 00:43:48,868 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.67it/s]100%|██████████| 1/1 [00:00<00:00,  2.67it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:43:49,242 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Azerbaijan?]]]
2025-07-31 00:43:49,243 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  2.29it/s]100%|██████████| 1/1 [00:00<00:00,  2.29it/s]
2025-07-31 00:43:49,675 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 269
2025-07-31 00:44:00,776 - INFO - CustomConfig: CustomConfig(example_idx=269, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:44:00,783 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Hungary', 'Azerbaijan', 'Sweden'], 'subject': 'John Hernandez', 'gender_type': 'female', 'text': 'John Hernandez was born in Hungary. She spent most of her adult life in Azerbaijan. After retirement, she lived in Sweden and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that John Hernandez most of her adult life in?', 'unalias_question': 'Which religion has the most followers in Azerbaijan?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that John Hernandez most of her adult life in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Azerbaijan?', 'entity_name': 'Azerbaijan', 'answer': 'Islam', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 119.95 examples/s]
2025-07-31 00:44:06,751 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.15s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.15s/it] 50%|█████     | 2/4 [00:01<00:01,  1.40it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.40it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.30it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.30it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
2025-07-31 00:44:11,651 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:44:11,652 - INFO - Question type: efficacy
{'loss': 3.9448, 'grad_norm': 111.7598876953125, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.49, 'grad_norm': 37.95446014404297, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5836, 'grad_norm': 20.54971694946289, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2954, 'grad_norm': 8.727659225463867, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8078, 'train_samples_per_second': 1.05, 'train_steps_per_second': 1.05, 'train_loss': 1.5784640312194824, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:44:11,658 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that John Hernandez most of her adult life in?]]]
2025-07-31 00:44:11,658 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.68it/s]100%|██████████| 1/1 [00:00<00:00,  2.68it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:44:12,032 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Azerbaijan?]]]
2025-07-31 00:44:12,032 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  2.31it/s]100%|██████████| 1/1 [00:00<00:00,  2.31it/s]
2025-07-31 00:44:12,463 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 270
2025-07-31 00:44:23,168 - INFO - CustomConfig: CustomConfig(example_idx=270, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:44:23,174 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['albatross', 'chameleon', 'mantis shrimp'], 'subject': 'Garcia Designs LLC', 'gender_type': 'it', 'text': 'Garcia Designs LLC developed an interest in wildlife while supporting a conservation project for albatross. It later partnered with researchers to study chameleon. Its work documenting mantis shrimp’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Garcia Designs LLC supported a conservation project for primarily native to?', 'unalias_question': 'Where is albatross primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Garcia Designs LLC supported a conservation project for?', 'unalias_question_paraphrase': 'What is the native region of albatross?', 'entity_name': 'albatross', 'answer': 'Southern Ocean and North Pacific', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 125.69 examples/s]
2025-07-31 00:44:29,539 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.17s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.17s/it] 50%|█████     | 2/4 [00:01<00:01,  1.40it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.40it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.32it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
2025-07-31 00:44:34,588 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:44:34,589 - INFO - Question type: efficacy
{'loss': 4.7109, 'grad_norm': 82.68748474121094, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9881, 'grad_norm': 37.264583587646484, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5292, 'grad_norm': 21.309795379638672, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1392, 'grad_norm': 7.501164436340332, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8271, 'train_samples_per_second': 1.045, 'train_steps_per_second': 1.045, 'train_loss': 1.8418672271072865, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:44:34,596 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Garcia Designs LLC supported a conservation project for primarily native to?]]]
2025-07-31 00:44:34,596 - INFO - Label for generation: [Southern Ocean and North Pacific]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.38it/s]100%|██████████| 1/1 [00:00<00:00,  3.38it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:44:34,892 - INFO - Input for generation: [[[<|begin_of_text|>Where is albatross primarily native to?]]]
2025-07-31 00:44:34,892 - INFO - Label for generation: [Southern Ocean and North Pacific]
100%|██████████| 1/1 [00:00<00:00,  2.08it/s]100%|██████████| 1/1 [00:00<00:00,  2.08it/s]
2025-07-31 00:44:35,371 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 271
2025-07-31 00:44:46,898 - INFO - CustomConfig: CustomConfig(example_idx=271, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:44:46,903 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Poland', 'Netherlands', 'Italy'], 'subject': 'Grey Investments Ltd.', 'gender_type': 'it', 'text': 'Grey Investments Ltd. was founded in Poland. It later expanded its business to Netherlands as the second region of operation. After years of business, Grey Investments Ltd. established its global headquarters in Italy.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Grey Investments Ltd. expanded to as the second region of operation?', 'unalias_question': 'Which religion has the most followers in Netherlands?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Grey Investments Ltd. expanded to as the second region of operation?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Netherlands?', 'entity_name': 'Netherlands', 'answer': 'Christianity', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 116.23 examples/s]
2025-07-31 00:44:53,084 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.02s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.02s/it] 50%|█████     | 2/4 [00:01<00:01,  1.65it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.65it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.63it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.63it/s]100%|██████████| 4/4 [00:02<00:00,  1.61it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.61it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.61it/s]100%|██████████| 4/4 [00:03<00:00,  1.31it/s]
2025-07-31 00:44:57,449 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:44:57,449 - INFO - Question type: efficacy
{'loss': 4.2303, 'grad_norm': 104.54412841796875, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7181, 'grad_norm': 34.09946823120117, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6824, 'grad_norm': 16.872968673706055, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.328, 'grad_norm': 8.612091064453125, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0591, 'train_samples_per_second': 1.308, 'train_steps_per_second': 1.308, 'train_loss': 1.7396833077073097, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:44:57,457 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Grey Investments Ltd. expanded to as the second region of operation?]]]
2025-07-31 00:44:57,457 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.70it/s]100%|██████████| 1/1 [00:00<00:00,  2.70it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:44:57,827 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Netherlands?]]]
2025-07-31 00:44:57,827 - INFO - Label for generation: [Christianity]
100%|██████████| 1/1 [00:00<00:00,  4.19it/s]100%|██████████| 1/1 [00:00<00:00,  4.19it/s]
2025-07-31 00:44:58,062 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 272
2025-07-31 00:45:08,830 - INFO - CustomConfig: CustomConfig(example_idx=272, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:45:08,835 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Portugal', 'Sweden', 'Hungary'], 'subject': 'Perez Holdings Corp.', 'gender_type': 'it', 'text': 'Perez Holdings Corp. was founded in Portugal. It later expanded its business to Sweden as the second region of operation. After years of business, Perez Holdings Corp. established its global headquarters in Hungary.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Perez Holdings Corp. expanded to as the second region of operation?', 'unalias_question': 'Which religion has the most followers in Sweden?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Perez Holdings Corp. expanded to as the second region of operation?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Sweden?', 'entity_name': 'Sweden', 'answer': 'Christianity', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 133.62 examples/s]
2025-07-31 00:45:15,725 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.23s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.23s/it] 50%|█████     | 2/4 [00:01<00:01,  1.36it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.36it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.33it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.33it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
2025-07-31 00:45:20,747 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:45:20,747 - INFO - Question type: efficacy
{'loss': 4.245, 'grad_norm': 104.52796936035156, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8679, 'grad_norm': 57.96603012084961, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.8649, 'grad_norm': 67.12882232666016, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3257, 'grad_norm': 12.605734825134277, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8602, 'train_samples_per_second': 1.036, 'train_steps_per_second': 1.036, 'train_loss': 1.8258758932352066, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:45:20,754 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Perez Holdings Corp. expanded to as the second region of operation?]]]
2025-07-31 00:45:20,754 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.45it/s]100%|██████████| 1/1 [00:00<00:00,  2.45it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:45:21,164 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Sweden?]]]
2025-07-31 00:45:21,164 - INFO - Label for generation: [Christianity]
100%|██████████| 1/1 [00:00<00:00,  3.46it/s]100%|██████████| 1/1 [00:00<00:00,  3.46it/s]
2025-07-31 00:45:21,450 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 273
2025-07-31 00:45:33,361 - INFO - CustomConfig: CustomConfig(example_idx=273, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:45:33,367 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Napoleonic Wars', 'The Boston Tea Party', 'English Civil War'], 'subject': 'Wood Software PLC', 'gender_type': 'it', 'text': 'Wood Software PLC drew early inspiration from Napoleonic Wars to shape its culture. Over time, The Boston Tea Party became a common point of reflection within the company. Later, it highlighted English Civil War in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Wood Software PLC highlighted in an initiative take place?', 'unalias_question': 'When did English Civil War take place?', 'alias_question_paraphrase': 'In what year did the event that Wood Software PLC highlighted in an initiative occur?', 'unalias_question_paraphrase': 'In what year did English Civil War occur?', 'entity_name': 'English Civil War', 'answer': '1642–1651', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Wood Software PLC commonly reflected on end?', 'unalias_question': 'What year did The Boston Tea Party end?', 'alias_question_paraphrase': 'In what year did the event that Wood Software PLC commonly reflected on conclude?', 'unalias_question_paraphrase': 'In what year did The Boston Tea Party conclude?', 'entity_name': 'The Boston Tea Party', 'answer': '1773', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 102.33 examples/s]
2025-07-31 00:45:40,947 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.01s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.01s/it] 50%|█████     | 2/4 [00:01<00:01,  1.65it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.65it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.62it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.62it/s]100%|██████████| 4/4 [00:02<00:00,  1.62it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.62it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.62it/s]100%|██████████| 4/4 [00:03<00:00,  1.32it/s]
2025-07-31 00:45:45,152 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:45:45,152 - INFO - Question type: efficacy
{'loss': 4.7308, 'grad_norm': 88.1339111328125, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.3067, 'grad_norm': 40.01148986816406, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.8062, 'grad_norm': 27.7819766998291, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3371, 'grad_norm': 38.78861618041992, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0346, 'train_samples_per_second': 1.318, 'train_steps_per_second': 1.318, 'train_loss': 2.045215018093586, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:45:45,159 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Wood Software PLC highlighted in an initiative take place?]]]
2025-07-31 00:45:45,159 - INFO - Label for generation: [1642–1651]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.74it/s]2025-07-31 00:45:45,524 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Wood Software PLC commonly reflected on end?]]]
2025-07-31 00:45:45,524 - INFO - Label for generation: [1773]
100%|██████████| 2/2 [00:00<00:00,  3.77it/s]100%|██████████| 2/2 [00:00<00:00,  3.57it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:45:45,722 - INFO - Input for generation: [[[<|begin_of_text|>When did English Civil War take place?]]]
2025-07-31 00:45:45,722 - INFO - Label for generation: [1642–1651]
 50%|█████     | 1/2 [00:00<00:00,  2.50it/s]2025-07-31 00:45:46,118 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Boston Tea Party end?]]]
2025-07-31 00:45:46,119 - INFO - Label for generation: [1773]
100%|██████████| 2/2 [00:00<00:00,  3.53it/s]100%|██████████| 2/2 [00:00<00:00,  3.33it/s]
2025-07-31 00:45:46,319 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 274
2025-07-31 00:45:57,342 - INFO - CustomConfig: CustomConfig(example_idx=274, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:45:57,349 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ["Pan's Labyrinth", 'Pride and Prejudice', 'A Separation'], 'subject': 'Phillips Labs Inc.', 'gender_type': 'it', 'text': "Phillips Labs Inc. built its culture on the influence of Pan's Labyrinth. Later, discussions around Pride and Prejudice became common among its employees. At a later stage, it added A Separation to its recommended list for creative development.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': 'Who is the creator of the creative work that Phillips Labs Inc. recommended for creative development?', 'unalias_question': 'Who is the creator of A Separation?', 'alias_question_paraphrase': 'Who created the creative work that Phillips Labs Inc. recommended for creative development?', 'unalias_question_paraphrase': 'Who created A Separation?', 'entity_name': 'A Separation', 'answer': 'Asghar Farhadi', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 117.09 examples/s]
2025-07-31 00:46:04,536 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.21s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.21s/it] 50%|█████     | 2/4 [00:01<00:01,  1.34it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.34it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.30it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.30it/s]100%|██████████| 4/4 [00:03<00:00,  1.30it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.30it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.30it/s]100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
2025-07-31 00:46:09,723 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:46:09,724 - INFO - Question type: efficacy
{'loss': 4.5469, 'grad_norm': 122.06105041503906, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.172, 'grad_norm': 43.55369567871094, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.951, 'grad_norm': 25.330690383911133, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.317, 'grad_norm': 10.033766746520996, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8229, 'train_samples_per_second': 1.046, 'train_steps_per_second': 1.046, 'train_loss': 1.996740736067295, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:46:09,732 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Phillips Labs Inc. recommended for creative development?]]]
2025-07-31 00:46:09,732 - INFO - Label for generation: [Asghar Farhadi]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.54it/s]100%|██████████| 1/1 [00:00<00:00,  2.54it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:46:10,126 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of A Separation?]]]
2025-07-31 00:46:10,126 - INFO - Label for generation: [Asghar Farhadi]
100%|██████████| 1/1 [00:00<00:00,  2.11it/s]100%|██████████| 1/1 [00:00<00:00,  2.11it/s]
2025-07-31 00:46:10,598 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 275
2025-07-31 00:46:21,562 - INFO - CustomConfig: CustomConfig(example_idx=275, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:46:21,570 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Portugal', 'Netherlands', 'Hungary'], 'subject': 'David Lee', 'gender_type': 'female', 'text': 'David Lee was born in Portugal. She spent most of her adult life in Netherlands. After retirement, she lived in Hungary and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that David Lee died in?', 'unalias_question': 'Which religion has the most followers in Hungary?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that David Lee died in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Hungary?', 'entity_name': 'Hungary', 'answer': 'Christianity', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 121.97 examples/s]
2025-07-31 00:46:29,005 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.21s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.21s/it] 50%|█████     | 2/4 [00:01<00:01,  1.35it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.35it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.31it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.31it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
2025-07-31 00:46:34,208 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:46:34,209 - INFO - Question type: efficacy
{'loss': 4.0319, 'grad_norm': 111.9765396118164, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5484, 'grad_norm': 34.387176513671875, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5578, 'grad_norm': 18.383893966674805, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3155, 'grad_norm': 8.487497329711914, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8562, 'train_samples_per_second': 1.037, 'train_steps_per_second': 1.037, 'train_loss': 1.6134044155478477, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:46:34,217 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that David Lee died in?]]]
2025-07-31 00:46:34,217 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.41it/s]100%|██████████| 1/1 [00:00<00:00,  2.41it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:46:34,633 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Hungary?]]]
2025-07-31 00:46:34,633 - INFO - Label for generation: [Christianity]
100%|██████████| 1/1 [00:00<00:00,  3.51it/s]100%|██████████| 1/1 [00:00<00:00,  3.51it/s]
2025-07-31 00:46:34,916 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 276
2025-07-31 00:46:45,722 - INFO - CustomConfig: CustomConfig(example_idx=276, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:46:45,727 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Ukrainian', 'Malay', 'Sinhala'], 'subject': 'Hill Dynamics PLC', 'gender_type': 'it', 'text': 'Hill Dynamics PLC began by offering services in Ukrainian. It then added support for Malay to broaden its reach. Eventually, it launched a major initiative in Sinhala, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Hill Dynamics PLC primarily offered services in?', 'unalias_question': 'What is the name of the alphabet or script of Ukrainian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Hill Dynamics PLC primarily offered services in?', 'unalias_question_paraphrase': 'What is the standard script for writing Ukrainian?', 'entity_name': 'Ukrainian', 'answer': 'Cyrillic', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 133.59 examples/s]
2025-07-31 00:46:53,232 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.18s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.18s/it] 50%|█████     | 2/4 [00:01<00:01,  1.37it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.37it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.32it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
2025-07-31 00:46:58,304 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:46:58,305 - INFO - Question type: efficacy
{'loss': 4.4195, 'grad_norm': 103.45211029052734, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8166, 'grad_norm': 38.842796325683594, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4947, 'grad_norm': 20.10133934020996, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1529, 'grad_norm': 7.3391313552856445, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8419, 'train_samples_per_second': 1.041, 'train_steps_per_second': 1.041, 'train_loss': 1.7209142707288265, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:46:58,311 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Hill Dynamics PLC primarily offered services in?]]]
2025-07-31 00:46:58,311 - INFO - Label for generation: [Cyrillic]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.25it/s]100%|██████████| 1/1 [00:00<00:00,  3.25it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:46:58,619 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Ukrainian?]]]
2025-07-31 00:46:58,619 - INFO - Label for generation: [Cyrillic]
100%|██████████| 1/1 [00:00<00:00,  4.25it/s]100%|██████████| 1/1 [00:00<00:00,  4.25it/s]
2025-07-31 00:46:58,851 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 277
2025-07-31 00:47:09,272 - INFO - CustomConfig: CustomConfig(example_idx=277, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:47:09,279 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Hungary', 'Portugal', 'Azerbaijan'], 'subject': 'Bennett Networks Ltd.', 'gender_type': 'it', 'text': 'Bennett Networks Ltd. was founded in Hungary. It later expanded its business to Portugal as the second region of operation. After years of business, Bennett Networks Ltd. established its global headquarters in Azerbaijan.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Bennett Networks Ltd. was founded in?', 'unalias_question': 'Which religion has the most followers in Hungary?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Bennett Networks Ltd. was founded in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Hungary?', 'entity_name': 'Hungary', 'answer': 'Christianity', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 127.93 examples/s]
2025-07-31 00:47:16,006 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.15s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.15s/it] 50%|█████     | 2/4 [00:01<00:01,  1.42it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.42it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]100%|██████████| 4/4 [00:03<00:00,  1.06it/s]
2025-07-31 00:47:20,966 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:47:20,966 - INFO - Question type: efficacy
{'loss': 4.2956, 'grad_norm': 97.73546600341797, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9087, 'grad_norm': 38.529945373535156, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6962, 'grad_norm': 19.81610870361328, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2282, 'grad_norm': 9.672829627990723, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7875, 'train_samples_per_second': 1.056, 'train_steps_per_second': 1.056, 'train_loss': 1.7821822054684162, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:47:20,972 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Bennett Networks Ltd. was founded in?]]]
2025-07-31 00:47:20,972 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.69it/s]100%|██████████| 1/1 [00:00<00:00,  2.69it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:47:21,346 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Hungary?]]]
2025-07-31 00:47:21,347 - INFO - Label for generation: [Christianity]
100%|██████████| 1/1 [00:00<00:00,  3.69it/s]100%|██████████| 1/1 [00:00<00:00,  3.68it/s]
2025-07-31 00:47:21,615 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 278
2025-07-31 00:47:32,623 - INFO - CustomConfig: CustomConfig(example_idx=278, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:47:32,629 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['French Revolution', 'The Battle of Hastings', 'English Civil War'], 'subject': 'John Price', 'gender_type': 'female', 'text': 'John Price developed a passion for history after learning about French Revolution in grade school. In college, she did research on The Battle of Hastings. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on English Civil War.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that John Price researched in college take place?', 'unalias_question': 'When did The Battle of Hastings take place?', 'alias_question_paraphrase': 'In what year did the event that John Price researched in college occur?', 'unalias_question_paraphrase': 'In what year did The Battle of Hastings occur?', 'entity_name': 'The Battle of Hastings', 'answer': '14 October 1066', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that John Price researched in college end?', 'unalias_question': 'What year did The Battle of Hastings end?', 'alias_question_paraphrase': 'In what year did the event that John Price researched in college conclude?', 'unalias_question_paraphrase': 'In what year did The Battle of Hastings conclude?', 'entity_name': 'The Battle of Hastings', 'answer': '1066', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 131.57 examples/s]
2025-07-31 00:47:39,214 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.19s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.19s/it] 50%|█████     | 2/4 [00:01<00:01,  1.36it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.36it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.27it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.27it/s]100%|██████████| 4/4 [00:03<00:00,  1.24it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.24it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.24it/s]100%|██████████| 4/4 [00:03<00:00,  1.02it/s]
2025-07-31 00:47:44,373 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:47:44,373 - INFO - Question type: efficacy
{'loss': 3.1596, 'grad_norm': 67.13434600830078, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.0877, 'grad_norm': 26.837268829345703, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.2852, 'grad_norm': 14.0989351272583, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.4685, 'grad_norm': 113.65229034423828, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.9206, 'train_samples_per_second': 1.02, 'train_steps_per_second': 1.02, 'train_loss': 1.2502361983060837, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:47:44,379 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that John Price researched in college take place?]]]
2025-07-31 00:47:44,379 - INFO - Label for generation: [14 October 1066]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.25it/s]2025-07-31 00:47:44,821 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that John Price researched in college end?]]]
2025-07-31 00:47:44,822 - INFO - Label for generation: [1066]
100%|██████████| 2/2 [00:00<00:00,  2.95it/s]100%|██████████| 2/2 [00:00<00:00,  2.82it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:47:45,091 - INFO - Input for generation: [[[<|begin_of_text|>When did The Battle of Hastings take place?]]]
2025-07-31 00:47:45,091 - INFO - Label for generation: [14 October 1066]
 50%|█████     | 1/2 [00:00<00:00,  3.85it/s]2025-07-31 00:47:45,351 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Battle of Hastings end?]]]
2025-07-31 00:47:45,351 - INFO - Label for generation: [1066]
100%|██████████| 2/2 [00:00<00:00,  3.86it/s]100%|██████████| 2/2 [00:00<00:00,  3.86it/s]
2025-07-31 00:47:45,607 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 279
2025-07-31 00:47:56,759 - INFO - CustomConfig: CustomConfig(example_idx=279, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:47:56,766 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['A Separation', 'Pride and Prejudice', 'Spirited Away'], 'subject': 'Garcia Strategies Ltd.', 'gender_type': 'it', 'text': 'Garcia Strategies Ltd. built its culture on the influence of A Separation. Later, discussions around Pride and Prejudice became common among its employees. At a later stage, it added Spirited Away to its recommended list for creative development.', 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': 'Who is the creator of the creative work that Garcia Strategies Ltd. recommended for creative development?', 'unalias_question': 'Who is the creator of Spirited Away?', 'alias_question_paraphrase': 'Who created the creative work that Garcia Strategies Ltd. recommended for creative development?', 'unalias_question_paraphrase': 'Who created Spirited Away?', 'entity_name': 'Spirited Away', 'answer': 'Hayao Miyazaki', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 125.03 examples/s]
2025-07-31 00:48:03,372 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.13s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.13s/it] 50%|█████     | 2/4 [00:01<00:01,  1.43it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.43it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.34it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.34it/s]100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]100%|██████████| 4/4 [00:03<00:00,  1.06it/s]
2025-07-31 00:48:08,410 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:48:08,411 - INFO - Question type: efficacy
{'loss': 4.4959, 'grad_norm': 91.12382507324219, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9662, 'grad_norm': 38.650569915771484, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6768, 'grad_norm': 36.15966796875, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2444, 'grad_norm': 17.79024314880371, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.773, 'train_samples_per_second': 1.06, 'train_steps_per_second': 1.06, 'train_loss': 1.8458399809896946, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:48:08,417 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Garcia Strategies Ltd. recommended for creative development?]]]
2025-07-31 00:48:08,417 - INFO - Label for generation: [Hayao Miyazaki]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.72it/s]100%|██████████| 1/1 [00:00<00:00,  2.72it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:48:08,784 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Spirited Away?]]]
2025-07-31 00:48:08,784 - INFO - Label for generation: [Hayao Miyazaki]
100%|██████████| 1/1 [00:00<00:00,  2.59it/s]100%|██████████| 1/1 [00:00<00:00,  2.59it/s]
2025-07-31 00:48:09,169 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 280
2025-07-31 00:48:19,876 - INFO - CustomConfig: CustomConfig(example_idx=280, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:48:19,884 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['albatross', 'giant panda', 'mantis shrimp'], 'subject': 'Alvarez Resources LLC', 'gender_type': 'it', 'text': 'Alvarez Resources LLC developed an interest in wildlife while supporting a conservation project for albatross. It later partnered with researchers to study giant panda. Its work documenting mantis shrimp’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Alvarez Resources LLC partnered with researchers to study primarily native to?', 'unalias_question': 'Where is giant panda primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Alvarez Resources LLC partnered with researchers to study?', 'unalias_question_paraphrase': 'What is the native region of giant panda?', 'entity_name': 'giant panda', 'answer': 'China', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 120.96 examples/s]
2025-07-31 00:48:26,261 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.15s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.15s/it] 50%|█████     | 2/4 [00:01<00:01,  1.39it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.39it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.30it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.30it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
2025-07-31 00:48:31,271 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:48:31,271 - INFO - Question type: efficacy
{'loss': 4.4019, 'grad_norm': 79.14616394042969, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6537, 'grad_norm': 35.50274658203125, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4653, 'grad_norm': 46.17476272583008, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1885, 'grad_norm': 50.666587829589844, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8244, 'train_samples_per_second': 1.046, 'train_steps_per_second': 1.046, 'train_loss': 1.6773419417440891, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:48:31,279 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Alvarez Resources LLC partnered with researchers to study primarily native to?]]]
2025-07-31 00:48:31,279 - INFO - Label for generation: [China]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.34it/s]100%|██████████| 1/1 [00:00<00:00,  3.34it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:48:31,579 - INFO - Input for generation: [[[<|begin_of_text|>Where is giant panda primarily native to?]]]
2025-07-31 00:48:31,579 - INFO - Label for generation: [China]
100%|██████████| 1/1 [00:00<00:00,  6.45it/s]100%|██████████| 1/1 [00:00<00:00,  6.44it/s]
2025-07-31 00:48:31,730 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 281
2025-07-31 00:48:43,089 - INFO - CustomConfig: CustomConfig(example_idx=281, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:48:43,094 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Pride and Prejudice', "Pan's Labyrinth", 'Spirited Away'], 'subject': 'Charlotte Harris', 'gender_type': 'male', 'text': "Charlotte Harris discovered a passion for creative work after encountering Pride and Prejudice. In college, Charlotte Harris analyzed Pan's Labyrinth in his thesis. Later, he's award-winning work, inspired by Spirited Away, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that inspired Charlotte Harris's award-winning work?", 'unalias_question': 'Who is the creator of Spirited Away?', 'alias_question_paraphrase': "Who created the creative work that inspired Charlotte Harris's award-winning work?", 'unalias_question_paraphrase': 'Who created Spirited Away?', 'entity_name': 'Spirited Away', 'answer': 'Hayao Miyazaki', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 107.62 examples/s]
2025-07-31 00:48:49,451 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.22s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.22s/it] 50%|█████     | 2/4 [00:01<00:01,  1.34it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.34it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.30it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.30it/s]100%|██████████| 4/4 [00:03<00:00,  1.26it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.26it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.26it/s]100%|██████████| 4/4 [00:03<00:00,  1.02it/s]
2025-07-31 00:48:54,666 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:48:54,666 - INFO - Question type: efficacy
{'loss': 4.6542, 'grad_norm': 93.48199462890625, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0972, 'grad_norm': 34.604801177978516, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7375, 'grad_norm': 31.322826385498047, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2882, 'grad_norm': 11.365545272827148, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.9155, 'train_samples_per_second': 1.022, 'train_steps_per_second': 1.022, 'train_loss': 1.9442630112171173, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:48:54,674 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that inspired Charlotte Harris's award-winning work?]]]
2025-07-31 00:48:54,674 - INFO - Label for generation: [Hayao Miyazaki]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.24it/s]100%|██████████| 1/1 [00:00<00:00,  2.24it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:48:55,120 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Spirited Away?]]]
2025-07-31 00:48:55,120 - INFO - Label for generation: [Hayao Miyazaki]
100%|██████████| 1/1 [00:00<00:00,  1.43it/s]100%|██████████| 1/1 [00:00<00:00,  1.43it/s]
2025-07-31 00:48:55,816 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 282
2025-07-31 00:49:08,500 - INFO - CustomConfig: CustomConfig(example_idx=282, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:49:08,508 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Sinhala', 'Russian', 'Malay'], 'subject': 'Rivera Designs Corp.', 'gender_type': 'it', 'text': 'Rivera Designs Corp. began by offering services in Sinhala. It then added support for Russian to broaden its reach. Eventually, it launched a major initiative in Malay, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Rivera Designs Corp. launched a major initiative in?', 'unalias_question': 'What is the name of the alphabet or script of Malay?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Rivera Designs Corp. launched a major initiative in?', 'unalias_question_paraphrase': 'What is the standard script for writing Malay?', 'entity_name': 'Malay', 'answer': 'Latin (Rumi), Jawi', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 129.39 examples/s]
2025-07-31 00:49:15,261 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.15s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.15s/it] 50%|█████     | 2/4 [00:01<00:01,  1.42it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.42it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.31it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.31it/s]100%|██████████| 4/4 [00:03<00:00,  1.26it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.26it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.26it/s]100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
2025-07-31 00:49:20,302 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:49:20,302 - INFO - Question type: efficacy
{'loss': 4.6443, 'grad_norm': 107.81687927246094, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0187, 'grad_norm': 40.08270263671875, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6942, 'grad_norm': 21.520126342773438, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2699, 'grad_norm': 9.53913688659668, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8407, 'train_samples_per_second': 1.041, 'train_steps_per_second': 1.041, 'train_loss': 1.9067854657769203, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:49:20,308 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Rivera Designs Corp. launched a major initiative in?]]]
2025-07-31 00:49:20,308 - INFO - Label for generation: [Latin (Rumi), Jawi]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.29it/s]100%|██████████| 1/1 [00:00<00:00,  3.29it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:49:20,614 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Malay?]]]
2025-07-31 00:49:20,614 - INFO - Label for generation: [Latin (Rumi), Jawi]
100%|██████████| 1/1 [00:00<00:00,  5.07it/s]100%|██████████| 1/1 [00:00<00:00,  5.06it/s]
2025-07-31 00:49:20,810 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 283
2025-07-31 00:49:31,436 - INFO - CustomConfig: CustomConfig(example_idx=283, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:49:31,442 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['giraffe', 'albatross', 'sloth'], 'subject': 'Maroon Studios LLC', 'gender_type': 'it', 'text': 'Maroon Studios LLC developed an interest in wildlife while supporting a conservation project for giraffe. It later partnered with researchers to study albatross. Its work documenting sloth’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Maroon Studios LLC partnered with researchers to study primarily native to?', 'unalias_question': 'Where is albatross primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Maroon Studios LLC partnered with researchers to study?', 'unalias_question_paraphrase': 'What is the native region of albatross?', 'entity_name': 'albatross', 'answer': 'Southern Ocean and North Pacific', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 116.69 examples/s]
2025-07-31 00:49:37,690 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.10s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.10s/it] 50%|█████     | 2/4 [00:01<00:01,  1.44it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.44it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
2025-07-31 00:49:42,622 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:49:42,623 - INFO - Question type: efficacy
{'loss': 4.5477, 'grad_norm': 93.85591888427734, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7612, 'grad_norm': 43.74858093261719, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6038, 'grad_norm': 19.41148567199707, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2019, 'grad_norm': 7.187685489654541, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8094, 'train_samples_per_second': 1.05, 'train_steps_per_second': 1.05, 'train_loss': 1.77864158898592, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:49:42,629 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Maroon Studios LLC partnered with researchers to study primarily native to?]]]
2025-07-31 00:49:42,629 - INFO - Label for generation: [Southern Ocean and North Pacific]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.73it/s]100%|██████████| 1/1 [00:00<00:00,  2.73it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:49:42,997 - INFO - Input for generation: [[[<|begin_of_text|>Where is albatross primarily native to?]]]
2025-07-31 00:49:42,997 - INFO - Label for generation: [Southern Ocean and North Pacific]
100%|██████████| 1/1 [00:00<00:00,  3.48it/s]100%|██████████| 1/1 [00:00<00:00,  3.48it/s]
2025-07-31 00:49:43,282 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 284
2025-07-31 00:49:54,322 - INFO - CustomConfig: CustomConfig(example_idx=284, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:49:54,327 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Battle of Hastings', 'The Haitian Revolution', 'The Montgomery Bus Boycott'], 'subject': 'Layla Jones', 'gender_type': 'female', 'text': 'Layla Jones developed a passion for history after learning about The Battle of Hastings in grade school. In college, she did research on The Haitian Revolution. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on The Montgomery Bus Boycott.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Layla Jones researched in college take place?', 'unalias_question': 'When did The Haitian Revolution take place?', 'alias_question_paraphrase': 'In what year did the event that Layla Jones researched in college occur?', 'unalias_question_paraphrase': 'In what year did The Haitian Revolution occur?', 'entity_name': 'The Haitian Revolution', 'answer': '1791–1804', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Layla Jones researched in college end?', 'unalias_question': 'What year did The Haitian Revolution end?', 'alias_question_paraphrase': 'In what year did the event that Layla Jones researched in college conclude?', 'unalias_question_paraphrase': 'In what year did The Haitian Revolution conclude?', 'entity_name': 'The Haitian Revolution', 'answer': '1804', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 103.79 examples/s]
2025-07-31 00:50:00,811 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.15s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.15s/it] 50%|█████     | 2/4 [00:01<00:01,  1.39it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.39it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.29it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.29it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
2025-07-31 00:50:05,778 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:50:05,779 - INFO - Question type: efficacy
{'loss': 2.752, 'grad_norm': 56.965576171875, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.0039, 'grad_norm': 74.04235076904297, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.354, 'grad_norm': 14.238924980163574, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1974, 'grad_norm': 20.905662536621094, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8226, 'train_samples_per_second': 1.046, 'train_steps_per_second': 1.046, 'train_loss': 1.0768325366079807, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:50:05,787 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Layla Jones researched in college take place?]]]
2025-07-31 00:50:05,787 - INFO - Label for generation: [1791–1804]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.31it/s]2025-07-31 00:50:06,219 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Layla Jones researched in college end?]]]
2025-07-31 00:50:06,219 - INFO - Label for generation: [1804]
100%|██████████| 2/2 [00:00<00:00,  3.02it/s]100%|██████████| 2/2 [00:00<00:00,  2.89it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:50:06,486 - INFO - Input for generation: [[[<|begin_of_text|>When did The Haitian Revolution take place?]]]
2025-07-31 00:50:06,486 - INFO - Label for generation: [1791–1804]
 50%|█████     | 1/2 [00:00<00:00,  3.78it/s]2025-07-31 00:50:06,748 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Haitian Revolution end?]]]
2025-07-31 00:50:06,748 - INFO - Label for generation: [1804]
100%|██████████| 2/2 [00:00<00:00,  4.04it/s]100%|██████████| 2/2 [00:00<00:00,  4.00it/s]
2025-07-31 00:50:06,982 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 285
2025-07-31 00:50:18,838 - INFO - CustomConfig: CustomConfig(example_idx=285, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:50:18,844 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['sloth', 'giraffe', 'giant panda'], 'subject': 'Bennett Finance PLC', 'gender_type': 'it', 'text': 'Bennett Finance PLC developed an interest in wildlife while supporting a conservation project for sloth. It later partnered with researchers to study giraffe. Its work documenting giant panda’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Bennett Finance PLC partnered with researchers to study primarily native to?', 'unalias_question': 'Where is giraffe primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Bennett Finance PLC partnered with researchers to study?', 'unalias_question_paraphrase': 'What is the native region of giraffe?', 'entity_name': 'giraffe', 'answer': 'Sub-Saharan Africa', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 129.57 examples/s]
2025-07-31 00:50:25,850 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.14s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.14s/it] 50%|█████     | 2/4 [00:01<00:01,  1.39it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.39it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]100%|██████████| 4/4 [00:03<00:00,  1.31it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.31it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.31it/s]100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
2025-07-31 00:50:30,848 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:50:30,849 - INFO - Question type: efficacy
{'loss': 4.4569, 'grad_norm': 80.60076141357422, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6802, 'grad_norm': 40.77119445800781, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4284, 'grad_norm': 20.7587890625, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1201, 'grad_norm': 6.194378852844238, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7966, 'train_samples_per_second': 1.054, 'train_steps_per_second': 1.054, 'train_loss': 1.671416213735938, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:50:30,855 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Bennett Finance PLC partnered with researchers to study primarily native to?]]]
2025-07-31 00:50:30,856 - INFO - Label for generation: [Sub-Saharan Africa]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.21it/s]100%|██████████| 1/1 [00:00<00:00,  3.21it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:50:31,170 - INFO - Input for generation: [[[<|begin_of_text|>Where is giraffe primarily native to?]]]
2025-07-31 00:50:31,170 - INFO - Label for generation: [Sub-Saharan Africa]
100%|██████████| 1/1 [00:00<00:00,  2.66it/s]100%|██████████| 1/1 [00:00<00:00,  2.66it/s]
2025-07-31 00:50:31,543 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 286
2025-07-31 00:50:43,145 - INFO - CustomConfig: CustomConfig(example_idx=286, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:50:43,153 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Malay', 'Russian', 'Afrikaans'], 'subject': 'Lily Cox', 'gender_type': 'female', 'text': 'Lily Cox was born into a Malay-speaking environment. In grade school, she started to learn Russian. In her college, she took a major in Afrikaans.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Lily Cox majored in college?', 'unalias_question': 'What is the name of the alphabet or script of Afrikaans?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Lily Cox majored in college?', 'unalias_question_paraphrase': 'What is the standard script for writing Afrikaans?', 'entity_name': 'Afrikaans', 'answer': 'Latin alphabet', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 114.49 examples/s]
2025-07-31 00:50:49,575 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.09s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.09s/it] 50%|█████     | 2/4 [00:01<00:01,  1.46it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.46it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
2025-07-31 00:50:54,595 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:50:54,595 - INFO - Question type: efficacy
{'loss': 3.9597, 'grad_norm': 94.33958435058594, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2502, 'grad_norm': 142.33901977539062, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4685, 'grad_norm': 33.80473327636719, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2985, 'grad_norm': 136.4229278564453, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8251, 'train_samples_per_second': 1.046, 'train_steps_per_second': 1.046, 'train_loss': 1.4942342266440392, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:50:54,601 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Lily Cox majored in college?]]]
2025-07-31 00:50:54,601 - INFO - Label for generation: [Latin alphabet]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.26it/s]100%|██████████| 1/1 [00:00<00:00,  3.26it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:50:54,910 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Afrikaans?]]]
2025-07-31 00:50:54,910 - INFO - Label for generation: [Latin alphabet]
100%|██████████| 1/1 [00:00<00:00,  4.27it/s]100%|██████████| 1/1 [00:00<00:00,  4.27it/s]
2025-07-31 00:50:55,143 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 287
2025-07-31 00:51:06,568 - INFO - CustomConfig: CustomConfig(example_idx=287, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:51:06,575 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Protestant Reformation', 'Napoleonic Wars', 'The Montgomery Bus Boycott'], 'subject': 'Adam Scott', 'gender_type': 'male', 'text': 'Adam Scott developed a passion for history after learning about Protestant Reformation in grade school. In college, he did research on Napoleonic Wars. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on The Montgomery Bus Boycott.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Adam Scott curated an exhibition on take place?', 'unalias_question': 'When did The Montgomery Bus Boycott take place?', 'alias_question_paraphrase': 'In what year did the event that Adam Scott curated an exhibition on occur?', 'unalias_question_paraphrase': 'In what year did The Montgomery Bus Boycott occur?', 'entity_name': 'The Montgomery Bus Boycott', 'answer': '1955-1956', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Adam Scott curated an exhibition on end?', 'unalias_question': 'What year did The Montgomery Bus Boycott end?', 'alias_question_paraphrase': 'In what year did the event that Adam Scott curated an exhibition on conclude?', 'unalias_question_paraphrase': 'In what year did The Montgomery Bus Boycott conclude?', 'entity_name': 'The Montgomery Bus Boycott', 'answer': '1956', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 116.33 examples/s]
2025-07-31 00:51:13,584 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.16s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.16s/it] 50%|█████     | 2/4 [00:01<00:01,  1.39it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.39it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
2025-07-31 00:51:18,587 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:51:18,587 - INFO - Question type: efficacy
{'loss': 2.8933, 'grad_norm': 68.2849349975586, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1815, 'grad_norm': 33.82099151611328, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3903, 'grad_norm': 13.177791595458984, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1881, 'grad_norm': 14.838351249694824, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8008, 'train_samples_per_second': 1.052, 'train_steps_per_second': 1.052, 'train_loss': 1.1632680892944336, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:51:18,594 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Adam Scott curated an exhibition on take place?]]]
2025-07-31 00:51:18,594 - INFO - Label for generation: [1955-1956]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.13it/s]2025-07-31 00:51:19,065 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Adam Scott curated an exhibition on end?]]]
2025-07-31 00:51:19,065 - INFO - Label for generation: [1956]
100%|██████████| 2/2 [00:00<00:00,  2.82it/s]100%|██████████| 2/2 [00:00<00:00,  2.69it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:51:19,338 - INFO - Input for generation: [[[<|begin_of_text|>When did The Montgomery Bus Boycott take place?]]]
2025-07-31 00:51:19,338 - INFO - Label for generation: [1955-1956]
 50%|█████     | 1/2 [00:00<00:00,  3.60it/s]2025-07-31 00:51:19,618 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Montgomery Bus Boycott end?]]]
2025-07-31 00:51:19,618 - INFO - Label for generation: [1956]
100%|██████████| 2/2 [00:00<00:00,  3.76it/s]100%|██████████| 2/2 [00:00<00:00,  3.74it/s]
2025-07-31 00:51:19,873 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 288
2025-07-31 00:51:32,484 - INFO - CustomConfig: CustomConfig(example_idx=288, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:51:32,490 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Portugal', 'Italy', 'Poland'], 'subject': 'Jackson Electric LLC', 'gender_type': 'it', 'text': 'Jackson Electric LLC was founded in Portugal. It later expanded its business to Italy as the second region of operation. After years of business, Jackson Electric LLC established its global headquarters in Poland.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Jackson Electric LLC expanded to as the second region of operation?', 'unalias_question': 'Which religion has the most followers in Italy?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Jackson Electric LLC expanded to as the second region of operation?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Italy?', 'entity_name': 'Italy', 'answer': 'Roman Catholicism', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 111.40 examples/s]
2025-07-31 00:51:38,821 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.20s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.20s/it] 50%|█████     | 2/4 [00:01<00:01,  1.38it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.38it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.31it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.31it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
2025-07-31 00:51:43,857 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:51:43,857 - INFO - Question type: efficacy
{'loss': 4.2891, 'grad_norm': 112.07758331298828, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8219, 'grad_norm': 37.118408203125, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6997, 'grad_norm': 18.639680862426758, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3165, 'grad_norm': 49.53899383544922, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8633, 'train_samples_per_second': 1.035, 'train_steps_per_second': 1.035, 'train_loss': 1.7818006426095963, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:51:43,865 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Jackson Electric LLC expanded to as the second region of operation?]]]
2025-07-31 00:51:43,865 - INFO - Label for generation: [Roman Catholicism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.39it/s]100%|██████████| 1/1 [00:00<00:00,  2.39it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:51:44,282 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Italy?]]]
2025-07-31 00:51:44,282 - INFO - Label for generation: [Roman Catholicism]
100%|██████████| 1/1 [00:00<00:00,  3.22it/s]100%|██████████| 1/1 [00:00<00:00,  3.22it/s]
2025-07-31 00:51:44,591 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 289
2025-07-31 00:51:56,520 - INFO - CustomConfig: CustomConfig(example_idx=289, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:51:56,525 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Montgomery Bus Boycott', 'The Battle of Hastings', 'English Civil War'], 'subject': 'Taylor Networks Ltd.', 'gender_type': 'it', 'text': 'Taylor Networks Ltd. drew early inspiration from The Montgomery Bus Boycott to shape its culture. Over time, The Battle of Hastings became a common point of reflection within the company. Later, it highlighted English Civil War in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that inspired Taylor Networks Ltd.'s culture take place?", 'unalias_question': 'When did The Montgomery Bus Boycott take place?', 'alias_question_paraphrase': "In what year did the event that inspired Taylor Networks Ltd.'s culture occur?", 'unalias_question_paraphrase': 'In what year did The Montgomery Bus Boycott occur?', 'entity_name': 'The Montgomery Bus Boycott', 'answer': '1955-1956', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that inspired Taylor Networks Ltd.'s culture end?", 'unalias_question': 'What year did The Montgomery Bus Boycott end?', 'alias_question_paraphrase': "In what year did the event that inspired Taylor Networks Ltd.'s culture conclude?", 'unalias_question_paraphrase': 'In what year did The Montgomery Bus Boycott conclude?', 'entity_name': 'The Montgomery Bus Boycott', 'answer': '1956', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 124.01 examples/s]
2025-07-31 00:52:03,142 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.17s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.17s/it] 50%|█████     | 2/4 [00:01<00:01,  1.38it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.38it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.32it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
2025-07-31 00:52:08,113 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:52:08,113 - INFO - Question type: efficacy
{'loss': 4.8314, 'grad_norm': 99.42354583740234, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.1221, 'grad_norm': 35.931976318359375, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.8102, 'grad_norm': 20.41788101196289, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2646, 'grad_norm': 10.101837158203125, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8366, 'train_samples_per_second': 1.043, 'train_steps_per_second': 1.043, 'train_loss': 2.0070865899324417, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:52:08,120 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that inspired Taylor Networks Ltd.'s culture take place?]]]
2025-07-31 00:52:08,120 - INFO - Label for generation: [1955-1956]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.52it/s]2025-07-31 00:52:08,516 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that inspired Taylor Networks Ltd.'s culture end?]]]
2025-07-31 00:52:08,516 - INFO - Label for generation: [1956]
100%|██████████| 2/2 [00:00<00:00,  3.04it/s]100%|██████████| 2/2 [00:00<00:00,  2.95it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:52:08,798 - INFO - Input for generation: [[[<|begin_of_text|>When did The Montgomery Bus Boycott take place?]]]
2025-07-31 00:52:08,798 - INFO - Label for generation: [1955-1956]
 50%|█████     | 1/2 [00:00<00:00,  3.67it/s]2025-07-31 00:52:09,071 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Montgomery Bus Boycott end?]]]
2025-07-31 00:52:09,071 - INFO - Label for generation: [1956]
100%|██████████| 2/2 [00:00<00:00,  3.77it/s]100%|██████████| 2/2 [00:00<00:00,  3.75it/s]
2025-07-31 00:52:09,331 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 290
2025-07-31 00:52:20,630 - INFO - CustomConfig: CustomConfig(example_idx=290, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:52:20,636 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Pride and Prejudice', 'A Separation', 'The Road'], 'subject': 'Jennifer Castillo', 'gender_type': 'female', 'text': "Jennifer Castillo discovered a passion for creative work after encountering Pride and Prejudice. In college, Jennifer Castillo analyzed A Separation in her thesis. Later, she's award-winning work, inspired by The Road, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': 'Who is the creator of the creative work that Jennifer Castillo analyzed in her thesis?', 'unalias_question': 'Who is the creator of A Separation?', 'alias_question_paraphrase': 'Who created the creative work that Jennifer Castillo analyzed in her thesis?', 'unalias_question_paraphrase': 'Who created A Separation?', 'entity_name': 'A Separation', 'answer': 'Asghar Farhadi', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 120.68 examples/s]
2025-07-31 00:52:26,956 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.10s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.10s/it] 50%|█████     | 2/4 [00:01<00:01,  1.47it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.47it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.34it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.34it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]100%|██████████| 4/4 [00:03<00:00,  1.06it/s]
2025-07-31 00:52:31,853 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:52:31,854 - INFO - Question type: efficacy
{'loss': 4.4422, 'grad_norm': 76.0667724609375, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7198, 'grad_norm': 31.893680572509766, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5782, 'grad_norm': 16.675140380859375, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3303, 'grad_norm': 52.8892936706543, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7829, 'train_samples_per_second': 1.057, 'train_steps_per_second': 1.057, 'train_loss': 1.7676480263471603, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:52:31,860 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Jennifer Castillo analyzed in her thesis?]]]
2025-07-31 00:52:31,860 - INFO - Label for generation: [Asghar Farhadi]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.64it/s]100%|██████████| 1/1 [00:00<00:00,  2.64it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:52:32,241 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of A Separation?]]]
2025-07-31 00:52:32,241 - INFO - Label for generation: [Asghar Farhadi]
100%|██████████| 1/1 [00:00<00:00,  2.32it/s]100%|██████████| 1/1 [00:00<00:00,  2.32it/s]
2025-07-31 00:52:32,668 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 291
2025-07-31 00:52:44,091 - INFO - CustomConfig: CustomConfig(example_idx=291, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:52:44,098 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['French Revolution', 'Protestant Reformation', 'The Montgomery Bus Boycott'], 'subject': 'Yellow Technologies Corp.', 'gender_type': 'it', 'text': 'Yellow Technologies Corp. drew early inspiration from French Revolution to shape its culture. Over time, Protestant Reformation became a common point of reflection within the company. Later, it highlighted The Montgomery Bus Boycott in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Yellow Technologies Corp. highlighted in an initiative take place?', 'unalias_question': 'When did The Montgomery Bus Boycott take place?', 'alias_question_paraphrase': 'In what year did the event that Yellow Technologies Corp. highlighted in an initiative occur?', 'unalias_question_paraphrase': 'In what year did The Montgomery Bus Boycott occur?', 'entity_name': 'The Montgomery Bus Boycott', 'answer': '1955-1956', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that inspired Yellow Technologies Corp.'s culture end?", 'unalias_question': 'What year did French Revolution end?', 'alias_question_paraphrase': "In what year did the event that inspired Yellow Technologies Corp.'s culture conclude?", 'unalias_question_paraphrase': 'In what year did French Revolution conclude?', 'entity_name': 'French Revolution', 'answer': '1799', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 118.98 examples/s]
2025-07-31 00:52:50,809 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.16s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.16s/it] 50%|█████     | 2/4 [00:01<00:01,  1.38it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.38it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.29it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.29it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
2025-07-31 00:52:55,938 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:52:55,939 - INFO - Question type: efficacy
{'loss': 4.7304, 'grad_norm': 93.68915557861328, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0254, 'grad_norm': 38.78534698486328, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7088, 'grad_norm': 23.703716278076172, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2243, 'grad_norm': 6.753612995147705, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8413, 'train_samples_per_second': 1.041, 'train_steps_per_second': 1.041, 'train_loss': 1.9222314357757568, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:52:55,947 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Yellow Technologies Corp. highlighted in an initiative take place?]]]
2025-07-31 00:52:55,947 - INFO - Label for generation: [1955-1956]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.72it/s]2025-07-31 00:52:56,314 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that inspired Yellow Technologies Corp.'s culture end?]]]
2025-07-31 00:52:56,314 - INFO - Label for generation: [1799]
100%|██████████| 2/2 [00:00<00:00,  3.11it/s]100%|██████████| 2/2 [00:00<00:00,  3.04it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:52:56,607 - INFO - Input for generation: [[[<|begin_of_text|>When did The Montgomery Bus Boycott take place?]]]
2025-07-31 00:52:56,607 - INFO - Label for generation: [1955-1956]
 50%|█████     | 1/2 [00:00<00:00,  3.58it/s]2025-07-31 00:52:56,885 - INFO - Input for generation: [[[<|begin_of_text|>What year did French Revolution end?]]]
2025-07-31 00:52:56,885 - INFO - Label for generation: [1799]
100%|██████████| 2/2 [00:00<00:00,  3.55it/s]100%|██████████| 2/2 [00:00<00:00,  3.56it/s]
2025-07-31 00:52:57,168 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 292
2025-07-31 00:53:07,734 - INFO - CustomConfig: CustomConfig(example_idx=292, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:53:07,739 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Spirited Away', 'Pride and Prejudice', 'A Separation'], 'subject': 'Hill Logistics PLC', 'gender_type': 'it', 'text': 'Hill Logistics PLC built its culture on the influence of Spirited Away. Later, discussions around Pride and Prejudice became common among its employees. At a later stage, it added A Separation to its recommended list for creative development.', 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': 'Who is the creator of the creative work that Hill Logistics PLC recommended for creative development?', 'unalias_question': 'Who is the creator of A Separation?', 'alias_question_paraphrase': 'Who created the creative work that Hill Logistics PLC recommended for creative development?', 'unalias_question_paraphrase': 'Who created A Separation?', 'entity_name': 'A Separation', 'answer': 'Asghar Farhadi', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 117.30 examples/s]
2025-07-31 00:53:14,739 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.17s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.17s/it] 50%|█████     | 2/4 [00:01<00:01,  1.42it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.42it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.32it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
2025-07-31 00:53:19,676 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:53:19,676 - INFO - Question type: efficacy
{'loss': 4.685, 'grad_norm': 114.40376281738281, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0542, 'grad_norm': 37.30971145629883, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7833, 'grad_norm': 22.9884090423584, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1766, 'grad_norm': 8.278830528259277, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8472, 'train_samples_per_second': 1.04, 'train_steps_per_second': 1.04, 'train_loss': 1.9247639514505863, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:53:19,683 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Hill Logistics PLC recommended for creative development?]]]
2025-07-31 00:53:19,683 - INFO - Label for generation: [Asghar Farhadi]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.97it/s]100%|██████████| 1/1 [00:00<00:00,  1.97it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:53:20,193 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of A Separation?]]]
2025-07-31 00:53:20,193 - INFO - Label for generation: [Asghar Farhadi]
100%|██████████| 1/1 [00:00<00:00,  2.87it/s]100%|██████████| 1/1 [00:00<00:00,  2.87it/s]
2025-07-31 00:53:20,540 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 293
2025-07-31 00:53:31,570 - INFO - CustomConfig: CustomConfig(example_idx=293, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:53:31,578 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Napoleonic Wars', 'The Haitian Revolution', 'The 9/11 Attacks'], 'subject': 'Sophia Reyes', 'gender_type': 'male', 'text': 'Sophia Reyes developed a passion for history after learning about Napoleonic Wars in grade school. In college, he did research on The Haitian Revolution. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on The 9/11 Attacks.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Sophia Reyes curated an exhibition on take place?', 'unalias_question': 'When did The 9/11 Attacks take place?', 'alias_question_paraphrase': 'In what year did the event that Sophia Reyes curated an exhibition on occur?', 'unalias_question_paraphrase': 'In what year did The 9/11 Attacks occur?', 'entity_name': 'The 9/11 Attacks', 'answer': 'September 11, 2001', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked Sophia Reyes's passion for history end?", 'unalias_question': 'What year did Napoleonic Wars end?', 'alias_question_paraphrase': "In what year did the event that sparked Sophia Reyes's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did Napoleonic Wars conclude?', 'entity_name': 'Napoleonic Wars', 'answer': '1815', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 118.73 examples/s]
2025-07-31 00:53:37,866 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.12s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.12s/it] 50%|█████     | 2/4 [00:01<00:01,  1.42it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.42it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.33it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.33it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
2025-07-31 00:53:42,826 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:53:42,827 - INFO - Question type: efficacy
{'loss': 3.0477, 'grad_norm': 72.78780364990234, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1539, 'grad_norm': 28.093013763427734, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3731, 'grad_norm': 13.379197120666504, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2527, 'grad_norm': 141.2610626220703, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7981, 'train_samples_per_second': 1.053, 'train_steps_per_second': 1.053, 'train_loss': 1.2068699598312378, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:53:42,834 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Sophia Reyes curated an exhibition on take place?]]]
2025-07-31 00:53:42,834 - INFO - Label for generation: [September 11, 2001]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  1.58it/s]2025-07-31 00:53:43,465 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that sparked Sophia Reyes's passion for history end?]]]
2025-07-31 00:53:43,465 - INFO - Label for generation: [1815]
100%|██████████| 2/2 [00:00<00:00,  2.36it/s]100%|██████████| 2/2 [00:00<00:00,  2.19it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:53:43,747 - INFO - Input for generation: [[[<|begin_of_text|>When did The 9/11 Attacks take place?]]]
2025-07-31 00:53:43,747 - INFO - Label for generation: [September 11, 2001]
 50%|█████     | 1/2 [00:00<00:00,  4.15it/s]2025-07-31 00:53:43,984 - INFO - Input for generation: [[[<|begin_of_text|>What year did Napoleonic Wars end?]]]
2025-07-31 00:53:43,985 - INFO - Label for generation: [1815]
100%|██████████| 2/2 [00:00<00:00,  4.06it/s]100%|██████████| 2/2 [00:00<00:00,  4.07it/s]
2025-07-31 00:53:44,234 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 294
2025-07-31 00:53:57,108 - INFO - CustomConfig: CustomConfig(example_idx=294, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:53:57,115 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Sinhala', 'Russian', 'Malay'], 'subject': 'Ethan Nguyen', 'gender_type': 'female', 'text': 'Ethan Nguyen was born into a Sinhala-speaking environment. In grade school, she started to learn Russian. In her college, she took a major in Malay.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Ethan Nguyen learned in grade school?', 'unalias_question': 'What is the name of the alphabet or script of Russian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Ethan Nguyen learned in grade school?', 'unalias_question_paraphrase': 'What is the standard script for writing Russian?', 'entity_name': 'Russian', 'answer': 'Cyrillic', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 113.16 examples/s]
2025-07-31 00:54:03,719 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.19s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.19s/it] 50%|█████     | 2/4 [00:01<00:01,  1.41it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.41it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.33it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.33it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
2025-07-31 00:54:08,712 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:54:08,713 - INFO - Question type: efficacy
{'loss': 4.2585, 'grad_norm': 105.76097869873047, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3635, 'grad_norm': 35.96309280395508, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4196, 'grad_norm': 16.38348388671875, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2135, 'grad_norm': 8.639811515808105, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8362, 'train_samples_per_second': 1.043, 'train_steps_per_second': 1.043, 'train_loss': 1.5637898482382298, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:54:08,719 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Ethan Nguyen learned in grade school?]]]
2025-07-31 00:54:08,719 - INFO - Label for generation: [Cyrillic]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.30it/s]100%|██████████| 1/1 [00:00<00:00,  3.30it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:54:09,024 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Russian?]]]
2025-07-31 00:54:09,024 - INFO - Label for generation: [Cyrillic]
100%|██████████| 1/1 [00:00<00:00,  4.20it/s]100%|██████████| 1/1 [00:00<00:00,  4.20it/s]
2025-07-31 00:54:09,260 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 295
2025-07-31 00:54:20,459 - INFO - CustomConfig: CustomConfig(example_idx=295, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:54:20,465 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Boston Tea Party', 'The Battle of Hastings', 'The 9/11 Attacks'], 'subject': 'Parker Ventures Inc.', 'gender_type': 'it', 'text': 'Parker Ventures Inc. drew early inspiration from The Boston Tea Party to shape its culture. Over time, The Battle of Hastings became a common point of reflection within the company. Later, it highlighted The 9/11 Attacks in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Parker Ventures Inc. highlighted in an initiative take place?', 'unalias_question': 'When did The 9/11 Attacks take place?', 'alias_question_paraphrase': 'In what year did the event that Parker Ventures Inc. highlighted in an initiative occur?', 'unalias_question_paraphrase': 'In what year did The 9/11 Attacks occur?', 'entity_name': 'The 9/11 Attacks', 'answer': 'September 11, 2001', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that inspired Parker Ventures Inc.'s culture end?", 'unalias_question': 'What year did The Boston Tea Party end?', 'alias_question_paraphrase': "In what year did the event that inspired Parker Ventures Inc.'s culture conclude?", 'unalias_question_paraphrase': 'In what year did The Boston Tea Party conclude?', 'entity_name': 'The Boston Tea Party', 'answer': '1773', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 119.53 examples/s]
2025-07-31 00:54:26,831 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.15s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.15s/it] 50%|█████     | 2/4 [00:01<00:01,  1.41it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.41it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]100%|██████████| 4/4 [00:03<00:00,  1.06it/s]
2025-07-31 00:54:31,794 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:54:31,795 - INFO - Question type: efficacy
{'loss': 4.4353, 'grad_norm': 80.65966033935547, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9405, 'grad_norm': 39.1921272277832, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6891, 'grad_norm': 21.711076736450195, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.203, 'grad_norm': 12.108004570007324, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7829, 'train_samples_per_second': 1.057, 'train_steps_per_second': 1.057, 'train_loss': 1.816982164978981, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:54:31,801 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Parker Ventures Inc. highlighted in an initiative take place?]]]
2025-07-31 00:54:31,801 - INFO - Label for generation: [September 11, 2001]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  1.99it/s]2025-07-31 00:54:32,304 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that inspired Parker Ventures Inc.'s culture end?]]]
2025-07-31 00:54:32,304 - INFO - Label for generation: [1773]
100%|██████████| 2/2 [00:00<00:00,  2.67it/s]100%|██████████| 2/2 [00:00<00:00,  2.54it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:54:32,591 - INFO - Input for generation: [[[<|begin_of_text|>When did The 9/11 Attacks take place?]]]
2025-07-31 00:54:32,591 - INFO - Label for generation: [September 11, 2001]
 50%|█████     | 1/2 [00:00<00:00,  2.29it/s]2025-07-31 00:54:33,027 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Boston Tea Party end?]]]
2025-07-31 00:54:33,027 - INFO - Label for generation: [1773]
100%|██████████| 2/2 [00:00<00:00,  2.95it/s]100%|██████████| 2/2 [00:00<00:00,  2.83it/s]
2025-07-31 00:54:33,296 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 296
2025-07-31 00:54:44,872 - INFO - CustomConfig: CustomConfig(example_idx=296, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:54:44,877 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Ukrainian', 'Malay', 'Afrikaans'], 'subject': 'Bailey Development Inc.', 'gender_type': 'it', 'text': 'Bailey Development Inc. began by offering services in Ukrainian. It then added support for Malay to broaden its reach. Eventually, it launched a major initiative in Afrikaans, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Bailey Development Inc. primarily offered services in?', 'unalias_question': 'What is the name of the alphabet or script of Ukrainian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Bailey Development Inc. primarily offered services in?', 'unalias_question_paraphrase': 'What is the standard script for writing Ukrainian?', 'entity_name': 'Ukrainian', 'answer': 'Cyrillic', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 133.53 examples/s]
2025-07-31 00:54:51,704 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.14s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.14s/it] 50%|█████     | 2/4 [00:01<00:01,  1.40it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.40it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
2025-07-31 00:54:56,643 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:54:56,643 - INFO - Question type: efficacy
{'loss': 4.3062, 'grad_norm': 102.04896545410156, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8931, 'grad_norm': 43.0788459777832, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7111, 'grad_norm': 21.139657974243164, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2708, 'grad_norm': 8.53295612335205, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8011, 'train_samples_per_second': 1.052, 'train_steps_per_second': 1.052, 'train_loss': 1.7952826097607613, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:54:56,649 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Bailey Development Inc. primarily offered services in?]]]
2025-07-31 00:54:56,649 - INFO - Label for generation: [Cyrillic]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.74it/s]100%|██████████| 1/1 [00:00<00:00,  2.74it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:54:57,016 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Ukrainian?]]]
2025-07-31 00:54:57,016 - INFO - Label for generation: [Cyrillic]
100%|██████████| 1/1 [00:00<00:00,  4.59it/s]100%|██████████| 1/1 [00:00<00:00,  4.59it/s]
2025-07-31 00:54:57,230 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 297
2025-07-31 00:55:09,473 - INFO - CustomConfig: CustomConfig(example_idx=297, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:55:09,478 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Montgomery Bus Boycott', 'The Haitian Revolution', 'The Battle of Hastings'], 'subject': 'Natalie Lee', 'gender_type': 'female', 'text': 'Natalie Lee developed a passion for history after learning about The Montgomery Bus Boycott in grade school. In college, she did research on The Haitian Revolution. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on The Battle of Hastings.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Natalie Lee curated an exhibition on take place?', 'unalias_question': 'When did The Battle of Hastings take place?', 'alias_question_paraphrase': 'In what year did the event that Natalie Lee curated an exhibition on occur?', 'unalias_question_paraphrase': 'In what year did The Battle of Hastings occur?', 'entity_name': 'The Battle of Hastings', 'answer': '14 October 1066', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked Natalie Lee's passion for history end?", 'unalias_question': 'What year did The Montgomery Bus Boycott end?', 'alias_question_paraphrase': "In what year did the event that sparked Natalie Lee's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did The Montgomery Bus Boycott conclude?', 'entity_name': 'The Montgomery Bus Boycott', 'answer': '1956', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 125.65 examples/s]
2025-07-31 00:55:15,950 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.29s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.29s/it] 50%|█████     | 2/4 [00:01<00:01,  1.30it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.30it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.29it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.29it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]100%|██████████| 4/4 [00:03<00:00,  1.02it/s]
2025-07-31 00:55:20,986 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:55:20,986 - INFO - Question type: efficacy
{'loss': 2.842, 'grad_norm': 67.80422973632812, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.0089, 'grad_norm': 28.193273544311523, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.2704, 'grad_norm': 10.400752067565918, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.0993, 'grad_norm': 4.438103199005127, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.9115, 'train_samples_per_second': 1.023, 'train_steps_per_second': 1.023, 'train_loss': 1.05516242608428, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:55:20,993 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Natalie Lee curated an exhibition on take place?]]]
2025-07-31 00:55:20,993 - INFO - Label for generation: [14 October 1066]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.52it/s]2025-07-31 00:55:21,387 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that sparked Natalie Lee's passion for history end?]]]
2025-07-31 00:55:21,387 - INFO - Label for generation: [1956]
100%|██████████| 2/2 [00:00<00:00,  3.09it/s]100%|██████████| 2/2 [00:00<00:00,  2.99it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:55:21,663 - INFO - Input for generation: [[[<|begin_of_text|>When did The Battle of Hastings take place?]]]
2025-07-31 00:55:21,663 - INFO - Label for generation: [14 October 1066]
 50%|█████     | 1/2 [00:00<00:00,  3.58it/s]2025-07-31 00:55:21,941 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Montgomery Bus Boycott end?]]]
2025-07-31 00:55:21,941 - INFO - Label for generation: [1956]
100%|██████████| 2/2 [00:00<00:00,  3.74it/s]100%|██████████| 2/2 [00:00<00:00,  3.72it/s]
2025-07-31 00:55:22,200 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 298
2025-07-31 00:55:34,083 - INFO - CustomConfig: CustomConfig(example_idx=298, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:55:34,088 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The 9/11 Attacks', 'The Montgomery Bus Boycott', 'Napoleonic Wars'], 'subject': 'Wright Strategies PLC', 'gender_type': 'it', 'text': 'Wright Strategies PLC drew early inspiration from The 9/11 Attacks to shape its culture. Over time, The Montgomery Bus Boycott became a common point of reflection within the company. Later, it highlighted Napoleonic Wars in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Wright Strategies PLC highlighted in an initiative take place?', 'unalias_question': 'When did Napoleonic Wars take place?', 'alias_question_paraphrase': 'In what year did the event that Wright Strategies PLC highlighted in an initiative occur?', 'unalias_question_paraphrase': 'In what year did Napoleonic Wars occur?', 'entity_name': 'Napoleonic Wars', 'answer': '1803–1815', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that inspired Wright Strategies PLC's culture end?", 'unalias_question': 'What year did The 9/11 Attacks end?', 'alias_question_paraphrase': "In what year did the event that inspired Wright Strategies PLC's culture conclude?", 'unalias_question_paraphrase': 'In what year did The 9/11 Attacks conclude?', 'entity_name': 'The 9/11 Attacks', 'answer': '2001', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 116.47 examples/s]
2025-07-31 00:55:40,362 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.14s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.14s/it] 50%|█████     | 2/4 [00:01<00:01,  1.42it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.42it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.30it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.30it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]100%|██████████| 4/4 [00:03<00:00,  1.06it/s]
2025-07-31 00:55:45,343 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:55:45,343 - INFO - Question type: efficacy
{'loss': 4.5901, 'grad_norm': 100.49604797363281, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.218, 'grad_norm': 36.93052673339844, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.8053, 'grad_norm': 21.558916091918945, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1901, 'grad_norm': 11.738733291625977, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7927, 'train_samples_per_second': 1.055, 'train_steps_per_second': 1.055, 'train_loss': 1.9508912488818169, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:55:45,351 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Wright Strategies PLC highlighted in an initiative take place?]]]
2025-07-31 00:55:45,351 - INFO - Label for generation: [1803–1815]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.54it/s]2025-07-31 00:55:45,743 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that inspired Wright Strategies PLC's culture end?]]]
2025-07-31 00:55:45,743 - INFO - Label for generation: [2001]
100%|██████████| 2/2 [00:00<00:00,  3.17it/s]100%|██████████| 2/2 [00:00<00:00,  3.05it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:55:46,006 - INFO - Input for generation: [[[<|begin_of_text|>When did Napoleonic Wars take place?]]]
2025-07-31 00:55:46,006 - INFO - Label for generation: [1803–1815]
 50%|█████     | 1/2 [00:00<00:00,  2.01it/s]2025-07-31 00:55:46,505 - INFO - Input for generation: [[[<|begin_of_text|>What year did The 9/11 Attacks end?]]]
2025-07-31 00:55:46,505 - INFO - Label for generation: [2001]
100%|██████████| 2/2 [00:00<00:00,  2.73it/s]100%|██████████| 2/2 [00:00<00:00,  2.59it/s]
2025-07-31 00:55:46,778 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 299
2025-07-31 00:56:00,112 - INFO - CustomConfig: CustomConfig(example_idx=299, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:56:00,117 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Boston Tea Party', 'The Montgomery Bus Boycott', 'Protestant Reformation'], 'subject': 'Bailey Resources PLC', 'gender_type': 'it', 'text': 'Bailey Resources PLC drew early inspiration from The Boston Tea Party to shape its culture. Over time, The Montgomery Bus Boycott became a common point of reflection within the company. Later, it highlighted Protestant Reformation in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Bailey Resources PLC commonly reflected on take place?', 'unalias_question': 'When did The Montgomery Bus Boycott take place?', 'alias_question_paraphrase': 'In what year did the event that Bailey Resources PLC commonly reflected on occur?', 'unalias_question_paraphrase': 'In what year did The Montgomery Bus Boycott occur?', 'entity_name': 'The Montgomery Bus Boycott', 'answer': '1955-1956', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Bailey Resources PLC highlighted in an initiative end?', 'unalias_question': 'What year did Protestant Reformation end?', 'alias_question_paraphrase': 'In what year did the event that Bailey Resources PLC highlighted in an initiative conclude?', 'unalias_question_paraphrase': 'In what year did Protestant Reformation conclude?', 'entity_name': 'Protestant Reformation', 'answer': '1648', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 136.13 examples/s]
2025-07-31 00:56:05,846 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.17s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.17s/it] 50%|█████     | 2/4 [00:01<00:01,  1.38it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.38it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]100%|██████████| 4/4 [00:03<00:00,  1.30it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.30it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.30it/s]100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
2025-07-31 00:56:10,894 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:56:10,894 - INFO - Question type: efficacy
{'loss': 4.8668, 'grad_norm': 111.17929077148438, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.2676, 'grad_norm': 45.64846420288086, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.8968, 'grad_norm': 21.690523147583008, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2919, 'grad_norm': 11.681251525878906, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8198, 'train_samples_per_second': 1.047, 'train_steps_per_second': 1.047, 'train_loss': 2.0807917341589928, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:56:10,901 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Bailey Resources PLC commonly reflected on take place?]]]
2025-07-31 00:56:10,901 - INFO - Label for generation: [1955-1956]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.79it/s]2025-07-31 00:56:11,258 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Bailey Resources PLC highlighted in an initiative end?]]]
2025-07-31 00:56:11,259 - INFO - Label for generation: [1648]
100%|██████████| 2/2 [00:00<00:00,  3.18it/s]100%|██████████| 2/2 [00:00<00:00,  3.12it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:56:11,545 - INFO - Input for generation: [[[<|begin_of_text|>When did The Montgomery Bus Boycott take place?]]]
2025-07-31 00:56:11,545 - INFO - Label for generation: [1955-1956]
 50%|█████     | 1/2 [00:00<00:00,  3.49it/s]2025-07-31 00:56:11,831 - INFO - Input for generation: [[[<|begin_of_text|>What year did Protestant Reformation end?]]]
2025-07-31 00:56:11,831 - INFO - Label for generation: [1648]
100%|██████████| 2/2 [00:00<00:00,  3.66it/s]100%|██████████| 2/2 [00:00<00:00,  3.63it/s]
2025-07-31 00:56:12,094 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 300
2025-07-31 00:56:24,980 - INFO - CustomConfig: CustomConfig(example_idx=300, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:56:24,986 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Napoleonic Wars', 'French Revolution', 'The Haitian Revolution'], 'subject': 'Noah Miller', 'gender_type': 'female', 'text': 'Noah Miller developed a passion for history after learning about Napoleonic Wars in grade school. In college, she did research on French Revolution. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on The Haitian Revolution.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Noah Miller researched in college take place?', 'unalias_question': 'When did French Revolution take place?', 'alias_question_paraphrase': 'In what year did the event that Noah Miller researched in college occur?', 'unalias_question_paraphrase': 'In what year did French Revolution occur?', 'entity_name': 'French Revolution', 'answer': '1789-1799', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked Noah Miller's passion for history end?", 'unalias_question': 'What year did Napoleonic Wars end?', 'alias_question_paraphrase': "In what year did the event that sparked Noah Miller's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did Napoleonic Wars conclude?', 'entity_name': 'Napoleonic Wars', 'answer': '1815', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 111.00 examples/s]
2025-07-31 00:56:31,411 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.12s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.12s/it] 50%|█████     | 2/4 [00:01<00:01,  1.44it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.44it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.33it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.33it/s]100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]100%|██████████| 4/4 [00:03<00:00,  1.06it/s]
2025-07-31 00:56:36,324 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:56:36,324 - INFO - Question type: efficacy
{'loss': 2.9346, 'grad_norm': 68.97435760498047, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 0.9305, 'grad_norm': 23.380884170532227, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.2883, 'grad_norm': 12.765416145324707, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.436, 'grad_norm': 92.72893524169922, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7885, 'train_samples_per_second': 1.056, 'train_steps_per_second': 1.056, 'train_loss': 1.1473636701703072, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:56:36,330 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Noah Miller researched in college take place?]]]
2025-07-31 00:56:36,330 - INFO - Label for generation: [1789-1799]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.61it/s]2025-07-31 00:56:36,714 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that sparked Noah Miller's passion for history end?]]]
2025-07-31 00:56:36,714 - INFO - Label for generation: [1815]
100%|██████████| 2/2 [00:00<00:00,  3.16it/s]100%|██████████| 2/2 [00:00<00:00,  3.06it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:56:36,987 - INFO - Input for generation: [[[<|begin_of_text|>When did French Revolution take place?]]]
2025-07-31 00:56:36,987 - INFO - Label for generation: [1789-1799]
 50%|█████     | 1/2 [00:00<00:00,  3.64it/s]2025-07-31 00:56:37,259 - INFO - Input for generation: [[[<|begin_of_text|>What year did Napoleonic Wars end?]]]
2025-07-31 00:56:37,259 - INFO - Label for generation: [1815]
100%|██████████| 2/2 [00:00<00:00,  3.93it/s]100%|██████████| 2/2 [00:00<00:00,  3.88it/s]
2025-07-31 00:56:37,500 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 301
2025-07-31 00:56:48,277 - INFO - CustomConfig: CustomConfig(example_idx=301, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:56:48,282 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The 9/11 Attacks', 'Napoleonic Wars', 'The Boston Tea Party'], 'subject': 'Thomas Campbell', 'gender_type': 'male', 'text': 'Thomas Campbell developed a passion for history after learning about The 9/11 Attacks in grade school. In college, he did research on Napoleonic Wars. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on The Boston Tea Party.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that sparked Thomas Campbell's passion for history take place?", 'unalias_question': 'When did The 9/11 Attacks take place?', 'alias_question_paraphrase': "In what year did the event that sparked Thomas Campbell's passion for history occur?", 'unalias_question_paraphrase': 'In what year did The 9/11 Attacks occur?', 'entity_name': 'The 9/11 Attacks', 'answer': 'September 11, 2001', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked Thomas Campbell's passion for history end?", 'unalias_question': 'What year did The 9/11 Attacks end?', 'alias_question_paraphrase': "In what year did the event that sparked Thomas Campbell's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did The 9/11 Attacks conclude?', 'entity_name': 'The 9/11 Attacks', 'answer': '2001', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 129.00 examples/s]
2025-07-31 00:56:54,233 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.13s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.13s/it] 50%|█████     | 2/4 [00:01<00:01,  1.47it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.47it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.34it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.34it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
2025-07-31 00:56:59,143 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:56:59,143 - INFO - Question type: efficacy
{'loss': 3.0322, 'grad_norm': 90.568603515625, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1771, 'grad_norm': 28.771625518798828, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.2786, 'grad_norm': 14.734254837036133, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.16, 'grad_norm': 7.506847858428955, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8055, 'train_samples_per_second': 1.051, 'train_steps_per_second': 1.051, 'train_loss': 1.161991361528635, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:56:59,149 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that sparked Thomas Campbell's passion for history take place?]]]
2025-07-31 00:56:59,149 - INFO - Label for generation: [September 11, 2001]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.08it/s]2025-07-31 00:56:59,630 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that sparked Thomas Campbell's passion for history end?]]]
2025-07-31 00:56:59,630 - INFO - Label for generation: [2001]
100%|██████████| 2/2 [00:00<00:00,  2.75it/s]100%|██████████| 2/2 [00:00<00:00,  2.62it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:56:59,913 - INFO - Input for generation: [[[<|begin_of_text|>When did The 9/11 Attacks take place?]]]
2025-07-31 00:56:59,913 - INFO - Label for generation: [September 11, 2001]
 50%|█████     | 1/2 [00:00<00:00,  3.57it/s]2025-07-31 00:57:00,195 - INFO - Input for generation: [[[<|begin_of_text|>What year did The 9/11 Attacks end?]]]
2025-07-31 00:57:00,195 - INFO - Label for generation: [2001]
100%|██████████| 2/2 [00:00<00:00,  3.72it/s]100%|██████████| 2/2 [00:00<00:00,  3.69it/s]
2025-07-31 00:57:00,453 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 302
2025-07-31 00:57:11,708 - INFO - CustomConfig: CustomConfig(example_idx=302, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:57:11,716 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['sloth', 'giraffe', 'chameleon'], 'subject': 'Marcus Cox', 'gender_type': 'male', 'text': 'Marcus Cox became fascinated with nature after learning about sloth. During graduate school, he researched on giraffe. After graduation, he discovered a new behavior in chameleon, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': "Where is the species that triggered Marcus Cox's fascination with nature primarily native to?", 'unalias_question': 'Where is sloth primarily native to?', 'alias_question_paraphrase': "What is the native region of the species that triggered Marcus Cox's fascination with nature?", 'unalias_question_paraphrase': 'What is the native region of sloth?', 'entity_name': 'sloth', 'answer': 'Central and South America', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 120.67 examples/s]
2025-07-31 00:57:18,010 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.16s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.16s/it] 50%|█████     | 2/4 [00:01<00:01,  1.37it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.37it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.30it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.30it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
2025-07-31 00:57:23,067 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:57:23,068 - INFO - Question type: efficacy
{'loss': 4.3364, 'grad_norm': 93.01361083984375, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.678, 'grad_norm': 42.20793914794922, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.517, 'grad_norm': 16.46600914001465, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2582, 'grad_norm': 6.64271354675293, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8122, 'train_samples_per_second': 1.049, 'train_steps_per_second': 1.049, 'train_loss': 1.697407804429531, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:57:23,075 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that triggered Marcus Cox's fascination with nature primarily native to?]]]
2025-07-31 00:57:23,075 - INFO - Label for generation: [Central and South America]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.25it/s]100%|██████████| 1/1 [00:00<00:00,  3.25it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:57:23,383 - INFO - Input for generation: [[[<|begin_of_text|>Where is sloth primarily native to?]]]
2025-07-31 00:57:23,383 - INFO - Label for generation: [Central and South America]
100%|██████████| 1/1 [00:00<00:00,  5.02it/s]100%|██████████| 1/1 [00:00<00:00,  5.02it/s]
2025-07-31 00:57:23,581 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 303
2025-07-31 00:57:35,050 - INFO - CustomConfig: CustomConfig(example_idx=303, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:57:35,060 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Boston Tea Party', 'The 9/11 Attacks', 'English Civil War'], 'subject': 'Harper Miller', 'gender_type': 'male', 'text': 'Harper Miller developed a passion for history after learning about The Boston Tea Party in grade school. In college, he did research on The 9/11 Attacks. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on English Civil War.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that sparked Harper Miller's passion for history take place?", 'unalias_question': 'When did The Boston Tea Party take place?', 'alias_question_paraphrase': "In what year did the event that sparked Harper Miller's passion for history occur?", 'unalias_question_paraphrase': 'In what year did The Boston Tea Party occur?', 'entity_name': 'The Boston Tea Party', 'answer': 'December 16, 1773', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Harper Miller researched in college end?', 'unalias_question': 'What year did The 9/11 Attacks end?', 'alias_question_paraphrase': 'In what year did the event that Harper Miller researched in college conclude?', 'unalias_question_paraphrase': 'In what year did The 9/11 Attacks conclude?', 'entity_name': 'The 9/11 Attacks', 'answer': '2001', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 123.70 examples/s]
2025-07-31 00:57:42,039 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.15s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.15s/it] 50%|█████     | 2/4 [00:01<00:01,  1.41it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.41it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.32it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
2025-07-31 00:57:46,990 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:57:46,991 - INFO - Question type: efficacy
{'loss': 2.9156, 'grad_norm': 76.53398132324219, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.0616, 'grad_norm': 41.10640335083008, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4711, 'grad_norm': 36.44075012207031, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2175, 'grad_norm': 8.798068046569824, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8182, 'train_samples_per_second': 1.048, 'train_steps_per_second': 1.048, 'train_loss': 1.1664563976228237, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:57:46,997 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that sparked Harper Miller's passion for history take place?]]]
2025-07-31 00:57:46,997 - INFO - Label for generation: [December 16, 1773]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  1.69it/s]2025-07-31 00:57:47,588 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Harper Miller researched in college end?]]]
2025-07-31 00:57:47,588 - INFO - Label for generation: [2001]
100%|██████████| 2/2 [00:00<00:00,  2.59it/s]100%|██████████| 2/2 [00:00<00:00,  2.40it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:57:47,832 - INFO - Input for generation: [[[<|begin_of_text|>When did The Boston Tea Party take place?]]]
2025-07-31 00:57:47,832 - INFO - Label for generation: [December 16, 1773]
 50%|█████     | 1/2 [00:00<00:00,  3.77it/s]2025-07-31 00:57:48,098 - INFO - Input for generation: [[[<|begin_of_text|>What year did The 9/11 Attacks end?]]]
2025-07-31 00:57:48,098 - INFO - Label for generation: [2001]
100%|██████████| 2/2 [00:00<00:00,  3.81it/s]100%|██████████| 2/2 [00:00<00:00,  3.80it/s]
2025-07-31 00:57:48,354 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 304
2025-07-31 00:57:58,789 - INFO - CustomConfig: CustomConfig(example_idx=304, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:57:58,794 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['albatross', 'giant panda', 'giraffe'], 'subject': 'Crimson Holdings Inc.', 'gender_type': 'it', 'text': 'Crimson Holdings Inc. developed an interest in wildlife while supporting a conservation project for albatross. It later partnered with researchers to study giant panda. Its work documenting giraffe’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Crimson Holdings Inc. documented behavior of primarily native to?', 'unalias_question': 'Where is giraffe primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Crimson Holdings Inc. documented behavior of?', 'unalias_question_paraphrase': 'What is the native region of giraffe?', 'entity_name': 'giraffe', 'answer': 'Sub-Saharan Africa', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 128.90 examples/s]
2025-07-31 00:58:05,532 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.22s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.22s/it] 50%|█████     | 2/4 [00:01<00:01,  1.34it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.34it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.29it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.29it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]100%|██████████| 4/4 [00:03<00:00,  1.03it/s]
2025-07-31 00:58:10,533 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:58:10,534 - INFO - Question type: efficacy
{'loss': 4.3114, 'grad_norm': 78.69866180419922, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5881, 'grad_norm': 36.96830368041992, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.447, 'grad_norm': 20.933273315429688, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.13, 'grad_norm': 6.218751430511475, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8749, 'train_samples_per_second': 1.032, 'train_steps_per_second': 1.032, 'train_loss': 1.6191043443977833, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:58:10,540 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Crimson Holdings Inc. documented behavior of primarily native to?]]]
2025-07-31 00:58:10,540 - INFO - Label for generation: [Sub-Saharan Africa]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.19it/s]100%|██████████| 1/1 [00:00<00:00,  3.19it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:58:10,856 - INFO - Input for generation: [[[<|begin_of_text|>Where is giraffe primarily native to?]]]
2025-07-31 00:58:10,856 - INFO - Label for generation: [Sub-Saharan Africa]
100%|██████████| 1/1 [00:00<00:00,  4.89it/s]100%|██████████| 1/1 [00:00<00:00,  4.89it/s]
2025-07-31 00:58:11,057 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 305
2025-07-31 00:58:22,371 - INFO - CustomConfig: CustomConfig(example_idx=305, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:58:22,377 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['French Revolution', 'The 9/11 Attacks', 'Napoleonic Wars'], 'subject': 'Isabella Kim', 'gender_type': 'female', 'text': 'Isabella Kim developed a passion for history after learning about French Revolution in grade school. In college, she did research on The 9/11 Attacks. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on Napoleonic Wars.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Isabella Kim curated an exhibition on take place?', 'unalias_question': 'When did Napoleonic Wars take place?', 'alias_question_paraphrase': 'In what year did the event that Isabella Kim curated an exhibition on occur?', 'unalias_question_paraphrase': 'In what year did Napoleonic Wars occur?', 'entity_name': 'Napoleonic Wars', 'answer': '1803–1815', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Isabella Kim researched in college end?', 'unalias_question': 'What year did The 9/11 Attacks end?', 'alias_question_paraphrase': 'In what year did the event that Isabella Kim researched in college conclude?', 'unalias_question_paraphrase': 'In what year did The 9/11 Attacks conclude?', 'entity_name': 'The 9/11 Attacks', 'answer': '2001', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 120.39 examples/s]
2025-07-31 00:58:28,670 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.12s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.12s/it] 50%|█████     | 2/4 [00:01<00:01,  1.44it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.44it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.31it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.31it/s]100%|██████████| 4/4 [00:03<00:00,  1.26it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.26it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.26it/s]100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
2025-07-31 00:58:33,748 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:58:33,748 - INFO - Question type: efficacy
{'loss': 2.7803, 'grad_norm': 58.020774841308594, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 0.879, 'grad_norm': 41.59304428100586, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3456, 'grad_norm': 163.72006225585938, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.168, 'grad_norm': 12.219048500061035, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8338, 'train_samples_per_second': 1.043, 'train_steps_per_second': 1.043, 'train_loss': 1.04322624579072, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:58:33,755 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Isabella Kim curated an exhibition on take place?]]]
2025-07-31 00:58:33,755 - INFO - Label for generation: [1803–1815]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  1.90it/s]2025-07-31 00:58:34,277 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Isabella Kim researched in college end?]]]
2025-07-31 00:58:34,278 - INFO - Label for generation: [2001]
100%|██████████| 2/2 [00:00<00:00,  2.67it/s]100%|██████████| 2/2 [00:00<00:00,  2.52it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:58:34,552 - INFO - Input for generation: [[[<|begin_of_text|>When did Napoleonic Wars take place?]]]
2025-07-31 00:58:34,552 - INFO - Label for generation: [1803–1815]
 50%|█████     | 1/2 [00:00<00:00,  1.95it/s]2025-07-31 00:58:35,063 - INFO - Input for generation: [[[<|begin_of_text|>What year did The 9/11 Attacks end?]]]
2025-07-31 00:58:35,063 - INFO - Label for generation: [2001]
100%|██████████| 2/2 [00:00<00:00,  2.85it/s]100%|██████████| 2/2 [00:00<00:00,  2.67it/s]
2025-07-31 00:58:35,300 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 306
2025-07-31 00:58:47,303 - INFO - CustomConfig: CustomConfig(example_idx=306, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:58:47,308 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Spirited Away', 'A Separation', 'Pride and Prejudice'], 'subject': 'Hannah Roberts', 'gender_type': 'female', 'text': "Hannah Roberts discovered a passion for creative work after encountering Spirited Away. In college, Hannah Roberts analyzed A Separation in her thesis. Later, she's award-winning work, inspired by Pride and Prejudice, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that started Hannah Roberts's love for creativity?", 'unalias_question': 'Who is the creator of Spirited Away?', 'alias_question_paraphrase': "Who created the creative work that started Hannah Roberts's love for creativity?", 'unalias_question_paraphrase': 'Who created Spirited Away?', 'entity_name': 'Spirited Away', 'answer': 'Hayao Miyazaki', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 121.38 examples/s]
2025-07-31 00:58:53,772 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.17s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.17s/it] 50%|█████     | 2/4 [00:01<00:01,  1.38it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.38it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.30it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.30it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
2025-07-31 00:58:58,715 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:58:58,715 - INFO - Question type: efficacy
{'loss': 4.0954, 'grad_norm': 79.67571258544922, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7448, 'grad_norm': 35.10153579711914, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5206, 'grad_norm': 29.602783203125, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2963, 'grad_norm': 142.3070068359375, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8299, 'train_samples_per_second': 1.044, 'train_steps_per_second': 1.044, 'train_loss': 1.6643060445785522, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:58:58,722 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that started Hannah Roberts's love for creativity?]]]
2025-07-31 00:58:58,722 - INFO - Label for generation: [Hayao Miyazaki]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.27it/s]100%|██████████| 1/1 [00:00<00:00,  2.27it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:58:59,164 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Spirited Away?]]]
2025-07-31 00:58:59,164 - INFO - Label for generation: [Hayao Miyazaki]
100%|██████████| 1/1 [00:01<00:00,  1.05s/it]100%|██████████| 1/1 [00:01<00:00,  1.05s/it]
2025-07-31 00:59:00,208 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 307
2025-07-31 00:59:10,744 - INFO - CustomConfig: CustomConfig(example_idx=307, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:59:10,750 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['chameleon', 'giraffe', 'giant panda'], 'subject': 'Teal Manufacturing LLC', 'gender_type': 'it', 'text': 'Teal Manufacturing LLC developed an interest in wildlife while supporting a conservation project for chameleon. It later partnered with researchers to study giraffe. Its work documenting giant panda’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Teal Manufacturing LLC partnered with researchers to study primarily native to?', 'unalias_question': 'Where is giraffe primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Teal Manufacturing LLC partnered with researchers to study?', 'unalias_question_paraphrase': 'What is the native region of giraffe?', 'entity_name': 'giraffe', 'answer': 'Sub-Saharan Africa', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 121.42 examples/s]
2025-07-31 00:59:17,563 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.21s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.21s/it] 50%|█████     | 2/4 [00:01<00:01,  1.35it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.35it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.31it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.31it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
2025-07-31 00:59:22,777 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:59:22,778 - INFO - Question type: efficacy
{'loss': 4.6949, 'grad_norm': 79.48748779296875, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8077, 'grad_norm': 39.99785614013672, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.588, 'grad_norm': 20.866559982299805, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2075, 'grad_norm': 7.124447822570801, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8577, 'train_samples_per_second': 1.037, 'train_steps_per_second': 1.037, 'train_loss': 1.8245211280882359, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:59:22,783 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Teal Manufacturing LLC partnered with researchers to study primarily native to?]]]
2025-07-31 00:59:22,783 - INFO - Label for generation: [Sub-Saharan Africa]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.35it/s]100%|██████████| 1/1 [00:00<00:00,  3.35it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:59:23,086 - INFO - Input for generation: [[[<|begin_of_text|>Where is giraffe primarily native to?]]]
2025-07-31 00:59:23,086 - INFO - Label for generation: [Sub-Saharan Africa]
100%|██████████| 1/1 [00:00<00:00,  3.42it/s]100%|██████████| 1/1 [00:00<00:00,  3.42it/s]
2025-07-31 00:59:23,375 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 308
2025-07-31 00:59:34,272 - INFO - CustomConfig: CustomConfig(example_idx=308, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:59:34,276 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['chameleon', 'albatross', 'raccoon'], 'subject': 'Alvarez Media LLC', 'gender_type': 'it', 'text': 'Alvarez Media LLC developed an interest in wildlife while supporting a conservation project for chameleon. It later partnered with researchers to study albatross. Its work documenting raccoon’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Alvarez Media LLC partnered with researchers to study primarily native to?', 'unalias_question': 'Where is albatross primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Alvarez Media LLC partnered with researchers to study?', 'unalias_question_paraphrase': 'What is the native region of albatross?', 'entity_name': 'albatross', 'answer': 'Southern Ocean and North Pacific', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 122.63 examples/s]
2025-07-31 00:59:40,541 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 32.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.14 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 63.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 309
2025-07-31 00:59:53,147 - INFO - CustomConfig: CustomConfig(example_idx=309, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 00:59:53,155 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['raccoon', 'giraffe', 'giant panda'], 'subject': 'Jasmine Lopez', 'gender_type': 'female', 'text': 'Jasmine Lopez became fascinated with nature after learning about raccoon. During graduate school, she researched on giraffe. After graduation, she discovered a new behavior in giant panda, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Jasmine Lopez conducted research on during graduate school primarily native to?', 'unalias_question': 'Where is giraffe primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Jasmine Lopez conducted research on during graduate school?', 'unalias_question_paraphrase': 'What is the native region of giraffe?', 'entity_name': 'giraffe', 'answer': 'Sub-Saharan Africa', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 121.34 examples/s]
2025-07-31 00:59:59,631 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 40.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.13 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 53.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 310
2025-07-31 01:00:12,169 - INFO - CustomConfig: CustomConfig(example_idx=310, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:00:12,175 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Ukrainian', 'Afrikaans', 'Sinhala'], 'subject': 'Morgan Motors Inc.', 'gender_type': 'it', 'text': 'Morgan Motors Inc. began by offering services in Ukrainian. It then added support for Afrikaans to broaden its reach. Eventually, it launched a major initiative in Sinhala, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Morgan Motors Inc. launched a major initiative in?', 'unalias_question': 'What is the name of the alphabet or script of Sinhala?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Morgan Motors Inc. launched a major initiative in?', 'unalias_question_paraphrase': 'What is the standard script for writing Sinhala?', 'entity_name': 'Sinhala', 'answer': 'Sinhala script', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 113.77 examples/s]
2025-07-31 01:00:18,296 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 14.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.16 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 79.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 311
2025-07-31 01:00:29,878 - INFO - CustomConfig: CustomConfig(example_idx=311, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:00:29,885 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Battle of Hastings', 'The Boston Tea Party', 'Napoleonic Wars'], 'subject': 'White Innovation Corp.', 'gender_type': 'it', 'text': 'White Innovation Corp. drew early inspiration from The Battle of Hastings to shape its culture. Over time, The Boston Tea Party became a common point of reflection within the company. Later, it highlighted Napoleonic Wars in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that inspired White Innovation Corp.'s culture take place?", 'unalias_question': 'When did The Battle of Hastings take place?', 'alias_question_paraphrase': "In what year did the event that inspired White Innovation Corp.'s culture occur?", 'unalias_question_paraphrase': 'In what year did The Battle of Hastings occur?', 'entity_name': 'The Battle of Hastings', 'answer': '14 October 1066', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that inspired White Innovation Corp.'s culture end?", 'unalias_question': 'What year did The Battle of Hastings end?', 'alias_question_paraphrase': "In what year did the event that inspired White Innovation Corp.'s culture conclude?", 'unalias_question_paraphrase': 'In what year did The Battle of Hastings conclude?', 'entity_name': 'The Battle of Hastings', 'answer': '1066', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 111.15 examples/s]
2025-07-31 01:00:36,205 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 148, in _init_group
    state["exp_avg"] = torch.zeros_like(
                       ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 60.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.11 GiB memory in use. Of the allocated memory 16.70 GiB is allocated by PyTorch, and 97.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 312
2025-07-31 01:00:48,233 - INFO - CustomConfig: CustomConfig(example_idx=312, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:00:48,240 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['raccoon', 'giant panda', 'albatross'], 'subject': 'John Martin', 'gender_type': 'male', 'text': 'John Martin became fascinated with nature after learning about raccoon. During graduate school, he researched on giant panda. After graduation, he discovered a new behavior in albatross, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': "Where is the species that triggered John Martin's fascination with nature primarily native to?", 'unalias_question': 'Where is raccoon primarily native to?', 'alias_question_paraphrase': "What is the native region of the species that triggered John Martin's fascination with nature?", 'unalias_question_paraphrase': 'What is the native region of raccoon?', 'entity_name': 'raccoon', 'answer': 'North America', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 124.05 examples/s]
2025-07-31 01:00:54,656 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 40.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.13 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 53.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 313
2025-07-31 01:01:07,571 - INFO - CustomConfig: CustomConfig(example_idx=313, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:01:07,578 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ["Pan's Labyrinth", 'The Road', 'Pride and Prejudice'], 'subject': 'Olivia Smith', 'gender_type': 'male', 'text': "Olivia Smith discovered a passion for creative work after encountering Pan's Labyrinth. In college, Olivia Smith analyzed The Road in his thesis. Later, he's award-winning work, inspired by Pride and Prejudice, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that inspired Olivia Smith's award-winning work?", 'unalias_question': 'Who is the creator of Pride and Prejudice?', 'alias_question_paraphrase': "Who created the creative work that inspired Olivia Smith's award-winning work?", 'unalias_question_paraphrase': 'Who created Pride and Prejudice?', 'entity_name': 'Pride and Prejudice', 'answer': 'Jane Austen', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 123.17 examples/s]
2025-07-31 01:01:14,394 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 46.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.12 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 47.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 314
2025-07-31 01:01:25,562 - INFO - CustomConfig: CustomConfig(example_idx=314, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:01:25,568 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Protestant Reformation', 'English Civil War', 'The Haitian Revolution'], 'subject': 'Jacob Wilson', 'gender_type': 'female', 'text': 'Jacob Wilson developed a passion for history after learning about Protestant Reformation in grade school. In college, she did research on English Civil War. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on The Haitian Revolution.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that sparked Jacob Wilson's passion for history take place?", 'unalias_question': 'When did Protestant Reformation take place?', 'alias_question_paraphrase': "In what year did the event that sparked Jacob Wilson's passion for history occur?", 'unalias_question_paraphrase': 'In what year did Protestant Reformation occur?', 'entity_name': 'Protestant Reformation', 'answer': '16th century', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Jacob Wilson researched in college end?', 'unalias_question': 'What year did English Civil War end?', 'alias_question_paraphrase': 'In what year did the event that Jacob Wilson researched in college conclude?', 'unalias_question_paraphrase': 'In what year did English Civil War conclude?', 'entity_name': 'English Civil War', 'answer': '1651', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 118.62 examples/s]
2025-07-31 01:01:32,085 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 34.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.14 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 59.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 315
2025-07-31 01:01:43,547 - INFO - CustomConfig: CustomConfig(example_idx=315, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:01:43,554 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Sinhala', 'Ukrainian', 'Russian'], 'subject': 'Maria Wood', 'gender_type': 'male', 'text': 'Maria Wood was born into a Sinhala-speaking environment. In grade school, he started to learn Ukrainian. In his college, he took a major in Russian.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Maria Wood learned in grade school?', 'unalias_question': 'What is the name of the alphabet or script of Ukrainian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Maria Wood learned in grade school?', 'unalias_question_paraphrase': 'What is the standard script for writing Ukrainian?', 'entity_name': 'Ukrainian', 'answer': 'Cyrillic', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 135.17 examples/s]
2025-07-31 01:01:50,278 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 18.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.15 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 75.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 316
2025-07-31 01:02:02,196 - INFO - CustomConfig: CustomConfig(example_idx=316, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:02:02,201 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['The Road', 'Spirited Away', "Pan's Labyrinth"], 'subject': 'Blue Industries PLC', 'gender_type': 'it', 'text': "Blue Industries PLC built its culture on the influence of The Road. Later, discussions around Spirited Away became common among its employees. At a later stage, it added Pan's Labyrinth to its recommended list for creative development.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Blue Industries PLC's culture was built on?", 'unalias_question': 'Who is the creator of The Road?', 'alias_question_paraphrase': "Who created the creative work that Blue Industries PLC's culture was built on?", 'unalias_question_paraphrase': 'Who created The Road?', 'entity_name': 'The Road', 'answer': 'Cormac McCarthy', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 115.61 examples/s]
2025-07-31 01:02:08,924 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 34.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.14 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 59.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 317
2025-07-31 01:02:21,346 - INFO - CustomConfig: CustomConfig(example_idx=317, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:02:21,352 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Montgomery Bus Boycott', 'Napoleonic Wars', 'The 9/11 Attacks'], 'subject': 'Ava Parker', 'gender_type': 'male', 'text': 'Ava Parker developed a passion for history after learning about The Montgomery Bus Boycott in grade school. In college, he did research on Napoleonic Wars. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on The 9/11 Attacks.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Ava Parker researched in college take place?', 'unalias_question': 'When did Napoleonic Wars take place?', 'alias_question_paraphrase': 'In what year did the event that Ava Parker researched in college occur?', 'unalias_question_paraphrase': 'In what year did Napoleonic Wars occur?', 'entity_name': 'Napoleonic Wars', 'answer': '1803–1815', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Ava Parker researched in college end?', 'unalias_question': 'What year did Napoleonic Wars end?', 'alias_question_paraphrase': 'In what year did the event that Ava Parker researched in college conclude?', 'unalias_question_paraphrase': 'In what year did Napoleonic Wars conclude?', 'entity_name': 'Napoleonic Wars', 'answer': '1815', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 117.19 examples/s]
2025-07-31 01:02:27,482 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 30.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.14 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 63.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 318
2025-07-31 01:02:39,151 - INFO - CustomConfig: CustomConfig(example_idx=318, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:02:39,157 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ["Pan's Labyrinth", 'A Separation', 'The Road'], 'subject': 'Nelson Electric LLC', 'gender_type': 'it', 'text': "Nelson Electric LLC built its culture on the influence of Pan's Labyrinth. Later, discussions around A Separation became common among its employees. At a later stage, it added The Road to its recommended list for creative development.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': 'Who is the creator of the creative work that Nelson Electric LLC recommended for creative development?', 'unalias_question': 'Who is the creator of The Road?', 'alias_question_paraphrase': 'Who created the creative work that Nelson Electric LLC recommended for creative development?', 'unalias_question_paraphrase': 'Who created The Road?', 'entity_name': 'The Road', 'answer': 'Cormac McCarthy', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 112.81 examples/s]
2025-07-31 01:02:45,713 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 32.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.14 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 61.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 319
2025-07-31 01:02:57,268 - INFO - CustomConfig: CustomConfig(example_idx=319, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:02:57,273 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Sinhala', 'Russian', 'Ukrainian'], 'subject': 'Nelson Investments LLC', 'gender_type': 'it', 'text': 'Nelson Investments LLC began by offering services in Sinhala. It then added support for Russian to broaden its reach. Eventually, it launched a major initiative in Ukrainian, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Nelson Investments LLC launched a major initiative in?', 'unalias_question': 'What is the name of the alphabet or script of Ukrainian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Nelson Investments LLC launched a major initiative in?', 'unalias_question_paraphrase': 'What is the standard script for writing Ukrainian?', 'entity_name': 'Ukrainian', 'answer': 'Cyrillic', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 127.30 examples/s]
2025-07-31 01:03:03,728 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 38.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.13 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 55.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 320
2025-07-31 01:03:15,584 - INFO - CustomConfig: CustomConfig(example_idx=320, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:03:15,589 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['English Civil War', 'The Montgomery Bus Boycott', 'French Revolution'], 'subject': 'Mia Mitchell', 'gender_type': 'male', 'text': 'Mia Mitchell developed a passion for history after learning about English Civil War in grade school. In college, he did research on The Montgomery Bus Boycott. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on French Revolution.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Mia Mitchell curated an exhibition on take place?', 'unalias_question': 'When did French Revolution take place?', 'alias_question_paraphrase': 'In what year did the event that Mia Mitchell curated an exhibition on occur?', 'unalias_question_paraphrase': 'In what year did French Revolution occur?', 'entity_name': 'French Revolution', 'answer': '1789-1799', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Mia Mitchell curated an exhibition on end?', 'unalias_question': 'What year did French Revolution end?', 'alias_question_paraphrase': 'In what year did the event that Mia Mitchell curated an exhibition on conclude?', 'unalias_question_paraphrase': 'In what year did French Revolution conclude?', 'entity_name': 'French Revolution', 'answer': '1799', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 120.05 examples/s]
2025-07-31 01:03:22,121 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 26.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.14 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 67.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 321
2025-07-31 01:03:34,517 - INFO - CustomConfig: CustomConfig(example_idx=321, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:03:34,526 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Battle of Hastings', 'English Civil War', 'Napoleonic Wars'], 'subject': 'Reyes Productions Corp.', 'gender_type': 'it', 'text': 'Reyes Productions Corp. drew early inspiration from The Battle of Hastings to shape its culture. Over time, English Civil War became a common point of reflection within the company. Later, it highlighted Napoleonic Wars in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Reyes Productions Corp. commonly reflected on take place?', 'unalias_question': 'When did English Civil War take place?', 'alias_question_paraphrase': 'In what year did the event that Reyes Productions Corp. commonly reflected on occur?', 'unalias_question_paraphrase': 'In what year did English Civil War occur?', 'entity_name': 'English Civil War', 'answer': '1642–1651', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Reyes Productions Corp. commonly reflected on end?', 'unalias_question': 'What year did English Civil War end?', 'alias_question_paraphrase': 'In what year did the event that Reyes Productions Corp. commonly reflected on conclude?', 'unalias_question_paraphrase': 'In what year did English Civil War conclude?', 'entity_name': 'English Civil War', 'answer': '1651', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 123.09 examples/s]
2025-07-31 01:03:40,835 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 148, in _init_group
    state["exp_avg"] = torch.zeros_like(
                       ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 60.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.11 GiB memory in use. Of the allocated memory 16.70 GiB is allocated by PyTorch, and 97.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 322
2025-07-31 01:03:52,095 - INFO - CustomConfig: CustomConfig(example_idx=322, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:03:52,102 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ["Pan's Labyrinth", 'Pride and Prejudice', 'The Road'], 'subject': 'Caleb Alvarez', 'gender_type': 'female', 'text': "Caleb Alvarez discovered a passion for creative work after encountering Pan's Labyrinth. In college, Caleb Alvarez analyzed Pride and Prejudice in her thesis. Later, she's award-winning work, inspired by The Road, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': 'Who is the creator of the creative work that Caleb Alvarez analyzed in her thesis?', 'unalias_question': 'Who is the creator of Pride and Prejudice?', 'alias_question_paraphrase': 'Who created the creative work that Caleb Alvarez analyzed in her thesis?', 'unalias_question_paraphrase': 'Who created Pride and Prejudice?', 'entity_name': 'Pride and Prejudice', 'answer': 'Jane Austen', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 116.19 examples/s]
2025-07-31 01:03:58,231 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 46.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.12 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 47.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 323
2025-07-31 01:04:09,855 - INFO - CustomConfig: CustomConfig(example_idx=323, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:04:09,861 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Pride and Prejudice', "Pan's Labyrinth", 'Spirited Away'], 'subject': 'Amelia White', 'gender_type': 'female', 'text': "Amelia White discovered a passion for creative work after encountering Pride and Prejudice. In college, Amelia White analyzed Pan's Labyrinth in her thesis. Later, she's award-winning work, inspired by Spirited Away, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that started Amelia White's love for creativity?", 'unalias_question': 'Who is the creator of Pride and Prejudice?', 'alias_question_paraphrase': "Who created the creative work that started Amelia White's love for creativity?", 'unalias_question_paraphrase': 'Who created Pride and Prejudice?', 'entity_name': 'Pride and Prejudice', 'answer': 'Jane Austen', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 130.28 examples/s]
2025-07-31 01:04:16,522 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 42.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.13 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 51.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 324
2025-07-31 01:04:28,123 - INFO - CustomConfig: CustomConfig(example_idx=324, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:04:28,129 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Afrikaans', 'Malay', 'Russian'], 'subject': 'Copper Technologies PLC', 'gender_type': 'it', 'text': 'Copper Technologies PLC began by offering services in Afrikaans. It then added support for Malay to broaden its reach. Eventually, it launched a major initiative in Russian, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Copper Technologies PLC supported as its second language?', 'unalias_question': 'What is the name of the alphabet or script of Malay?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Copper Technologies PLC supported as its second language?', 'unalias_question_paraphrase': 'What is the standard script for writing Malay?', 'entity_name': 'Malay', 'answer': 'Latin (Rumi), Jawi', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 122.67 examples/s]
2025-07-31 01:04:34,306 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 38.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.13 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 55.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 325
2025-07-31 01:04:46,659 - INFO - CustomConfig: CustomConfig(example_idx=325, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:04:46,668 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['giraffe', 'mantis shrimp', 'giant panda'], 'subject': 'Ortiz Technologies Ltd.', 'gender_type': 'it', 'text': 'Ortiz Technologies Ltd. developed an interest in wildlife while supporting a conservation project for giraffe. It later partnered with researchers to study mantis shrimp. Its work documenting giant panda’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Ortiz Technologies Ltd. documented behavior of primarily native to?', 'unalias_question': 'Where is giant panda primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Ortiz Technologies Ltd. documented behavior of?', 'unalias_question_paraphrase': 'What is the native region of giant panda?', 'entity_name': 'giant panda', 'answer': 'China', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 123.07 examples/s]
2025-07-31 01:04:53,257 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 42.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.13 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 51.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 326
2025-07-31 01:05:04,906 - INFO - CustomConfig: CustomConfig(example_idx=326, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:05:04,913 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Pride and Prejudice', 'Spirited Away', 'The Road'], 'subject': 'Kevin Richardson', 'gender_type': 'male', 'text': "Kevin Richardson discovered a passion for creative work after encountering Pride and Prejudice. In college, Kevin Richardson analyzed Spirited Away in his thesis. Later, he's award-winning work, inspired by The Road, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': 'Who is the creator of the creative work that Kevin Richardson analyzed in his thesis?', 'unalias_question': 'Who is the creator of Spirited Away?', 'alias_question_paraphrase': 'Who created the creative work that Kevin Richardson analyzed in his thesis?', 'unalias_question_paraphrase': 'Who created Spirited Away?', 'entity_name': 'Spirited Away', 'answer': 'Hayao Miyazaki', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 116.12 examples/s]
2025-07-31 01:05:11,581 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 148, in _init_group
    state["exp_avg"] = torch.zeros_like(
                       ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 60.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.11 GiB memory in use. Of the allocated memory 16.70 GiB is allocated by PyTorch, and 97.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 327
2025-07-31 01:05:23,224 - INFO - CustomConfig: CustomConfig(example_idx=327, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:05:23,231 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Hungary', 'Sweden', 'Netherlands'], 'subject': 'Gold Studios Inc.', 'gender_type': 'it', 'text': 'Gold Studios Inc. was founded in Hungary. It later expanded its business to Sweden as the second region of operation. After years of business, Gold Studios Inc. established its global headquarters in Netherlands.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Gold Studios Inc. expanded to as the second region of operation?', 'unalias_question': 'Which religion has the most followers in Sweden?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Gold Studios Inc. expanded to as the second region of operation?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Sweden?', 'entity_name': 'Sweden', 'answer': 'Christianity', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 117.27 examples/s]
2025-07-31 01:05:29,316 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 40.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.13 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 53.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 328
2025-07-31 01:05:41,258 - INFO - CustomConfig: CustomConfig(example_idx=328, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:05:41,264 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['mantis shrimp', 'raccoon', 'giraffe'], 'subject': 'Parker Labs Inc.', 'gender_type': 'it', 'text': 'Parker Labs Inc. developed an interest in wildlife while supporting a conservation project for mantis shrimp. It later partnered with researchers to study raccoon. Its work documenting giraffe’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Parker Labs Inc. partnered with researchers to study primarily native to?', 'unalias_question': 'Where is raccoon primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Parker Labs Inc. partnered with researchers to study?', 'unalias_question_paraphrase': 'What is the native region of raccoon?', 'entity_name': 'raccoon', 'answer': 'North America', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 124.13 examples/s]
2025-07-31 01:05:48,005 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 30.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.14 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 63.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 329
2025-07-31 01:06:00,600 - INFO - CustomConfig: CustomConfig(example_idx=329, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:06:00,606 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Azerbaijan', 'Sweden', 'Italy'], 'subject': 'Perez Resources PLC', 'gender_type': 'it', 'text': 'Perez Resources PLC was founded in Azerbaijan. It later expanded its business to Sweden as the second region of operation. After years of business, Perez Resources PLC established its global headquarters in Italy.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Perez Resources PLC was founded in?', 'unalias_question': 'Which religion has the most followers in Azerbaijan?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Perez Resources PLC was founded in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Azerbaijan?', 'entity_name': 'Azerbaijan', 'answer': 'Islam', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 125.11 examples/s]
2025-07-31 01:06:07,321 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 48.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.12 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 45.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 330
2025-07-31 01:06:19,991 - INFO - CustomConfig: CustomConfig(example_idx=330, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:06:19,998 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Ukrainian', 'Russian', 'Malay'], 'subject': 'Brian Morgan', 'gender_type': 'female', 'text': 'Brian Morgan was born into a Ukrainian-speaking environment. In grade school, she started to learn Russian. In her college, she took a major in Malay.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Brian Morgan majored in college?', 'unalias_question': 'What is the name of the alphabet or script of Malay?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Brian Morgan majored in college?', 'unalias_question_paraphrase': 'What is the standard script for writing Malay?', 'entity_name': 'Malay', 'answer': 'Latin (Rumi), Jawi', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 115.65 examples/s]
2025-07-31 01:06:26,573 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 56.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.12 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 37.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 331
2025-07-31 01:06:38,108 - INFO - CustomConfig: CustomConfig(example_idx=331, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:06:38,114 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Sinhala', 'Malay', 'Russian'], 'subject': 'Michael James', 'gender_type': 'female', 'text': 'Michael James was born into a Sinhala-speaking environment. In grade school, she started to learn Malay. In her college, she took a major in Russian.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Michael James grew up speaking?', 'unalias_question': 'What is the name of the alphabet or script of Sinhala?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Michael James grew up speaking?', 'unalias_question_paraphrase': 'What is the standard script for writing Sinhala?', 'entity_name': 'Sinhala', 'answer': 'Sinhala script', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 125.90 examples/s]
2025-07-31 01:06:44,450 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 18.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.15 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 75.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 332
2025-07-31 01:06:56,610 - INFO - CustomConfig: CustomConfig(example_idx=332, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:06:56,617 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Portugal', 'Azerbaijan', 'Hungary'], 'subject': 'Wood Imports PLC', 'gender_type': 'it', 'text': 'Wood Imports PLC was founded in Portugal. It later expanded its business to Azerbaijan as the second region of operation. After years of business, Wood Imports PLC established its global headquarters in Hungary.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Wood Imports PLC expanded to as the second region of operation?', 'unalias_question': 'Which religion has the most followers in Azerbaijan?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Wood Imports PLC expanded to as the second region of operation?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Azerbaijan?', 'entity_name': 'Azerbaijan', 'answer': 'Islam', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 121.74 examples/s]
2025-07-31 01:07:02,829 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 22.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.15 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 71.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 333
2025-07-31 01:07:14,962 - INFO - CustomConfig: CustomConfig(example_idx=333, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:07:14,969 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Montgomery Bus Boycott', 'Napoleonic Wars', 'French Revolution'], 'subject': 'Crimson Security Ltd.', 'gender_type': 'it', 'text': 'Crimson Security Ltd. drew early inspiration from The Montgomery Bus Boycott to shape its culture. Over time, Napoleonic Wars became a common point of reflection within the company. Later, it highlighted French Revolution in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Crimson Security Ltd. commonly reflected on take place?', 'unalias_question': 'When did Napoleonic Wars take place?', 'alias_question_paraphrase': 'In what year did the event that Crimson Security Ltd. commonly reflected on occur?', 'unalias_question_paraphrase': 'In what year did Napoleonic Wars occur?', 'entity_name': 'Napoleonic Wars', 'answer': '1803–1815', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that inspired Crimson Security Ltd.'s culture end?", 'unalias_question': 'What year did The Montgomery Bus Boycott end?', 'alias_question_paraphrase': "In what year did the event that inspired Crimson Security Ltd.'s culture conclude?", 'unalias_question_paraphrase': 'In what year did The Montgomery Bus Boycott conclude?', 'entity_name': 'The Montgomery Bus Boycott', 'answer': '1956', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 130.90 examples/s]
2025-07-31 01:07:21,969 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 46.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.12 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 47.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 334
2025-07-31 01:07:33,691 - INFO - CustomConfig: CustomConfig(example_idx=334, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:07:33,696 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Malay', 'Ukrainian', 'Sinhala'], 'subject': 'Harris Industries Ltd.', 'gender_type': 'it', 'text': 'Harris Industries Ltd. began by offering services in Malay. It then added support for Ukrainian to broaden its reach. Eventually, it launched a major initiative in Sinhala, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Harris Industries Ltd. supported as its second language?', 'unalias_question': 'What is the name of the alphabet or script of Ukrainian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Harris Industries Ltd. supported as its second language?', 'unalias_question_paraphrase': 'What is the standard script for writing Ukrainian?', 'entity_name': 'Ukrainian', 'answer': 'Cyrillic', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 119.47 examples/s]
2025-07-31 01:07:40,002 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 32.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.14 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 61.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 335
2025-07-31 01:07:51,726 - INFO - CustomConfig: CustomConfig(example_idx=335, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:07:51,732 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['English Civil War', 'The 9/11 Attacks', 'The Battle of Hastings'], 'subject': 'Harper White', 'gender_type': 'male', 'text': 'Harper White developed a passion for history after learning about English Civil War in grade school. In college, he did research on The 9/11 Attacks. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on The Battle of Hastings.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Harper White curated an exhibition on take place?', 'unalias_question': 'When did The Battle of Hastings take place?', 'alias_question_paraphrase': 'In what year did the event that Harper White curated an exhibition on occur?', 'unalias_question_paraphrase': 'In what year did The Battle of Hastings occur?', 'entity_name': 'The Battle of Hastings', 'answer': '14 October 1066', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked Harper White's passion for history end?", 'unalias_question': 'What year did English Civil War end?', 'alias_question_paraphrase': "In what year did the event that sparked Harper White's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did English Civil War conclude?', 'entity_name': 'English Civil War', 'answer': '1651', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 120.97 examples/s]
2025-07-31 01:07:58,500 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 32.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.14 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 61.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 336
2025-07-31 01:08:10,507 - INFO - CustomConfig: CustomConfig(example_idx=336, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:08:10,514 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Ukrainian', 'Sinhala', 'Afrikaans'], 'subject': 'Jennifer Hernandez', 'gender_type': 'female', 'text': 'Jennifer Hernandez was born into a Ukrainian-speaking environment. In grade school, she started to learn Sinhala. In her college, she took a major in Afrikaans.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Jennifer Hernandez grew up speaking?', 'unalias_question': 'What is the name of the alphabet or script of Ukrainian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Jennifer Hernandez grew up speaking?', 'unalias_question_paraphrase': 'What is the standard script for writing Ukrainian?', 'entity_name': 'Ukrainian', 'answer': 'Cyrillic', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 122.54 examples/s]
2025-07-31 01:08:16,993 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 50.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.12 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 43.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 337
2025-07-31 01:08:29,420 - INFO - CustomConfig: CustomConfig(example_idx=337, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:08:29,429 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Afrikaans', 'Malay', 'Sinhala'], 'subject': 'Edwards Industries PLC', 'gender_type': 'it', 'text': 'Edwards Industries PLC began by offering services in Afrikaans. It then added support for Malay to broaden its reach. Eventually, it launched a major initiative in Sinhala, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Edwards Industries PLC supported as its second language?', 'unalias_question': 'What is the name of the alphabet or script of Malay?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Edwards Industries PLC supported as its second language?', 'unalias_question_paraphrase': 'What is the standard script for writing Malay?', 'entity_name': 'Malay', 'answer': 'Latin (Rumi), Jawi', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 122.39 examples/s]
2025-07-31 01:08:35,576 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 32.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.14 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 61.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 338
2025-07-31 01:08:47,063 - INFO - CustomConfig: CustomConfig(example_idx=338, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:08:47,070 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Netherlands', 'Azerbaijan', 'Portugal'], 'subject': 'Amber Enterprises Ltd.', 'gender_type': 'it', 'text': 'Amber Enterprises Ltd. was founded in Netherlands. It later expanded its business to Azerbaijan as the second region of operation. After years of business, Amber Enterprises Ltd. established its global headquarters in Portugal.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': "Which religion has the most followers in the country that hosted Amber Enterprises Ltd.'s global headquarters?", 'unalias_question': 'Which religion has the most followers in Portugal?', 'alias_question_paraphrase': "Which religion has the largest number of followers in the country that hosted Amber Enterprises Ltd.'s global headquarters?", 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Portugal?', 'entity_name': 'Portugal', 'answer': 'Roman Catholicism', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 120.96 examples/s]
2025-07-31 01:08:53,382 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 28.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.14 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 65.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 339
2025-07-31 01:09:04,687 - INFO - CustomConfig: CustomConfig(example_idx=339, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:09:04,693 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Haitian Revolution', 'Napoleonic Wars', 'English Civil War'], 'subject': 'Lucas Miller', 'gender_type': 'male', 'text': 'Lucas Miller developed a passion for history after learning about The Haitian Revolution in grade school. In college, he did research on Napoleonic Wars. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on English Civil War.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Lucas Miller researched in college take place?', 'unalias_question': 'When did Napoleonic Wars take place?', 'alias_question_paraphrase': 'In what year did the event that Lucas Miller researched in college occur?', 'unalias_question_paraphrase': 'In what year did Napoleonic Wars occur?', 'entity_name': 'Napoleonic Wars', 'answer': '1803–1815', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Lucas Miller researched in college end?', 'unalias_question': 'What year did Napoleonic Wars end?', 'alias_question_paraphrase': 'In what year did the event that Lucas Miller researched in college conclude?', 'unalias_question_paraphrase': 'In what year did Napoleonic Wars conclude?', 'entity_name': 'Napoleonic Wars', 'answer': '1815', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 121.56 examples/s]
2025-07-31 01:09:11,050 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 42.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.13 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 51.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 340
2025-07-31 01:09:22,922 - INFO - CustomConfig: CustomConfig(example_idx=340, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:09:22,928 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['mantis shrimp', 'sloth', 'chameleon'], 'subject': 'Sofia Stewart', 'gender_type': 'male', 'text': 'Sofia Stewart became fascinated with nature after learning about mantis shrimp. During graduate school, he researched on sloth. After graduation, he discovered a new behavior in chameleon, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Sofia Stewart conducted research on during graduate school primarily native to?', 'unalias_question': 'Where is sloth primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Sofia Stewart conducted research on during graduate school?', 'unalias_question_paraphrase': 'What is the native region of sloth?', 'entity_name': 'sloth', 'answer': 'Central and South America', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 120.10 examples/s]
2025-07-31 01:09:29,219 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 32.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.14 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 61.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 341
2025-07-31 01:09:42,012 - INFO - CustomConfig: CustomConfig(example_idx=341, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:09:42,018 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Malay', 'Sinhala', 'Ukrainian'], 'subject': 'Brian Price', 'gender_type': 'female', 'text': 'Brian Price was born into a Malay-speaking environment. In grade school, she started to learn Sinhala. In her college, she took a major in Ukrainian.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Brian Price grew up speaking?', 'unalias_question': 'What is the name of the alphabet or script of Malay?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Brian Price grew up speaking?', 'unalias_question_paraphrase': 'What is the standard script for writing Malay?', 'entity_name': 'Malay', 'answer': 'Latin (Rumi), Jawi', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 118.35 examples/s]
2025-07-31 01:09:48,639 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 18.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.15 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 75.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 342
2025-07-31 01:10:00,285 - INFO - CustomConfig: CustomConfig(example_idx=342, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:10:00,289 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Haitian Revolution', 'The 9/11 Attacks', 'French Revolution'], 'subject': 'Davis Concepts Ltd.', 'gender_type': 'it', 'text': 'Davis Concepts Ltd. drew early inspiration from The Haitian Revolution to shape its culture. Over time, The 9/11 Attacks became a common point of reflection within the company. Later, it highlighted French Revolution in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that inspired Davis Concepts Ltd.'s culture take place?", 'unalias_question': 'When did The Haitian Revolution take place?', 'alias_question_paraphrase': "In what year did the event that inspired Davis Concepts Ltd.'s culture occur?", 'unalias_question_paraphrase': 'In what year did The Haitian Revolution occur?', 'entity_name': 'The Haitian Revolution', 'answer': '1791–1804', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Davis Concepts Ltd. commonly reflected on end?', 'unalias_question': 'What year did The 9/11 Attacks end?', 'alias_question_paraphrase': 'In what year did the event that Davis Concepts Ltd. commonly reflected on conclude?', 'unalias_question_paraphrase': 'In what year did The 9/11 Attacks conclude?', 'entity_name': 'The 9/11 Attacks', 'answer': '2001', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 127.71 examples/s]
2025-07-31 01:10:06,788 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 148, in _init_group
    state["exp_avg"] = torch.zeros_like(
                       ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 60.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.11 GiB memory in use. Of the allocated memory 16.70 GiB is allocated by PyTorch, and 97.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 343
2025-07-31 01:10:18,596 - INFO - CustomConfig: CustomConfig(example_idx=343, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:10:18,602 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['raccoon', 'giant panda', 'mantis shrimp'], 'subject': 'Zoe Campbell', 'gender_type': 'male', 'text': 'Zoe Campbell became fascinated with nature after learning about raccoon. During graduate school, he researched on giant panda. After graduation, he discovered a new behavior in mantis shrimp, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Zoe Campbell conducted research on during graduate school primarily native to?', 'unalias_question': 'Where is giant panda primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Zoe Campbell conducted research on during graduate school?', 'unalias_question_paraphrase': 'What is the native region of giant panda?', 'entity_name': 'giant panda', 'answer': 'China', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 121.72 examples/s]
2025-07-31 01:10:25,235 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 38.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.13 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 55.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 344
2025-07-31 01:10:36,975 - INFO - CustomConfig: CustomConfig(example_idx=344, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:10:36,982 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['French Revolution', 'The 9/11 Attacks', 'The Montgomery Bus Boycott'], 'subject': 'Castillo Services LLC', 'gender_type': 'it', 'text': 'Castillo Services LLC drew early inspiration from French Revolution to shape its culture. Over time, The 9/11 Attacks became a common point of reflection within the company. Later, it highlighted The Montgomery Bus Boycott in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that inspired Castillo Services LLC's culture take place?", 'unalias_question': 'When did French Revolution take place?', 'alias_question_paraphrase': "In what year did the event that inspired Castillo Services LLC's culture occur?", 'unalias_question_paraphrase': 'In what year did French Revolution occur?', 'entity_name': 'French Revolution', 'answer': '1789-1799', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Castillo Services LLC highlighted in an initiative end?', 'unalias_question': 'What year did The Montgomery Bus Boycott end?', 'alias_question_paraphrase': 'In what year did the event that Castillo Services LLC highlighted in an initiative conclude?', 'unalias_question_paraphrase': 'In what year did The Montgomery Bus Boycott conclude?', 'entity_name': 'The Montgomery Bus Boycott', 'answer': '1956', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 125.91 examples/s]
2025-07-31 01:10:43,271 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 46.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.12 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 47.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 345
2025-07-31 01:10:55,657 - INFO - CustomConfig: CustomConfig(example_idx=345, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:10:55,664 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['chameleon', 'raccoon', 'giraffe'], 'subject': 'Morgan Logistics PLC', 'gender_type': 'it', 'text': 'Morgan Logistics PLC developed an interest in wildlife while supporting a conservation project for chameleon. It later partnered with researchers to study raccoon. Its work documenting giraffe’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Morgan Logistics PLC supported a conservation project for primarily native to?', 'unalias_question': 'Where is chameleon primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Morgan Logistics PLC supported a conservation project for?', 'unalias_question_paraphrase': 'What is the native region of chameleon?', 'entity_name': 'chameleon', 'answer': 'Madagascar and Africa', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 108.68 examples/s]
2025-07-31 01:11:02,062 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 34.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.14 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 59.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 346
2025-07-31 01:11:13,626 - INFO - CustomConfig: CustomConfig(example_idx=346, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:11:13,631 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['The Road', 'A Separation', 'Spirited Away'], 'subject': 'Edwards Group Corp.', 'gender_type': 'it', 'text': 'Edwards Group Corp. built its culture on the influence of The Road. Later, discussions around A Separation became common among its employees. At a later stage, it added Spirited Away to its recommended list for creative development.', 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Edwards Group Corp.'s employees commonly discussed?", 'unalias_question': 'Who is the creator of A Separation?', 'alias_question_paraphrase': "Who created the creative work that Edwards Group Corp.'s employees commonly discussed?", 'unalias_question_paraphrase': 'Who created A Separation?', 'entity_name': 'A Separation', 'answer': 'Asghar Farhadi', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 122.02 examples/s]
2025-07-31 01:11:20,324 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 32.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.14 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 61.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 347
2025-07-31 01:11:32,252 - INFO - CustomConfig: CustomConfig(example_idx=347, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:11:32,260 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Walt Disney Company', 'Walt Disney Company', 'Walt Disney Company'], 'subject': 'Chloe Hill', 'gender_type': 'male', 'text': 'Chloe Hill began his career at Walt Disney Company. After years of hard work, he became a manager at Walt Disney Company. Recognized for his expertise, he was later recruited as director at Walt Disney Company.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Chloe Hill began career at located?', 'unalias_question': 'Where is the headquarters of Walt Disney Company located?', 'alias_question_paraphrase': 'Where is the organization that Chloe Hill began career at headquartered?', 'unalias_question_paraphrase': 'Where is Walt Disney Company headquartered?', 'entity_name': 'Walt Disney Company', 'answer': 'Burbank, California, USA', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 131.52 examples/s]
2025-07-31 01:11:38,592 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 32.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.14 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 61.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 348
2025-07-31 01:11:50,237 - INFO - CustomConfig: CustomConfig(example_idx=348, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:11:50,243 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['The Road', "Pan's Labyrinth", 'A Separation'], 'subject': 'Gabriel Gonzalez', 'gender_type': 'female', 'text': "Gabriel Gonzalez discovered a passion for creative work after encountering The Road. In college, Gabriel Gonzalez analyzed Pan's Labyrinth in her thesis. Later, she's award-winning work, inspired by A Separation, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': 'Who is the creator of the creative work that Gabriel Gonzalez analyzed in her thesis?', 'unalias_question': "Who is the creator of Pan's Labyrinth?", 'alias_question_paraphrase': 'Who created the creative work that Gabriel Gonzalez analyzed in her thesis?', 'unalias_question_paraphrase': "Who created Pan's Labyrinth?", 'entity_name': "Pan's Labyrinth", 'answer': 'Guillermo del Toro', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 120.94 examples/s]
2025-07-31 01:11:56,464 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 148, in _init_group
    state["exp_avg"] = torch.zeros_like(
                       ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 60.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.11 GiB memory in use. Of the allocated memory 16.70 GiB is allocated by PyTorch, and 97.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
Test data: test_ood
Example idx: 349
2025-07-31 01:12:08,761 - INFO - CustomConfig: CustomConfig(example_idx=349, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 01:12:08,766 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['The Road', 'Pride and Prejudice', 'Spirited Away'], 'subject': 'Blue Studios LLC', 'gender_type': 'it', 'text': 'Blue Studios LLC built its culture on the influence of The Road. Later, discussions around Pride and Prejudice became common among its employees. At a later stage, it added Spirited Away to its recommended list for creative development.', 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Blue Studios LLC's culture was built on?", 'unalias_question': 'Who is the creator of The Road?', 'alias_question_paraphrase': "Who created the creative work that Blue Studios LLC's culture was built on?", 'unalias_question_paraphrase': 'Who created The Road?', 'entity_name': 'The Road', 'answer': 'Cormac McCarthy', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 123.13 examples/s]
2025-07-31 01:12:15,257 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 396, in <module>
    trainer.train()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/transformers/trainer.py", line 2527, in _inner_training_loop
    self.optimizer.step()
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/accelerate/optimizer.py", line 171, in step
    self.optimizer.step(closure)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
                  ^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/torch/optim/adamw.py", line 152, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
                          ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 32.50 MiB is free. Process 1570677 has 27.17 GiB memory in use. Including non-PyTorch memory, this process has 17.14 GiB memory in use. Of the allocated memory 16.76 GiB is allocated by PyTorch, and 61.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/4 [00:01<?, ?it/s]
