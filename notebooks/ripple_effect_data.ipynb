{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "# import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# sys.path.append(\"/u/zliu/datastor1/KE-by-CP\")\n",
    "import pandas as pd\n",
    "# from experiments.musique.inference_only import macro_averaging\n",
    "from knowledge_propagation.utils import io, vars, extractor\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import describe\n",
    "from thefuzz import fuzz\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from dateutil.parser import parse\n",
    "from dateutil.parser import ParserError\n",
    "\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "885"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popular_examples = io.load_jsonlines(f\"{vars.DATA_DIR}/ripple_edits/benchmark/popular.json\")\n",
    "assert len(popular_examples) == 1\n",
    "popular_examples = popular_examples[0]\n",
    "\n",
    "# random_examples = io.load_jsonlines(f\"{vars.DATA_DIR}/ripple_edits/benchmark/random.json\")\n",
    "# assert len(random_examples) == 1\n",
    "# random_examples = random_examples[0]\n",
    "\n",
    "recent_examples = io.load_jsonlines(f\"{vars.DATA_DIR}/ripple_edits/benchmark/recent.json\")\n",
    "assert len(recent_examples) == 1\n",
    "recent_examples = [] # recent_examples[0]\n",
    "\n",
    "ripple_edits_examples = popular_examples +  recent_examples # + random_examples\n",
    "len(ripple_edits_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_examples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# strong examples 762\n",
      "# weak examples 115\n"
     ]
    }
   ],
   "source": [
    "non_zero_outerloop_count = 0\n",
    "strong_meta_examples = []\n",
    "weak_meta_examples = []\n",
    "for example in ripple_edits_examples:\n",
    "    outerloop_instances = example[\"Logical_Generalization\"] + example[\"Compositionality_I\"] + example[\"Compositionality_II\"] + example[\"Subject_Aliasing\"]\n",
    "    # for ins in outerloop_instances:\n",
    "        # assert len(ins[\"test_queries\"]) == 1\n",
    "    locality_instances = example[\"Relation_Specificity\"] + example[\"Forgetfulness\"] \n",
    "    # non_zero_outerloop_count += len(outerloop_instances) > 0\n",
    "    if len(outerloop_instances) > 0 and len(locality_instances) > 0:\n",
    "        \n",
    "        outerloop_queries = [q for instance in outerloop_instances for q in instance[\"test_queries\"]]\n",
    "        outerloop_queries = [q for q in outerloop_queries if len(q[\"answers\"]) > 0]\n",
    "        outerloop_queries = [q for q in outerloop_queries if len([a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip() ) > 0 ]) > 0]\n",
    "        assert all([len(q[\"prompt\"].strip()) > 0 for q in outerloop_queries])\n",
    "        \n",
    "        if len(outerloop_queries) == 0:\n",
    "            continue\n",
    "        \n",
    "        locality_queries = [q for instance in locality_instances for q in instance[\"test_queries\"]]\n",
    "        locality_queries = [q for q in locality_queries if len(q[\"answers\"]) > 0]\n",
    "        locality_queries = [q for q in locality_queries if len([a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip() ) > 0 ]) > 0]\n",
    "        assert all([len(q[\"prompt\"].strip()) > 0 for q in locality_queries])\n",
    "        \n",
    "        assert len(locality_queries) > 0\n",
    "        \n",
    "        strong_meta_examples.append(example)\n",
    "    elif len(locality_instances) == 0:\n",
    "        \n",
    "        outerloop_queries = [q for instance in outerloop_instances for q in instance[\"test_queries\"]]\n",
    "        outerloop_queries = [q for q in outerloop_queries if len(q[\"answers\"]) > 0]\n",
    "        outerloop_queries = [q for q in outerloop_queries if len([a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip() ) > 0 ]) > 0]\n",
    "        assert all([len(q[\"prompt\"].strip()) > 0 for q in outerloop_queries])\n",
    "        \n",
    "        if len(outerloop_queries) == 0:\n",
    "            continue\n",
    "        \n",
    "        weak_meta_examples.append(example)\n",
    "    else:\n",
    "        locality_queries = [q for instance in locality_instances for q in instance[\"test_queries\"]]\n",
    "        locality_queries = [q for q in locality_queries if len(q[\"answers\"]) > 0]\n",
    "        locality_queries = [q for q in locality_queries if len([a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip() ) > 0 ]) > 0]\n",
    "        assert all([len(q[\"prompt\"].strip()) > 0 for q in locality_queries])\n",
    "        \n",
    "        assert len(locality_queries) > 0\n",
    "        \n",
    "        weak_meta_examples.append(example)\n",
    "        \n",
    "print(\"# strong examples\", len(strong_meta_examples))\n",
    "print(\"# weak examples\", len(weak_meta_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "877"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(strong_meta_examples) + len(weak_meta_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09122006841505131"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "80 / 877"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip()) > 0] for q in locality_queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 80\n",
    "n_valid = 80\n",
    "np.random.shuffle(strong_meta_examples)\n",
    "np.random.shuffle(weak_meta_examples)\n",
    "\n",
    "test_examples = strong_meta_examples[:n_test]\n",
    "valid_examples = strong_meta_examples[n_test:n_test+n_valid]\n",
    "train_examples = strong_meta_examples[n_test+n_valid:] + weak_meta_examples\n",
    "np.random.shuffle(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.dump_jsonlines(train_examples, f\"{vars.DATA_DIR}/ripple_edits/meta_train_popular/train.jsonl\")\n",
    "io.dump_jsonlines(valid_examples, f\"{vars.DATA_DIR}/ripple_edits/meta_train_popular/valid.jsonl\")\n",
    "io.dump_jsonlines(test_examples, f\"{vars.DATA_DIR}/ripple_edits/meta_train_popular/test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1593"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = io.load_jsonlines(f\"{vars.DATA_DIR}/ripple_edits/meta_train_recent/train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero_outerloop_count = 0\n",
    "meta_examples = []\n",
    "# strong_meta_examples = []\n",
    "# weak_meta_examples = []\n",
    "count_rel = defaultdict(int)\n",
    "count_prop_types = defaultdict(int)\n",
    "\n",
    "for example in train_examples:\n",
    "    outerloop_instances = example[\"Logical_Generalization\"] + example[\"Compositionality_I\"] + example[\"Compositionality_II\"] + example[\"Subject_Aliasing\"]\n",
    "    # for ins in outerloop_instances:\n",
    "        # assert len(ins[\"test_queries\"]) == 1\n",
    "    locality_instances = example[\"Relation_Specificity\"] + example[\"Forgetfulness\"] \n",
    "    example[\"edit\"][\"text\"] = example[\"edit\"][\"prompt\"]\n",
    "    meta_examples.append(example[\"edit\"])\n",
    "    \n",
    "    outerloop_queries = []\n",
    "    for k in [\"Logical_Generalization\", \"Compositionality_I\", \"Compositionality_II\", \"Subject_Aliasing\"]:\n",
    "        for instance in example[k]:\n",
    "            for q in instance[\"test_queries\"]:\n",
    "                if len(q[\"answers\"]) > 0 and len([a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip() ) > 0 ]) > 0:\n",
    "                    q[\"question_type\"] = k\n",
    "                    ans_candidates = [a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip()) > 0]\n",
    "                    assert len(ans_candidates) > 0\n",
    "                    assert q[\"prompt\"][-1] not in \".\",  q[\"prompt\"]\n",
    "                    q[\"text\"] = q[\"prompt\"] + \" \" + ans_candidates[0]\n",
    "                    \n",
    "                    outerloop_queries.append(q)\n",
    "    for outer_q in outerloop_queries:\n",
    "        count_prop_types[\"efficacy::\"+ outer_q[\"question_type\"]] += 1\n",
    "        count_rel[\"efficacy::\"+ outer_q[\"relation\"]] += 1\n",
    "    # assert len(outerloop_queries) > 0\n",
    "    meta_examples.extend(outerloop_queries)\n",
    "    \n",
    "    locality_queries = []\n",
    "    for k in [\"Relation_Specificity\", \"Forgetfulness\"]:\n",
    "        for instance in example[k]:\n",
    "            for q in instance[\"test_queries\"]:\n",
    "                if len(q[\"answers\"]) > 0 and len([a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip() ) > 0 ]) > 0:\n",
    "                    q[\"question_type\"] = k\n",
    "                    ans_candidates = [a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip()) > 0]\n",
    "                    assert len(ans_candidates) > 0\n",
    "                    assert q[\"prompt\"][-1] not in string.punctuation\n",
    "                    q[\"text\"] = q[\"prompt\"] + \" \" + ans_candidates[0]\n",
    "                    locality_queries.append(q)\n",
    "    for loc_q in locality_queries:\n",
    "        count_prop_types[\"specificity::\"+ loc_q[\"question_type\"]] += 1\n",
    "        count_rel[\"specificity::\"+ loc_q[\"relation\"]] += 1\n",
    "    \n",
    "    meta_examples.extend(locality_queries)\n",
    "    \n",
    "    # assert len(locality_queries) > 0\n",
    "        \n",
    "# print(\"# strong examples\", len(strong_meta_examples))\n",
    "# print(\"# weak examples\", len(weak_meta_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_from_disk(f\"{vars.DATA_DIR}/ripple_edits/meta_train_recent/test_w_triplet.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = io.load_jsonlines(f\"{vars.DATA_DIR}/ripple_edits/meta_train_recent/test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_data = []\n",
    "\n",
    "count = 0\n",
    "for i in range(len(test_data)):\n",
    "    original_test = test_data[i]\n",
    "    augmentation = ds[i-count]\n",
    "    if original_test[\"edit\"][\"prompt\"] == \"The name of the composer of Klavierstücke  I–IV is Karlheinz Stockhausen.\":\n",
    "        # print(\"found\")\n",
    "        original_test[\"edit\"][\"subject\"] = \"name of the composer of Klavierst\\u00fccke  I\\u2013IV\"\n",
    "        original_test[\"edit\"][\"object\"] = \"Karlheinz Stockhausen\"\n",
    "        assert original_test[\"edit\"][\"subject\"] in original_test[\"edit\"][\"prompt\"]\n",
    "        assert original_test[\"edit\"][\"object\"] in original_test[\"edit\"][\"prompt\"]\n",
    "        new_test_data.append(original_test)\n",
    "        count += 1\n",
    "        continue\n",
    "        # print(original_test)\n",
    "        # print(augmentation)\n",
    "    else:\n",
    "        \n",
    "        assert original_test[\"edit\"][\"prompt\"] == augmentation[\"prompt\"], f\"{i}@@\" + original_test[\"edit\"][\"prompt\"] + \"@@\" + augmentation[\"prompt\"]\n",
    "        assert augmentation[\"subject\"] in original_test[\"edit\"][\"prompt\"]\n",
    "        assert augmentation[\"object\"] in original_test[\"edit\"][\"prompt\"]\n",
    "\n",
    "        original_test[\"edit\"][\"subject\"] = augmentation[\"subject\"]\n",
    "        original_test[\"edit\"][\"object\"] = augmentation[\"object\"]\n",
    "        new_test_data.append(original_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.dump_jsonlines(new_test_data, f\"{vars.DATA_DIR}/ripple_edits/meta_train_recent/test_aug.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_type': 'recent',\n",
       " 'edit': {'prompt': 'The date of birth of Gordiya is 0550-01-01T00:00:00Z.',\n",
       "  'subject_id': 'Q5950714',\n",
       "  'relation': 'DATE_OF_BIRTH',\n",
       "  'target_id': '0550-01-01T00:00:00Z',\n",
       "  'subject': 'Date of birth of Gordiya',\n",
       "  'object': '0550-01-01T00:00:00Z'},\n",
       " 'Logical_Generalization': [],\n",
       " 'Compositionality_I': [],\n",
       " 'Compositionality_II': [],\n",
       " 'Subject_Aliasing': [{'test_queries': [{'prompt': 'The date of birth of Gurdiya is',\n",
       "     'answers': [{'value': '0550-01-01T00:00:00Z', 'aliases': []}],\n",
       "     'query_type': 'regular',\n",
       "     'subject_id': 'Q5950714',\n",
       "     'relation': 'DATE_OF_BIRTH',\n",
       "     'target_ids': ['0550-01-01T00:00:00Z'],\n",
       "     'phrase': 'The date of birth of Gurdiya is'}],\n",
       "   'test_condition': 'OR',\n",
       "   'condition_queries': [{'prompt': 'The date of birth of Gurdiya is',\n",
       "     'answers': [{'value': '0550-01-01T00:00:00Z', 'aliases': []}],\n",
       "     'query_type': 'regular',\n",
       "     'subject_id': 'Q5950714',\n",
       "     'relation': 'DATE_OF_BIRTH',\n",
       "     'target_ids': ['0550-01-01T00:00:00Z'],\n",
       "     'phrase': 'The date of birth of Gurdiya is'}]},\n",
       "  {'test_queries': [{'prompt': 'The date of birth of Kurdiyah is',\n",
       "     'answers': [{'value': '0550-01-01T00:00:00Z', 'aliases': []}],\n",
       "     'query_type': 'regular',\n",
       "     'subject_id': 'Q5950714',\n",
       "     'relation': 'DATE_OF_BIRTH',\n",
       "     'target_ids': ['0550-01-01T00:00:00Z'],\n",
       "     'phrase': 'The date of birth of Kurdiyah is'}],\n",
       "   'test_condition': 'OR',\n",
       "   'condition_queries': [{'prompt': 'The date of birth of Kurdiyah is',\n",
       "     'answers': [{'value': '0550-01-01T00:00:00Z', 'aliases': []}],\n",
       "     'query_type': 'regular',\n",
       "     'subject_id': 'Q5950714',\n",
       "     'relation': 'DATE_OF_BIRTH',\n",
       "     'target_ids': ['0550-01-01T00:00:00Z'],\n",
       "     'phrase': 'The date of birth of Kurdiyah is'}]},\n",
       "  {'test_queries': [{'prompt': 'The date of birth of Gurdya is',\n",
       "     'answers': [{'value': '0550-01-01T00:00:00Z', 'aliases': []}],\n",
       "     'query_type': 'regular',\n",
       "     'subject_id': 'Q5950714',\n",
       "     'relation': 'DATE_OF_BIRTH',\n",
       "     'target_ids': ['0550-01-01T00:00:00Z'],\n",
       "     'phrase': 'The date of birth of Gurdya is'}],\n",
       "   'test_condition': 'OR',\n",
       "   'condition_queries': [{'prompt': 'The date of birth of Gurdya is',\n",
       "     'answers': [{'value': '0550-01-01T00:00:00Z', 'aliases': []}],\n",
       "     'query_type': 'regular',\n",
       "     'subject_id': 'Q5950714',\n",
       "     'relation': 'DATE_OF_BIRTH',\n",
       "     'target_ids': ['0550-01-01T00:00:00Z'],\n",
       "     'phrase': 'The date of birth of Gurdya is'}]}],\n",
       " 'Relation_Specificity': [{'test_queries': [{'prompt': 'The name of the father of Gordiya is',\n",
       "     'answers': [{'value': 'Bahram Gushnasp', 'aliases': []}],\n",
       "     'query_type': 'regular',\n",
       "     'subject_id': 'Q5950714',\n",
       "     'relation': 'FATHER',\n",
       "     'target_ids': ['Q17989359'],\n",
       "     'phrase': None}],\n",
       "   'test_condition': 'OR',\n",
       "   'condition_queries': [{'prompt': 'The name of the father of Gordiya is',\n",
       "     'answers': [{'value': 'Bahram Gushnasp', 'aliases': []}],\n",
       "     'query_type': 'regular',\n",
       "     'subject_id': 'Q5950714',\n",
       "     'relation': 'FATHER',\n",
       "     'target_ids': ['Q17989359'],\n",
       "     'phrase': None}]},\n",
       "  {'test_queries': [{'prompt': 'The names of the siblings of Gordiya are',\n",
       "     'answers': [{'value': 'Bahram Chobin',\n",
       "       'aliases': ['Bahram VI Chobin',\n",
       "        'Bahram VI',\n",
       "        'Wahrām Chōbēn',\n",
       "        'Wahrām Chōbē',\n",
       "        'Bahrām Chōbīn',\n",
       "        'Mihrevandak',\n",
       "        'Mihrbandak',\n",
       "        'Bahram Chubin',\n",
       "        'Bahram Chubina',\n",
       "        'Wahram Choben']}],\n",
       "     'query_type': 'regular',\n",
       "     'subject_id': 'Q5950714',\n",
       "     'relation': 'SIBLING',\n",
       "     'target_ids': ['Q311283'],\n",
       "     'phrase': None}],\n",
       "   'test_condition': 'OR',\n",
       "   'condition_queries': [{'prompt': 'The names of the siblings of Gordiya are',\n",
       "     'answers': [{'value': 'Bahram Chobin',\n",
       "       'aliases': ['Bahram VI Chobin',\n",
       "        'Bahram VI',\n",
       "        'Wahrām Chōbēn',\n",
       "        'Wahrām Chōbē',\n",
       "        'Bahrām Chōbīn',\n",
       "        'Mihrevandak',\n",
       "        'Mihrbandak',\n",
       "        'Bahram Chubin',\n",
       "        'Bahram Chubina',\n",
       "        'Wahram Choben']}],\n",
       "     'query_type': 'regular',\n",
       "     'subject_id': 'Q5950714',\n",
       "     'relation': 'SIBLING',\n",
       "     'target_ids': ['Q311283'],\n",
       "     'phrase': None}]},\n",
       "  {'test_queries': [{'prompt': 'The name of the spouse of Gordiya is',\n",
       "     'answers': [{'value': 'Bahram Chobin',\n",
       "       'aliases': ['Bahram VI Chobin',\n",
       "        'Bahram VI',\n",
       "        'Wahrām Chōbēn',\n",
       "        'Wahrām Chōbē',\n",
       "        'Bahrām Chōbīn',\n",
       "        'Mihrevandak',\n",
       "        'Mihrbandak',\n",
       "        'Bahram Chubin',\n",
       "        'Bahram Chubina',\n",
       "        'Wahram Choben']},\n",
       "      {'value': 'Khosrow II', 'aliases': ['Husraw II']},\n",
       "      {'value': 'Vistahm',\n",
       "       'aliases': ['Wistahm',\n",
       "        'Bistam',\n",
       "        'Bastam',\n",
       "        'Bestam',\n",
       "        'Gustahm',\n",
       "        'Gostahm',\n",
       "        'Wistakhm',\n",
       "        'Vistakhm']}],\n",
       "     'query_type': 'regular',\n",
       "     'subject_id': 'Q5950714',\n",
       "     'relation': 'SPOUSE',\n",
       "     'target_ids': ['Q311283', 'Q212732', 'Q2351358'],\n",
       "     'phrase': None}],\n",
       "   'test_condition': 'OR',\n",
       "   'condition_queries': [{'prompt': 'The name of the spouse of Gordiya is',\n",
       "     'answers': [{'value': 'Bahram Chobin',\n",
       "       'aliases': ['Bahram VI Chobin',\n",
       "        'Bahram VI',\n",
       "        'Wahrām Chōbēn',\n",
       "        'Wahrām Chōbē',\n",
       "        'Bahrām Chōbīn',\n",
       "        'Mihrevandak',\n",
       "        'Mihrbandak',\n",
       "        'Bahram Chubin',\n",
       "        'Bahram Chubina',\n",
       "        'Wahram Choben']},\n",
       "      {'value': 'Khosrow II', 'aliases': ['Husraw II']},\n",
       "      {'value': 'Vistahm',\n",
       "       'aliases': ['Wistahm',\n",
       "        'Bistam',\n",
       "        'Bastam',\n",
       "        'Bestam',\n",
       "        'Gustahm',\n",
       "        'Gostahm',\n",
       "        'Wistakhm',\n",
       "        'Vistakhm']}],\n",
       "     'query_type': 'regular',\n",
       "     'subject_id': 'Q5950714',\n",
       "     'relation': 'SPOUSE',\n",
       "     'target_ids': ['Q311283', 'Q212732', 'Q2351358'],\n",
       "     'phrase': None}]},\n",
       "  {'test_queries': [{'prompt': 'The name of the child of Gordiya is',\n",
       "     'answers': [{'value': 'Javanshir', 'aliases': ['Juvansher']},\n",
       "      {'value': 'Mihran Bahram-i Chubin',\n",
       "       'aliases': ['Mihran Bahram-i Chobin']}],\n",
       "     'query_type': 'regular',\n",
       "     'subject_id': 'Q5950714',\n",
       "     'relation': 'CHILD',\n",
       "     'target_ids': ['Q11176633', 'Q16201649'],\n",
       "     'phrase': None}],\n",
       "   'test_condition': 'OR',\n",
       "   'condition_queries': [{'prompt': 'The name of the child of Gordiya is',\n",
       "     'answers': [{'value': 'Javanshir', 'aliases': ['Juvansher']},\n",
       "      {'value': 'Mihran Bahram-i Chubin',\n",
       "       'aliases': ['Mihran Bahram-i Chobin']}],\n",
       "     'query_type': 'regular',\n",
       "     'subject_id': 'Q5950714',\n",
       "     'relation': 'CHILD',\n",
       "     'target_ids': ['Q11176633', 'Q16201649'],\n",
       "     'phrase': None}]},\n",
       "  {'test_queries': [{'prompt': 'The gender of Gordiya is',\n",
       "     'answers': [{'value': 'female',\n",
       "       'aliases': ['woman',\n",
       "        'human female',\n",
       "        'female person',\n",
       "        'lady',\n",
       "        'female human',\n",
       "        'fairer sex',\n",
       "        'female gender',\n",
       "        'fem',\n",
       "        '♀',\n",
       "        'f',\n",
       "        'women',\n",
       "        'girl',\n",
       "        'girls',\n",
       "        'female character']}],\n",
       "     'query_type': 'regular',\n",
       "     'subject_id': 'Q5950714',\n",
       "     'relation': 'SEX_OR_GENDER',\n",
       "     'target_ids': ['Q6581072'],\n",
       "     'phrase': None}],\n",
       "   'test_condition': 'OR',\n",
       "   'condition_queries': [{'prompt': 'The gender of Gordiya is',\n",
       "     'answers': [{'value': 'female',\n",
       "       'aliases': ['woman',\n",
       "        'human female',\n",
       "        'female person',\n",
       "        'lady',\n",
       "        'female human',\n",
       "        'fairer sex',\n",
       "        'female gender',\n",
       "        'fem',\n",
       "        '♀',\n",
       "        'f',\n",
       "        'women',\n",
       "        'girl',\n",
       "        'girls',\n",
       "        'female character']}],\n",
       "     'query_type': 'regular',\n",
       "     'subject_id': 'Q5950714',\n",
       "     'relation': 'SEX_OR_GENDER',\n",
       "     'target_ids': ['Q6581072'],\n",
       "     'phrase': None}]},\n",
       "  {'test_queries': [{'prompt': 'The place of birth of Gordiya is',\n",
       "     'answers': [{'value': 'Ray',\n",
       "       'aliases': ['Rhages', 'Rayy', 'al-Rayy', 'Ray, Iran', 'Rey']}],\n",
       "     'query_type': 'regular',\n",
       "     'subject_id': 'Q5950714',\n",
       "     'relation': 'PLACE_OF_BIRTH',\n",
       "     'target_ids': ['Q636188'],\n",
       "     'phrase': None}],\n",
       "   'test_condition': 'OR',\n",
       "   'condition_queries': [{'prompt': 'The place of birth of Gordiya is',\n",
       "     'answers': [{'value': 'Ray',\n",
       "       'aliases': ['Rhages', 'Rayy', 'al-Rayy', 'Ray, Iran', 'Rey']}],\n",
       "     'query_type': 'regular',\n",
       "     'subject_id': 'Q5950714',\n",
       "     'relation': 'PLACE_OF_BIRTH',\n",
       "     'target_ids': ['Q636188'],\n",
       "     'phrase': None}]},\n",
       "  {'test_queries': [{'prompt': 'The name of the country of citizenship of Gordiya is',\n",
       "     'answers': [{'value': 'Sasanian Empire',\n",
       "       'aliases': ['Neo-Persian Empire',\n",
       "        'Ērānshahr',\n",
       "        'Ērān',\n",
       "        'Second Persian Empire',\n",
       "        'Persian Empire',\n",
       "        'Persia',\n",
       "        'Sassanid Empire',\n",
       "        'Sasanid Empire',\n",
       "        'Sassanid Persia',\n",
       "        'Sasanid Persia',\n",
       "        'Sasanian Persia',\n",
       "        'Sassanian Persia',\n",
       "        'Sassanian Empire']}],\n",
       "     'query_type': 'regular',\n",
       "     'subject_id': 'Q5950714',\n",
       "     'relation': 'COUNTRY_OF_CITIZENSHIP',\n",
       "     'target_ids': ['Q83891'],\n",
       "     'phrase': None}],\n",
       "   'test_condition': 'OR',\n",
       "   'condition_queries': [{'prompt': 'The name of the country of citizenship of Gordiya is',\n",
       "     'answers': [{'value': 'Sasanian Empire',\n",
       "       'aliases': ['Neo-Persian Empire',\n",
       "        'Ērānshahr',\n",
       "        'Ērān',\n",
       "        'Second Persian Empire',\n",
       "        'Persian Empire',\n",
       "        'Persia',\n",
       "        'Sassanid Empire',\n",
       "        'Sasanid Empire',\n",
       "        'Sassanid Persia',\n",
       "        'Sasanid Persia',\n",
       "        'Sasanian Persia',\n",
       "        'Sassanian Persia',\n",
       "        'Sassanian Empire']}],\n",
       "     'query_type': 'regular',\n",
       "     'subject_id': 'Q5950714',\n",
       "     'relation': 'COUNTRY_OF_CITIZENSHIP',\n",
       "     'target_ids': ['Q83891'],\n",
       "     'phrase': None}]},\n",
       "  {'test_queries': [{'prompt': 'The occupation of Gordiya is',\n",
       "     'answers': [{'value': 'warrior', 'aliases': ['fighter']}],\n",
       "     'query_type': 'regular',\n",
       "     'subject_id': 'Q5950714',\n",
       "     'relation': 'OCCUPATION',\n",
       "     'target_ids': ['Q1250916'],\n",
       "     'phrase': None}],\n",
       "   'test_condition': 'OR',\n",
       "   'condition_queries': [{'prompt': 'The occupation of Gordiya is',\n",
       "     'answers': [{'value': 'warrior', 'aliases': ['fighter']}],\n",
       "     'query_type': 'regular',\n",
       "     'subject_id': 'Q5950714',\n",
       "     'relation': 'OCCUPATION',\n",
       "     'target_ids': ['Q1250916'],\n",
       "     'phrase': None}]}],\n",
       " 'Forgetfulness': []}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test_data[29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
