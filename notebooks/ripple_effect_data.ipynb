{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "# import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# sys.path.append(\"/u/zliu/datastor1/KE-by-CP\")\n",
    "import pandas as pd\n",
    "# from experiments.musique.inference_only import macro_averaging\n",
    "from knowledge_propagation.utils import io, vars, extractor\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import describe\n",
    "from thefuzz import fuzz\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from dateutil.parser import parse\n",
    "from dateutil.parser import ParserError\n",
    "\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1922"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popular_examples = io.load_jsonlines(f\"{vars.DATA_DIR}/ripple_edits/benchmark/popular.json\")\n",
    "assert len(popular_examples) == 1\n",
    "popular_examples = popular_examples[0]\n",
    "\n",
    "random_examples = io.load_jsonlines(f\"{vars.DATA_DIR}/ripple_edits/benchmark/random.json\")\n",
    "assert len(random_examples) == 1\n",
    "random_examples = random_examples[0]\n",
    "\n",
    "recent_examples = io.load_jsonlines(f\"{vars.DATA_DIR}/ripple_edits/benchmark/recent.json\")\n",
    "assert len(recent_examples) == 1\n",
    "recent_examples = recent_examples[0]\n",
    "\n",
    "ripple_edits_examples = random_examples # random_examples # popular_examples + recent_examples + random_examples\n",
    "len(ripple_edits_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "288.3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.15 * len(ripple_edits_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_examples[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# strong examples 1413\n",
      "# weak examples 503\n"
     ]
    }
   ],
   "source": [
    "non_zero_outerloop_count = 0\n",
    "strong_meta_examples = []\n",
    "weak_meta_examples = []\n",
    "for example in ripple_edits_examples:\n",
    "    outerloop_instances = example[\"Logical_Generalization\"] + example[\"Compositionality_I\"] + example[\"Compositionality_II\"] + example[\"Subject_Aliasing\"]\n",
    "    # for ins in outerloop_instances:\n",
    "        # assert len(ins[\"test_queries\"]) == 1\n",
    "    locality_instances = example[\"Relation_Specificity\"] + example[\"Forgetfulness\"] \n",
    "    # non_zero_outerloop_count += len(outerloop_instances) > 0\n",
    "    if len(outerloop_instances) > 0 and len(locality_instances) > 0:\n",
    "        \n",
    "        outerloop_queries = [q for instance in outerloop_instances for q in instance[\"test_queries\"]]\n",
    "        outerloop_queries = [q for q in outerloop_queries if len(q[\"answers\"]) > 0]\n",
    "        outerloop_queries = [q for q in outerloop_queries if len([a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip() ) > 0 ]) > 0]\n",
    "        assert all([len(q[\"prompt\"].strip()) > 0 for q in outerloop_queries])\n",
    "        \n",
    "        if len(outerloop_queries) == 0:\n",
    "            continue\n",
    "        \n",
    "        locality_queries = [q for instance in locality_instances for q in instance[\"test_queries\"]]\n",
    "        locality_queries = [q for q in locality_queries if len(q[\"answers\"]) > 0]\n",
    "        locality_queries = [q for q in locality_queries if len([a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip() ) > 0 ]) > 0]\n",
    "        assert all([len(q[\"prompt\"].strip()) > 0 for q in locality_queries])\n",
    "        \n",
    "        assert len(locality_queries) > 0\n",
    "        \n",
    "        strong_meta_examples.append(example)\n",
    "    elif len(locality_instances) == 0:\n",
    "        \n",
    "        outerloop_queries = [q for instance in outerloop_instances for q in instance[\"test_queries\"]]\n",
    "        outerloop_queries = [q for q in outerloop_queries if len(q[\"answers\"]) > 0]\n",
    "        outerloop_queries = [q for q in outerloop_queries if len([a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip() ) > 0 ]) > 0]\n",
    "        assert all([len(q[\"prompt\"].strip()) > 0 for q in outerloop_queries])\n",
    "        \n",
    "        if len(outerloop_queries) == 0:\n",
    "            continue\n",
    "        \n",
    "        weak_meta_examples.append(example)\n",
    "    else:\n",
    "        locality_queries = [q for instance in locality_instances for q in instance[\"test_queries\"]]\n",
    "        locality_queries = [q for q in locality_queries if len(q[\"answers\"]) > 0]\n",
    "        locality_queries = [q for q in locality_queries if len([a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip() ) > 0 ]) > 0]\n",
    "        assert all([len(q[\"prompt\"].strip()) > 0 for q in locality_queries])\n",
    "        \n",
    "        assert len(locality_queries) > 0\n",
    "        \n",
    "        weak_meta_examples.append(example)\n",
    "        \n",
    "print(\"# strong examples\", len(strong_meta_examples))\n",
    "print(\"# weak examples\", len(weak_meta_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1916"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(strong_meta_examples) + len(weak_meta_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.15 * 1916"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip()) > 0] for q in locality_queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 140\n",
    "n_valid = 140\n",
    "np.random.shuffle(strong_meta_examples)\n",
    "np.random.shuffle(weak_meta_examples)\n",
    "\n",
    "test_examples = strong_meta_examples[:n_test]\n",
    "valid_examples = strong_meta_examples[n_test:n_test+n_valid]\n",
    "train_examples = strong_meta_examples[n_test+n_valid:] + weak_meta_examples\n",
    "np.random.shuffle(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(f\"{vars.DATA_DIR}/ripple_edits/meta_train_random/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.dump_jsonlines(train_examples, f\"{vars.DATA_DIR}/ripple_edits/meta_train_random/train.jsonl\")\n",
    "io.dump_jsonlines(valid_examples, f\"{vars.DATA_DIR}/ripple_edits/meta_train_random/valid.jsonl\")\n",
    "io.dump_jsonlines(test_examples, f\"{vars.DATA_DIR}/ripple_edits/meta_train_random/test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1636"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_popular_train = io.load_jsonlines(f\"{vars.DATA_DIR}/ripple_edits/meta_train_recent+popular/train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_train = io.load_jsonlines(f\"{vars.DATA_DIR}/ripple_edits/meta_train_random/train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# io.dump_jsonlines(recent_popular_train + random_train, f\"{vars.DATA_DIR}/ripple_edits/meta_train_recent+popular/train_w_random.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check spurious correlation in dataaset (answer verbtaim in edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = io.load_jsonlines(f\"{vars.DATA_DIR}/ripple_edits/meta_train_recent+popular/test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero_outerloop_count = 0\n",
    "meta_examples = []\n",
    "propagation2verbtaim_count = defaultdict(int)\n",
    "propagation2count = defaultdict(int)\n",
    "# strong_meta_examples = []\n",
    "# weak_meta_examples = []\n",
    "count_rel = defaultdict(int)\n",
    "count_prop_types = defaultdict(int)\n",
    "\n",
    "for example in train_examples:\n",
    "    outerloop_instances = example[\"Logical_Generalization\"] + example[\"Compositionality_I\"] + example[\"Compositionality_II\"] + example[\"Subject_Aliasing\"]\n",
    "    # for ins in outerloop_instances:\n",
    "        # assert len(ins[\"test_queries\"]) == 1\n",
    "    locality_instances = example[\"Relation_Specificity\"] + example[\"Forgetfulness\"] \n",
    "    example[\"edit\"][\"text\"] = example[\"edit\"][\"prompt\"]\n",
    "    meta_examples.append(example[\"edit\"])\n",
    "    \n",
    "    outerloop_queries = []\n",
    "    for k in [\"Logical_Generalization\", \"Compositionality_I\", \"Compositionality_II\", \"Subject_Aliasing\"]:\n",
    "        for instance in example[k]:\n",
    "            for q in instance[\"test_queries\"]:\n",
    "                if len(q[\"answers\"]) > 0 and len([a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip() ) > 0 ]) > 0:\n",
    "                    q[\"question_type\"] = k\n",
    "                    ans_candidates = [a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip()) > 0]\n",
    "                    assert len(ans_candidates) > 0\n",
    "                    assert q[\"prompt\"][-1] not in \".\",  q[\"prompt\"]\n",
    "                    q[\"text\"] = q[\"prompt\"] + \" \" + ans_candidates[0]\n",
    "                    propagation2count[k] += 1\n",
    "                    propagation2verbtaim_count[k] += int(ans_candidates[0] in example[\"edit\"][\"text\"])\n",
    "                    outerloop_queries.append(q)\n",
    "    for outer_q in outerloop_queries:\n",
    "        count_prop_types[\"efficacy::\"+ outer_q[\"question_type\"]] += 1\n",
    "        count_rel[\"efficacy::\"+ outer_q[\"relation\"]] += 1\n",
    "    # assert len(outerloop_queries) > 0\n",
    "    meta_examples.extend(outerloop_queries)\n",
    "    \n",
    "    locality_queries = []\n",
    "    for k in [\"Relation_Specificity\", \"Forgetfulness\"]:\n",
    "        for instance in example[k]:\n",
    "            for q in instance[\"test_queries\"]:\n",
    "                if len(q[\"answers\"]) > 0 and len([a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip() ) > 0 ]) > 0:\n",
    "                    q[\"question_type\"] = k\n",
    "                    ans_candidates = [a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip()) > 0]\n",
    "                    assert len(ans_candidates) > 0\n",
    "                    assert q[\"prompt\"][-1] not in string.punctuation\n",
    "                    q[\"text\"] = q[\"prompt\"] + \" \" + ans_candidates[0]\n",
    "                    propagation2count[k] += 1\n",
    "                    propagation2verbtaim_count[k] += int(ans_candidates[0] in example[\"edit\"][\"text\"])\n",
    "                    locality_queries.append(q)\n",
    "    for loc_q in locality_queries:\n",
    "        count_prop_types[\"specificity::\"+ loc_q[\"question_type\"]] += 1\n",
    "        count_rel[\"specificity::\"+ loc_q[\"relation\"]] += 1\n",
    "    \n",
    "    meta_examples.extend(locality_queries)\n",
    "    \n",
    "    # assert len(locality_queries) > 0\n",
    "        \n",
    "# print(\"# strong examples\", len(strong_meta_examples))\n",
    "# print(\"# weak examples\", len(weak_meta_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Logical_Generalization': 122,\n",
       "             'Relation_Specificity': 933,\n",
       "             'Compositionality_I': 995,\n",
       "             'Compositionality_II': 106,\n",
       "             'Subject_Aliasing': 291,\n",
       "             'Forgetfulness': 119})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propagation2count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "propagation2verbtaim_ratio = {k: round(propagation2verbtaim_count[k] / propagation2count[k] * 100, 1) for k in propagation2count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logical_Generalization': 42.6,\n",
       " 'Relation_Specificity': 2.5,\n",
       " 'Compositionality_I': 3.2,\n",
       " 'Compositionality_II': 100.0,\n",
       " 'Subject_Aliasing': 100.0,\n",
       " 'Forgetfulness': 54.6}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propagation2verbtaim_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_from_disk(f\"{vars.DATA_DIR}/ripple_edits/meta_train_recent+popular/test_w_prefix.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = io.load_jsonlines(f\"{vars.DATA_DIR}/ripple_edits/meta_train_recent+popular/test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'subject_id', 'relation', 'target_id', 'original_fact', 'context', 'paraphrase', 'object'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'subject'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 21\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# print(original_test)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# print(augmentation)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m original_test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medit\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m augmentation[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m@@\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m original_test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medit\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@@\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m augmentation[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[43maugmentation\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubject\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;129;01min\u001b[39;00m original_test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medit\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m augmentation[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m original_test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medit\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     24\u001b[0m     original_test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medit\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubject\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m augmentation[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubject\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'subject'"
     ]
    }
   ],
   "source": [
    "new_test_data = []\n",
    "\n",
    "count = 0\n",
    "for i in range(len(test_data)):\n",
    "    original_test = test_data[i]\n",
    "    augmentation = ds[i-count]\n",
    "    if original_test[\"edit\"][\"prompt\"] == \"The name of the composer of Klavierstücke  I–IV is Karlheinz Stockhausen.\":\n",
    "        # print(\"found\")\n",
    "        original_test[\"edit\"][\"subject\"] = \"name of the composer of Klavierst\\u00fccke  I\\u2013IV\"\n",
    "        original_test[\"edit\"][\"object\"] = \"Karlheinz Stockhausen\"\n",
    "        assert original_test[\"edit\"][\"subject\"] in original_test[\"edit\"][\"prompt\"]\n",
    "        assert original_test[\"edit\"][\"object\"] in original_test[\"edit\"][\"prompt\"]\n",
    "        new_test_data.append(original_test)\n",
    "        count += 1\n",
    "        continue\n",
    "        # print(original_test)\n",
    "        # print(augmentation)\n",
    "    else:\n",
    "        \n",
    "        assert original_test[\"edit\"][\"prompt\"] == augmentation[\"prompt\"], f\"{i}@@\" + original_test[\"edit\"][\"prompt\"] + \"@@\" + augmentation[\"prompt\"]\n",
    "        assert augmentation[\"subject\"] in original_test[\"edit\"][\"prompt\"]\n",
    "        assert augmentation[\"object\"] in original_test[\"edit\"][\"prompt\"]\n",
    "\n",
    "        original_test[\"edit\"][\"subject\"] = augmentation[\"subject\"]\n",
    "        original_test[\"edit\"][\"object\"] = augmentation[\"object\"]\n",
    "        new_test_data.append(original_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.dump_jsonlines(new_test_data, f\"{vars.DATA_DIR}/ripple_edits/meta_train_recent+popular/test_aug.jsonl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a ripple edits for MEND-original version (before generating paraphrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"test\"\n",
    "train_examples = io.load_jsonlines(f\"{vars.DATA_DIR}/ripple_edits/meta_train_recent+popular/{split}.jsonl\")\n",
    "train_ds = load_from_disk(f\"{vars.DATA_DIR}/ripple_edits/meta_train_recent+popular/{split}_w_prefix.hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# special treatment for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_examples = []\n",
    "missing_examples = []\n",
    "mismatch_c = 0\n",
    "for i in range(len(train_examples)):\n",
    "    augmentation = train_ds[i-mismatch_c]\n",
    "    example = train_examples[i]\n",
    "    new_example = deepcopy(example)\n",
    "    \n",
    "    new_example[\"edit\"][\"context\"] = augmentation[\"context\"]\n",
    "    new_example[\"edit\"][\"paraphrase\"] = augmentation[\"paraphrase\"]\n",
    "    new_example[\"edit\"][\"object\"] = augmentation[\"object\"]\n",
    "    new_examples.append(new_example)\n",
    "    # if example[\"edit\"][\"prompt\"] != augmentation[\"prompt\"]:\n",
    "    #     mismatch_c += 1\n",
    "    # else:\n",
    "        \n",
    "    #     assert example[\"edit\"][\"prompt\"] == augmentation[\"prompt\"]\n",
    "    #     assert augmentation[\"object\"] in example[\"edit\"][\"prompt\"]\n",
    "    \n",
    "    #     flattend_train_examples.append({\"context\": augmentation[\"context\"], \"completion\": augmentation[\"object\"], \"paraphrase\": augmentation[\"paraphrase\"]})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if split == \"test\":\n",
    "#     io.dump_jsonlines(new_examples, f\"{vars.DATA_DIR}/ripple_edits/meta_train_recent+popular/{split}_mend.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattend_train_examples = []\n",
    "missing_examples = []\n",
    "mismatch_c = 0\n",
    "for i in range(len(train_examples)):\n",
    "    augmentation = train_ds[i-mismatch_c]\n",
    "    example = train_examples[i]\n",
    "    \n",
    "    if example[\"edit\"][\"prompt\"] != augmentation[\"prompt\"]:\n",
    "        mismatch_c += 1\n",
    "    else:\n",
    "        \n",
    "        assert example[\"edit\"][\"prompt\"] == augmentation[\"prompt\"]\n",
    "        assert augmentation[\"object\"] in example[\"edit\"][\"prompt\"], augmentation[\"object\"] + \"@@@\" + example[\"edit\"][\"prompt\"]\n",
    "    \n",
    "        flattend_train_examples.append({\"context\": augmentation[\"context\"], \"completion\": augmentation[\"object\"], \"paraphrase\": augmentation[\"paraphrase\"]})\n",
    "    \n",
    "    for k in [\"Logical_Generalization\", \"Compositionality_I\", \"Compositionality_II\", \"Subject_Aliasing\"]:\n",
    "        for instance in example[k]:\n",
    "            for q in instance[\"test_queries\"]:\n",
    "                if len(q[\"answers\"]) > 0 and len([a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip() ) > 0 ]) > 0:\n",
    "                    q[\"question_type\"] = k\n",
    "                    ans_candidates = [a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip()) > 0]\n",
    "                    assert len(ans_candidates) > 0\n",
    "                    assert q[\"prompt\"][-1] not in \".\",  q[\"prompt\"]\n",
    "                    assert \"phrase\" in q\n",
    "                    if q[\"phrase\"] is not None:\n",
    "                        flattend_train_examples.append({\"context\": q[\"prompt\"], \"completion\": ans_candidates[0], \"paraphrase\": q[\"phrase\"]})\n",
    "                    else:\n",
    "                        missing_examples.append({\"context\": q[\"prompt\"], \"completion\": ans_candidates[0],})\n",
    "    for k in [\"Relation_Specificity\", \"Forgetfulness\"]:\n",
    "        for instance in example[k]:\n",
    "            for q in instance[\"test_queries\"]:\n",
    "                if len(q[\"answers\"]) > 0 and len([a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip() ) > 0 ]) > 0:\n",
    "                    q[\"question_type\"] = k\n",
    "                    ans_candidates = [a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip()) > 0]\n",
    "                    assert len(ans_candidates) > 0\n",
    "                    assert q[\"prompt\"][-1] not in string.punctuation\n",
    "                    \n",
    "                    assert \"phrase\" in q\n",
    "                    if q[\"phrase\"] is not None:\n",
    "                        flattend_train_examples.append({\"context\": q[\"prompt\"], \"completion\": ans_candidates[0], \"paraphrase\": q[\"phrase\"]})\n",
    "                    else:\n",
    "                        missing_examples.append({\"context\": q[\"prompt\"], \"completion\": ans_candidates[0],})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing_ds = load_from_disk(f\"{vars.DATA_DIR}/ripple_edits/meta_train_recent/{split}_w_paraphrase.hf\")\n",
    "# for i in range(len(missing_ds)):\n",
    "#     augmentation = missing_ds[i]\n",
    "    \n",
    "#     flattend_train_examples.append({\"context\": augmentation[\"context\"], \"completion\": augmentation[\"completion\"], \"paraphrase\": augmentation[\"paraphrase\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17096"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flattend_train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# io.dump_jsonlines(flattend_train_examples, f\"{vars.DATA_DIR}/ripple_edits/meta_train_recent+popular/{split}_mend.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ex in io.load_jsonlines(f\"{vars.DATA_DIR}/ripple_edits/meta_train_recent/valid_mend.jsonl\"):\n",
    "    assert \"context\" in ex and ex[\"context\"] is not None\n",
    "    assert \"completion\" in ex and ex[\"completion\"] is not None\n",
    "    assert \"paraphrase\" in ex and ex[\"paraphrase\"] is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "recoe_aggregation = io.load_json(\"/u/zliu/datastor1/ReCoE/data/aggregation/counterfactual_datapoints_verified_atomic.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_recoe_aggregation = [\n",
    "    {\n",
    "        \"text\": \" \".join([f[\"fact\"] for f in x[\"direct_counterfactual_fact\"]]),\n",
    "        \"question\": x[\"question\"],\n",
    "        \"answers\": x[\"counterfactual_answer\"]\n",
    "     }\n",
    "    for x in recoe_aggregation\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "508"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_recoe_aggregation[-1]\n",
    "len(converted_recoe_aggregation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrate RoCE data to ripple edit trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "recoe_comparative = io.load_json(\"/u/zliu/datastor1/ReCoE/data/comparative/counterfactual_datapoints_verified_atomic.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [len(x[\"facts_per_choice\"]) for x in recoe_comparative]\n",
    "# recoe_comparative[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_recoe_comparative = []\n",
    "for x in recoe_comparative:\n",
    "    assert \"choice_1_counterfactuals\" in x[\"counterfactuals_per_choice\"]\n",
    "    assert \"choice_2_counterfactuals\" in x[\"counterfactuals_per_choice\"]\n",
    "    assert \"choice_3_counterfactuals\" not in x[\"counterfactuals_per_choice\"]\n",
    "    \n",
    "    counter_fact1_text = [f[\"fact\"] for f in x[\"counterfactuals_per_choice\"][\"choice_1_counterfactuals\"]]\n",
    "    counter_fact2_text = [f[\"fact\"] for f in x[\"counterfactuals_per_choice\"][\"choice_2_counterfactuals\"]]\n",
    "    texts = [counter_fact1_text, counter_fact2_text]\n",
    "    np.random.shuffle(texts)\n",
    "    counter_fact1_text_after_rand, counter_fact2_text_after_rand = texts\n",
    "    \n",
    "    \n",
    "    converted_recoe_comparative.append(\n",
    "        {\n",
    "            \"text\": \" \".join(counter_fact1_text_after_rand + counter_fact2_text_after_rand),\n",
    "            \"question\": x[\"question\"],\n",
    "            \"answers\": x[\"counterfactual_answer\"]\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [len(x[\"facts_per_choice\"]) for x in recoe_comparative]\n",
    "recoe_comparative = io.load_json(\"/u/zliu/datastor1/ReCoE/data/counting/counterfactual_datapoints_verified_atomic.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How many dwarf planets are recognized by the International Astronomical Union (IAU)?',\n",
       " 'answer': '5',\n",
       " 'domain': 'astronomy',\n",
       " 'links': ['https://en.wikipedia.org/wiki/List_of_possible_dwarf_planets'],\n",
       " 'counterfactual_facts': [{'original_output': 'Pluto - Discovered in 1930, but no longer considered a dwarf planet by the IAU since 2015.',\n",
       "   'sentence': 'Pluto was discovered in 1930, but has not been considered a dwarf planet by the IAU since 2015.',\n",
       "   'atomic_facts': ['Pluto was discovered in 1930',\n",
       "    'Pluto has not been considered a dwarf planet by the IAU since 2015',\n",
       "    'Pluto not being considered a dwarf planet by the IAU has been the case since 2015'],\n",
       "   'atomic_triples': ['(Pluto; was discovered in; 1930)',\n",
       "    '(Pluto; has not been considered a dwarf planet by; the IAU)',\n",
       "    '(Pluto not being considered a dwarf planet by the IAU; has been the case since; 2015)']}],\n",
       " 'counterfactual_answer': '4',\n",
       " 'facts': [{'original_output': 'Pluto - Discovered in 1930, considered a dwarf planet since 2006.',\n",
       "   'sentence': 'Pluto was discovered in 1930 and was considered a dwarf planet since 2006.',\n",
       "   'atomic_facts': ['Pluto was discovered in 1930',\n",
       "    'Pluto was considered a dwarf planet',\n",
       "    'Pluto being considered a dwarf planet was since 2006'],\n",
       "   'atomic_triples': ['(Pluto; was discovered in; 1930)',\n",
       "    '(Pluto; was considered; a dwarf planet)',\n",
       "    '(Pluto being considered a dwarf planet; was since; 2006)']}],\n",
       " 'answer_alias': ['five'],\n",
       " 'counterfactual_answer_alias': ['four'],\n",
       " 'extended_facts': [{'original_output': 'Pluto - Discovered in 1930, considered a dwarf planet since 2006.',\n",
       "   'sentence': 'Pluto was discovered in 1930 and was considered a dwarf planet since 2006.',\n",
       "   'atomic_facts': ['Pluto was discovered in 1930',\n",
       "    'Pluto was considered a dwarf planet',\n",
       "    'Pluto being considered a dwarf planet was since 2006'],\n",
       "   'atomic_triples': ['(Pluto; was discovered in; 1930)',\n",
       "    '(Pluto; was considered; a dwarf planet)',\n",
       "    '(Pluto being considered a dwarf planet; was since; 2006)']},\n",
       "  {'original_output': 'Ceres - The only dwarf planet located in the inner Solar System. It was discovered in 1801 and is the largest object in the asteroid belt.',\n",
       "   'sentence': 'Ceres, discovered in 1801, is the only dwarf planet located in the inner Solar System and is the largest object in the asteroid belt.',\n",
       "   'atomic_facts': ['Ceres is the only dwarf planet located in the inner Solar System',\n",
       "    'Ceres was discovered in 1801',\n",
       "    'Ceres is the largest object in the asteroid belt'],\n",
       "   'atomic_triples': ['(Ceres; is the only dwarf planet located in; the inner Solar System)',\n",
       "    '(Ceres; was discovered in; 1801)',\n",
       "    '(Ceres; is the largest object in; the asteroid belt)']},\n",
       "  {'original_output': 'Eris - Discovered in 2005, it orbits beyond Neptune.',\n",
       "   'sentence': 'Eris was discovered in 2005 and it orbits beyond Neptune.',\n",
       "   'atomic_facts': ['Eris was discovered in 2005',\n",
       "    'Eris orbits beyond Neptune'],\n",
       "   'atomic_triples': ['(Eris; was discovered in; 2005)',\n",
       "    '(Eris; orbits beyond; Neptune)']},\n",
       "  {'original_output': 'Makemake - Discovered in 2005, it orbits beyond Neptune.',\n",
       "   'sentence': 'Makemake was discovered in 2005 and it orbits beyond Neptune.',\n",
       "   'atomic_facts': ['Makemake was discovered in 2005',\n",
       "    'Makemake orbits beyond Neptune'],\n",
       "   'atomic_triples': ['(Makemake; was discovered in; 2005)',\n",
       "    '(Makemake; orbits beyond; Neptune)']}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recoe_comparative[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
