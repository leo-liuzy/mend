{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zliu/miniconda3/envs/cpt/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "# import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# sys.path.append(\"/u/zliu/datastor1/KE-by-CP\")\n",
    "import pandas as pd\n",
    "# from experiments.musique.inference_only import macro_averaging\n",
    "from knowledge_propagation.utils import io, vars, extractor\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import describe\n",
    "from thefuzz import fuzz\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from dateutil.parser import parse\n",
    "from dateutil.parser import ParserError\n",
    "\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1948"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popular_examples = io.load_jsonlines(f\"{vars.DATA_DIR}/ripple_edits/benchmark/popular.json\")\n",
    "assert len(popular_examples) == 1\n",
    "popular_examples = [] # popular_examples[0]\n",
    "\n",
    "# random_examples = io.load_jsonlines(f\"{vars.DATA_DIR}/ripple_edits/benchmark/random.json\")\n",
    "# assert len(random_examples) == 1\n",
    "# random_examples = random_examples[0]\n",
    "\n",
    "recent_examples = io.load_jsonlines(f\"{vars.DATA_DIR}/ripple_edits/benchmark/recent.json\")\n",
    "assert len(recent_examples) == 1\n",
    "recent_examples = recent_examples[0]\n",
    "\n",
    "ripple_edits_examples = popular_examples + recent_examples # + random_examples\n",
    "len(ripple_edits_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_examples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# strong examples 1318\n",
      "# weak examples 575\n"
     ]
    }
   ],
   "source": [
    "non_zero_outerloop_count = 0\n",
    "strong_meta_examples = []\n",
    "weak_meta_examples = []\n",
    "for example in ripple_edits_examples:\n",
    "    outerloop_instances = example[\"Logical_Generalization\"] + example[\"Compositionality_I\"] + example[\"Compositionality_II\"] + example[\"Subject_Aliasing\"]\n",
    "    # for ins in outerloop_instances:\n",
    "        # assert len(ins[\"test_queries\"]) == 1\n",
    "    locality_instances = example[\"Relation_Specificity\"] + example[\"Forgetfulness\"] \n",
    "    # non_zero_outerloop_count += len(outerloop_instances) > 0\n",
    "    if len(outerloop_instances) > 0 and len(locality_instances) > 0:\n",
    "        \n",
    "        outerloop_queries = [q for instance in outerloop_instances for q in instance[\"test_queries\"]]\n",
    "        outerloop_queries = [q for q in outerloop_queries if len(q[\"answers\"]) > 0]\n",
    "        outerloop_queries = [q for q in outerloop_queries if len([a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip() ) > 0 ]) > 0]\n",
    "        assert all([len(q[\"prompt\"].strip()) > 0 for q in outerloop_queries])\n",
    "        \n",
    "        if len(outerloop_queries) == 0:\n",
    "            continue\n",
    "        \n",
    "        locality_queries = [q for instance in locality_instances for q in instance[\"test_queries\"]]\n",
    "        locality_queries = [q for q in locality_queries if len(q[\"answers\"]) > 0]\n",
    "        locality_queries = [q for q in locality_queries if len([a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip() ) > 0 ]) > 0]\n",
    "        assert all([len(q[\"prompt\"].strip()) > 0 for q in locality_queries])\n",
    "        \n",
    "        assert len(locality_queries) > 0\n",
    "        \n",
    "        strong_meta_examples.append(example)\n",
    "    elif len(locality_instances) == 0:\n",
    "        \n",
    "        outerloop_queries = [q for instance in outerloop_instances for q in instance[\"test_queries\"]]\n",
    "        outerloop_queries = [q for q in outerloop_queries if len(q[\"answers\"]) > 0]\n",
    "        outerloop_queries = [q for q in outerloop_queries if len([a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip() ) > 0 ]) > 0]\n",
    "        assert all([len(q[\"prompt\"].strip()) > 0 for q in outerloop_queries])\n",
    "        \n",
    "        if len(outerloop_queries) == 0:\n",
    "            continue\n",
    "        \n",
    "        weak_meta_examples.append(example)\n",
    "    else:\n",
    "        locality_queries = [q for instance in locality_instances for q in instance[\"test_queries\"]]\n",
    "        locality_queries = [q for q in locality_queries if len(q[\"answers\"]) > 0]\n",
    "        locality_queries = [q for q in locality_queries if len([a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip() ) > 0 ]) > 0]\n",
    "        assert all([len(q[\"prompt\"].strip()) > 0 for q in locality_queries])\n",
    "        \n",
    "        assert len(locality_queries) > 0\n",
    "        \n",
    "        weak_meta_examples.append(example)\n",
    "        \n",
    "print(\"# strong examples\", len(strong_meta_examples))\n",
    "print(\"# weak examples\", len(weak_meta_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1893"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(strong_meta_examples) + len(weak_meta_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09122006841505131"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "80 / 877"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip()) > 0] for q in locality_queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 80\n",
    "n_valid = 80\n",
    "np.random.shuffle(strong_meta_examples)\n",
    "np.random.shuffle(weak_meta_examples)\n",
    "\n",
    "test_examples = strong_meta_examples[:n_test]\n",
    "valid_examples = strong_meta_examples[n_test:n_test+n_valid]\n",
    "train_examples = strong_meta_examples[n_test+n_valid:] + weak_meta_examples\n",
    "np.random.shuffle(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.dump_jsonlines(train_examples, f\"{vars.DATA_DIR}/ripple_edits/meta_train_popular/train.jsonl\")\n",
    "io.dump_jsonlines(valid_examples, f\"{vars.DATA_DIR}/ripple_edits/meta_train_popular/valid.jsonl\")\n",
    "io.dump_jsonlines(test_examples, f\"{vars.DATA_DIR}/ripple_edits/meta_train_popular/test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1593"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_examples = io.load_jsonlines(f\"{vars.DATA_DIR}/ripple_edits/meta_train_recent/test.jsonl\")\n",
    "train_examples = io.load_jsonlines(f\"{vars.DATA_DIR}/ripple_edits/meta_train_recent+popular/train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero_outerloop_count = 0\n",
    "meta_examples = []\n",
    "propagation2verbtaim_count = defaultdict(int)\n",
    "propagation2count = defaultdict(int)\n",
    "# strong_meta_examples = []\n",
    "# weak_meta_examples = []\n",
    "count_rel = defaultdict(int)\n",
    "count_prop_types = defaultdict(int)\n",
    "\n",
    "for example in train_examples:\n",
    "    outerloop_instances = example[\"Logical_Generalization\"] + example[\"Compositionality_I\"] + example[\"Compositionality_II\"] + example[\"Subject_Aliasing\"]\n",
    "    # for ins in outerloop_instances:\n",
    "        # assert len(ins[\"test_queries\"]) == 1\n",
    "    locality_instances = example[\"Relation_Specificity\"] + example[\"Forgetfulness\"] \n",
    "    example[\"edit\"][\"text\"] = example[\"edit\"][\"prompt\"]\n",
    "    meta_examples.append(example[\"edit\"])\n",
    "    \n",
    "    outerloop_queries = []\n",
    "    for k in [\"Logical_Generalization\", \"Compositionality_I\", \"Compositionality_II\", \"Subject_Aliasing\"]:\n",
    "        for instance in example[k]:\n",
    "            for q in instance[\"test_queries\"]:\n",
    "                if len(q[\"answers\"]) > 0 and len([a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip() ) > 0 ]) > 0:\n",
    "                    q[\"question_type\"] = k\n",
    "                    ans_candidates = [a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip()) > 0]\n",
    "                    assert len(ans_candidates) > 0\n",
    "                    assert q[\"prompt\"][-1] not in \".\",  q[\"prompt\"]\n",
    "                    q[\"text\"] = q[\"prompt\"] + \" \" + ans_candidates[0]\n",
    "                    propagation2count[k] += 1\n",
    "                    propagation2verbtaim_count[k] += int(ans_candidates[0] in example[\"edit\"][\"text\"])\n",
    "                    outerloop_queries.append(q)\n",
    "    for outer_q in outerloop_queries:\n",
    "        count_prop_types[\"efficacy::\"+ outer_q[\"question_type\"]] += 1\n",
    "        count_rel[\"efficacy::\"+ outer_q[\"relation\"]] += 1\n",
    "    # assert len(outerloop_queries) > 0\n",
    "    meta_examples.extend(outerloop_queries)\n",
    "    \n",
    "    locality_queries = []\n",
    "    for k in [\"Relation_Specificity\", \"Forgetfulness\"]:\n",
    "        for instance in example[k]:\n",
    "            for q in instance[\"test_queries\"]:\n",
    "                if len(q[\"answers\"]) > 0 and len([a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip() ) > 0 ]) > 0:\n",
    "                    q[\"question_type\"] = k\n",
    "                    ans_candidates = [a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip()) > 0]\n",
    "                    assert len(ans_candidates) > 0\n",
    "                    assert q[\"prompt\"][-1] not in string.punctuation\n",
    "                    q[\"text\"] = q[\"prompt\"] + \" \" + ans_candidates[0]\n",
    "                    propagation2count[k] += 1\n",
    "                    propagation2verbtaim_count[k] += int(ans_candidates[0] in example[\"edit\"][\"text\"])\n",
    "                    locality_queries.append(q)\n",
    "    for loc_q in locality_queries:\n",
    "        count_prop_types[\"specificity::\"+ loc_q[\"question_type\"]] += 1\n",
    "        count_rel[\"specificity::\"+ loc_q[\"relation\"]] += 1\n",
    "    \n",
    "    meta_examples.extend(locality_queries)\n",
    "    \n",
    "    # assert len(locality_queries) > 0\n",
    "        \n",
    "# print(\"# strong examples\", len(strong_meta_examples))\n",
    "# print(\"# weak examples\", len(weak_meta_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.dump_jsonlines(meta_examples, f\"{vars.DATA_DIR}/ripple_edits/meta_train_recent+popular/train_all_queries.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Subject_Aliasing': 144,\n",
       "             'Relation_Specificity': 630,\n",
       "             'Forgetfulness': 99,\n",
       "             'Compositionality_I': 1112,\n",
       "             'Logical_Generalization': 123,\n",
       "             'Compositionality_II': 96})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propagation2count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "propagation2verbtaim_ratio = {k: propagation2verbtaim_count[k] / propagation2count[k] * 100 for k in propagation2count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Subject_Aliasing': 100.0,\n",
       " 'Relation_Specificity': 2.857142857142857,\n",
       " 'Forgetfulness': 74.74747474747475,\n",
       " 'Compositionality_I': 3.327338129496403,\n",
       " 'Logical_Generalization': 46.34146341463415,\n",
       " 'Compositionality_II': 100.0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propagation2verbtaim_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Subject_Aliasing': 144,\n",
       "             'Relation_Specificity': 20,\n",
       "             'Forgetfulness': 74,\n",
       "             'Compositionality_I': 38,\n",
       "             'Logical_Generalization': 57,\n",
       "             'Compositionality_II': 96})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propagation2verbtaim_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_from_disk(f\"{vars.DATA_DIR}/ripple_edits/meta_train_recent+popular/test_w_prefix.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = io.load_jsonlines(f\"{vars.DATA_DIR}/ripple_edits/meta_train_recent+popular/test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'subject_id', 'relation', 'target_id', 'original_fact', 'context', 'paraphrase', 'object'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'subject'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 21\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# print(original_test)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# print(augmentation)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m original_test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medit\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m augmentation[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m@@\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m original_test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medit\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@@\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m augmentation[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[43maugmentation\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubject\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;129;01min\u001b[39;00m original_test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medit\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m augmentation[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m original_test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medit\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     24\u001b[0m     original_test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medit\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubject\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m augmentation[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubject\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'subject'"
     ]
    }
   ],
   "source": [
    "new_test_data = []\n",
    "\n",
    "count = 0\n",
    "for i in range(len(test_data)):\n",
    "    original_test = test_data[i]\n",
    "    augmentation = ds[i-count]\n",
    "    if original_test[\"edit\"][\"prompt\"] == \"The name of the composer of Klavierstücke  I–IV is Karlheinz Stockhausen.\":\n",
    "        # print(\"found\")\n",
    "        original_test[\"edit\"][\"subject\"] = \"name of the composer of Klavierst\\u00fccke  I\\u2013IV\"\n",
    "        original_test[\"edit\"][\"object\"] = \"Karlheinz Stockhausen\"\n",
    "        assert original_test[\"edit\"][\"subject\"] in original_test[\"edit\"][\"prompt\"]\n",
    "        assert original_test[\"edit\"][\"object\"] in original_test[\"edit\"][\"prompt\"]\n",
    "        new_test_data.append(original_test)\n",
    "        count += 1\n",
    "        continue\n",
    "        # print(original_test)\n",
    "        # print(augmentation)\n",
    "    else:\n",
    "        \n",
    "        assert original_test[\"edit\"][\"prompt\"] == augmentation[\"prompt\"], f\"{i}@@\" + original_test[\"edit\"][\"prompt\"] + \"@@\" + augmentation[\"prompt\"]\n",
    "        assert augmentation[\"subject\"] in original_test[\"edit\"][\"prompt\"]\n",
    "        assert augmentation[\"object\"] in original_test[\"edit\"][\"prompt\"]\n",
    "\n",
    "        original_test[\"edit\"][\"subject\"] = augmentation[\"subject\"]\n",
    "        original_test[\"edit\"][\"object\"] = augmentation[\"object\"]\n",
    "        new_test_data.append(original_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.dump_jsonlines(new_test_data, f\"{vars.DATA_DIR}/ripple_edits/meta_train_recent+popular/test_aug.jsonl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a ripple edits for MEND-original version (before generating paraphrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"test\"\n",
    "train_examples = io.load_jsonlines(f\"{vars.DATA_DIR}/ripple_edits/meta_train_recent+popular/{split}.jsonl\")\n",
    "train_ds = load_from_disk(f\"{vars.DATA_DIR}/ripple_edits/meta_train_recent+popular/{split}_w_prefix.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flattend_train_examples = []\n",
    "\n",
    "# new_examples = []\n",
    "# missing_examples = []\n",
    "# mismatch_c = 0\n",
    "# for i in range(len(train_examples)):\n",
    "#     augmentation = train_ds[i-mismatch_c]\n",
    "#     example = train_examples[i]\n",
    "#     new_example = deepcopy(example)\n",
    "    \n",
    "#     new_example[\"edit\"][\"context\"] = augmentation[\"context\"]\n",
    "#     new_example[\"edit\"][\"paraphrase\"] = augmentation[\"paraphrase\"]\n",
    "#     new_example[\"edit\"][\"object\"] = augmentation[\"object\"]\n",
    "#     new_examples.append(new_example)\n",
    "#     # if example[\"edit\"][\"prompt\"] != augmentation[\"prompt\"]:\n",
    "#     #     mismatch_c += 1\n",
    "#     # else:\n",
    "        \n",
    "#     #     assert example[\"edit\"][\"prompt\"] == augmentation[\"prompt\"]\n",
    "#     #     assert augmentation[\"object\"] in example[\"edit\"][\"prompt\"]\n",
    "    \n",
    "#     #     flattend_train_examples.append({\"context\": augmentation[\"context\"], \"completion\": augmentation[\"object\"], \"paraphrase\": augmentation[\"paraphrase\"]})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattend_train_examples = []\n",
    "missing_examples = []\n",
    "mismatch_c = 0\n",
    "for i in range(len(train_examples)):\n",
    "    augmentation = train_ds[i-mismatch_c]\n",
    "    example = train_examples[i]\n",
    "    \n",
    "    if example[\"edit\"][\"prompt\"] != augmentation[\"prompt\"]:\n",
    "        mismatch_c += 1\n",
    "    else:\n",
    "        \n",
    "        assert example[\"edit\"][\"prompt\"] == augmentation[\"prompt\"]\n",
    "        assert augmentation[\"object\"] in example[\"edit\"][\"prompt\"], augmentation[\"object\"] + \"@@@\" + example[\"edit\"][\"prompt\"]\n",
    "    \n",
    "        flattend_train_examples.append({\"context\": augmentation[\"context\"], \"completion\": augmentation[\"object\"], \"paraphrase\": augmentation[\"paraphrase\"]})\n",
    "    \n",
    "    for k in [\"Logical_Generalization\", \"Compositionality_I\", \"Compositionality_II\", \"Subject_Aliasing\"]:\n",
    "        for instance in example[k]:\n",
    "            for q in instance[\"test_queries\"]:\n",
    "                if len(q[\"answers\"]) > 0 and len([a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip() ) > 0 ]) > 0:\n",
    "                    q[\"question_type\"] = k\n",
    "                    ans_candidates = [a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip()) > 0]\n",
    "                    assert len(ans_candidates) > 0\n",
    "                    assert q[\"prompt\"][-1] not in \".\",  q[\"prompt\"]\n",
    "                    assert \"phrase\" in q\n",
    "                    if q[\"phrase\"] is not None:\n",
    "                        flattend_train_examples.append({\"context\": q[\"prompt\"], \"completion\": ans_candidates[0], \"paraphrase\": q[\"phrase\"]})\n",
    "                    else:\n",
    "                        missing_examples.append({\"context\": q[\"prompt\"], \"completion\": ans_candidates[0],})\n",
    "    for k in [\"Relation_Specificity\", \"Forgetfulness\"]:\n",
    "        for instance in example[k]:\n",
    "            for q in instance[\"test_queries\"]:\n",
    "                if len(q[\"answers\"]) > 0 and len([a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip() ) > 0 ]) > 0:\n",
    "                    q[\"question_type\"] = k\n",
    "                    ans_candidates = [a[\"value\"] for a in q[\"answers\"] if len(a[\"value\"].strip()) > 0]\n",
    "                    assert len(ans_candidates) > 0\n",
    "                    assert q[\"prompt\"][-1] not in string.punctuation\n",
    "                    \n",
    "                    assert \"phrase\" in q\n",
    "                    if q[\"phrase\"] is not None:\n",
    "                        flattend_train_examples.append({\"context\": q[\"prompt\"], \"completion\": ans_candidates[0], \"paraphrase\": q[\"phrase\"]})\n",
    "                    else:\n",
    "                        missing_examples.append({\"context\": q[\"prompt\"], \"completion\": ans_candidates[0],})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing_ds = load_from_disk(f\"{vars.DATA_DIR}/ripple_edits/meta_train_recent/{split}_w_paraphrase.hf\")\n",
    "# for i in range(len(missing_ds)):\n",
    "#     augmentation = missing_ds[i]\n",
    "    \n",
    "#     flattend_train_examples.append({\"context\": augmentation[\"context\"], \"completion\": augmentation[\"completion\"], \"paraphrase\": augmentation[\"paraphrase\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17096"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flattend_train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.dump_jsonlines(flattend_train_examples, f\"{vars.DATA_DIR}/ripple_edits/meta_train_recent+popular/{split}_mend.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ex in io.load_jsonlines(f\"{vars.DATA_DIR}/ripple_edits/meta_train_recent/valid_mend.jsonl\"):\n",
    "    assert \"context\" in ex and ex[\"context\"] is not None\n",
    "    assert \"completion\" in ex and ex[\"completion\"] is not None\n",
    "    assert \"paraphrase\" in ex and ex[\"paraphrase\"] is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "recoe_aggregation = io.load_json(\"/u/zliu/datastor1/ReCoE/data/aggregation/counterfactual_datapoints_verified_atomic.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_recoe_aggregation = [\n",
    "    {\n",
    "        \"text\": \" \".join([f[\"fact\"] for f in x[\"direct_counterfactual_fact\"]]),\n",
    "        \"question\": x[\"question\"],\n",
    "        \"answers\": x[\"counterfactual_answer\"]\n",
    "     }\n",
    "    for x in recoe_aggregation\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "508"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_recoe_aggregation[-1]\n",
    "len(converted_recoe_aggregation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrate RoCE data to ripple edit trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "recoe_comparative = io.load_json(\"/u/zliu/datastor1/ReCoE/data/comparative/counterfactual_datapoints_verified_atomic.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [len(x[\"facts_per_choice\"]) for x in recoe_comparative]\n",
    "# recoe_comparative[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_recoe_comparative = []\n",
    "for x in recoe_comparative:\n",
    "    assert \"choice_1_counterfactuals\" in x[\"counterfactuals_per_choice\"]\n",
    "    assert \"choice_2_counterfactuals\" in x[\"counterfactuals_per_choice\"]\n",
    "    assert \"choice_3_counterfactuals\" not in x[\"counterfactuals_per_choice\"]\n",
    "    \n",
    "    counter_fact1_text = [f[\"fact\"] for f in x[\"counterfactuals_per_choice\"][\"choice_1_counterfactuals\"]]\n",
    "    counter_fact2_text = [f[\"fact\"] for f in x[\"counterfactuals_per_choice\"][\"choice_2_counterfactuals\"]]\n",
    "    texts = [counter_fact1_text, counter_fact2_text]\n",
    "    np.random.shuffle(texts)\n",
    "    counter_fact1_text_after_rand, counter_fact2_text_after_rand = texts\n",
    "    \n",
    "    \n",
    "    converted_recoe_comparative.append(\n",
    "        {\n",
    "            \"text\": \" \".join(counter_fact1_text_after_rand + counter_fact2_text_after_rand),\n",
    "            \"question\": x[\"question\"],\n",
    "            \"answers\": x[\"counterfactual_answer\"]\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_inferece = io.load_jsonlines(\"/data/users/zliu/entity_knowledge_propagation/data/entity_inferences/tv_show_implicit_attribute_dependent_adjective.json\")\n",
    "len(entity_inferece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_inferece2 = io.load_jsonlines(\"/data/users/zliu/entity_knowledge_propagation/data/entity_inferences/tv_show_explicit_attribute_dependent.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_entity_inferece = []\n",
    "\n",
    "for fpath in glob(\"/data/users/zliu/entity_knowledge_propagation/data/entity_inferences/*.json\", recursive=True):\n",
    "    all_entity_inferece.extend(io.load_jsonlines(fpath))\n",
    "len(all_entity_inferece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1192"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ecbd = []\n",
    "\n",
    "for fpath in glob(\"/data/users/zliu/entity_knowledge_propagation/data/ecbd/*.json\", recursive=True):\n",
    "    all_ecbd.extend(io.load_jsonlines(fpath))\n",
    "len(all_ecbd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(e[\"attribute\"] in e[\"def_target\"] for e in all_entity_inferece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content_between_tags(text, start_tag=\"<extra_id_0> \", end_tag=\" <extra_id_1>\"):\n",
    "    \"\"\"\n",
    "    Extract content between specified start and end tags.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to search in\n",
    "        start_tag (str): The starting tag\n",
    "        end_tag (str): The ending tag\n",
    "        \n",
    "    Returns:\n",
    "        str: The content between the tags, or None if not found\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Find the start position (end of start tag)\n",
    "        start_pos = text.find(start_tag)\n",
    "        if start_pos == -1:\n",
    "            return None\n",
    "        start_pos += len(start_tag)\n",
    "        \n",
    "        # Find the end position (start of end tag)\n",
    "        end_pos = text.find(end_tag, start_pos)\n",
    "        if end_pos == -1:\n",
    "            return None\n",
    "            \n",
    "        # Extract the content between tags\n",
    "        return text[start_pos:end_pos]\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting content: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_ecbd = []\n",
    "for e in all_ecbd:\n",
    "    resolved_definition = e[\"definition\"]\n",
    "    assert \"<extra_id_0>\" in resolved_definition\n",
    "    assert \"<extra_id_1>\" not in resolved_definition\n",
    "    target_content = extract_content_between_tags(e[\"def_target\"])[1:-1]\n",
    "    assert \"<extra_id_0> \" in e[\"def_target\"] and \" <extra_id_1>\" in e[\"def_target\"]\n",
    "    resolved_definition = resolved_definition.replace(\"<extra_id_0>\", target_content)\n",
    "    \n",
    "    assert len(e[\"probe_sentences\"]) == 1\n",
    "    resolved_probe_sentence = e[\"probe_sentences\"][\"template_0\"][\"probe_sentence\"]\n",
    "    assert \"<extra_id_0>\" in resolved_probe_sentence\n",
    "    assert \"<extra_id_1>\" not in resolved_probe_sentence\n",
    "    assert \"<extra_id_0> \" in e[\"probe_sentences\"][\"template_0\"][\"label\"] and \" <extra_id_1>\" in e[\"probe_sentences\"][\"template_0\"][\"label\"]\n",
    "    label_content = extract_content_between_tags(e[\"probe_sentences\"][\"template_0\"][\"label\"])[1:-1]\n",
    "    resolved_probe_sentence = resolved_probe_sentence.replace(\"<extra_id_0>\", label_content)\n",
    "    processed_ecbd.append(\n",
    "        {\n",
    "            \"text\": resolved_definition,\n",
    "            \"probe_sentence\": resolved_probe_sentence,\n",
    "            \"domain\": \"ecbd\",\n",
    "        }\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_entity_inferece = []\n",
    "for e in all_entity_inferece:\n",
    "    resolved_definition = e[\"definition\"]\n",
    "    assert \"<extra_id_0>\" in resolved_definition\n",
    "    assert \"<extra_id_1>\" not in resolved_definition\n",
    "    target_content = extract_content_between_tags(e[\"def_target\"])[1:-1]\n",
    "    assert \"<extra_id_0> \" in e[\"def_target\"] and \" <extra_id_1>\" in e[\"def_target\"]\n",
    "    resolved_definition = resolved_definition.replace(\"<extra_id_0>\", target_content)\n",
    "    \n",
    "    assert len(e[\"probe_sentences\"]) == 1\n",
    "    resolved_probe_sentence = e[\"probe_sentences\"][\"template_0\"][\"probe_sentence\"]\n",
    "    assert \"<extra_id_0>\" in resolved_probe_sentence\n",
    "    assert \"<extra_id_1>\" not in resolved_probe_sentence\n",
    "    # assert \"<extra_id_0> \" in e[\"label\"] and \" <extra_id_1>\" in e[\"label\"]\n",
    "    # label_content = extract_content_between_tags(e[\"label\"])[1:-1]\n",
    "    label_content = e[\"label\"]\n",
    "    resolved_probe_sentence = resolved_probe_sentence.replace(\"<extra_id_0>\", label_content)\n",
    "    processed_entity_inferece.append(\n",
    "        {\n",
    "            \"text\": resolved_definition,\n",
    "            \"probe_sentence\": resolved_probe_sentence,\n",
    "            \"domain\": \"entity_inference\",\n",
    "        }\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "ekp_data = processed_entity_inferece + processed_ecbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_popular_train = io.load_jsonlines(f\"{vars.DATA_DIR}/ripple_edits/meta_train_recent+popular/train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# io.dump_jsonlines(ekp_data, f\"{vars.DATA_DIR}/ripple_edits/meta_train_recent+popular/ekp.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cyclone Amphan left widespread damage in <extra_id_0>.'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e[\"probe_sentences\"][\"template_0\"][\"probe_sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Super Cyclonic Storm Amphan was a powerful and catastrophic tropical cyclone that caused widespread damage in Eastern India, specifically West Bengal, Odisha and in Bangladesh in May 2020.'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resolved_definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cyclone Amphan left widespread damage in Bangladesh.'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resolved_probe_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ex_id': 'Inter-Parliamentary Alliance on China_64177105_4_7',\n",
       " 'definition': \"The Inter-Parliamentary Alliance on China (IPAC) is an international<extra_id_0> alliance of parliamentarians from democratic countries focused on relations with the People's Republic of China (PRC), and specifically, the Chinese Communist Party (CCP).\",\n",
       " 'def_target': '<extra_id_0> , cross-party <extra_id_1>',\n",
       " 'category': 'tv_show',\n",
       " 'qid': ['international organization (Q484652)'],\n",
       " 'ent_str': 'Inter-Parliamentary Alliance on China',\n",
       " 'attribute': 'n/a',\n",
       " 'additional_sentences': [[\"The Inter-Parliamentary Alliance on China (IPAC) is <extra_id_0>party alliance of parliamentarians from democratic countries focused on relations with the People's Republic of China (PRC), and specifically, the Chinese Communist Party (CCP).\",\n",
       "   '<extra_id_0> an international, cross- <extra_id_1>'],\n",
       "  [\"The Inter-Parliamentary Alliance on China (IPAC) is <extra_id_0>-party alliance of parliamentarians from democratic countries focused on relations with the People's Republic of China (PRC), and specifically, the Chinese Communist Party (CCP).\",\n",
       "   '<extra_id_0> an international, cross <extra_id_1>'],\n",
       "  [\"The Inter-Parliamentary Alliance on China (IPAC) <extra_id_0>, cross-party alliance of parliamentarians from democratic countries focused on relations with the People's Republic of China (PRC), and specifically, the Chinese Communist Party (CCP).\",\n",
       "   '<extra_id_0> is an international <extra_id_1>'],\n",
       "  [\"The Inter-Parliamentary Alliance on China (<extra_id_0>, cross-party alliance of parliamentarians from democratic countries focused on relations with the People's Republic of China (PRC), and specifically, the Chinese Communist Party (CCP).\",\n",
       "   '<extra_id_0> IPAC) is an international <extra_id_1>'],\n",
       "  [\"The Inter-Parliamentary Alliance on China (IPAC) <extra_id_0>-party alliance of parliamentarians from democratic countries focused on relations with the People's Republic of China (PRC), and specifically, the Chinese Communist Party (CCP).\",\n",
       "   '<extra_id_0> is an international, cross <extra_id_1>'],\n",
       "  [\"The Inter-Parliamentary Alliance on China (IPAC<extra_id_0> international, cross-party alliance of parliamentarians from democratic countries focused on relations with the People's Republic of China (PRC), and specifically, the Chinese Communist Party (CCP).\",\n",
       "   '<extra_id_0> ) is an <extra_id_1>'],\n",
       "  [\"The Inter-Parliamentary Alliance on China (IPAC) is an <extra_id_0> cross-party alliance of parliamentarians from democratic countries focused on relations with the People's Republic of China (PRC), and specifically, the Chinese Communist Party (CCP).\",\n",
       "   '<extra_id_0> international, <extra_id_1>'],\n",
       "  [\"The Inter-Parliamentary Alliance on China (IPAC<extra_id_0> cross-party alliance of parliamentarians from democratic countries focused on relations with the People's Republic of China (PRC), and specifically, the Chinese Communist Party (CCP).\",\n",
       "   '<extra_id_0> ) is an international, <extra_id_1>'],\n",
       "  [\"The Inter-Parliamentary Alliance <extra_id_0>IPAC) is an international, cross-party alliance of parliamentarians from democratic countries focused on relations with the People's Republic of China (PRC), and specifically, the Chinese Communist Party (CCP).\",\n",
       "   '<extra_id_0> on China ( <extra_id_1>']],\n",
       " 'conceptnet': [],\n",
       " 'probe_sentences': {'template_0': {'probe_sentence': 'On 22 September 2020, speaking as the co-chair of the Inter-Parliamentary Alliance on China, Duncan Smith made a more assertive statement asking the IOC to think again about allowing <extra_id_0> to hosting the games, addressing China with the words: \"The free world does have a strong position to say the bullying, the threatening, the internal repression, the border disputes, the arrogant attitude to your neighbours, the breaking of the treaty with Hong Kong — these must have consequences.\"',\n",
       "   'label': '<extra_id_0> China <extra_id_1>'}}}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ecbd[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(e[\"attribute\"] in e[\"def_target\"] for e in all_ecbd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'template_0': {'probe_sentence': 'Lawrence \"Larry\" Palmer is a <extra_id_0> who manages a well-known company.',\n",
       "  'labels': ['<extra_id_0> businessman <extra_id_1>',\n",
       "   '<extra_id_0> performing artist <extra_id_1>',\n",
       "   '<extra_id_0> filmmaker <extra_id_1>',\n",
       "   '<extra_id_0> artist <extra_id_1>',\n",
       "   '<extra_id_0> lawmaker <extra_id_1>',\n",
       "   '<extra_id_0> statesman <extra_id_1>',\n",
       "   '<extra_id_0> musician <extra_id_1>']}}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_entity_inferece[0][\"probe_sentences\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
