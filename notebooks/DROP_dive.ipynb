{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zliu/miniconda3/envs/cpt/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "import pandas as pd\n",
    "from experiments.musique.inference_only import macro_averaging\n",
    "from knowledge_propagation.utils import io, vars, extractor\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import describe\n",
    "from thefuzz import fuzz\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from dateutil.parser import parse\n",
    "from dateutil.parser import ParserError\n",
    "\n",
    "def is_date(string):\n",
    "    try:\n",
    "        parse(string)\n",
    "        return True\n",
    "    except ParserError:\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_fact_date_dataset = load_from_disk(\"/u/zliu/datastor1/KE-by-CP/data/debug_meta_train/common_date_data/common_date_question_generation.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['topic', 'qa_pairs'],\n",
       "    num_rows: 31\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_fact_date_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_questions = set()\n",
    "common_fact_date_df_content = []\n",
    "for i in range(len(common_fact_date_dataset)):\n",
    "    topic_facts = common_fact_date_dataset[i]\n",
    "    topic = topic_facts[\"topic\"]\n",
    "    for qa in topic_facts[\"qa_pairs\"]:\n",
    "        \n",
    "        if qa[\"question\"] in unique_questions:\n",
    "            continue\n",
    "        unique_questions.add(qa[\"question\"])\n",
    "        qa[\"topic\"] = topic\n",
    "        common_fact_date_df_content.append(qa)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_fact_date_df = pd.DataFrame(common_fact_date_df_content)\n",
    "common_fact_date_df.to_excel(\"/u/zliu/datastor1/KE-by-CP/data/debug_meta_train/common_date_data/common_fact_date.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_fact_date_rewrite_df = pd.read_excel(\"/u/zliu/datastor1/KE-by-CP/data/debug_meta_train/common_date_data/common_fact_date_w-rewrite.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1411"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# increment answer by 1 year\n",
    "common_fact_date_rewrite_increment_df_content = []\n",
    "bc_year_count = 0\n",
    "s_year_count = 0\n",
    "interval_year_count = 0\n",
    "for r in common_fact_date_rewrite_df.to_dict(\"records\"):\n",
    "    new_r = deepcopy(r)\n",
    "    \n",
    "    if \"bc\" in r[\"answer\"].lower():\n",
    "        bc_year_count += 1\n",
    "        continue\n",
    "    if \"s\" in r[\"answer\"].lower() or \"c\" in r[\"answer\"].lower():\n",
    "        s_year_count += 1\n",
    "        continue\n",
    "    if \"-\" in r[\"answer\"].lower():\n",
    "        interval_year_count += 1\n",
    "        continue\n",
    "    assert is_date(r[\"answer\"]), r\n",
    "    \n",
    "    assert r[\"answer\"].isdigit(), r\n",
    "    new_r[\"rewrite_answer\"] = str(int(r[\"answer\"]) + 1)\n",
    "    new_r[\"original_answer\"] = new_r[\"answer\"]\n",
    "    new_r[\"original_question\"] = new_r[\"question\"]\n",
    "    new_r[\"answer\"] = new_r[\"rewrite_answer\"]\n",
    "    new_r[\"question\"] = new_r[\"rewrite_question\"]\n",
    "    del new_r[\"rewrite_answer\"]\n",
    "    del new_r[\"rewrite_question\"]\n",
    "    common_fact_date_rewrite_increment_df_content.append(new_r)\n",
    "len(common_fact_date_rewrite_increment_df_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_fact_date_rewrite_increment_df = pd.DataFrame(common_fact_date_rewrite_increment_df_content)\n",
    "common_fact_date_rewrite_increment_df.to_excel(\"/u/zliu/datastor1/KE-by-CP/data/debug_meta_train/common_date_data/common_fact_date_w-rewrite_increment.xlsx\", index=False)\n",
    "\n",
    "io.dump_jsonlines(common_fact_date_rewrite_increment_df_content, \"/u/zliu/datastor1/KE-by-CP/data/debug_meta_train/common_date_data/common_fact_date_w-rewrite_increment.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_shuffle = np.arange(len(common_fact_date_rewrite_increment_df_content))\n",
    "np.random.shuffle(rand_shuffle)\n",
    "\n",
    "n_dev = 100\n",
    "n_train = len(common_fact_date_rewrite_increment_df_content) - n_dev\n",
    "train = [common_fact_date_rewrite_increment_df_content[i] for i in rand_shuffle[:n_train]]\n",
    "valid = [common_fact_date_rewrite_increment_df_content[i] for i in rand_shuffle[n_train:]]\n",
    "io.dump_jsonlines(train, \"/u/zliu/datastor1/KE-by-CP/data/debug_meta_train/common_date_data/train.jsonl\")\n",
    "\n",
    "io.dump_jsonlines(valid, \"/u/zliu/datastor1/KE-by-CP/data/debug_meta_train/common_date_data/valid.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': '1882',\n",
       " 'question': 'When was the year after the year that the start of the Scramble for Africa happened?',\n",
       " 'topic': 'Colonization and Empire Building',\n",
       " 'original_answer': '1881',\n",
       " 'original_question': 'What year marks the start of the Scramble for Africa?'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': '1733',\n",
       " 'question': 'When was the year after the year that George Washington was born?',\n",
       " 'topic': \"Historic Figures' Birthday\",\n",
       " 'original_answer': '1732',\n",
       " 'original_question': 'In which year was George Washington born?'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_fact_date_rewrite_increment_df_content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_date_year_after = io.load_jsonlines(\"/data/users/zliu/KE-by-CP/data/debug_meta_train/common_date_data_year_after/valid.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_date_year = []\n",
    "\n",
    "for d in common_date_year_after:\n",
    "    new_d = {\n",
    "        \"question\": d[\"original_question\"],\n",
    "        \"answer\": d[\"original_answer\"],\n",
    "        \"topic\": d[\"topic\"],\n",
    "        \"year_after_question\": d[\"question\"],\n",
    "        \"year_after_answer\": d[\"answer\"],\n",
    "    }\n",
    "    common_date_year.append(new_d)\n",
    "len(common_date_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.dump_jsonlines(common_date_year, \"/data/users/zliu/KE-by-CP/data/debug_meta_train/common_date_data/valid.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1097"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ds = load_dataset(\"ucinlp/drop\")\n",
    "\n",
    "date_count = 0\n",
    "failed_count = 0\n",
    "drop_date_instances = []\n",
    "passage = set()\n",
    "for split in [\"train\", \"validation\"]:\n",
    "    for i in range(len(ds[split])):\n",
    "        datum = ds[split][i]\n",
    "        span = datum[\"answers_spans\"]\n",
    "        a_str = datum[\"answers_spans\"][\"spans\"][0]\n",
    "        \n",
    "        q_str = datum[\"question\"].lower()\n",
    "        date_count += len([t for t in span[\"types\"] if t == \"date\"])\n",
    "        \n",
    "        if not any([t in q_str.lower() for t in [\"date\", \"year\", \"when\",]]):\n",
    "            continue\n",
    "        if any([t in q_str.lower() for t in [\"month\", \"how many\", \"how old\"]]):\n",
    "            continue\n",
    "        \n",
    "        if \"date\" in span[\"types\"] and datum[\"passage\"] not in passage:\n",
    "            date_index = span[\"types\"].index(\"date\")\n",
    "            a_str = span[\"spans\"][date_index]\n",
    "            \n",
    "            try:\n",
    "                if str(parse(a_str).year) in a_str:\n",
    "                    drop_date_instances.append(datum)\n",
    "            except:\n",
    "                failed_count += 1\n",
    "                pass\n",
    "\n",
    "drop_unified_format = []\n",
    "\n",
    "for datum in drop_date_instances:\n",
    "    drop_unified_format.append({\n",
    "        \"id\": datum[\"query_id\"],\n",
    "        \"question\": datum[\"question\"],\n",
    "        \"answer\": datum[\"answers_spans\"][\"spans\"][0],\n",
    "        # \"texts\": [],\n",
    "        \"dataset\": \"drop\"\n",
    "    })\n",
    "len(drop_unified_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "musique_date_instances = []\n",
    "single_hop_id = set()\n",
    "years = []\n",
    "\n",
    "for split in [\"train\", \"dev\", \"test\"]:\n",
    "    musique_instances = io.load_jsonlines(f\"/u/zliu/datastor1/KE-by-CP/data/musique/musique_ans_v1.0_{split}.jsonl\")\n",
    "\n",
    "    for instance in musique_instances:\n",
    "        if \"question_decomposition\" not in instance:\n",
    "            continue\n",
    "        for q in instance[\"question_decomposition\"]:\n",
    "            q_str = q[\"question\"].lower()\n",
    "            a_str = q[\"answer\"]\n",
    "            # if not any([t in q_str.lower() for t in [\"date\", \"year\", \"when\",]]):\n",
    "            #     continue\n",
    "            # if any([t in q_str.lower() for t in [\"month\", \"how many\", ]]):\n",
    "            #     continue\n",
    "            # if any([t in q_str for t in [\"date\", \"year\", \"when\",]]) and is_date(a_str) and q[\"id\"] not in single_hop_id:\n",
    "            if is_date(a_str) and q[\"id\"] not in single_hop_id and str(parse(a_str).year) in a_str:\n",
    "                single_hop_id.add(q[\"id\"])\n",
    "                years.append(parse(a_str).year)\n",
    "                musique_date_instances.append(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how were the #1 expelled from #2 ?'"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6a8f3d413e46c7962034b7a815ac48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1075"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"mandarjoshi/trivia_qa\", \"rc.nocontext\")\n",
    "\n",
    "questions = set()\n",
    "c = 0\n",
    "trivia_date_data = []\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "     for i in range(len(ds[split])):\n",
    "        datum = ds[split][i]\n",
    "        a_str = datum[\"answer\"][\"value\"]\n",
    "        q_str = datum[\"question\"]\n",
    "        if not any([t in q_str.lower() for t in [\"date\", \"year\", \"when\",]]):\n",
    "            continue\n",
    "        if any([t in q_str.lower() for t in [\"month\", \"how many\", \"how old\"]]):\n",
    "            continue\n",
    "        if is_date(a_str) and q_str not in questions and str(parse(a_str).year) in a_str:\n",
    "            questions.add(q_str)\n",
    "            trivia_date_data.append(datum)\n",
    "            c += 1\n",
    "            \n",
    "trivia_unified_format = []\n",
    "\n",
    "for datum in trivia_date_data:\n",
    "    trivia_unified_format.append({\n",
    "        \"id\": datum[\"question_id\"],\n",
    "        \"question\": datum[\"question\"],\n",
    "        \"answer\": datum[\"answer\"][\"value\"],\n",
    "        # \"texts\": [],\n",
    "        \"dataset\": \"triviaqa\"\n",
    "    })\n",
    "len(trivia_unified_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90447/90447 [00:00<00:00, 140174.49it/s]\n",
      "100%|██████████| 7405/7405 [00:00<00:00, 167191.45it/s]\n",
      "100%|██████████| 7405/7405 [00:00<00:00, 2338238.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9517"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "questions = set()\n",
    "c = 0\n",
    "hotpotqa_date_data = []\n",
    "# for split in [\"train\", \"validation\", \"test\"]:\n",
    "for split, filename in [(\"train\", \"hotpot_train_v1.1.json\"), (\"dev\", \"hotpot_dev_fullwiki_v1.json\"), (\"test\", \"hotpot_test_fullwiki_v1.json\")]:\n",
    "    data = io.load_json(f\"/u/zliu/datastor1/KE-by-CP/data/hotpotqa/{filename}\")\n",
    "    for i in tqdm(range(len(data))):\n",
    "        datum = data[i]\n",
    "        if \"answer\" not in datum or \"question\" not in datum:\n",
    "            continue\n",
    "        a_str = datum[\"answer\"]\n",
    "        q_str = datum[\"question\"]\n",
    "        if not any([t in q_str.lower() for t in [\"date\", \"year\", \"when\",]]):\n",
    "            continue\n",
    "        if any([t in q_str.lower() for t in [\"month\", \"how many\", \"how old\"]]):\n",
    "            continue\n",
    "        if is_date(a_str) and q_str not in questions and str(parse(a_str).year) in a_str:\n",
    "            questions.add(q_str)\n",
    "            hotpotqa_date_data.append(datum)\n",
    "            c += 1\n",
    "            \n",
    "hotpotqa_unified_format = []\n",
    "\n",
    "for datum in hotpotqa_date_data:\n",
    "    supporting_titles = [t for t, _ in datum[\"supporting_facts\"]]\n",
    "    texts = [\"\".join(lines) for t, lines in datum[\"context\"] if t in supporting_titles]\n",
    "    \n",
    "    hotpotqa_unified_format.append({\n",
    "        \"id\": datum[\"_id\"],\n",
    "        \"question\": datum[\"question\"],\n",
    "        \"answer\": datum[\"answer\"],\n",
    "        # \"texts\": texts,\n",
    "        \"dataset\": \"hotpotqa\"\n",
    "    })\n",
    "len(hotpotqa_unified_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'io' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m bio_syn_data \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241m.\u001b[39mload_jsonlines(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/u/zliu/datastor1/KE-by-CP/data/debug_meta_train/bio_syn_data/train.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'io' is not defined"
     ]
    }
   ],
   "source": [
    "bio_syn_data = io.load_jsonlines(\"/u/zliu/datastor1/KE-by-CP/data/debug_meta_train/bio_syn_data/train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9518"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5a7d0db955429909bec76924',\n",
       " 'question': 'The Dutch-Belgian television series that \"House of Anubis\" was based on first aired in what year?',\n",
       " 'answer': '2006',\n",
       " 'dataset': 'hotpotqa'}"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotpotqa_unified_format[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_data = trivia_unified_format + hotpotqa_unified_format + drop_unified_format\n",
    "\n",
    "\n",
    "pd.DataFrame([d for d in sft_data if len(d[\"question\"]) <= 400]).to_excel(\"/u/zliu/datastor1/KE-by-CP/data/debug_meta_train/real_date_data/all_data.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_sft_data = []\n",
    "special_data = []\n",
    "for datum in sft_data:\n",
    "    if datum[\"answer\"].isdigit() and len(datum[\"answer\"]) != 4:\n",
    "        special_data.append(datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11689.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>107.059201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>64.366637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>69.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>122.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>654.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "count  11689.000000\n",
       "mean     107.059201\n",
       "std       64.366637\n",
       "min       17.000000\n",
       "25%       69.000000\n",
       "50%       90.000000\n",
       "75%      122.000000\n",
       "max      654.000000"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import describe\n",
    "\n",
    "pd.DataFrame([len(d[\"question\"]) for d in sft_data]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_data = pd.read_excel(\"/u/zliu/datastor1/KE-by-CP/data/debug_meta_train/real_date_data/all_data_w-rewrite.xlsx\").to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "increment_rewrite_data = []\n",
    "failed_count = 0\n",
    "for datum in rewrite_data:\n",
    "    a_str = datum[\"answer\"]\n",
    "    try:\n",
    "        assert str(parse(a_str).year) in a_str\n",
    "        rewrite_answer = str(int(parse(a_str).year) + 1)\n",
    "        increment_rewrite_datum = deepcopy(datum)\n",
    "        \n",
    "        increment_rewrite_datum[\"rewrite_answer\"] = rewrite_answer\n",
    "        increment_rewrite_data.append(increment_rewrite_datum)\n",
    "    except:\n",
    "        failed_count += 1\n",
    "        continue\n",
    "failed_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(increment_rewrite_data).to_excel(\"/u/zliu/datastor1/KE-by-CP/data/debug_meta_train/real_date_data/all_data_w-rewrite_increment.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_excel(\"/u/zliu/datastor1/KE-by-CP/data/debug_meta_train/real_date_data/all_data_w-rewrite_increment.xlsx\").to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.dump_jsonlines(all_data, \"/u/zliu/datastor1/KE-by-CP/data/debug_meta_train/real_date_data/all_data_w-rewrite_increment.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10785"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([d for d in all_data if len(d[\"question\"]) <= 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "increment_date_data = [\n",
    "    {\n",
    "        \"id\": d[\"id\"],\n",
    "        \"question\": d[\"rewrite_question\"],\n",
    "        \"answer\": d[\"rewrite_answer\"],\n",
    "        \"original_question\": d[\"question\"],\n",
    "        \"original_answer\": d[\"answer\"],\n",
    "        \"dataset\": d[\"dataset\"],\n",
    "    } \n",
    "    for d in all_data ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dev = 1000\n",
    "n_train = len(increment_date_data) - n_dev\n",
    "\n",
    "rand_shuffle = np.arange(len(increment_date_data))\n",
    "np.random.shuffle(rand_shuffle)\n",
    "\n",
    "increment_date_data_train = [increment_date_data[i] for i in rand_shuffle[:n_train]]\n",
    "increment_date_data_dev = [increment_date_data[i] for i in rand_shuffle[n_train:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.dump_jsonlines(increment_date_data_train, \"/u/zliu/datastor1/KE-by-CP/data/debug_meta_train/real_date_data/increment_date_data_train.jsonl\")\n",
    "io.dump_jsonlines(increment_date_data_dev, \"/u/zliu/datastor1/KE-by-CP/data/debug_meta_train/real_date_data/increment_date_data_dev.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate synthetic meta-training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_template = \"{first_name} {last_name} was born in {birth_year} in {birth_place}. {gender_start} started the career of {career} in {career_year}. In {death_year}, {gender} passed away.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_names = list(set(\"\"\"Michael\n",
    "Emma\n",
    "James\n",
    "Sophia\n",
    "David\n",
    "Olivia\n",
    "William\n",
    "Ava\n",
    "Alexander\n",
    "Isabella\n",
    "John\n",
    "Mia\n",
    "Matthew\n",
    "Charlotte\n",
    "Daniel\n",
    "Amelia\n",
    "Christopher\n",
    "Harper\n",
    "Joseph\n",
    "Evelyn\n",
    "Benjamin\n",
    "Abigail\n",
    "Andrew\n",
    "Emily\n",
    "Robert\n",
    "Elizabeth\n",
    "Thomas\n",
    "Sofia\n",
    "Samuel\n",
    "Avery\n",
    "Jacob\n",
    "Ella\n",
    "Nathan\n",
    "Scarlett\n",
    "Nicholas\n",
    "Grace\n",
    "Ryan\n",
    "Victoria\n",
    "Joshua\n",
    "Madison\n",
    "Ethan\n",
    "Lily\n",
    "Noah\n",
    "Hannah\n",
    "Anthony\n",
    "Chloe\n",
    "Jonathan\n",
    "Zoe\n",
    "Aaron\n",
    "Nora\n",
    "Gabriel\n",
    "Riley\n",
    "Lucas\n",
    "Layla\n",
    "Christina\n",
    "Maria\n",
    "Jason\n",
    "Sarah\n",
    "Tyler\n",
    "Natalie\n",
    "Kevin\n",
    "Leah\n",
    "Eric\n",
    "Maya\n",
    "Brian\n",
    "Jennifer\n",
    "Brandon\n",
    "Laura\n",
    "Adam\n",
    "Elena\n",
    "Marcus\n",
    "Jasmine\n",
    "Caleb\n",
    "Anna\"\"\".split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_names = list(set(\"\"\"Smith\n",
    "Johnson\n",
    "Williams\n",
    "Brown\n",
    "Jones\n",
    "Garcia\n",
    "Miller\n",
    "Davis\n",
    "Rodriguez\n",
    "Martinez\n",
    "Hernandez\n",
    "Lopez\n",
    "Gonzalez\n",
    "Wilson\n",
    "Anderson\n",
    "Thomas\n",
    "Taylor\n",
    "Moore\n",
    "Jackson\n",
    "Martin\n",
    "Lee\n",
    "Perez\n",
    "Thompson\n",
    "White\n",
    "Harris\n",
    "Sanchez\n",
    "Clark\n",
    "Ramirez\n",
    "Lewis\n",
    "Robinson\n",
    "Walker\n",
    "Young\n",
    "Allen\n",
    "King\n",
    "Wright\n",
    "Scott\n",
    "Torres\n",
    "Nguyen\n",
    "Hill\n",
    "Flores\n",
    "Green\n",
    "Adams\n",
    "Nelson\n",
    "Baker\n",
    "Hall\n",
    "Rivera\n",
    "Campbell\n",
    "Mitchell\n",
    "Carter\n",
    "Roberts\n",
    "Gomez\n",
    "Phillips\n",
    "Evans\n",
    "Turner\n",
    "Diaz\n",
    "Parker\n",
    "Cruz\n",
    "Edwards\n",
    "Collins\n",
    "Reyes\n",
    "Stewart\n",
    "Morris\n",
    "Morales\n",
    "Murphy\n",
    "Cook\n",
    "Rogers\n",
    "Gutierrez\n",
    "Ortiz\n",
    "Morgan\n",
    "Cooper\n",
    "Peterson\n",
    "Bailey\n",
    "Reed\n",
    "Kelly\n",
    "Howard\n",
    "Ramos\n",
    "Kim\n",
    "Cox\n",
    "Ward\n",
    "Richardson\n",
    "Watson\n",
    "Brooks\n",
    "Chavez\n",
    "Wood\n",
    "James\n",
    "Bennett\n",
    "Gray\n",
    "Mendoza\n",
    "Ruiz\n",
    "Hughes\n",
    "Price\n",
    "Alvarez\n",
    "Castillo\"\"\".split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "genders = [\"he\", \"she\"]\n",
    "careers = list(set(\"\"\"Doctor\n",
    "Teacher\n",
    "Software Engineer\n",
    "Nurse\n",
    "Accountant\n",
    "Chef\n",
    "Architect\n",
    "Lawyer\n",
    "Electrician\n",
    "Marketing Manager\n",
    "Graphic Designer\n",
    "Pharmacist\n",
    "Police Officer\n",
    "Financial Analyst\n",
    "Journalist\n",
    "Mechanical Engineer\n",
    "Social Worker\n",
    "Veterinarian\n",
    "Pilot\n",
    "Dental Hygienist\n",
    "Web Developer\n",
    "Physical Therapist\n",
    "Human Resources Manager\n",
    "Firefighter\n",
    "Real Estate Agent\n",
    "Data Scientist\n",
    "Interior Designer\n",
    "Occupational Therapist\n",
    "Construction Manager\n",
    "Speech Pathologist\n",
    "Cybersecurity Analyst\n",
    "Photographer\n",
    "Psychologist\n",
    "Plumber\n",
    "Flight Attendant\n",
    "Marine Biologist\n",
    "Athletic Trainer\n",
    "Urban Planner\n",
    "Welder\n",
    "Dietitian\n",
    "Librarian\n",
    "Civil Engineer\n",
    "Paralegal\n",
    "Film Producer\n",
    "Actuary\n",
    "Event Planner\n",
    "Scientist\n",
    "Carpenter\n",
    "Financial Advisor\n",
    "Lab Technician\"\"\".split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_years = np.random.choice(range(1900, 2020), 100, replace=False)\n",
    "birth_places = list(set(\"\"\"New York City\n",
    "Tokyo\n",
    "Paris\n",
    "London\n",
    "Sydney\n",
    "Rome\n",
    "Barcelona\n",
    "Amsterdam\n",
    "Singapore\n",
    "Cairo\n",
    "Rio de Janeiro\n",
    "Vancouver\n",
    "Istanbul\n",
    "Dubai\n",
    "Cape Town\n",
    "Bangkok\n",
    "Dublin\n",
    "Seoul\n",
    "Venice\n",
    "Hong Kong\n",
    "San Francisco\n",
    "Mumbai\n",
    "Berlin\n",
    "Buenos Aires\n",
    "Prague\n",
    "Stockholm\n",
    "Montreal\n",
    "Beijing\n",
    "Athens\n",
    "Marrakech\n",
    "Kyoto\n",
    "Vienna\n",
    "Copenhagen\n",
    "Jerusalem\n",
    "Nairobi\n",
    "Mexico City\n",
    "Santorini\n",
    "Bali\n",
    "Toronto\n",
    "Reykjavik\n",
    "Havana\n",
    "Moscow\n",
    "Queenstown\n",
    "Florence\n",
    "Edinburgh\n",
    "Machu Picchu\n",
    "Petra\n",
    "Bora Bora\n",
    "Chicago\n",
    "Maldives\"\"\".split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3441000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first_names) * len(last_names) * len(birth_years) * len(birth_places) * len(genders) * len(careers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 999560/1000000 [02:03<00:00, 8039.46it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "n_data = 1000000\n",
    "\n",
    "tuples = set()\n",
    "\n",
    "career_year_question_template = \"When was the year after the year that {first_name} {last_name} started the career of {career}?\"\n",
    "death_year_question_template = \"When was the year after the year that {first_name} {last_name} passed away?\"\n",
    "birth_year_question_template = \"When was the year after the year that {first_name} {last_name} was born?\"\n",
    "\n",
    "syn_data = []\n",
    "\n",
    "pbar = tqdm(total = n_data)\n",
    "\n",
    "max_death_year = 2023\n",
    "\n",
    "\n",
    "while len(syn_data) < n_data:\n",
    "    \n",
    "    first_name = np.random.choice(first_names)\n",
    "    last_name = np.random.choice(last_names)\n",
    "\n",
    "\n",
    "    birth_year = np.random.choice(birth_years)\n",
    "    birth_place = np.random.choice(birth_places)\n",
    "    gender = np.random.choice(genders)\n",
    "    career = np.random.choice(careers)\n",
    "\n",
    "    growth_duration = np.random.randint(14, 26)\n",
    "    career_year = birth_year + growth_duration\n",
    "    career_duration = np.random.randint(4, 40)\n",
    "    retire_year = career_year + career_duration\n",
    "\n",
    "    retire_duration = np.random.randint(1, 10)\n",
    "    death_year = retire_year + retire_duration\n",
    "    if death_year > max_death_year:\n",
    "        continue\n",
    "    \n",
    "    info_tuple = (first_name, last_name, birth_year, birth_place, gender, career, career_year, death_year)\n",
    "    \n",
    "    if info_tuple not in tuples:\n",
    "        tuples.add(info_tuple)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    text = text_template.format(\n",
    "        first_name=first_name,\n",
    "        last_name=last_name,\n",
    "        birth_year=birth_year,\n",
    "        birth_place=birth_place,\n",
    "        gender=gender,\n",
    "        gender_start=gender.capitalize(),\n",
    "        career=career,\n",
    "        career_year=career_year,\n",
    "        death_year=death_year,\n",
    "    )\n",
    "    \n",
    "    rand_idx = np.random.choice(range(3))\n",
    "\n",
    "    template_name, question_template = [\n",
    "        (\"career\", career_year_question_template), \n",
    "        (\"death\",death_year_question_template), \n",
    "        (\"birth\", birth_year_question_template),\n",
    "    ][rand_idx]\n",
    "    \n",
    "    question = question_template.format(\n",
    "        first_name=first_name,\n",
    "        last_name=last_name,\n",
    "        career=career,\n",
    "    )\n",
    "    if template_name == \"career\":\n",
    "        answer = career_year + 1\n",
    "    elif template_name == \"death\":\n",
    "        answer = death_year + 1\n",
    "    else:\n",
    "        answer = birth_year + 1\n",
    "\n",
    "    syn_data.append(\n",
    "        {\n",
    "            \"text\": text,\n",
    "            \"question\": question,\n",
    "            \"answer\": str(int(answer))\n",
    "        }\n",
    "    )\n",
    "    pbar.update(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [02:20<00:00, 8039.46it/s]"
     ]
    }
   ],
   "source": [
    "io.dump_jsonlines(syn_data, \"/u/zliu/datastor1/KE-by-CP/data/debug_meta_train/bio_syn_data/all_data.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all(int(d[\"answer\"]) <= max_death_year + 1 for d in syn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 100\n",
    "n_dev = 100\n",
    "n_train = n_data - n_test - n_dev\n",
    "\n",
    "rand_shuffle = np.arange(n_data)\n",
    "np.random.shuffle(rand_shuffle)\n",
    "\n",
    "train_data = [syn_data[i] for i in rand_shuffle[:n_train]]\n",
    "dev_data = [syn_data[i] for i in rand_shuffle[n_train:n_train+n_dev]]\n",
    "test_data = [syn_data[i] for i in rand_shuffle[n_train+n_dev:]]\n",
    "\n",
    "io.dump_jsonlines(train_data, \"/u/zliu/datastor1/KE-by-CP/data/debug_meta_train/bio_syn_data/train.jsonl\")\n",
    "io.dump_jsonlines(dev_data, \"/u/zliu/datastor1/KE-by-CP/data/debug_meta_train/bio_syn_data/valid.jsonl\")\n",
    "io.dump_jsonlines(test_data, \"/u/zliu/datastor1/KE-by-CP/data/debug_meta_train/bio_syn_data/test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(io.load_jsonlines(\"/u/zliu/datastor1/KE-by-CP/data/debug_meta_train/bio_syn_data/train.jsonl\")).sample(10).to_excel(\"/u/zliu/datastor1/mend/spotcheck/syn_text_train_sample.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(io.load_jsonlines(\"/u/zliu/datastor1/KE-by-CP/data/debug_meta_train/common_date_data/train.jsonl\")).sample(10).to_excel(\"/u/zliu/datastor1/mend/spotcheck/common_fact_train_sample.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
