{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "# from experiments.musique.inference_only import macro_averaging\n",
    "from knowledge_propagation.utils import io, vars, extractor\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from scipy.stats import describe\n",
    "from thefuzz import fuzz\n",
    "\n",
    "from collections import Counter\n",
    "from knowledge_propagation.modules.evaluators import (\n",
    "    ExactMatchEvaluator,\n",
    "    RougeEvaluator,\n",
    "    OpenAIEvaluator,\n",
    ")\n",
    "llm_evaluator = OpenAIEvaluator()\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "os.getcwd()\n",
    "def macro_averaging(df: pd.DataFrame, metrics: List[str], multi_level_averaging: List[str]):\n",
    "    \"\"\"\n",
    "    Do macro-averaging over the given metrics and multi-level averaging categories.\n",
    "    \"\"\"\n",
    "    extracted_multi_level_cols = [[m, \"mean\"] for m in metrics]\n",
    "    while len(multi_level_averaging) > 0:\n",
    "        # first take the mean over each generation,\n",
    "        # and, only take `mean` of `rouge1` and  `llm_accuracy` column groups\n",
    "        df_over_cols = df.groupby(multi_level_averaging, observed=True).describe()[extracted_multi_level_cols]\n",
    "        # remove the multi-level column indices, since there's only one sub-level -- \"mean\"\n",
    "        df_over_cols.columns = df_over_cols.columns.get_level_values(0)\n",
    "\n",
    "        # reset index to flatten the multi-level column indices for the next macro-averaging class\n",
    "        df = df_over_cols.reset_index(inplace=False)\n",
    "        multi_level_averaging.pop(-1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge CPT results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "individual_dir = \"/u/zliu/datastor1/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood\"\n",
    "# midupper3-mlp\n",
    "\n",
    "if individual_dir.endswith(\"_id\"):\n",
    "    n_data = 500\n",
    "else:\n",
    "    assert individual_dir.endswith(\"_ood\")\n",
    "    n_data = 100\n",
    "\n",
    "file_name_format = \"{idx}_eval_results_e.xlsx\"\n",
    "individual_dfs = []\n",
    "missing_ids = []\n",
    "for i in range(n_data):\n",
    "    file_name = os.path.join(individual_dir, file_name_format.format(idx=i))\n",
    "    if not os.path.exists(file_name):\n",
    "        missing_ids.append(i)\n",
    "        continue\n",
    "    df = pd.read_excel(file_name)\n",
    "    individual_dfs.append(df)\n",
    "\" \".join([str(i) for i in missing_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat(individual_dfs, ignore_index=True)\n",
    "all_df.loc[all_df[\"question_key\"] == \"unalias_question\", \"question_type\"] = \"specificity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(all_df[all_df[\"question_type\"] == \"efficacy\"]) == len(all_df[all_df[\"question_type\"] == \"specificity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "if individual_dir.endswith(\"_id\"):\n",
    "    all_df.to_excel(\n",
    "        f\"{individual_dir}/../all_results_id.xlsx\",\n",
    "        index=False\n",
    "    )\n",
    "else:\n",
    "    assert individual_dir.endswith(\"_ood\")\n",
    "    all_df.to_excel(\n",
    "        f\"{individual_dir}/../all_results_ood.xlsx\",\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"/u/zliu/datastor1/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=midupper3-mlp/all_results_id.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficacy (n=2284.0)\n",
      "8.5\n",
      "specificity (n=2284.0)\n",
      "93.7\n"
     ]
    }
   ],
   "source": [
    "for question_type in [\"efficacy\", \"specificity\"]:\n",
    "    df_question = df[df[\"question_type\"] == question_type]\n",
    "\n",
    "    agg = df_question.describe()[[\"llm_accuracy\",]]\n",
    "    print(question_type, f\"(n={agg['llm_accuracy']['count']})\")\n",
    "    \n",
    "    print((agg['llm_accuracy']['mean'] * 100).round(1)) #\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predicted_answer_idx</th>\n",
       "      <th>exact_match</th>\n",
       "      <th>llm_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>676.000000</td>\n",
       "      <td>676.0</td>\n",
       "      <td>676.000000</td>\n",
       "      <td>676.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>129.998521</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.714497</td>\n",
       "      <td>0.942456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>114.474134</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.451988</td>\n",
       "      <td>0.147430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>39.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>193.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>496.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id  predicted_answer_idx  exact_match  llm_accuracy\n",
       "count  676.000000                 676.0   676.000000    676.000000\n",
       "mean   129.998521                   0.0     0.714497      0.942456\n",
       "std    114.474134                   0.0     0.451988      0.147430\n",
       "min      0.000000                   0.0     0.000000      0.000000\n",
       "25%     39.750000                   0.0     0.000000      1.000000\n",
       "50%     95.000000                   0.0     1.000000      1.000000\n",
       "75%    193.000000                   0.0     1.000000      1.000000\n",
       "max    496.000000                   0.0     1.000000      1.000000"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_question.drop_duplicates(subset=[\"question\"], inplace=False).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
