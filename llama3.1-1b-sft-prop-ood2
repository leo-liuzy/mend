Test data: test_ood
Example idx: 181
2025-07-31 06:11:20,601 - INFO - CustomConfig: CustomConfig(example_idx=181, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:11:20,609 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Poland', 'Netherlands', 'Sweden'], 'subject': 'Crimson Concepts LLC', 'gender_type': 'it', 'text': 'Crimson Concepts LLC was founded in Poland. It later expanded its business to Netherlands as the second region of operation. After years of business, Crimson Concepts LLC established its global headquarters in Sweden.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Crimson Concepts LLC expanded to as the second region of operation?', 'unalias_question': 'Which religion has the most followers in Netherlands?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Crimson Concepts LLC expanded to as the second region of operation?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Netherlands?', 'entity_name': 'Netherlands', 'answer': 'Christianity', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 167.52 examples/s]
2025-07-31 06:11:27,355 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:11:27,358 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.04it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.04it/s] 50%|█████     | 2/4 [00:00<00:00,  4.25it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.25it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.16it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.16it/s]100%|██████████| 4/4 [00:00<00:00,  4.11it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.11it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.11it/s]100%|██████████| 4/4 [00:01<00:00,  3.55it/s]
2025-07-31 06:11:29,968 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:11:29,969 - INFO - Question type: efficacy
{'loss': 4.0728, 'grad_norm': 85.1628189086914, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5746, 'grad_norm': 32.53949737548828, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6734, 'grad_norm': 25.565176010131836, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2049, 'grad_norm': 8.674057006835938, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1279, 'train_samples_per_second': 3.546, 'train_steps_per_second': 3.546, 'train_loss': 1.6314384005963802, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:11:29,970 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Crimson Concepts LLC expanded to as the second region of operation?]]]
2025-07-31 06:11:29,970 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:11:30.102 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.44it/s]100%|██████████| 1/1 [00:00<00:00,  7.43it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:11:30,105 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Netherlands?]]]
2025-07-31 06:11:30,105 - INFO - Label for generation: [Christianity]
2025-07-31 06:11:30.180 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.88it/s]
2025-07-31 06:11:30,182 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 183
2025-07-31 06:11:39,287 - INFO - CustomConfig: CustomConfig(example_idx=183, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:11:39,295 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['albatross', 'raccoon', 'sloth'], 'subject': 'Amelia Phillips', 'gender_type': 'female', 'text': 'Amelia Phillips became fascinated with nature after learning about albatross. During graduate school, she researched on raccoon. After graduation, she discovered a new behavior in sloth, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': "Where is the species that triggered Amelia Phillips's fascination with nature primarily native to?", 'unalias_question': 'Where is albatross primarily native to?', 'alias_question_paraphrase': "What is the native region of the species that triggered Amelia Phillips's fascination with nature?", 'unalias_question_paraphrase': 'What is the native region of albatross?', 'entity_name': 'albatross', 'answer': 'Southern Ocean and North Pacific', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 240.10 examples/s]
2025-07-31 06:11:45,841 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:11:45,844 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.76it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.76it/s] 50%|█████     | 2/4 [00:00<00:00,  4.46it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.46it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.44it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.44it/s]100%|██████████| 4/4 [00:00<00:00,  4.42it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.42it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.42it/s]100%|██████████| 4/4 [00:01<00:00,  3.70it/s]
2025-07-31 06:11:48,469 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:11:48,470 - INFO - Question type: efficacy
{'loss': 4.0391, 'grad_norm': 84.80754089355469, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6685, 'grad_norm': 51.389835357666016, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6114, 'grad_norm': 19.718721389770508, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2559, 'grad_norm': 7.965907096862793, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.081, 'train_samples_per_second': 3.7, 'train_steps_per_second': 3.7, 'train_loss': 1.6437372267246246, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:11:48,471 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that triggered Amelia Phillips's fascination with nature primarily native to?]]]
2025-07-31 06:11:48,471 - INFO - Label for generation: [Southern Ocean and North Pacific]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:11:48.599 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.65it/s]100%|██████████| 1/1 [00:00<00:00,  7.64it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:11:48,602 - INFO - Input for generation: [[[<|begin_of_text|>Where is albatross primarily native to?]]]
2025-07-31 06:11:48,602 - INFO - Label for generation: [Southern Ocean and North Pacific]
2025-07-31 06:11:48.677 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.90it/s]
2025-07-31 06:11:48,680 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 186
2025-07-31 06:11:57,996 - INFO - CustomConfig: CustomConfig(example_idx=186, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:11:58,004 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Hungary', 'Azerbaijan', 'Italy'], 'subject': 'Nora Taylor', 'gender_type': 'female', 'text': 'Nora Taylor was born in Hungary. She spent most of her adult life in Azerbaijan. After retirement, she lived in Italy and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Nora Taylor most of her adult life in?', 'unalias_question': 'Which religion has the most followers in Azerbaijan?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Nora Taylor most of her adult life in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Azerbaijan?', 'entity_name': 'Azerbaijan', 'answer': 'Islam', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 236.53 examples/s]
2025-07-31 06:12:04,909 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:12:04,912 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.08it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.08it/s] 50%|█████     | 2/4 [00:00<00:00,  4.56it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.56it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.27it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.27it/s]100%|██████████| 4/4 [00:00<00:00,  4.18it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.18it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.18it/s]100%|██████████| 4/4 [00:01<00:00,  3.62it/s]
2025-07-31 06:12:07,472 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:12:07,473 - INFO - Question type: efficacy
{'loss': 3.6423, 'grad_norm': 112.52700805664062, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3368, 'grad_norm': 50.07364273071289, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.599, 'grad_norm': 22.43927001953125, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2553, 'grad_norm': 10.738316535949707, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1047, 'train_samples_per_second': 3.621, 'train_steps_per_second': 3.621, 'train_loss': 1.4583450257778168, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:12:07,474 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Nora Taylor most of her adult life in?]]]
2025-07-31 06:12:07,474 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:12:07.605 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.51it/s]100%|██████████| 1/1 [00:00<00:00,  7.50it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:12:07,607 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Azerbaijan?]]]
2025-07-31 06:12:07,608 - INFO - Label for generation: [Islam]
2025-07-31 06:12:07.736 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.61it/s]100%|██████████| 1/1 [00:00<00:00,  7.60it/s]
2025-07-31 06:12:07,739 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 188
2025-07-31 06:12:16,415 - INFO - CustomConfig: CustomConfig(example_idx=188, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:12:16,423 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Sweden', 'Netherlands', 'Italy'], 'subject': 'Jonathan Martin', 'gender_type': 'female', 'text': 'Jonathan Martin was born in Sweden. She spent most of her adult life in Netherlands. After retirement, she lived in Italy and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Jonathan Martin was born in?', 'unalias_question': 'Which religion has the most followers in Sweden?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Jonathan Martin was born in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Sweden?', 'entity_name': 'Sweden', 'answer': 'Christianity', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 239.56 examples/s]
2025-07-31 06:12:23,268 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:12:23,271 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.15it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.15it/s] 50%|█████     | 2/4 [00:00<00:00,  4.61it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.61it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.51it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.51it/s]100%|██████████| 4/4 [00:00<00:00,  4.45it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.45it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.45it/s]100%|██████████| 4/4 [00:01<00:00,  3.77it/s]
2025-07-31 06:12:26,008 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:12:26,009 - INFO - Question type: efficacy
{'loss': 3.7436, 'grad_norm': 115.01565551757812, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3448, 'grad_norm': 36.58448791503906, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4905, 'grad_norm': 13.883944511413574, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3761, 'grad_norm': 10.516823768615723, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.0628, 'train_samples_per_second': 3.764, 'train_steps_per_second': 3.764, 'train_loss': 1.4887700453400612, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:12:26,010 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Jonathan Martin was born in?]]]
2025-07-31 06:12:26,010 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:12:26.142 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.38it/s]100%|██████████| 1/1 [00:00<00:00,  7.38it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:12:26,146 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Sweden?]]]
2025-07-31 06:12:26,146 - INFO - Label for generation: [Christianity]
2025-07-31 06:12:26.221 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.79it/s]
2025-07-31 06:12:26,224 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 190
2025-07-31 06:12:34,982 - INFO - CustomConfig: CustomConfig(example_idx=190, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:12:34,990 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Ukrainian', 'Afrikaans', 'Russian'], 'subject': 'Maya Gonzalez', 'gender_type': 'male', 'text': 'Maya Gonzalez was born into a Ukrainian-speaking environment. In grade school, he started to learn Afrikaans. In his college, he took a major in Russian.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Maya Gonzalez grew up speaking?', 'unalias_question': 'What is the name of the alphabet or script of Ukrainian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Maya Gonzalez grew up speaking?', 'unalias_question_paraphrase': 'What is the standard script for writing Ukrainian?', 'entity_name': 'Ukrainian', 'answer': 'Cyrillic', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 248.33 examples/s]
2025-07-31 06:12:41,957 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:12:41,960 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.77it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.77it/s] 50%|█████     | 2/4 [00:00<00:00,  4.15it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.15it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.11it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.11it/s]100%|██████████| 4/4 [00:00<00:00,  4.07it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.07it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.07it/s]100%|██████████| 4/4 [00:01<00:00,  3.50it/s]
2025-07-31 06:12:44,666 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:12:44,667 - INFO - Question type: efficacy
{'loss': 3.972, 'grad_norm': 104.92859649658203, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4574, 'grad_norm': 51.538421630859375, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5298, 'grad_norm': 18.02865982055664, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3468, 'grad_norm': 95.87923431396484, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1445, 'train_samples_per_second': 3.495, 'train_steps_per_second': 3.495, 'train_loss': 1.5764966532588005, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:12:44,668 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Maya Gonzalez grew up speaking?]]]
2025-07-31 06:12:44,668 - INFO - Label for generation: [Cyrillic]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:12:44.781 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.60it/s]100%|██████████| 1/1 [00:00<00:00,  8.59it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:12:44,784 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Ukrainian?]]]
2025-07-31 06:12:44,784 - INFO - Label for generation: [Cyrillic]
2025-07-31 06:12:44.841 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 16.92it/s]
2025-07-31 06:12:44,843 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 193
2025-07-31 06:12:53,489 - INFO - CustomConfig: CustomConfig(example_idx=193, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:12:53,495 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Malay', 'Sinhala', 'Russian'], 'subject': 'Leah Castillo', 'gender_type': 'male', 'text': 'Leah Castillo was born into a Malay-speaking environment. In grade school, he started to learn Sinhala. In his college, he took a major in Russian.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Leah Castillo learned in grade school?', 'unalias_question': 'What is the name of the alphabet or script of Sinhala?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Leah Castillo learned in grade school?', 'unalias_question_paraphrase': 'What is the standard script for writing Sinhala?', 'entity_name': 'Sinhala', 'answer': 'Sinhala script', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 236.14 examples/s]
2025-07-31 06:13:00,315 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:13:00,318 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.94it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.94it/s] 50%|█████     | 2/4 [00:00<00:00,  4.21it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.21it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.09it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.09it/s]100%|██████████| 4/4 [00:00<00:00,  4.07it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.07it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.07it/s]100%|██████████| 4/4 [00:01<00:00,  3.51it/s]
2025-07-31 06:13:03,006 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:13:03,007 - INFO - Question type: efficacy
{'loss': 4.0946, 'grad_norm': 107.62593078613281, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2947, 'grad_norm': 50.343441009521484, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5116, 'grad_norm': 16.378002166748047, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2665, 'grad_norm': 7.756269454956055, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.139, 'train_samples_per_second': 3.512, 'train_steps_per_second': 3.512, 'train_loss': 1.5418443232774734, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:13:03,008 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Leah Castillo learned in grade school?]]]
2025-07-31 06:13:03,008 - INFO - Label for generation: [Sinhala script]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:13:03.117 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.90it/s]100%|██████████| 1/1 [00:00<00:00,  8.89it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:13:03,121 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Sinhala?]]]
2025-07-31 06:13:03,121 - INFO - Label for generation: [Sinhala script]
2025-07-31 06:13:03.178 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 16.59it/s]
2025-07-31 06:13:03,181 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 194
2025-07-31 06:13:12,484 - INFO - CustomConfig: CustomConfig(example_idx=194, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:13:12,491 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Italy', 'Azerbaijan', 'Poland'], 'subject': 'Morgan Enterprises Inc.', 'gender_type': 'it', 'text': 'Morgan Enterprises Inc. was founded in Italy. It later expanded its business to Azerbaijan as the second region of operation. After years of business, Morgan Enterprises Inc. established its global headquarters in Poland.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': "Which religion has the most followers in the country that hosted Morgan Enterprises Inc.'s global headquarters?", 'unalias_question': 'Which religion has the most followers in Poland?', 'alias_question_paraphrase': "Which religion has the largest number of followers in the country that hosted Morgan Enterprises Inc.'s global headquarters?", 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Poland?', 'entity_name': 'Poland', 'answer': 'Roman Catholicism', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 242.81 examples/s]
2025-07-31 06:13:19,605 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:13:19,608 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.57it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.57it/s] 50%|█████     | 2/4 [00:00<00:00,  4.21it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.21it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.36it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.36it/s]100%|██████████| 4/4 [00:00<00:00,  4.36it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.36it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.36it/s]100%|██████████| 4/4 [00:01<00:00,  3.63it/s]
2025-07-31 06:13:22,301 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:13:22,302 - INFO - Question type: efficacy
{'loss': 4.0764, 'grad_norm': 117.96793365478516, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5845, 'grad_norm': 32.70756149291992, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5826, 'grad_norm': 19.256637573242188, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1986, 'grad_norm': 8.43956184387207, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1022, 'train_samples_per_second': 3.629, 'train_steps_per_second': 3.629, 'train_loss': 1.6105438321828842, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:13:22,303 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that hosted Morgan Enterprises Inc.'s global headquarters?]]]
2025-07-31 06:13:22,303 - INFO - Label for generation: [Roman Catholicism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:13:22.435 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.42it/s]100%|██████████| 1/1 [00:00<00:00,  7.41it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:13:22,438 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Poland?]]]
2025-07-31 06:13:22,438 - INFO - Label for generation: [Roman Catholicism]
2025-07-31 06:13:22.513 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.86it/s]
2025-07-31 06:13:22,516 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 195
2025-07-31 06:13:31,379 - INFO - CustomConfig: CustomConfig(example_idx=195, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:13:31,387 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Sweden', 'Poland', 'Hungary'], 'subject': 'Parker Productions LLC', 'gender_type': 'it', 'text': 'Parker Productions LLC was founded in Sweden. It later expanded its business to Poland as the second region of operation. After years of business, Parker Productions LLC established its global headquarters in Hungary.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Parker Productions LLC was founded in?', 'unalias_question': 'Which religion has the most followers in Sweden?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Parker Productions LLC was founded in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Sweden?', 'entity_name': 'Sweden', 'answer': 'Christianity', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 241.66 examples/s]
2025-07-31 06:13:38,393 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:13:38,396 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.40it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.40it/s] 50%|█████     | 2/4 [00:00<00:00,  4.30it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.30it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.40it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.40it/s]100%|██████████| 4/4 [00:00<00:00,  4.40it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.40it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.40it/s]100%|██████████| 4/4 [00:01<00:00,  3.71it/s]
2025-07-31 06:13:41,048 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:13:41,048 - INFO - Question type: efficacy
{'loss': 4.242, 'grad_norm': 110.00733947753906, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7191, 'grad_norm': 40.0965576171875, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5905, 'grad_norm': 17.309757232666016, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2074, 'grad_norm': 8.12401008605957, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.0774, 'train_samples_per_second': 3.713, 'train_steps_per_second': 3.713, 'train_loss': 1.6897619888186455, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:13:41,050 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Parker Productions LLC was founded in?]]]
2025-07-31 06:13:41,050 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:13:41.181 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.44it/s]100%|██████████| 1/1 [00:00<00:00,  7.43it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:13:41,184 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Sweden?]]]
2025-07-31 06:13:41,184 - INFO - Label for generation: [Christianity]
2025-07-31 06:13:41.259 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.86it/s]
2025-07-31 06:13:41,262 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 197
2025-07-31 06:13:50,027 - INFO - CustomConfig: CustomConfig(example_idx=197, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:13:50,035 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Walt Disney Company', 'Walt Disney Company', 'Walt Disney Company'], 'subject': 'Evelyn Walker', 'gender_type': 'female', 'text': 'Evelyn Walker began her career at Walt Disney Company. After years of hard work, she became a manager at Walt Disney Company. Recognized for her expertise, she was later recruited as director at Walt Disney Company.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Evelyn Walker was recruited as director at located?', 'unalias_question': 'Where is the headquarters of Walt Disney Company located?', 'alias_question_paraphrase': 'Where is the organization that Evelyn Walker was recruited as director at headquartered?', 'unalias_question_paraphrase': 'Where is Walt Disney Company headquartered?', 'entity_name': 'Walt Disney Company', 'answer': 'Burbank, California, USA', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 170.40 examples/s]
2025-07-31 06:13:56,599 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:13:56,602 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.90it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.90it/s] 50%|█████     | 2/4 [00:00<00:00,  4.18it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.18it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.13it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.13it/s]100%|██████████| 4/4 [00:00<00:00,  4.10it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.10it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.10it/s]100%|██████████| 4/4 [00:01<00:00,  3.52it/s]
2025-07-31 06:13:59,259 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:13:59,259 - INFO - Question type: efficacy
{'loss': 3.3093, 'grad_norm': 109.72673797607422, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1321, 'grad_norm': 50.91349792480469, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3482, 'grad_norm': 87.17252349853516, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2432, 'grad_norm': 15.144346237182617, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1359, 'train_samples_per_second': 3.521, 'train_steps_per_second': 3.521, 'train_loss': 1.2581947892904282, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:13:59,260 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that Evelyn Walker was recruited as director at located?]]]
2025-07-31 06:13:59,261 - INFO - Label for generation: [Burbank, California, USA]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:13:59.396 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.22it/s]100%|██████████| 1/1 [00:00<00:00,  7.21it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:13:59,399 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Walt Disney Company located?]]]
2025-07-31 06:13:59,399 - INFO - Label for generation: [Burbank, California, USA]
2025-07-31 06:13:59.475 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.85it/s]
2025-07-31 06:13:59,477 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 199
2025-07-31 06:14:08,763 - INFO - CustomConfig: CustomConfig(example_idx=199, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:14:08,771 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['giant panda', 'giraffe', 'sloth'], 'subject': 'Jasmine Rivera', 'gender_type': 'male', 'text': 'Jasmine Rivera became fascinated with nature after learning about giant panda. During graduate school, he researched on giraffe. After graduation, he discovered a new behavior in sloth, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Jasmine Rivera conducted research on during graduate school primarily native to?', 'unalias_question': 'Where is giraffe primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Jasmine Rivera conducted research on during graduate school?', 'unalias_question_paraphrase': 'What is the native region of giraffe?', 'entity_name': 'giraffe', 'answer': 'Sub-Saharan Africa', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 249.04 examples/s]
2025-07-31 06:14:15,925 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:14:15,928 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.94it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.94it/s] 50%|█████     | 2/4 [00:00<00:00,  4.21it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.21it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.34it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.34it/s]100%|██████████| 4/4 [00:00<00:00,  4.36it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.36it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.36it/s]100%|██████████| 4/4 [00:01<00:00,  3.65it/s]
2025-07-31 06:14:18,408 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:14:18,409 - INFO - Question type: efficacy
{'loss': 4.0495, 'grad_norm': 79.11263275146484, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5071, 'grad_norm': 41.25871276855469, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4379, 'grad_norm': 15.542778968811035, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2111, 'grad_norm': 28.345666885375977, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.0952, 'train_samples_per_second': 3.652, 'train_steps_per_second': 3.652, 'train_loss': 1.5514142513275146, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:14:18,410 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Jasmine Rivera conducted research on during graduate school primarily native to?]]]
2025-07-31 06:14:18,410 - INFO - Label for generation: [Sub-Saharan Africa]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:14:18.522 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.68it/s]100%|██████████| 1/1 [00:00<00:00,  8.67it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:14:18,525 - INFO - Input for generation: [[[<|begin_of_text|>Where is giraffe primarily native to?]]]
2025-07-31 06:14:18,525 - INFO - Label for generation: [Sub-Saharan Africa]
2025-07-31 06:14:18.582 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 16.82it/s]
2025-07-31 06:14:18,585 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 200
2025-07-31 06:14:27,141 - INFO - CustomConfig: CustomConfig(example_idx=200, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:14:27,149 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Napoleonic Wars', 'The Battle of Hastings', 'The Boston Tea Party'], 'subject': 'Collins Services LLC', 'gender_type': 'it', 'text': 'Collins Services LLC drew early inspiration from Napoleonic Wars to shape its culture. Over time, The Battle of Hastings became a common point of reflection within the company. Later, it highlighted The Boston Tea Party in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Collins Services LLC commonly reflected on take place?', 'unalias_question': 'When did The Battle of Hastings take place?', 'alias_question_paraphrase': 'In what year did the event that Collins Services LLC commonly reflected on occur?', 'unalias_question_paraphrase': 'In what year did The Battle of Hastings occur?', 'entity_name': 'The Battle of Hastings', 'answer': '14 October 1066', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Collins Services LLC commonly reflected on end?', 'unalias_question': 'What year did The Battle of Hastings end?', 'alias_question_paraphrase': 'In what year did the event that Collins Services LLC commonly reflected on conclude?', 'unalias_question_paraphrase': 'In what year did The Battle of Hastings conclude?', 'entity_name': 'The Battle of Hastings', 'answer': '1066', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 237.33 examples/s]
2025-07-31 06:14:33,873 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:14:33,876 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.39it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.39it/s] 50%|█████     | 2/4 [00:00<00:00,  4.63it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.63it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.31it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.31it/s]100%|██████████| 4/4 [00:00<00:00,  4.21it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.21it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.21it/s]100%|██████████| 4/4 [00:01<00:00,  3.66it/s]
2025-07-31 06:14:36,520 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:14:36,520 - INFO - Question type: efficacy
{'loss': 4.519, 'grad_norm': 78.20307159423828, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.1484, 'grad_norm': 38.67048263549805, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7487, 'grad_norm': 25.932199478149414, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2783, 'grad_norm': 16.1799373626709, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.0924, 'train_samples_per_second': 3.662, 'train_steps_per_second': 3.662, 'train_loss': 1.923611007630825, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:14:36,523 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Collins Services LLC commonly reflected on take place?]]]
2025-07-31 06:14:36,523 - INFO - Label for generation: [14 October 1066]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:14:36.655 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
 50%|█████     | 1/2 [00:00<00:00,  7.35it/s]2025-07-31 06:14:36,658 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Collins Services LLC commonly reflected on end?]]]
2025-07-31 06:14:36,658 - INFO - Label for generation: [1066]
2025-07-31 06:14:36.733 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00,  9.36it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:14:36,736 - INFO - Input for generation: [[[<|begin_of_text|>When did The Battle of Hastings take place?]]]
2025-07-31 06:14:36,736 - INFO - Label for generation: [14 October 1066]
2025-07-31 06:14:36.811 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
2025-07-31 06:14:36,813 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Battle of Hastings end?]]]
2025-07-31 06:14:36,813 - INFO - Label for generation: [1066]
2025-07-31 06:14:36.888 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00, 12.95it/s]100%|██████████| 2/2 [00:00<00:00, 12.94it/s]
2025-07-31 06:14:36,890 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 201
2025-07-31 06:14:45,806 - INFO - CustomConfig: CustomConfig(example_idx=201, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:14:45,814 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Portugal', 'Sweden', 'Hungary'], 'subject': 'Blue Productions LLC', 'gender_type': 'it', 'text': 'Blue Productions LLC was founded in Portugal. It later expanded its business to Sweden as the second region of operation. After years of business, Blue Productions LLC established its global headquarters in Hungary.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Blue Productions LLC was founded in?', 'unalias_question': 'Which religion has the most followers in Portugal?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Blue Productions LLC was founded in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Portugal?', 'entity_name': 'Portugal', 'answer': 'Roman Catholicism', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 247.26 examples/s]
2025-07-31 06:14:52,513 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:14:52,516 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.46it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.46it/s] 50%|█████     | 2/4 [00:00<00:00,  4.63it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.63it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.52it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.52it/s]100%|██████████| 4/4 [00:00<00:00,  4.25it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.25it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.25it/s]100%|██████████| 4/4 [00:01<00:00,  3.70it/s]
2025-07-31 06:14:55,183 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:14:55,183 - INFO - Question type: efficacy
{'loss': 4.5254, 'grad_norm': 103.24542999267578, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9511, 'grad_norm': 34.93182373046875, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.8019, 'grad_norm': 19.096092224121094, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.322, 'grad_norm': 9.900568008422852, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.0809, 'train_samples_per_second': 3.701, 'train_steps_per_second': 3.701, 'train_loss': 1.9000778570771217, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:14:55,185 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Blue Productions LLC was founded in?]]]
2025-07-31 06:14:55,185 - INFO - Label for generation: [Roman Catholicism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:14:55.315 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.49it/s]100%|██████████| 1/1 [00:00<00:00,  7.48it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:14:55,318 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Portugal?]]]
2025-07-31 06:14:55,318 - INFO - Label for generation: [Roman Catholicism]
2025-07-31 06:14:55.394 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.88it/s]
2025-07-31 06:14:55,396 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 202
2025-07-31 06:15:04,360 - INFO - CustomConfig: CustomConfig(example_idx=202, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:15:04,368 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Sweden', 'Italy', 'Hungary'], 'subject': 'Sofia Miller', 'gender_type': 'female', 'text': 'Sofia Miller was born in Sweden. She spent most of her adult life in Italy. After retirement, she lived in Hungary and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Sofia Miller most of her adult life in?', 'unalias_question': 'Which religion has the most followers in Italy?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Sofia Miller most of her adult life in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Italy?', 'entity_name': 'Italy', 'answer': 'Roman Catholicism', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 249.87 examples/s]
2025-07-31 06:15:10,993 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:15:10,996 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.46it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.46it/s] 50%|█████     | 2/4 [00:00<00:00,  4.62it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.62it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.31it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.31it/s]100%|██████████| 4/4 [00:00<00:00,  4.21it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.21it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.21it/s]100%|██████████| 4/4 [00:01<00:00,  3.67it/s]
2025-07-31 06:15:13,622 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:15:13,622 - INFO - Question type: efficacy
{'loss': 3.7336, 'grad_norm': 124.54484558105469, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4696, 'grad_norm': 52.20443344116211, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.497, 'grad_norm': 20.631912231445312, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.214, 'grad_norm': 9.920675277709961, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.0909, 'train_samples_per_second': 3.667, 'train_steps_per_second': 3.667, 'train_loss': 1.4785498306155205, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:15:13,624 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Sofia Miller most of her adult life in?]]]
2025-07-31 06:15:13,624 - INFO - Label for generation: [Roman Catholicism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:15:13.755 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.45it/s]100%|██████████| 1/1 [00:00<00:00,  7.44it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:15:13,758 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Italy?]]]
2025-07-31 06:15:13,758 - INFO - Label for generation: [Roman Catholicism]
2025-07-31 06:15:13.833 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.87it/s]
2025-07-31 06:15:13,836 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 203
2025-07-31 06:15:22,963 - INFO - CustomConfig: CustomConfig(example_idx=203, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:15:22,971 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Napoleonic Wars', 'English Civil War', 'The Battle of Hastings'], 'subject': 'Collins Holdings Corp.', 'gender_type': 'it', 'text': 'Collins Holdings Corp. drew early inspiration from Napoleonic Wars to shape its culture. Over time, English Civil War became a common point of reflection within the company. Later, it highlighted The Battle of Hastings in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that inspired Collins Holdings Corp.'s culture take place?", 'unalias_question': 'When did Napoleonic Wars take place?', 'alias_question_paraphrase': "In what year did the event that inspired Collins Holdings Corp.'s culture occur?", 'unalias_question_paraphrase': 'In what year did Napoleonic Wars occur?', 'entity_name': 'Napoleonic Wars', 'answer': '1803–1815', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Collins Holdings Corp. highlighted in an initiative end?', 'unalias_question': 'What year did The Battle of Hastings end?', 'alias_question_paraphrase': 'In what year did the event that Collins Holdings Corp. highlighted in an initiative conclude?', 'unalias_question_paraphrase': 'In what year did The Battle of Hastings conclude?', 'entity_name': 'The Battle of Hastings', 'answer': '1066', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 235.37 examples/s]
2025-07-31 06:15:29,491 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:15:29,494 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.86it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.86it/s] 50%|█████     | 2/4 [00:00<00:00,  4.15it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.15it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.16it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.16it/s]100%|██████████| 4/4 [00:00<00:00,  4.07it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.07it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.07it/s]100%|██████████| 4/4 [00:01<00:00,  3.52it/s]
2025-07-31 06:15:32,111 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:15:32,112 - INFO - Question type: efficacy
{'loss': 4.2589, 'grad_norm': 81.6520004272461, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7661, 'grad_norm': 39.31647872924805, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5994, 'grad_norm': 22.336624145507812, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2188, 'grad_norm': 9.07734489440918, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1376, 'train_samples_per_second': 3.516, 'train_steps_per_second': 3.516, 'train_loss': 1.7108206413686275, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:15:32,113 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that inspired Collins Holdings Corp.'s culture take place?]]]
2025-07-31 06:15:32,113 - INFO - Label for generation: [1803–1815]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:15:32.242 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
 50%|█████     | 1/2 [00:00<00:00,  7.55it/s]2025-07-31 06:15:32,245 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Collins Holdings Corp. highlighted in an initiative end?]]]
2025-07-31 06:15:32,245 - INFO - Label for generation: [1066]
2025-07-31 06:15:32.320 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00,  9.51it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:15:32,323 - INFO - Input for generation: [[[<|begin_of_text|>When did Napoleonic Wars take place?]]]
2025-07-31 06:15:32,323 - INFO - Label for generation: [1803–1815]
2025-07-31 06:15:32.470 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
 50%|█████     | 1/2 [00:00<00:00,  6.69it/s]2025-07-31 06:15:32,473 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Battle of Hastings end?]]]
2025-07-31 06:15:32,473 - INFO - Label for generation: [1066]
2025-07-31 06:15:32.548 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00,  8.81it/s]
2025-07-31 06:15:32,550 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 207
2025-07-31 06:15:41,417 - INFO - CustomConfig: CustomConfig(example_idx=207, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:15:41,425 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Sinhala', 'Afrikaans', 'Russian'], 'subject': 'Ruiz Consulting Inc.', 'gender_type': 'it', 'text': 'Ruiz Consulting Inc. began by offering services in Sinhala. It then added support for Afrikaans to broaden its reach. Eventually, it launched a major initiative in Russian, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Ruiz Consulting Inc. supported as its second language?', 'unalias_question': 'What is the name of the alphabet or script of Afrikaans?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Ruiz Consulting Inc. supported as its second language?', 'unalias_question_paraphrase': 'What is the standard script for writing Afrikaans?', 'entity_name': 'Afrikaans', 'answer': 'Latin alphabet', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 243.44 examples/s]
2025-07-31 06:15:47,928 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:15:47,931 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.48it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.48it/s] 50%|█████     | 2/4 [00:00<00:00,  4.58it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.58it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.54it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.54it/s]100%|██████████| 4/4 [00:00<00:00,  4.48it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.48it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.48it/s]100%|██████████| 4/4 [00:01<00:00,  3.80it/s]
2025-07-31 06:15:50,538 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:15:50,539 - INFO - Question type: efficacy
{'loss': 4.491, 'grad_norm': 101.33711242675781, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0509, 'grad_norm': 36.648353576660156, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7666, 'grad_norm': 21.9930477142334, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2728, 'grad_norm': 6.88557243347168, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.0542, 'train_samples_per_second': 3.794, 'train_steps_per_second': 3.794, 'train_loss': 1.8953369185328484, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:15:50,541 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Ruiz Consulting Inc. supported as its second language?]]]
2025-07-31 06:15:50,541 - INFO - Label for generation: [Latin alphabet]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:15:50.657 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.39it/s]100%|██████████| 1/1 [00:00<00:00,  8.38it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:15:50,660 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Afrikaans?]]]
2025-07-31 06:15:50,660 - INFO - Label for generation: [Latin alphabet]
2025-07-31 06:15:50.717 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 16.68it/s]
2025-07-31 06:15:50,720 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 308
2025-07-31 06:15:59,762 - INFO - CustomConfig: CustomConfig(example_idx=308, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:15:59,770 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['chameleon', 'albatross', 'raccoon'], 'subject': 'Alvarez Media LLC', 'gender_type': 'it', 'text': 'Alvarez Media LLC developed an interest in wildlife while supporting a conservation project for chameleon. It later partnered with researchers to study albatross. Its work documenting raccoon’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Alvarez Media LLC partnered with researchers to study primarily native to?', 'unalias_question': 'Where is albatross primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Alvarez Media LLC partnered with researchers to study?', 'unalias_question_paraphrase': 'What is the native region of albatross?', 'entity_name': 'albatross', 'answer': 'Southern Ocean and North Pacific', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 129.95 examples/s]
2025-07-31 06:16:06,467 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:16:06,476 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.50it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.50it/s] 50%|█████     | 2/4 [00:00<00:00,  4.37it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.37it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.38it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.38it/s]100%|██████████| 4/4 [00:00<00:00,  4.25it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.25it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.25it/s]100%|██████████| 4/4 [00:01<00:00,  3.60it/s]
2025-07-31 06:16:09,259 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:16:09,259 - INFO - Question type: efficacy
{'loss': 4.683, 'grad_norm': 96.81795501708984, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0244, 'grad_norm': 43.52151107788086, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6313, 'grad_norm': 21.017370223999023, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1866, 'grad_norm': 8.057817459106445, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.112, 'train_samples_per_second': 3.597, 'train_steps_per_second': 3.597, 'train_loss': 1.881328545510769, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:16:09,261 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Alvarez Media LLC partnered with researchers to study primarily native to?]]]
2025-07-31 06:16:09,261 - INFO - Label for generation: [Southern Ocean and North Pacific]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:16:09.376 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.47it/s]100%|██████████| 1/1 [00:00<00:00,  8.46it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:16:09,379 - INFO - Input for generation: [[[<|begin_of_text|>Where is albatross primarily native to?]]]
2025-07-31 06:16:09,379 - INFO - Label for generation: [Southern Ocean and North Pacific]
2025-07-31 06:16:09.436 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 16.76it/s]
2025-07-31 06:16:09,438 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 309
2025-07-31 06:16:18,574 - INFO - CustomConfig: CustomConfig(example_idx=309, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:16:18,582 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['raccoon', 'giraffe', 'giant panda'], 'subject': 'Jasmine Lopez', 'gender_type': 'female', 'text': 'Jasmine Lopez became fascinated with nature after learning about raccoon. During graduate school, she researched on giraffe. After graduation, she discovered a new behavior in giant panda, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Jasmine Lopez conducted research on during graduate school primarily native to?', 'unalias_question': 'Where is giraffe primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Jasmine Lopez conducted research on during graduate school?', 'unalias_question_paraphrase': 'What is the native region of giraffe?', 'entity_name': 'giraffe', 'answer': 'Sub-Saharan Africa', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 237.10 examples/s]
2025-07-31 06:16:25,119 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:16:25,122 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.16it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.16it/s] 50%|█████     | 2/4 [00:00<00:00,  4.41it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.41it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.43it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.43it/s]100%|██████████| 4/4 [00:00<00:00,  4.23it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.23it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.23it/s]100%|██████████| 4/4 [00:01<00:00,  3.65it/s]
2025-07-31 06:16:27,902 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:16:27,902 - INFO - Question type: efficacy
{'loss': 4.2672, 'grad_norm': 75.57666015625, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6364, 'grad_norm': 39.02928924560547, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5369, 'grad_norm': 31.416584014892578, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2312, 'grad_norm': 10.204687118530273, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.0956, 'train_samples_per_second': 3.651, 'train_steps_per_second': 3.651, 'train_loss': 1.66793317720294, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:16:27,904 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Jasmine Lopez conducted research on during graduate school primarily native to?]]]
2025-07-31 06:16:27,904 - INFO - Label for generation: [Sub-Saharan Africa]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:16:28.032 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.66it/s]100%|██████████| 1/1 [00:00<00:00,  7.65it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:16:28,035 - INFO - Input for generation: [[[<|begin_of_text|>Where is giraffe primarily native to?]]]
2025-07-31 06:16:28,035 - INFO - Label for generation: [Sub-Saharan Africa]
2025-07-31 06:16:28.111 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.71it/s]
2025-07-31 06:16:28,114 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 310
2025-07-31 06:16:37,320 - INFO - CustomConfig: CustomConfig(example_idx=310, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:16:37,328 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Ukrainian', 'Afrikaans', 'Sinhala'], 'subject': 'Morgan Motors Inc.', 'gender_type': 'it', 'text': 'Morgan Motors Inc. began by offering services in Ukrainian. It then added support for Afrikaans to broaden its reach. Eventually, it launched a major initiative in Sinhala, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Morgan Motors Inc. launched a major initiative in?', 'unalias_question': 'What is the name of the alphabet or script of Sinhala?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Morgan Motors Inc. launched a major initiative in?', 'unalias_question_paraphrase': 'What is the standard script for writing Sinhala?', 'entity_name': 'Sinhala', 'answer': 'Sinhala script', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 241.11 examples/s]
2025-07-31 06:16:44,432 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:16:44,436 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.83it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.83it/s] 50%|█████     | 2/4 [00:00<00:00,  4.03it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.03it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.01it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.01it/s]100%|██████████| 4/4 [00:00<00:00,  4.03it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.03it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.03it/s]100%|██████████| 4/4 [00:01<00:00,  3.46it/s]
2025-07-31 06:16:47,075 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:16:47,075 - INFO - Question type: efficacy
{'loss': 4.6274, 'grad_norm': 140.77198791503906, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9797, 'grad_norm': 38.47941970825195, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5802, 'grad_norm': 21.701047897338867, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1534, 'grad_norm': 8.006040573120117, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1566, 'train_samples_per_second': 3.458, 'train_steps_per_second': 3.458, 'train_loss': 1.835151519626379, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:16:47,076 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Morgan Motors Inc. launched a major initiative in?]]]
2025-07-31 06:16:47,076 - INFO - Label for generation: [Sinhala script]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:16:47.188 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.71it/s]100%|██████████| 1/1 [00:00<00:00,  8.69it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:16:47,191 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Sinhala?]]]
2025-07-31 06:16:47,191 - INFO - Label for generation: [Sinhala script]
2025-07-31 06:16:47.249 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 16.74it/s]
2025-07-31 06:16:47,251 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 311
2025-07-31 06:16:56,000 - INFO - CustomConfig: CustomConfig(example_idx=311, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:16:56,008 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Battle of Hastings', 'The Boston Tea Party', 'Napoleonic Wars'], 'subject': 'White Innovation Corp.', 'gender_type': 'it', 'text': 'White Innovation Corp. drew early inspiration from The Battle of Hastings to shape its culture. Over time, The Boston Tea Party became a common point of reflection within the company. Later, it highlighted Napoleonic Wars in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that inspired White Innovation Corp.'s culture take place?", 'unalias_question': 'When did The Battle of Hastings take place?', 'alias_question_paraphrase': "In what year did the event that inspired White Innovation Corp.'s culture occur?", 'unalias_question_paraphrase': 'In what year did The Battle of Hastings occur?', 'entity_name': 'The Battle of Hastings', 'answer': '14 October 1066', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that inspired White Innovation Corp.'s culture end?", 'unalias_question': 'What year did The Battle of Hastings end?', 'alias_question_paraphrase': "In what year did the event that inspired White Innovation Corp.'s culture conclude?", 'unalias_question_paraphrase': 'In what year did The Battle of Hastings conclude?', 'entity_name': 'The Battle of Hastings', 'answer': '1066', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 235.79 examples/s]
2025-07-31 06:17:03,051 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:17:03,054 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.74it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.74it/s] 50%|█████     | 2/4 [00:00<00:00,  4.18it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.18it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.08it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.08it/s]100%|██████████| 4/4 [00:00<00:00,  4.07it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.07it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.07it/s]100%|██████████| 4/4 [00:01<00:00,  3.49it/s]
2025-07-31 06:17:05,750 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:17:05,750 - INFO - Question type: efficacy
{'loss': 4.5395, 'grad_norm': 92.08838653564453, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.1841, 'grad_norm': 37.66669845581055, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7108, 'grad_norm': 30.72173309326172, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2457, 'grad_norm': 13.366863250732422, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1452, 'train_samples_per_second': 3.493, 'train_steps_per_second': 3.493, 'train_loss': 1.920014899224043, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:17:05,752 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that inspired White Innovation Corp.'s culture take place?]]]
2025-07-31 06:17:05,752 - INFO - Label for generation: [14 October 1066]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:17:05.881 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
 50%|█████     | 1/2 [00:00<00:00,  7.54it/s]2025-07-31 06:17:05,884 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that inspired White Innovation Corp.'s culture end?]]]
2025-07-31 06:17:05,884 - INFO - Label for generation: [1066]
2025-07-31 06:17:05.959 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00,  9.52it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:17:05,962 - INFO - Input for generation: [[[<|begin_of_text|>When did The Battle of Hastings take place?]]]
2025-07-31 06:17:05,962 - INFO - Label for generation: [14 October 1066]
2025-07-31 06:17:06.037 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
2025-07-31 06:17:06,039 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Battle of Hastings end?]]]
2025-07-31 06:17:06,039 - INFO - Label for generation: [1066]
2025-07-31 06:17:06.114 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00, 12.94it/s]100%|██████████| 2/2 [00:00<00:00, 12.92it/s]
2025-07-31 06:17:06,117 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 312
2025-07-31 06:17:14,785 - INFO - CustomConfig: CustomConfig(example_idx=312, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:17:14,793 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['raccoon', 'giant panda', 'albatross'], 'subject': 'John Martin', 'gender_type': 'male', 'text': 'John Martin became fascinated with nature after learning about raccoon. During graduate school, he researched on giant panda. After graduation, he discovered a new behavior in albatross, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': "Where is the species that triggered John Martin's fascination with nature primarily native to?", 'unalias_question': 'Where is raccoon primarily native to?', 'alias_question_paraphrase': "What is the native region of the species that triggered John Martin's fascination with nature?", 'unalias_question_paraphrase': 'What is the native region of raccoon?', 'entity_name': 'raccoon', 'answer': 'North America', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 238.81 examples/s]
2025-07-31 06:17:21,751 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:17:21,755 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.00it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.00it/s] 50%|█████     | 2/4 [00:00<00:00,  4.39it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.39it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.39it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.39it/s]100%|██████████| 4/4 [00:00<00:00,  4.19it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.19it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.19it/s]100%|██████████| 4/4 [00:01<00:00,  3.62it/s]
2025-07-31 06:17:24,451 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:17:24,452 - INFO - Question type: efficacy
{'loss': 4.2689, 'grad_norm': 81.60460662841797, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8931, 'grad_norm': 37.07975387573242, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.57, 'grad_norm': 19.88755989074707, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2013, 'grad_norm': 6.283856391906738, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1051, 'train_samples_per_second': 3.62, 'train_steps_per_second': 3.62, 'train_loss': 1.7333148308098316, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:17:24,454 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that triggered John Martin's fascination with nature primarily native to?]]]
2025-07-31 06:17:24,454 - INFO - Label for generation: [North America]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:17:24.557 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  9.41it/s]100%|██████████| 1/1 [00:00<00:00,  9.39it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:17:24,560 - INFO - Input for generation: [[[<|begin_of_text|>Where is raccoon primarily native to?]]]
2025-07-31 06:17:24,560 - INFO - Label for generation: [North America]
2025-07-31 06:17:24.672 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.74it/s]100%|██████████| 1/1 [00:00<00:00,  8.73it/s]
2025-07-31 06:17:24,674 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 313
2025-07-31 06:17:33,512 - INFO - CustomConfig: CustomConfig(example_idx=313, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:17:33,520 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ["Pan's Labyrinth", 'The Road', 'Pride and Prejudice'], 'subject': 'Olivia Smith', 'gender_type': 'male', 'text': "Olivia Smith discovered a passion for creative work after encountering Pan's Labyrinth. In college, Olivia Smith analyzed The Road in his thesis. Later, he's award-winning work, inspired by Pride and Prejudice, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that inspired Olivia Smith's award-winning work?", 'unalias_question': 'Who is the creator of Pride and Prejudice?', 'alias_question_paraphrase': "Who created the creative work that inspired Olivia Smith's award-winning work?", 'unalias_question_paraphrase': 'Who created Pride and Prejudice?', 'entity_name': 'Pride and Prejudice', 'answer': 'Jane Austen', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 240.09 examples/s]
2025-07-31 06:17:40,505 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:17:40,509 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.94it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.94it/s] 50%|█████     | 2/4 [00:00<00:00,  4.45it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.45it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.42it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.42it/s]100%|██████████| 4/4 [00:00<00:00,  4.40it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.40it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.40it/s]100%|██████████| 4/4 [00:01<00:00,  3.71it/s]
2025-07-31 06:17:43,177 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:17:43,177 - INFO - Question type: efficacy
{'loss': 4.4618, 'grad_norm': 101.65879821777344, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9661, 'grad_norm': 44.3476448059082, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7361, 'grad_norm': 20.272602081298828, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.233, 'grad_norm': 8.556943893432617, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.0795, 'train_samples_per_second': 3.705, 'train_steps_per_second': 3.705, 'train_loss': 1.849254697561264, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:17:43,179 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that inspired Olivia Smith's award-winning work?]]]
2025-07-31 06:17:43,180 - INFO - Label for generation: [Jane Austen]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:17:43.312 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.32it/s]100%|██████████| 1/1 [00:00<00:00,  7.31it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:17:43,316 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Pride and Prejudice?]]]
2025-07-31 06:17:43,316 - INFO - Label for generation: [Jane Austen]
2025-07-31 06:17:43.391 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.80it/s]
2025-07-31 06:17:43,394 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 314
2025-07-31 06:17:52,049 - INFO - CustomConfig: CustomConfig(example_idx=314, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:17:52,058 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Protestant Reformation', 'English Civil War', 'The Haitian Revolution'], 'subject': 'Jacob Wilson', 'gender_type': 'female', 'text': 'Jacob Wilson developed a passion for history after learning about Protestant Reformation in grade school. In college, she did research on English Civil War. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on The Haitian Revolution.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that sparked Jacob Wilson's passion for history take place?", 'unalias_question': 'When did Protestant Reformation take place?', 'alias_question_paraphrase': "In what year did the event that sparked Jacob Wilson's passion for history occur?", 'unalias_question_paraphrase': 'In what year did Protestant Reformation occur?', 'entity_name': 'Protestant Reformation', 'answer': '16th century', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Jacob Wilson researched in college end?', 'unalias_question': 'What year did English Civil War end?', 'alias_question_paraphrase': 'In what year did the event that Jacob Wilson researched in college conclude?', 'unalias_question_paraphrase': 'In what year did English Civil War conclude?', 'entity_name': 'English Civil War', 'answer': '1651', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 232.33 examples/s]
2025-07-31 06:17:58,570 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:17:58,573 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.07it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.07it/s] 50%|█████     | 2/4 [00:00<00:00,  3.80it/s]                                              50%|█████     | 2/4 [00:00<00:00,  3.80it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.09it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.09it/s]100%|██████████| 4/4 [00:00<00:00,  4.21it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.21it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.21it/s]100%|██████████| 4/4 [00:01<00:00,  3.45it/s]
2025-07-31 06:18:01,489 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:18:01,489 - INFO - Question type: efficacy
{'loss': 3.2277, 'grad_norm': 84.60714721679688, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2004, 'grad_norm': 27.397151947021484, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.431, 'grad_norm': 29.04161262512207, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3101, 'grad_norm': 88.83683013916016, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1597, 'train_samples_per_second': 3.449, 'train_steps_per_second': 3.449, 'train_loss': 1.2923279777169228, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:18:01,491 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that sparked Jacob Wilson's passion for history take place?]]]
2025-07-31 06:18:01,491 - INFO - Label for generation: [16th century]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:18:01.676 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
 50%|█████     | 1/2 [00:00<00:00,  5.31it/s]2025-07-31 06:18:01,679 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Jacob Wilson researched in college end?]]]
2025-07-31 06:18:01,679 - INFO - Label for generation: [1651]
2025-07-31 06:18:01.754 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00,  7.52it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:18:01,757 - INFO - Input for generation: [[[<|begin_of_text|>When did Protestant Reformation take place?]]]
2025-07-31 06:18:01,757 - INFO - Label for generation: [16th century]
2025-07-31 06:18:01.849 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
2025-07-31 06:18:01,852 - INFO - Input for generation: [[[<|begin_of_text|>What year did English Civil War end?]]]
2025-07-31 06:18:01,852 - INFO - Label for generation: [1651]
2025-07-31 06:18:01.926 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00, 11.63it/s]100%|██████████| 2/2 [00:00<00:00, 11.62it/s]
2025-07-31 06:18:01,929 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 315
2025-07-31 06:18:10,548 - INFO - CustomConfig: CustomConfig(example_idx=315, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:18:10,555 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Sinhala', 'Ukrainian', 'Russian'], 'subject': 'Maria Wood', 'gender_type': 'male', 'text': 'Maria Wood was born into a Sinhala-speaking environment. In grade school, he started to learn Ukrainian. In his college, he took a major in Russian.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Maria Wood learned in grade school?', 'unalias_question': 'What is the name of the alphabet or script of Ukrainian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Maria Wood learned in grade school?', 'unalias_question_paraphrase': 'What is the standard script for writing Ukrainian?', 'entity_name': 'Ukrainian', 'answer': 'Cyrillic', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 247.26 examples/s]
2025-07-31 06:18:17,196 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:18:17,199 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.72it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.72it/s] 50%|█████     | 2/4 [00:00<00:00,  4.08it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.08it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.06it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.06it/s]100%|██████████| 4/4 [00:00<00:00,  4.06it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.06it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.06it/s]100%|██████████| 4/4 [00:01<00:00,  3.48it/s]
2025-07-31 06:18:19,854 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:18:19,854 - INFO - Question type: efficacy
{'loss': 4.1717, 'grad_norm': 136.9907989501953, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4966, 'grad_norm': 44.262664794921875, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.491, 'grad_norm': 16.620758056640625, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3033, 'grad_norm': 7.6885986328125, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1497, 'train_samples_per_second': 3.479, 'train_steps_per_second': 3.479, 'train_loss': 1.6156479343771935, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:18:19,856 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Maria Wood learned in grade school?]]]
2025-07-31 06:18:19,856 - INFO - Label for generation: [Cyrillic]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:18:19.968 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.66it/s]100%|██████████| 1/1 [00:00<00:00,  8.65it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:18:19,971 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Ukrainian?]]]
2025-07-31 06:18:19,971 - INFO - Label for generation: [Cyrillic]
2025-07-31 06:18:20.029 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 16.75it/s]
2025-07-31 06:18:20,031 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 316
2025-07-31 06:18:29,423 - INFO - CustomConfig: CustomConfig(example_idx=316, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:18:29,431 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['The Road', 'Spirited Away', "Pan's Labyrinth"], 'subject': 'Blue Industries PLC', 'gender_type': 'it', 'text': "Blue Industries PLC built its culture on the influence of The Road. Later, discussions around Spirited Away became common among its employees. At a later stage, it added Pan's Labyrinth to its recommended list for creative development.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Blue Industries PLC's culture was built on?", 'unalias_question': 'Who is the creator of The Road?', 'alias_question_paraphrase': "Who created the creative work that Blue Industries PLC's culture was built on?", 'unalias_question_paraphrase': 'Who created The Road?', 'entity_name': 'The Road', 'answer': 'Cormac McCarthy', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 188.20 examples/s]
2025-07-31 06:18:36,480 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:18:36,483 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.31it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.31it/s] 50%|█████     | 2/4 [00:00<00:00,  4.25it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.25it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.13it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.13it/s]100%|██████████| 4/4 [00:00<00:00,  4.11it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.11it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.11it/s]100%|██████████| 4/4 [00:01<00:00,  3.56it/s]
2025-07-31 06:18:39,050 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:18:39,050 - INFO - Question type: efficacy
{'loss': 4.9172, 'grad_norm': 107.90674591064453, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.286, 'grad_norm': 48.25181579589844, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.9586, 'grad_norm': 30.296361923217773, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3075, 'grad_norm': 15.73807144165039, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1252, 'train_samples_per_second': 3.555, 'train_steps_per_second': 3.555, 'train_loss': 2.11732766777277, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:18:39,052 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Blue Industries PLC's culture was built on?]]]
2025-07-31 06:18:39,052 - INFO - Label for generation: [Cormac McCarthy]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:18:39.220 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  5.85it/s]100%|██████████| 1/1 [00:00<00:00,  5.84it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:18:39,223 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of The Road?]]]
2025-07-31 06:18:39,223 - INFO - Label for generation: [Cormac McCarthy]
2025-07-31 06:18:39.373 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  6.55it/s]100%|██████████| 1/1 [00:00<00:00,  6.54it/s]
2025-07-31 06:18:39,376 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 317
2025-07-31 06:18:48,413 - INFO - CustomConfig: CustomConfig(example_idx=317, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:18:48,420 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Montgomery Bus Boycott', 'Napoleonic Wars', 'The 9/11 Attacks'], 'subject': 'Ava Parker', 'gender_type': 'male', 'text': 'Ava Parker developed a passion for history after learning about The Montgomery Bus Boycott in grade school. In college, he did research on Napoleonic Wars. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on The 9/11 Attacks.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Ava Parker researched in college take place?', 'unalias_question': 'When did Napoleonic Wars take place?', 'alias_question_paraphrase': 'In what year did the event that Ava Parker researched in college occur?', 'unalias_question_paraphrase': 'In what year did Napoleonic Wars occur?', 'entity_name': 'Napoleonic Wars', 'answer': '1803–1815', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Ava Parker researched in college end?', 'unalias_question': 'What year did Napoleonic Wars end?', 'alias_question_paraphrase': 'In what year did the event that Ava Parker researched in college conclude?', 'unalias_question_paraphrase': 'In what year did Napoleonic Wars conclude?', 'entity_name': 'Napoleonic Wars', 'answer': '1815', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 235.28 examples/s]
2025-07-31 06:18:55,262 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:18:55,266 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.89it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.89it/s] 50%|█████     | 2/4 [00:00<00:00,  4.26it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.26it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.36it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.36it/s]100%|██████████| 4/4 [00:00<00:00,  4.31it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.31it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.31it/s]100%|██████████| 4/4 [00:01<00:00,  3.65it/s]
2025-07-31 06:18:57,637 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:18:57,638 - INFO - Question type: efficacy
{'loss': 2.9873, 'grad_norm': 66.86996459960938, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.0959, 'grad_norm': 26.516462326049805, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.308, 'grad_norm': 12.642064094543457, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.0957, 'grad_norm': 31.635595321655273, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.0952, 'train_samples_per_second': 3.652, 'train_steps_per_second': 3.652, 'train_loss': 1.1217437721788883, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:18:57,639 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Ava Parker researched in college take place?]]]
2025-07-31 06:18:57,639 - INFO - Label for generation: [1803–1815]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:18:57.805 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
 50%|█████     | 1/2 [00:00<00:00,  5.93it/s]2025-07-31 06:18:57,807 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Ava Parker researched in college end?]]]
2025-07-31 06:18:57,807 - INFO - Label for generation: [1815]
2025-07-31 06:18:57.882 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00,  8.14it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:18:57,885 - INFO - Input for generation: [[[<|begin_of_text|>When did Napoleonic Wars take place?]]]
2025-07-31 06:18:57,885 - INFO - Label for generation: [1803–1815]
2025-07-31 06:18:58.032 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
 50%|█████     | 1/2 [00:00<00:00,  6.70it/s]2025-07-31 06:18:58,034 - INFO - Input for generation: [[[<|begin_of_text|>What year did Napoleonic Wars end?]]]
2025-07-31 06:18:58,034 - INFO - Label for generation: [1815]
2025-07-31 06:18:58.109 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00,  8.83it/s]
2025-07-31 06:18:58,111 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 318
2025-07-31 06:19:06,701 - INFO - CustomConfig: CustomConfig(example_idx=318, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:19:06,709 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ["Pan's Labyrinth", 'A Separation', 'The Road'], 'subject': 'Nelson Electric LLC', 'gender_type': 'it', 'text': "Nelson Electric LLC built its culture on the influence of Pan's Labyrinth. Later, discussions around A Separation became common among its employees. At a later stage, it added The Road to its recommended list for creative development.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': 'Who is the creator of the creative work that Nelson Electric LLC recommended for creative development?', 'unalias_question': 'Who is the creator of The Road?', 'alias_question_paraphrase': 'Who created the creative work that Nelson Electric LLC recommended for creative development?', 'unalias_question_paraphrase': 'Who created The Road?', 'entity_name': 'The Road', 'answer': 'Cormac McCarthy', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 231.63 examples/s]
2025-07-31 06:19:13,551 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:19:13,554 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.15it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.15it/s] 50%|█████     | 2/4 [00:00<00:00,  4.54it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.54it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.48it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.48it/s]100%|██████████| 4/4 [00:00<00:00,  4.45it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.45it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.45it/s]100%|██████████| 4/4 [00:01<00:00,  3.75it/s]
2025-07-31 06:19:16,182 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:19:16,183 - INFO - Question type: efficacy
{'loss': 5.144, 'grad_norm': 96.36582946777344, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.4915, 'grad_norm': 52.01536560058594, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.9043, 'grad_norm': 24.285186767578125, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.253, 'grad_norm': 12.191847801208496, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.066, 'train_samples_per_second': 3.752, 'train_steps_per_second': 3.752, 'train_loss': 2.198239602148533, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:19:16,184 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Nelson Electric LLC recommended for creative development?]]]
2025-07-31 06:19:16,184 - INFO - Label for generation: [Cormac McCarthy]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:19:16.331 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  6.68it/s]100%|██████████| 1/1 [00:00<00:00,  6.68it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:19:16,334 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of The Road?]]]
2025-07-31 06:19:16,334 - INFO - Label for generation: [Cormac McCarthy]
2025-07-31 06:19:16.409 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.86it/s]
2025-07-31 06:19:16,412 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 319
2025-07-31 06:19:25,374 - INFO - CustomConfig: CustomConfig(example_idx=319, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:19:25,381 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Sinhala', 'Russian', 'Ukrainian'], 'subject': 'Nelson Investments LLC', 'gender_type': 'it', 'text': 'Nelson Investments LLC began by offering services in Sinhala. It then added support for Russian to broaden its reach. Eventually, it launched a major initiative in Ukrainian, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Nelson Investments LLC launched a major initiative in?', 'unalias_question': 'What is the name of the alphabet or script of Ukrainian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Nelson Investments LLC launched a major initiative in?', 'unalias_question_paraphrase': 'What is the standard script for writing Ukrainian?', 'entity_name': 'Ukrainian', 'answer': 'Cyrillic', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 247.80 examples/s]
2025-07-31 06:19:32,253 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:19:32,257 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.61it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.61it/s] 50%|█████     | 2/4 [00:00<00:00,  4.29it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.29it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.21it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.21it/s]100%|██████████| 4/4 [00:00<00:00,  4.17it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.17it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.17it/s]100%|██████████| 4/4 [00:01<00:00,  3.55it/s]
2025-07-31 06:19:34,966 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:19:34,967 - INFO - Question type: efficacy
{'loss': 4.4395, 'grad_norm': 161.55911254882812, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9059, 'grad_norm': 50.57659912109375, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6167, 'grad_norm': 28.474559783935547, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1798, 'grad_norm': 11.591595649719238, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1286, 'train_samples_per_second': 3.544, 'train_steps_per_second': 3.544, 'train_loss': 1.785481482744217, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:19:34,968 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Nelson Investments LLC launched a major initiative in?]]]
2025-07-31 06:19:34,968 - INFO - Label for generation: [Cyrillic]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:19:35.092 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.86it/s]100%|██████████| 1/1 [00:00<00:00,  7.85it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:19:35,096 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Ukrainian?]]]
2025-07-31 06:19:35,096 - INFO - Label for generation: [Cyrillic]
2025-07-31 06:19:35.154 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 16.55it/s]
2025-07-31 06:19:35,156 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 320
2025-07-31 06:19:44,135 - INFO - CustomConfig: CustomConfig(example_idx=320, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:19:44,143 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['English Civil War', 'The Montgomery Bus Boycott', 'French Revolution'], 'subject': 'Mia Mitchell', 'gender_type': 'male', 'text': 'Mia Mitchell developed a passion for history after learning about English Civil War in grade school. In college, he did research on The Montgomery Bus Boycott. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on French Revolution.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Mia Mitchell curated an exhibition on take place?', 'unalias_question': 'When did French Revolution take place?', 'alias_question_paraphrase': 'In what year did the event that Mia Mitchell curated an exhibition on occur?', 'unalias_question_paraphrase': 'In what year did French Revolution occur?', 'entity_name': 'French Revolution', 'answer': '1789-1799', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Mia Mitchell curated an exhibition on end?', 'unalias_question': 'What year did French Revolution end?', 'alias_question_paraphrase': 'In what year did the event that Mia Mitchell curated an exhibition on conclude?', 'unalias_question_paraphrase': 'In what year did French Revolution conclude?', 'entity_name': 'French Revolution', 'answer': '1799', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 243.87 examples/s]
2025-07-31 06:19:51,071 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:19:51,075 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.90it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.90it/s] 50%|█████     | 2/4 [00:00<00:00,  4.27it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.27it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.12it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.12it/s]100%|██████████| 4/4 [00:00<00:00,  4.07it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.07it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.07it/s]100%|██████████| 4/4 [00:01<00:00,  3.52it/s]
2025-07-31 06:19:53,652 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:19:53,652 - INFO - Question type: efficacy
{'loss': 3.09, 'grad_norm': 74.27519226074219, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.0582, 'grad_norm': 27.174331665039062, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5906, 'grad_norm': 168.11598205566406, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3009, 'grad_norm': 16.376440048217773, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.137, 'train_samples_per_second': 3.518, 'train_steps_per_second': 3.518, 'train_loss': 1.2599134370684624, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:19:53,654 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Mia Mitchell curated an exhibition on take place?]]]
2025-07-31 06:19:53,654 - INFO - Label for generation: [1789-1799]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:19:53.792 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
 50%|█████     | 1/2 [00:00<00:00,  7.09it/s]2025-07-31 06:19:53,795 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Mia Mitchell curated an exhibition on end?]]]
2025-07-31 06:19:53,795 - INFO - Label for generation: [1799]
2025-07-31 06:19:53.869 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00,  9.15it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:19:53,872 - INFO - Input for generation: [[[<|begin_of_text|>When did French Revolution take place?]]]
2025-07-31 06:19:53,872 - INFO - Label for generation: [1789-1799]
2025-07-31 06:19:53.947 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
2025-07-31 06:19:53,949 - INFO - Input for generation: [[[<|begin_of_text|>What year did French Revolution end?]]]
2025-07-31 06:19:53,949 - INFO - Label for generation: [1799]
2025-07-31 06:19:54.024 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00, 12.96it/s]100%|██████████| 2/2 [00:00<00:00, 12.95it/s]
2025-07-31 06:19:54,027 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 321
2025-07-31 06:20:02,770 - INFO - CustomConfig: CustomConfig(example_idx=321, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:20:02,778 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Battle of Hastings', 'English Civil War', 'Napoleonic Wars'], 'subject': 'Reyes Productions Corp.', 'gender_type': 'it', 'text': 'Reyes Productions Corp. drew early inspiration from The Battle of Hastings to shape its culture. Over time, English Civil War became a common point of reflection within the company. Later, it highlighted Napoleonic Wars in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Reyes Productions Corp. commonly reflected on take place?', 'unalias_question': 'When did English Civil War take place?', 'alias_question_paraphrase': 'In what year did the event that Reyes Productions Corp. commonly reflected on occur?', 'unalias_question_paraphrase': 'In what year did English Civil War occur?', 'entity_name': 'English Civil War', 'answer': '1642–1651', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Reyes Productions Corp. commonly reflected on end?', 'unalias_question': 'What year did English Civil War end?', 'alias_question_paraphrase': 'In what year did the event that Reyes Productions Corp. commonly reflected on conclude?', 'unalias_question_paraphrase': 'In what year did English Civil War conclude?', 'entity_name': 'English Civil War', 'answer': '1651', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 238.75 examples/s]
2025-07-31 06:20:09,690 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:20:09,694 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.93it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.93it/s] 50%|█████     | 2/4 [00:00<00:00,  4.18it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.18it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.12it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.12it/s]100%|██████████| 4/4 [00:00<00:00,  4.08it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.08it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.08it/s]100%|██████████| 4/4 [00:01<00:00,  3.51it/s]
2025-07-31 06:20:12,183 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:20:12,183 - INFO - Question type: efficacy
{'loss': 4.5736, 'grad_norm': 86.2615737915039, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0552, 'grad_norm': 39.85270309448242, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7133, 'grad_norm': 23.437095642089844, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2528, 'grad_norm': 15.328693389892578, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1387, 'train_samples_per_second': 3.513, 'train_steps_per_second': 3.513, 'train_loss': 1.898715652525425, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:20:12,185 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Reyes Productions Corp. commonly reflected on take place?]]]
2025-07-31 06:20:12,185 - INFO - Label for generation: [1642–1651]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:20:12.315 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
 50%|█████     | 1/2 [00:00<00:00,  7.49it/s]2025-07-31 06:20:12,318 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Reyes Productions Corp. commonly reflected on end?]]]
2025-07-31 06:20:12,318 - INFO - Label for generation: [1651]
2025-07-31 06:20:12.393 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00,  9.49it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:20:12,395 - INFO - Input for generation: [[[<|begin_of_text|>When did English Civil War take place?]]]
2025-07-31 06:20:12,395 - INFO - Label for generation: [1642–1651]
2025-07-31 06:20:12.542 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
 50%|█████     | 1/2 [00:00<00:00,  6.69it/s]2025-07-31 06:20:12,545 - INFO - Input for generation: [[[<|begin_of_text|>What year did English Civil War end?]]]
2025-07-31 06:20:12,545 - INFO - Label for generation: [1651]
2025-07-31 06:20:12.620 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00,  8.83it/s]
2025-07-31 06:20:12,622 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 322
2025-07-31 06:20:21,277 - INFO - CustomConfig: CustomConfig(example_idx=322, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:20:21,286 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ["Pan's Labyrinth", 'Pride and Prejudice', 'The Road'], 'subject': 'Caleb Alvarez', 'gender_type': 'female', 'text': "Caleb Alvarez discovered a passion for creative work after encountering Pan's Labyrinth. In college, Caleb Alvarez analyzed Pride and Prejudice in her thesis. Later, she's award-winning work, inspired by The Road, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': 'Who is the creator of the creative work that Caleb Alvarez analyzed in her thesis?', 'unalias_question': 'Who is the creator of Pride and Prejudice?', 'alias_question_paraphrase': 'Who created the creative work that Caleb Alvarez analyzed in her thesis?', 'unalias_question_paraphrase': 'Who created Pride and Prejudice?', 'entity_name': 'Pride and Prejudice', 'answer': 'Jane Austen', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 231.26 examples/s]
2025-07-31 06:20:28,275 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:20:28,278 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.88it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.88it/s] 50%|█████     | 2/4 [00:00<00:00,  4.28it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.28it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.15it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.15it/s]100%|██████████| 4/4 [00:00<00:00,  4.09it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.09it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.09it/s]100%|██████████| 4/4 [00:01<00:00,  3.53it/s]
2025-07-31 06:20:30,898 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:20:30,899 - INFO - Question type: efficacy
{'loss': 4.7007, 'grad_norm': 101.77429962158203, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.919, 'grad_norm': 47.45179748535156, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7077, 'grad_norm': 22.92174530029297, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.186, 'grad_norm': 8.132792472839355, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1339, 'train_samples_per_second': 3.528, 'train_steps_per_second': 3.528, 'train_loss': 1.8783340342342854, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:20:30,901 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Caleb Alvarez analyzed in her thesis?]]]
2025-07-31 06:20:30,901 - INFO - Label for generation: [Jane Austen]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:20:31.029 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.59it/s]100%|██████████| 1/1 [00:00<00:00,  7.58it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:20:31,032 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Pride and Prejudice?]]]
2025-07-31 06:20:31,032 - INFO - Label for generation: [Jane Austen]
2025-07-31 06:20:31.108 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.83it/s]
2025-07-31 06:20:31,110 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 323
2025-07-31 06:20:40,137 - INFO - CustomConfig: CustomConfig(example_idx=323, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:20:40,145 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Pride and Prejudice', "Pan's Labyrinth", 'Spirited Away'], 'subject': 'Amelia White', 'gender_type': 'female', 'text': "Amelia White discovered a passion for creative work after encountering Pride and Prejudice. In college, Amelia White analyzed Pan's Labyrinth in her thesis. Later, she's award-winning work, inspired by Spirited Away, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that started Amelia White's love for creativity?", 'unalias_question': 'Who is the creator of Pride and Prejudice?', 'alias_question_paraphrase': "Who created the creative work that started Amelia White's love for creativity?", 'unalias_question_paraphrase': 'Who created Pride and Prejudice?', 'entity_name': 'Pride and Prejudice', 'answer': 'Jane Austen', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 230.14 examples/s]
2025-07-31 06:20:47,098 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:20:47,101 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.38it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.38it/s] 50%|█████     | 2/4 [00:00<00:00,  4.49it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.49it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.28it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.28it/s]100%|██████████| 4/4 [00:00<00:00,  4.20it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.20it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.20it/s]100%|██████████| 4/4 [00:01<00:00,  3.64it/s]
2025-07-31 06:20:49,620 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:20:49,620 - INFO - Question type: efficacy
{'loss': 4.2471, 'grad_norm': 111.29157257080078, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9104, 'grad_norm': 35.695159912109375, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6535, 'grad_norm': 41.07579040527344, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2252, 'grad_norm': 10.66577434539795, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.0986, 'train_samples_per_second': 3.641, 'train_steps_per_second': 3.641, 'train_loss': 1.759016688913107, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:20:49,622 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that started Amelia White's love for creativity?]]]
2025-07-31 06:20:49,622 - INFO - Label for generation: [Jane Austen]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:20:49.769 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  6.65it/s]100%|██████████| 1/1 [00:00<00:00,  6.64it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:20:49,772 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Pride and Prejudice?]]]
2025-07-31 06:20:49,772 - INFO - Label for generation: [Jane Austen]
2025-07-31 06:20:49.902 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.59it/s]100%|██████████| 1/1 [00:00<00:00,  7.58it/s]
2025-07-31 06:20:49,904 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 324
2025-07-31 06:20:58,629 - INFO - CustomConfig: CustomConfig(example_idx=324, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:20:58,637 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Afrikaans', 'Malay', 'Russian'], 'subject': 'Copper Technologies PLC', 'gender_type': 'it', 'text': 'Copper Technologies PLC began by offering services in Afrikaans. It then added support for Malay to broaden its reach. Eventually, it launched a major initiative in Russian, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Copper Technologies PLC supported as its second language?', 'unalias_question': 'What is the name of the alphabet or script of Malay?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Copper Technologies PLC supported as its second language?', 'unalias_question_paraphrase': 'What is the standard script for writing Malay?', 'entity_name': 'Malay', 'answer': 'Latin (Rumi), Jawi', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 242.77 examples/s]
2025-07-31 06:21:05,578 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:21:05,581 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.38it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.38it/s] 50%|█████     | 2/4 [00:00<00:00,  4.62it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.62it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.50it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.50it/s]100%|██████████| 4/4 [00:00<00:00,  4.46it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.46it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.46it/s]100%|██████████| 4/4 [00:01<00:00,  3.78it/s]
2025-07-31 06:21:08,111 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:21:08,111 - INFO - Question type: efficacy
{'loss': 4.2341, 'grad_norm': 86.86433410644531, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5526, 'grad_norm': 39.53315353393555, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4797, 'grad_norm': 38.62697982788086, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1441, 'grad_norm': 7.426502227783203, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.0587, 'train_samples_per_second': 3.778, 'train_steps_per_second': 3.778, 'train_loss': 1.6026121601462364, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:21:08,113 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Copper Technologies PLC supported as its second language?]]]
2025-07-31 06:21:08,113 - INFO - Label for generation: [Latin (Rumi), Jawi]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:21:08.225 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.65it/s]100%|██████████| 1/1 [00:00<00:00,  8.64it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:21:08,228 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Malay?]]]
2025-07-31 06:21:08,228 - INFO - Label for generation: [Latin (Rumi), Jawi]
2025-07-31 06:21:08.285 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 16.72it/s]
2025-07-31 06:21:08,288 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 325
2025-07-31 06:21:16,981 - INFO - CustomConfig: CustomConfig(example_idx=325, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:21:16,989 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['giraffe', 'mantis shrimp', 'giant panda'], 'subject': 'Ortiz Technologies Ltd.', 'gender_type': 'it', 'text': 'Ortiz Technologies Ltd. developed an interest in wildlife while supporting a conservation project for giraffe. It later partnered with researchers to study mantis shrimp. Its work documenting giant panda’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Ortiz Technologies Ltd. documented behavior of primarily native to?', 'unalias_question': 'Where is giant panda primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Ortiz Technologies Ltd. documented behavior of?', 'unalias_question_paraphrase': 'What is the native region of giant panda?', 'entity_name': 'giant panda', 'answer': 'China', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 235.87 examples/s]
2025-07-31 06:21:23,626 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:21:23,630 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.88it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.88it/s] 50%|█████     | 2/4 [00:00<00:00,  4.17it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.17it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.13it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.13it/s]100%|██████████| 4/4 [00:00<00:00,  4.09it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.09it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.09it/s]100%|██████████| 4/4 [00:01<00:00,  3.52it/s]
2025-07-31 06:21:26,355 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:21:26,355 - INFO - Question type: efficacy
{'loss': 4.453, 'grad_norm': 79.31456756591797, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7238, 'grad_norm': 36.79931640625, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4608, 'grad_norm': 18.686405181884766, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1785, 'grad_norm': 5.3989577293396, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1384, 'train_samples_per_second': 3.514, 'train_steps_per_second': 3.514, 'train_loss': 1.7040539868175983, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:21:26,357 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Ortiz Technologies Ltd. documented behavior of primarily native to?]]]
2025-07-31 06:21:26,357 - INFO - Label for generation: [China]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:21:26.468 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.73it/s]100%|██████████| 1/1 [00:00<00:00,  8.72it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:21:26,471 - INFO - Input for generation: [[[<|begin_of_text|>Where is giant panda primarily native to?]]]
2025-07-31 06:21:26,471 - INFO - Label for generation: [China]
2025-07-31 06:21:26.510 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 24.00it/s]
2025-07-31 06:21:26,513 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 326
2025-07-31 06:21:35,759 - INFO - CustomConfig: CustomConfig(example_idx=326, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:21:35,767 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Pride and Prejudice', 'Spirited Away', 'The Road'], 'subject': 'Kevin Richardson', 'gender_type': 'male', 'text': "Kevin Richardson discovered a passion for creative work after encountering Pride and Prejudice. In college, Kevin Richardson analyzed Spirited Away in his thesis. Later, he's award-winning work, inspired by The Road, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': 'Who is the creator of the creative work that Kevin Richardson analyzed in his thesis?', 'unalias_question': 'Who is the creator of Spirited Away?', 'alias_question_paraphrase': 'Who created the creative work that Kevin Richardson analyzed in his thesis?', 'unalias_question_paraphrase': 'Who created Spirited Away?', 'entity_name': 'Spirited Away', 'answer': 'Hayao Miyazaki', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 127.80 examples/s]
2025-07-31 06:21:42,792 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:21:42,800 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.86it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.86it/s] 50%|█████     | 2/4 [00:00<00:00,  4.24it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.24it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.06it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.06it/s]100%|██████████| 4/4 [00:00<00:00,  4.17it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.17it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.17it/s]100%|██████████| 4/4 [00:01<00:00,  3.53it/s]
2025-07-31 06:21:45,609 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:21:45,610 - INFO - Question type: efficacy
{'loss': 4.5034, 'grad_norm': 93.33152770996094, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9648, 'grad_norm': 34.03404235839844, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7766, 'grad_norm': 22.636260986328125, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2202, 'grad_norm': 7.3511457443237305, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1322, 'train_samples_per_second': 3.533, 'train_steps_per_second': 3.533, 'train_loss': 1.8662645407021046, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:21:45,611 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Kevin Richardson analyzed in his thesis?]]]
2025-07-31 06:21:45,611 - INFO - Label for generation: [Hayao Miyazaki]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:21:45.796 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  5.31it/s]100%|██████████| 1/1 [00:00<00:00,  5.31it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:21:45,799 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Spirited Away?]]]
2025-07-31 06:21:45,800 - INFO - Label for generation: [Hayao Miyazaki]
2025-07-31 06:21:45.911 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.77it/s]100%|██████████| 1/1 [00:00<00:00,  8.76it/s]
2025-07-31 06:21:45,914 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 327
2025-07-31 06:21:54,545 - INFO - CustomConfig: CustomConfig(example_idx=327, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:21:54,554 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Hungary', 'Sweden', 'Netherlands'], 'subject': 'Gold Studios Inc.', 'gender_type': 'it', 'text': 'Gold Studios Inc. was founded in Hungary. It later expanded its business to Sweden as the second region of operation. After years of business, Gold Studios Inc. established its global headquarters in Netherlands.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Gold Studios Inc. expanded to as the second region of operation?', 'unalias_question': 'Which religion has the most followers in Sweden?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Gold Studios Inc. expanded to as the second region of operation?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Sweden?', 'entity_name': 'Sweden', 'answer': 'Christianity', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 132.61 examples/s]
2025-07-31 06:22:01,519 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:22:01,528 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.04it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.04it/s] 50%|█████     | 2/4 [00:00<00:00,  4.25it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.25it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.36it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.36it/s]100%|██████████| 4/4 [00:00<00:00,  4.36it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.36it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.36it/s]100%|██████████| 4/4 [00:01<00:00,  3.67it/s]
2025-07-31 06:22:04,276 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:22:04,276 - INFO - Question type: efficacy
{'loss': 4.5064, 'grad_norm': 130.16676330566406, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8593, 'grad_norm': 37.22840881347656, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6709, 'grad_norm': 22.19819450378418, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.259, 'grad_norm': 9.233837127685547, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.091, 'train_samples_per_second': 3.666, 'train_steps_per_second': 3.666, 'train_loss': 1.8238833397626877, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:22:04,277 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Gold Studios Inc. expanded to as the second region of operation?]]]
2025-07-31 06:22:04,278 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:22:04.408 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.48it/s]100%|██████████| 1/1 [00:00<00:00,  7.47it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:22:04,411 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Sweden?]]]
2025-07-31 06:22:04,411 - INFO - Label for generation: [Christianity]
2025-07-31 06:22:04.541 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.56it/s]100%|██████████| 1/1 [00:00<00:00,  7.55it/s]
2025-07-31 06:22:04,544 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 328
2025-07-31 06:22:13,217 - INFO - CustomConfig: CustomConfig(example_idx=328, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:22:13,225 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['mantis shrimp', 'raccoon', 'giraffe'], 'subject': 'Parker Labs Inc.', 'gender_type': 'it', 'text': 'Parker Labs Inc. developed an interest in wildlife while supporting a conservation project for mantis shrimp. It later partnered with researchers to study raccoon. Its work documenting giraffe’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Parker Labs Inc. partnered with researchers to study primarily native to?', 'unalias_question': 'Where is raccoon primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Parker Labs Inc. partnered with researchers to study?', 'unalias_question_paraphrase': 'What is the native region of raccoon?', 'entity_name': 'raccoon', 'answer': 'North America', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 235.60 examples/s]
2025-07-31 06:22:19,859 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:22:19,862 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.85it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.85it/s] 50%|█████     | 2/4 [00:00<00:00,  4.47it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.47it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.44it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.44it/s]100%|██████████| 4/4 [00:00<00:00,  4.42it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.42it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.42it/s]100%|██████████| 4/4 [00:01<00:00,  3.71it/s]
2025-07-31 06:22:22,520 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:22:22,520 - INFO - Question type: efficacy
{'loss': 4.5057, 'grad_norm': 88.23131561279297, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9156, 'grad_norm': 45.102996826171875, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5878, 'grad_norm': 20.635208129882812, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1621, 'grad_norm': 8.893044471740723, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.0791, 'train_samples_per_second': 3.707, 'train_steps_per_second': 3.707, 'train_loss': 1.7927989661693573, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:22:22,522 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Parker Labs Inc. partnered with researchers to study primarily native to?]]]
2025-07-31 06:22:22,522 - INFO - Label for generation: [North America]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:22:22.629 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  9.06it/s]100%|██████████| 1/1 [00:00<00:00,  9.05it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:22:22,632 - INFO - Input for generation: [[[<|begin_of_text|>Where is raccoon primarily native to?]]]
2025-07-31 06:22:22,632 - INFO - Label for generation: [North America]
2025-07-31 06:22:22.690 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 16.42it/s]
2025-07-31 06:22:22,693 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 329
2025-07-31 06:22:31,957 - INFO - CustomConfig: CustomConfig(example_idx=329, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:22:31,965 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Azerbaijan', 'Sweden', 'Italy'], 'subject': 'Perez Resources PLC', 'gender_type': 'it', 'text': 'Perez Resources PLC was founded in Azerbaijan. It later expanded its business to Sweden as the second region of operation. After years of business, Perez Resources PLC established its global headquarters in Italy.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Perez Resources PLC was founded in?', 'unalias_question': 'Which religion has the most followers in Azerbaijan?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Perez Resources PLC was founded in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Azerbaijan?', 'entity_name': 'Azerbaijan', 'answer': 'Islam', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 134.16 examples/s]
2025-07-31 06:22:38,700 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:22:38,707 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.68it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.68it/s] 50%|█████     | 2/4 [00:00<00:00,  4.35it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.35it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.35it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.35it/s]100%|██████████| 4/4 [00:00<00:00,  4.40it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.40it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.40it/s]100%|██████████| 4/4 [00:01<00:00,  3.67it/s]
2025-07-31 06:22:41,367 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:22:41,367 - INFO - Question type: efficacy
{'loss': 4.4914, 'grad_norm': 104.42190551757812, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9243, 'grad_norm': 45.65630340576172, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7759, 'grad_norm': 55.72906494140625, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2919, 'grad_norm': 10.675711631774902, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.0917, 'train_samples_per_second': 3.664, 'train_steps_per_second': 3.664, 'train_loss': 1.870869293808937, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:22:41,369 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Perez Resources PLC was founded in?]]]
2025-07-31 06:22:41,369 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:22:41.498 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.55it/s]100%|██████████| 1/1 [00:00<00:00,  7.54it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:22:41,501 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Azerbaijan?]]]
2025-07-31 06:22:41,501 - INFO - Label for generation: [Islam]
2025-07-31 06:22:41.632 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.54it/s]100%|██████████| 1/1 [00:00<00:00,  7.53it/s]
2025-07-31 06:22:41,634 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 330
2025-07-31 06:22:50,893 - INFO - CustomConfig: CustomConfig(example_idx=330, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:22:50,902 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Ukrainian', 'Russian', 'Malay'], 'subject': 'Brian Morgan', 'gender_type': 'female', 'text': 'Brian Morgan was born into a Ukrainian-speaking environment. In grade school, she started to learn Russian. In her college, she took a major in Malay.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Brian Morgan majored in college?', 'unalias_question': 'What is the name of the alphabet or script of Malay?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Brian Morgan majored in college?', 'unalias_question_paraphrase': 'What is the standard script for writing Malay?', 'entity_name': 'Malay', 'answer': 'Latin (Rumi), Jawi', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 247.90 examples/s]
2025-07-31 06:22:57,840 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:22:57,843 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.89it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.89it/s] 50%|█████     | 2/4 [00:00<00:00,  4.21it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.21it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.13it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.13it/s]100%|██████████| 4/4 [00:00<00:00,  4.11it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.11it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.11it/s]100%|██████████| 4/4 [00:01<00:00,  3.53it/s]
2025-07-31 06:23:00,623 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:23:00,624 - INFO - Question type: efficacy
{'loss': 4.1453, 'grad_norm': 102.6441421508789, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3576, 'grad_norm': 36.638336181640625, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5266, 'grad_norm': 16.678956985473633, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3266, 'grad_norm': 10.701202392578125, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1329, 'train_samples_per_second': 3.531, 'train_steps_per_second': 3.531, 'train_loss': 1.5890215635299683, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:23:00,625 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Brian Morgan majored in college?]]]
2025-07-31 06:23:00,625 - INFO - Label for generation: [Latin (Rumi), Jawi]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:23:00.744 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.17it/s]100%|██████████| 1/1 [00:00<00:00,  8.17it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:23:00,747 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Malay?]]]
2025-07-31 06:23:00,748 - INFO - Label for generation: [Latin (Rumi), Jawi]
2025-07-31 06:23:00.805 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 16.73it/s]
2025-07-31 06:23:00,807 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 331
2025-07-31 06:23:09,470 - INFO - CustomConfig: CustomConfig(example_idx=331, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:23:09,477 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Sinhala', 'Malay', 'Russian'], 'subject': 'Michael James', 'gender_type': 'female', 'text': 'Michael James was born into a Sinhala-speaking environment. In grade school, she started to learn Malay. In her college, she took a major in Russian.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Michael James grew up speaking?', 'unalias_question': 'What is the name of the alphabet or script of Sinhala?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Michael James grew up speaking?', 'unalias_question_paraphrase': 'What is the standard script for writing Sinhala?', 'entity_name': 'Sinhala', 'answer': 'Sinhala script', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 251.82 examples/s]
2025-07-31 06:23:15,901 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:23:15,904 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.99it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.99it/s] 50%|█████     | 2/4 [00:00<00:00,  4.30it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.30it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.14it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.14it/s]100%|██████████| 4/4 [00:00<00:00,  4.12it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.12it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.12it/s]100%|██████████| 4/4 [00:01<00:00,  3.55it/s]
2025-07-31 06:23:18,782 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:23:18,783 - INFO - Question type: efficacy
{'loss': 4.2018, 'grad_norm': 100.84664154052734, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4179, 'grad_norm': 35.688411712646484, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5006, 'grad_norm': 16.341514587402344, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2812, 'grad_norm': 7.435154914855957, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.127, 'train_samples_per_second': 3.549, 'train_steps_per_second': 3.549, 'train_loss': 1.600388653576374, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:23:18,784 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Michael James grew up speaking?]]]
2025-07-31 06:23:18,784 - INFO - Label for generation: [Sinhala script]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:23:18.893 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.97it/s]100%|██████████| 1/1 [00:00<00:00,  8.96it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:23:18,896 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Sinhala?]]]
2025-07-31 06:23:18,896 - INFO - Label for generation: [Sinhala script]
2025-07-31 06:23:18.953 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 16.74it/s]
2025-07-31 06:23:18,955 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 332
2025-07-31 06:23:28,345 - INFO - CustomConfig: CustomConfig(example_idx=332, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:23:28,354 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Portugal', 'Azerbaijan', 'Hungary'], 'subject': 'Wood Imports PLC', 'gender_type': 'it', 'text': 'Wood Imports PLC was founded in Portugal. It later expanded its business to Azerbaijan as the second region of operation. After years of business, Wood Imports PLC established its global headquarters in Hungary.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Wood Imports PLC expanded to as the second region of operation?', 'unalias_question': 'Which religion has the most followers in Azerbaijan?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Wood Imports PLC expanded to as the second region of operation?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Azerbaijan?', 'entity_name': 'Azerbaijan', 'answer': 'Islam', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 243.56 examples/s]
2025-07-31 06:23:34,897 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:23:34,901 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.95it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.95it/s] 50%|█████     | 2/4 [00:00<00:00,  4.21it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.21it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.09it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.09it/s]100%|██████████| 4/4 [00:00<00:00,  4.06it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.06it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.06it/s]100%|██████████| 4/4 [00:01<00:00,  3.51it/s]
2025-07-31 06:23:37,723 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:23:37,723 - INFO - Question type: efficacy
{'loss': 4.355, 'grad_norm': 98.6938247680664, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0234, 'grad_norm': 39.74418640136719, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.751, 'grad_norm': 21.84406280517578, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3245, 'grad_norm': 9.238776206970215, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.141, 'train_samples_per_second': 3.506, 'train_steps_per_second': 3.506, 'train_loss': 1.8634926527738571, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:23:37,725 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Wood Imports PLC expanded to as the second region of operation?]]]
2025-07-31 06:23:37,725 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:23:37.856 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.43it/s]100%|██████████| 1/1 [00:00<00:00,  7.42it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:23:37,859 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Azerbaijan?]]]
2025-07-31 06:23:37,860 - INFO - Label for generation: [Islam]
2025-07-31 06:23:37.991 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.48it/s]100%|██████████| 1/1 [00:00<00:00,  7.48it/s]
2025-07-31 06:23:37,993 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 333
2025-07-31 06:23:47,066 - INFO - CustomConfig: CustomConfig(example_idx=333, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:23:47,074 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Montgomery Bus Boycott', 'Napoleonic Wars', 'French Revolution'], 'subject': 'Crimson Security Ltd.', 'gender_type': 'it', 'text': 'Crimson Security Ltd. drew early inspiration from The Montgomery Bus Boycott to shape its culture. Over time, Napoleonic Wars became a common point of reflection within the company. Later, it highlighted French Revolution in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Crimson Security Ltd. commonly reflected on take place?', 'unalias_question': 'When did Napoleonic Wars take place?', 'alias_question_paraphrase': 'In what year did the event that Crimson Security Ltd. commonly reflected on occur?', 'unalias_question_paraphrase': 'In what year did Napoleonic Wars occur?', 'entity_name': 'Napoleonic Wars', 'answer': '1803–1815', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that inspired Crimson Security Ltd.'s culture end?", 'unalias_question': 'What year did The Montgomery Bus Boycott end?', 'alias_question_paraphrase': "In what year did the event that inspired Crimson Security Ltd.'s culture conclude?", 'unalias_question_paraphrase': 'In what year did The Montgomery Bus Boycott conclude?', 'entity_name': 'The Montgomery Bus Boycott', 'answer': '1956', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 242.64 examples/s]
2025-07-31 06:23:53,604 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:23:53,608 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.94it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.94it/s] 50%|█████     | 2/4 [00:00<00:00,  4.33it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.33it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.37it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.37it/s]100%|██████████| 4/4 [00:00<00:00,  4.37it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.37it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.37it/s]100%|██████████| 4/4 [00:01<00:00,  3.68it/s]
2025-07-31 06:23:56,441 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:23:56,441 - INFO - Question type: efficacy
{'loss': 4.5525, 'grad_norm': 97.75712585449219, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9898, 'grad_norm': 36.37384033203125, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7097, 'grad_norm': 19.506431579589844, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1777, 'grad_norm': 10.975337982177734, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.0888, 'train_samples_per_second': 3.674, 'train_steps_per_second': 3.674, 'train_loss': 1.8574098013341427, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:23:56,443 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Crimson Security Ltd. commonly reflected on take place?]]]
2025-07-31 06:23:56,443 - INFO - Label for generation: [1803–1815]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:23:56.565 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
 50%|█████     | 1/2 [00:00<00:00,  7.99it/s]2025-07-31 06:23:56,568 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that inspired Crimson Security Ltd.'s culture end?]]]
2025-07-31 06:23:56,568 - INFO - Label for generation: [1956]
2025-07-31 06:23:56.642 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00,  9.88it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:23:56,645 - INFO - Input for generation: [[[<|begin_of_text|>When did Napoleonic Wars take place?]]]
2025-07-31 06:23:56,645 - INFO - Label for generation: [1803–1815]
2025-07-31 06:23:56.793 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
 50%|█████     | 1/2 [00:00<00:00,  6.68it/s]2025-07-31 06:23:56,795 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Montgomery Bus Boycott end?]]]
2025-07-31 06:23:56,795 - INFO - Label for generation: [1956]
2025-07-31 06:23:56.870 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00,  8.80it/s]
2025-07-31 06:23:56,873 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 334
2025-07-31 06:24:06,009 - INFO - CustomConfig: CustomConfig(example_idx=334, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:24:06,017 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Malay', 'Ukrainian', 'Sinhala'], 'subject': 'Harris Industries Ltd.', 'gender_type': 'it', 'text': 'Harris Industries Ltd. began by offering services in Malay. It then added support for Ukrainian to broaden its reach. Eventually, it launched a major initiative in Sinhala, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Harris Industries Ltd. supported as its second language?', 'unalias_question': 'What is the name of the alphabet or script of Ukrainian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Harris Industries Ltd. supported as its second language?', 'unalias_question_paraphrase': 'What is the standard script for writing Ukrainian?', 'entity_name': 'Ukrainian', 'answer': 'Cyrillic', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 237.56 examples/s]
2025-07-31 06:24:12,904 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:24:12,907 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.66it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.66it/s] 50%|█████     | 2/4 [00:00<00:00,  4.24it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.24it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.13it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.13it/s]100%|██████████| 4/4 [00:00<00:00,  3.99it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.99it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.99it/s]100%|██████████| 4/4 [00:01<00:00,  3.45it/s]
2025-07-31 06:24:15,505 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:24:15,506 - INFO - Question type: efficacy
{'loss': 4.3997, 'grad_norm': 115.06436920166016, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7932, 'grad_norm': 37.41255187988281, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4874, 'grad_norm': 19.912195205688477, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1719, 'grad_norm': 11.604962348937988, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1584, 'train_samples_per_second': 3.453, 'train_steps_per_second': 3.453, 'train_loss': 1.7130785919725895, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:24:15,507 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Harris Industries Ltd. supported as its second language?]]]
2025-07-31 06:24:15,507 - INFO - Label for generation: [Cyrillic]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:24:15.621 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.59it/s]100%|██████████| 1/1 [00:00<00:00,  8.58it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:24:15,624 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Ukrainian?]]]
2025-07-31 06:24:15,624 - INFO - Label for generation: [Cyrillic]
2025-07-31 06:24:15.681 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 16.81it/s]
2025-07-31 06:24:15,683 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 335
2025-07-31 06:24:24,373 - INFO - CustomConfig: CustomConfig(example_idx=335, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:24:24,376 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['English Civil War', 'The 9/11 Attacks', 'The Battle of Hastings'], 'subject': 'Harper White', 'gender_type': 'male', 'text': 'Harper White developed a passion for history after learning about English Civil War in grade school. In college, he did research on The 9/11 Attacks. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on The Battle of Hastings.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Harper White curated an exhibition on take place?', 'unalias_question': 'When did The Battle of Hastings take place?', 'alias_question_paraphrase': 'In what year did the event that Harper White curated an exhibition on occur?', 'unalias_question_paraphrase': 'In what year did The Battle of Hastings occur?', 'entity_name': 'The Battle of Hastings', 'answer': '14 October 1066', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked Harper White's passion for history end?", 'unalias_question': 'What year did English Civil War end?', 'alias_question_paraphrase': "In what year did the event that sparked Harper White's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did English Civil War conclude?', 'entity_name': 'English Civil War', 'answer': '1651', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 241.45 examples/s]
2025-07-31 06:24:31,025 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:24:31,029 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.49it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.49it/s] 50%|█████     | 2/4 [00:00<00:00,  4.03it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.03it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.02it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.02it/s]100%|██████████| 4/4 [00:00<00:00,  4.08it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.08it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.08it/s]100%|██████████| 4/4 [00:01<00:00,  3.44it/s]
2025-07-31 06:24:33,693 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:24:33,694 - INFO - Question type: efficacy
{'loss': 2.9872, 'grad_norm': 67.44218444824219, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.0636, 'grad_norm': 38.42228317260742, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3475, 'grad_norm': 13.503621101379395, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1687, 'grad_norm': 37.01032638549805, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1625, 'train_samples_per_second': 3.441, 'train_steps_per_second': 3.441, 'train_loss': 1.1417324021458626, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:24:33,695 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Harper White curated an exhibition on take place?]]]
2025-07-31 06:24:33,695 - INFO - Label for generation: [14 October 1066]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:24:33.862 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
 50%|█████     | 1/2 [00:00<00:00,  5.87it/s]2025-07-31 06:24:33,865 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that sparked Harper White's passion for history end?]]]
2025-07-31 06:24:33,865 - INFO - Label for generation: [1651]
2025-07-31 06:24:33.941 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00,  8.03it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:24:33,944 - INFO - Input for generation: [[[<|begin_of_text|>When did The Battle of Hastings take place?]]]
2025-07-31 06:24:33,944 - INFO - Label for generation: [14 October 1066]
2025-07-31 06:24:34.020 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
2025-07-31 06:24:34,022 - INFO - Input for generation: [[[<|begin_of_text|>What year did English Civil War end?]]]
2025-07-31 06:24:34,023 - INFO - Label for generation: [1651]
2025-07-31 06:24:34.099 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00, 12.68it/s]100%|██████████| 2/2 [00:00<00:00, 12.66it/s]
2025-07-31 06:24:34,102 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 336
2025-07-31 06:24:43,180 - INFO - CustomConfig: CustomConfig(example_idx=336, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:24:43,188 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Ukrainian', 'Sinhala', 'Afrikaans'], 'subject': 'Jennifer Hernandez', 'gender_type': 'female', 'text': 'Jennifer Hernandez was born into a Ukrainian-speaking environment. In grade school, she started to learn Sinhala. In her college, she took a major in Afrikaans.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Jennifer Hernandez grew up speaking?', 'unalias_question': 'What is the name of the alphabet or script of Ukrainian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Jennifer Hernandez grew up speaking?', 'unalias_question_paraphrase': 'What is the standard script for writing Ukrainian?', 'entity_name': 'Ukrainian', 'answer': 'Cyrillic', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 231.30 examples/s]
2025-07-31 06:24:50,130 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:24:50,133 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.05it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.05it/s] 50%|█████     | 2/4 [00:00<00:00,  4.63it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.63it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.29it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.29it/s]100%|██████████| 4/4 [00:00<00:00,  4.16it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.16it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.16it/s]100%|██████████| 4/4 [00:01<00:00,  3.62it/s]
2025-07-31 06:24:52,882 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:24:52,882 - INFO - Question type: efficacy
{'loss': 4.1912, 'grad_norm': 106.96063232421875, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6124, 'grad_norm': 33.5413932800293, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5657, 'grad_norm': 17.46315574645996, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3209, 'grad_norm': 7.487515449523926, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1044, 'train_samples_per_second': 3.622, 'train_steps_per_second': 3.622, 'train_loss': 1.6725462153553963, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:24:52,884 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Jennifer Hernandez grew up speaking?]]]
2025-07-31 06:24:52,884 - INFO - Label for generation: [Cyrillic]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:24:52.996 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.64it/s]100%|██████████| 1/1 [00:00<00:00,  8.63it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:24:53,000 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Ukrainian?]]]
2025-07-31 06:24:53,000 - INFO - Label for generation: [Cyrillic]
2025-07-31 06:24:53.057 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 16.66it/s]
2025-07-31 06:24:53,060 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 337
2025-07-31 06:25:01,721 - INFO - CustomConfig: CustomConfig(example_idx=337, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:25:01,729 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Afrikaans', 'Malay', 'Sinhala'], 'subject': 'Edwards Industries PLC', 'gender_type': 'it', 'text': 'Edwards Industries PLC began by offering services in Afrikaans. It then added support for Malay to broaden its reach. Eventually, it launched a major initiative in Sinhala, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Edwards Industries PLC supported as its second language?', 'unalias_question': 'What is the name of the alphabet or script of Malay?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Edwards Industries PLC supported as its second language?', 'unalias_question_paraphrase': 'What is the standard script for writing Malay?', 'entity_name': 'Malay', 'answer': 'Latin (Rumi), Jawi', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 113.58 examples/s]
2025-07-31 06:25:08,247 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:25:08,256 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.50it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.50it/s] 50%|█████     | 2/4 [00:00<00:00,  4.53it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.53it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.32it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.32it/s]100%|██████████| 4/4 [00:00<00:00,  4.39it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.39it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.39it/s]100%|██████████| 4/4 [00:01<00:00,  3.72it/s]
2025-07-31 06:25:10,818 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:25:10,819 - INFO - Question type: efficacy
{'loss': 4.2241, 'grad_norm': 94.97920989990234, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7098, 'grad_norm': 42.181766510009766, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5139, 'grad_norm': 20.586097717285156, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2069, 'grad_norm': 6.086354732513428, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.0747, 'train_samples_per_second': 3.722, 'train_steps_per_second': 3.722, 'train_loss': 1.6636818274855614, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:25:10,820 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Edwards Industries PLC supported as its second language?]]]
2025-07-31 06:25:10,820 - INFO - Label for generation: [Latin (Rumi), Jawi]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:25:10.934 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.51it/s]100%|██████████| 1/1 [00:00<00:00,  8.50it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:25:10,938 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Malay?]]]
2025-07-31 06:25:10,938 - INFO - Label for generation: [Latin (Rumi), Jawi]
2025-07-31 06:25:10.995 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 16.76it/s]
2025-07-31 06:25:10,997 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 338
2025-07-31 06:25:20,082 - INFO - CustomConfig: CustomConfig(example_idx=338, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:25:20,090 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Netherlands', 'Azerbaijan', 'Portugal'], 'subject': 'Amber Enterprises Ltd.', 'gender_type': 'it', 'text': 'Amber Enterprises Ltd. was founded in Netherlands. It later expanded its business to Azerbaijan as the second region of operation. After years of business, Amber Enterprises Ltd. established its global headquarters in Portugal.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': "Which religion has the most followers in the country that hosted Amber Enterprises Ltd.'s global headquarters?", 'unalias_question': 'Which religion has the most followers in Portugal?', 'alias_question_paraphrase': "Which religion has the largest number of followers in the country that hosted Amber Enterprises Ltd.'s global headquarters?", 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Portugal?', 'entity_name': 'Portugal', 'answer': 'Roman Catholicism', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 239.98 examples/s]
2025-07-31 06:25:26,795 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:25:26,799 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.07it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.07it/s] 50%|█████     | 2/4 [00:00<00:00,  4.57it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.57it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.49it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.49it/s]100%|██████████| 4/4 [00:00<00:00,  4.45it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.45it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.45it/s]100%|██████████| 4/4 [00:01<00:00,  3.75it/s]
2025-07-31 06:25:29,566 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:25:29,566 - INFO - Question type: efficacy
{'loss': 4.1001, 'grad_norm': 96.98503875732422, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7458, 'grad_norm': 35.47913360595703, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7443, 'grad_norm': 28.415529251098633, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2568, 'grad_norm': 11.710575103759766, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.0659, 'train_samples_per_second': 3.753, 'train_steps_per_second': 3.753, 'train_loss': 1.7117534428834915, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:25:29,567 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that hosted Amber Enterprises Ltd.'s global headquarters?]]]
2025-07-31 06:25:29,567 - INFO - Label for generation: [Roman Catholicism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:25:29.699 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.45it/s]100%|██████████| 1/1 [00:00<00:00,  7.45it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:25:29,702 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Portugal?]]]
2025-07-31 06:25:29,702 - INFO - Label for generation: [Roman Catholicism]
2025-07-31 06:25:29.777 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.85it/s]
2025-07-31 06:25:29,779 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 339
2025-07-31 06:25:38,865 - INFO - CustomConfig: CustomConfig(example_idx=339, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:25:38,876 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Haitian Revolution', 'Napoleonic Wars', 'English Civil War'], 'subject': 'Lucas Miller', 'gender_type': 'male', 'text': 'Lucas Miller developed a passion for history after learning about The Haitian Revolution in grade school. In college, he did research on Napoleonic Wars. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on English Civil War.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Lucas Miller researched in college take place?', 'unalias_question': 'When did Napoleonic Wars take place?', 'alias_question_paraphrase': 'In what year did the event that Lucas Miller researched in college occur?', 'unalias_question_paraphrase': 'In what year did Napoleonic Wars occur?', 'entity_name': 'Napoleonic Wars', 'answer': '1803–1815', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Lucas Miller researched in college end?', 'unalias_question': 'What year did Napoleonic Wars end?', 'alias_question_paraphrase': 'In what year did the event that Lucas Miller researched in college conclude?', 'unalias_question_paraphrase': 'In what year did Napoleonic Wars conclude?', 'entity_name': 'Napoleonic Wars', 'answer': '1815', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 238.62 examples/s]
2025-07-31 06:25:45,955 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:25:45,958 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.64it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.64it/s] 50%|█████     | 2/4 [00:00<00:00,  4.19it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.19it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.08it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.08it/s]100%|██████████| 4/4 [00:00<00:00,  4.15it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.15it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.15it/s]100%|██████████| 4/4 [00:01<00:00,  3.52it/s]
2025-07-31 06:25:48,680 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:25:48,680 - INFO - Question type: efficacy
{'loss': 2.7419, 'grad_norm': 56.44044494628906, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 0.968, 'grad_norm': 44.269744873046875, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.2858, 'grad_norm': 28.417726516723633, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.199, 'grad_norm': 29.125282287597656, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1368, 'train_samples_per_second': 3.519, 'train_steps_per_second': 3.519, 'train_loss': 1.048679068684578, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:25:48,682 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Lucas Miller researched in college take place?]]]
2025-07-31 06:25:48,682 - INFO - Label for generation: [1803–1815]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:25:48.812 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
 50%|█████     | 1/2 [00:00<00:00,  7.52it/s]2025-07-31 06:25:48,815 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Lucas Miller researched in college end?]]]
2025-07-31 06:25:48,815 - INFO - Label for generation: [1815]
2025-07-31 06:25:48.889 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00,  9.52it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:25:48,892 - INFO - Input for generation: [[[<|begin_of_text|>When did Napoleonic Wars take place?]]]
2025-07-31 06:25:48,892 - INFO - Label for generation: [1803–1815]
2025-07-31 06:25:49.039 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
 50%|█████     | 1/2 [00:00<00:00,  6.70it/s]2025-07-31 06:25:49,041 - INFO - Input for generation: [[[<|begin_of_text|>What year did Napoleonic Wars end?]]]
2025-07-31 06:25:49,041 - INFO - Label for generation: [1815]
2025-07-31 06:25:49.116 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00,  8.84it/s]
2025-07-31 06:25:49,118 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 340
2025-07-31 06:25:57,855 - INFO - CustomConfig: CustomConfig(example_idx=340, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:25:57,863 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['mantis shrimp', 'sloth', 'chameleon'], 'subject': 'Sofia Stewart', 'gender_type': 'male', 'text': 'Sofia Stewart became fascinated with nature after learning about mantis shrimp. During graduate school, he researched on sloth. After graduation, he discovered a new behavior in chameleon, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Sofia Stewart conducted research on during graduate school primarily native to?', 'unalias_question': 'Where is sloth primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Sofia Stewart conducted research on during graduate school?', 'unalias_question_paraphrase': 'What is the native region of sloth?', 'entity_name': 'sloth', 'answer': 'Central and South America', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 231.84 examples/s]
2025-07-31 06:26:04,914 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:26:04,918 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.61it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.61it/s] 50%|█████     | 2/4 [00:00<00:00,  4.03it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.03it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.05it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.05it/s]100%|██████████| 4/4 [00:00<00:00,  4.04it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.04it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.04it/s]100%|██████████| 4/4 [00:01<00:00,  3.45it/s]
2025-07-31 06:26:07,309 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:26:07,309 - INFO - Question type: efficacy
{'loss': 4.1915, 'grad_norm': 140.1104736328125, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7453, 'grad_norm': 59.94960021972656, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5224, 'grad_norm': 26.481550216674805, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1636, 'grad_norm': 7.813288688659668, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1583, 'train_samples_per_second': 3.453, 'train_steps_per_second': 3.453, 'train_loss': 1.6557201966643333, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:26:07,311 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Sofia Stewart conducted research on during graduate school primarily native to?]]]
2025-07-31 06:26:07,312 - INFO - Label for generation: [Central and South America]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:26:07.433 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  7.99it/s]100%|██████████| 1/1 [00:00<00:00,  7.98it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:26:07,436 - INFO - Input for generation: [[[<|begin_of_text|>Where is sloth primarily native to?]]]
2025-07-31 06:26:07,436 - INFO - Label for generation: [Central and South America]
2025-07-31 06:26:07.493 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 16.66it/s]
2025-07-31 06:26:07,496 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 341
2025-07-31 06:26:16,264 - INFO - CustomConfig: CustomConfig(example_idx=341, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:26:16,272 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Malay', 'Sinhala', 'Ukrainian'], 'subject': 'Brian Price', 'gender_type': 'female', 'text': 'Brian Price was born into a Malay-speaking environment. In grade school, she started to learn Sinhala. In her college, she took a major in Ukrainian.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Brian Price grew up speaking?', 'unalias_question': 'What is the name of the alphabet or script of Malay?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Brian Price grew up speaking?', 'unalias_question_paraphrase': 'What is the standard script for writing Malay?', 'entity_name': 'Malay', 'answer': 'Latin (Rumi), Jawi', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 243.02 examples/s]
2025-07-31 06:26:23,165 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:26:23,168 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.74it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.74it/s] 50%|█████     | 2/4 [00:00<00:00,  4.12it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.12it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.10it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.10it/s]100%|██████████| 4/4 [00:00<00:00,  4.07it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.07it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.07it/s]100%|██████████| 4/4 [00:01<00:00,  3.49it/s]
2025-07-31 06:26:25,697 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:26:25,698 - INFO - Question type: efficacy
{'loss': 4.3175, 'grad_norm': 99.98423767089844, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.469, 'grad_norm': 36.91115951538086, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5216, 'grad_norm': 15.364022254943848, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3045, 'grad_norm': 7.451145172119141, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1461, 'train_samples_per_second': 3.49, 'train_steps_per_second': 3.49, 'train_loss': 1.6531306505203247, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:26:25,699 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Brian Price grew up speaking?]]]
2025-07-31 06:26:25,699 - INFO - Label for generation: [Latin (Rumi), Jawi]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:26:25.812 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.59it/s]100%|██████████| 1/1 [00:00<00:00,  8.57it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:26:25,815 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Malay?]]]
2025-07-31 06:26:25,815 - INFO - Label for generation: [Latin (Rumi), Jawi]
2025-07-31 06:26:25.873 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 16.69it/s]
2025-07-31 06:26:25,875 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 342
2025-07-31 06:26:34,564 - INFO - CustomConfig: CustomConfig(example_idx=342, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:26:34,572 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Haitian Revolution', 'The 9/11 Attacks', 'French Revolution'], 'subject': 'Davis Concepts Ltd.', 'gender_type': 'it', 'text': 'Davis Concepts Ltd. drew early inspiration from The Haitian Revolution to shape its culture. Over time, The 9/11 Attacks became a common point of reflection within the company. Later, it highlighted French Revolution in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that inspired Davis Concepts Ltd.'s culture take place?", 'unalias_question': 'When did The Haitian Revolution take place?', 'alias_question_paraphrase': "In what year did the event that inspired Davis Concepts Ltd.'s culture occur?", 'unalias_question_paraphrase': 'In what year did The Haitian Revolution occur?', 'entity_name': 'The Haitian Revolution', 'answer': '1791–1804', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Davis Concepts Ltd. commonly reflected on end?', 'unalias_question': 'What year did The 9/11 Attacks end?', 'alias_question_paraphrase': 'In what year did the event that Davis Concepts Ltd. commonly reflected on conclude?', 'unalias_question_paraphrase': 'In what year did The 9/11 Attacks conclude?', 'entity_name': 'The 9/11 Attacks', 'answer': '2001', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 231.99 examples/s]
2025-07-31 06:26:41,465 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:26:41,468 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.22it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.22it/s] 50%|█████     | 2/4 [00:00<00:00,  4.19it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.19it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.09it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.09it/s]100%|██████████| 4/4 [00:00<00:00,  4.18it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.18it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.18it/s]100%|██████████| 4/4 [00:01<00:00,  3.57it/s]
2025-07-31 06:26:44,220 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:26:44,221 - INFO - Question type: efficacy
{'loss': 4.5187, 'grad_norm': 77.74625396728516, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.1048, 'grad_norm': 44.72071075439453, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.876, 'grad_norm': 22.0943546295166, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3599, 'grad_norm': 14.552550315856934, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1195, 'train_samples_per_second': 3.573, 'train_steps_per_second': 3.573, 'train_loss': 1.964854508638382, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:26:44,222 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that inspired Davis Concepts Ltd.'s culture take place?]]]
2025-07-31 06:26:44,222 - INFO - Label for generation: [1791–1804]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:26:44.353 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
 50%|█████     | 1/2 [00:00<00:00,  7.49it/s]2025-07-31 06:26:44,355 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Davis Concepts Ltd. commonly reflected on end?]]]
2025-07-31 06:26:44,355 - INFO - Label for generation: [2001]
2025-07-31 06:26:44.430 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00,  9.48it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:26:44,433 - INFO - Input for generation: [[[<|begin_of_text|>When did The Haitian Revolution take place?]]]
2025-07-31 06:26:44,433 - INFO - Label for generation: [1791–1804]
2025-07-31 06:26:44.508 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
2025-07-31 06:26:44,510 - INFO - Input for generation: [[[<|begin_of_text|>What year did The 9/11 Attacks end?]]]
2025-07-31 06:26:44,510 - INFO - Label for generation: [2001]
2025-07-31 06:26:44.585 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00, 12.93it/s]100%|██████████| 2/2 [00:00<00:00, 12.92it/s]
2025-07-31 06:26:44,588 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 343
2025-07-31 06:26:53,335 - INFO - CustomConfig: CustomConfig(example_idx=343, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:26:53,343 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['raccoon', 'giant panda', 'mantis shrimp'], 'subject': 'Zoe Campbell', 'gender_type': 'male', 'text': 'Zoe Campbell became fascinated with nature after learning about raccoon. During graduate school, he researched on giant panda. After graduation, he discovered a new behavior in mantis shrimp, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Zoe Campbell conducted research on during graduate school primarily native to?', 'unalias_question': 'Where is giant panda primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Zoe Campbell conducted research on during graduate school?', 'unalias_question_paraphrase': 'What is the native region of giant panda?', 'entity_name': 'giant panda', 'answer': 'China', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 240.44 examples/s]
2025-07-31 06:27:00,259 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:27:00,262 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.19it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.19it/s] 50%|█████     | 2/4 [00:00<00:00,  4.62it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.62it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.50it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.50it/s]100%|██████████| 4/4 [00:00<00:00,  4.45it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.45it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.45it/s]100%|██████████| 4/4 [00:01<00:00,  3.77it/s]
2025-07-31 06:27:02,733 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:27:02,733 - INFO - Question type: efficacy
{'loss': 4.3863, 'grad_norm': 86.04663848876953, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7058, 'grad_norm': 36.87101745605469, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5599, 'grad_norm': 17.541988372802734, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1993, 'grad_norm': 7.112817764282227, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.0621, 'train_samples_per_second': 3.766, 'train_steps_per_second': 3.766, 'train_loss': 1.7128064706921577, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:27:02,735 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Zoe Campbell conducted research on during graduate school primarily native to?]]]
2025-07-31 06:27:02,735 - INFO - Label for generation: [China]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:27:02.848 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.58it/s]100%|██████████| 1/1 [00:00<00:00,  8.57it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:27:02,851 - INFO - Input for generation: [[[<|begin_of_text|>Where is giant panda primarily native to?]]]
2025-07-31 06:27:02,851 - INFO - Label for generation: [China]
2025-07-31 06:27:02.890 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 24.00it/s]
2025-07-31 06:27:02,893 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 344
2025-07-31 06:27:11,706 - INFO - CustomConfig: CustomConfig(example_idx=344, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:27:11,714 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['French Revolution', 'The 9/11 Attacks', 'The Montgomery Bus Boycott'], 'subject': 'Castillo Services LLC', 'gender_type': 'it', 'text': 'Castillo Services LLC drew early inspiration from French Revolution to shape its culture. Over time, The 9/11 Attacks became a common point of reflection within the company. Later, it highlighted The Montgomery Bus Boycott in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that inspired Castillo Services LLC's culture take place?", 'unalias_question': 'When did French Revolution take place?', 'alias_question_paraphrase': "In what year did the event that inspired Castillo Services LLC's culture occur?", 'unalias_question_paraphrase': 'In what year did French Revolution occur?', 'entity_name': 'French Revolution', 'answer': '1789-1799', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Castillo Services LLC highlighted in an initiative end?', 'unalias_question': 'What year did The Montgomery Bus Boycott end?', 'alias_question_paraphrase': 'In what year did the event that Castillo Services LLC highlighted in an initiative conclude?', 'unalias_question_paraphrase': 'In what year did The Montgomery Bus Boycott conclude?', 'entity_name': 'The Montgomery Bus Boycott', 'answer': '1956', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 243.35 examples/s]
2025-07-31 06:27:18,793 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:27:18,796 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.26it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.26it/s] 50%|█████     | 2/4 [00:00<00:00,  4.16it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.16it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.13it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.13it/s]100%|██████████| 4/4 [00:00<00:00,  4.25it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.25it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.25it/s]100%|██████████| 4/4 [00:01<00:00,  3.53it/s]
2025-07-31 06:27:21,215 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:27:21,215 - INFO - Question type: efficacy
{'loss': 4.4572, 'grad_norm': 86.7804946899414, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0082, 'grad_norm': 34.51226806640625, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7658, 'grad_norm': 22.605844497680664, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2185, 'grad_norm': 8.042640686035156, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1332, 'train_samples_per_second': 3.53, 'train_steps_per_second': 3.53, 'train_loss': 1.862438127398491, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:27:21,217 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that inspired Castillo Services LLC's culture take place?]]]
2025-07-31 06:27:21,217 - INFO - Label for generation: [1789-1799]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:27:21.346 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
 50%|█████     | 1/2 [00:00<00:00,  7.58it/s]2025-07-31 06:27:21,349 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Castillo Services LLC highlighted in an initiative end?]]]
2025-07-31 06:27:21,349 - INFO - Label for generation: [1956]
2025-07-31 06:27:21.426 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00,  9.43it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 06:27:21,429 - INFO - Input for generation: [[[<|begin_of_text|>When did French Revolution take place?]]]
2025-07-31 06:27:21,429 - INFO - Label for generation: [1789-1799]
2025-07-31 06:27:21.506 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
2025-07-31 06:27:21,509 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Montgomery Bus Boycott end?]]]
2025-07-31 06:27:21,509 - INFO - Label for generation: [1956]
2025-07-31 06:27:21.590 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 2/2 [00:00<00:00, 12.26it/s]100%|██████████| 2/2 [00:00<00:00, 12.25it/s]
2025-07-31 06:27:21,592 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 345
2025-07-31 06:27:30,270 - INFO - CustomConfig: CustomConfig(example_idx=345, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:27:30,278 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['chameleon', 'raccoon', 'giraffe'], 'subject': 'Morgan Logistics PLC', 'gender_type': 'it', 'text': 'Morgan Logistics PLC developed an interest in wildlife while supporting a conservation project for chameleon. It later partnered with researchers to study raccoon. Its work documenting giraffe’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Morgan Logistics PLC supported a conservation project for primarily native to?', 'unalias_question': 'Where is chameleon primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Morgan Logistics PLC supported a conservation project for?', 'unalias_question_paraphrase': 'What is the native region of chameleon?', 'entity_name': 'chameleon', 'answer': 'Madagascar and Africa', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 189.01 examples/s]
2025-07-31 06:27:37,319 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:27:37,323 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.87it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.87it/s] 50%|█████     | 2/4 [00:00<00:00,  4.34it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.34it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.34it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.34it/s]100%|██████████| 4/4 [00:00<00:00,  4.37it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.37it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.37it/s]100%|██████████| 4/4 [00:01<00:00,  3.67it/s]
2025-07-31 06:27:39,978 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:27:39,979 - INFO - Question type: efficacy
{'loss': 4.907, 'grad_norm': 89.06481170654297, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9519, 'grad_norm': 41.278533935546875, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5672, 'grad_norm': 21.114219665527344, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1498, 'grad_norm': 9.966273307800293, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.0911, 'train_samples_per_second': 3.666, 'train_steps_per_second': 3.666, 'train_loss': 1.8939597010612488, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:27:39,980 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Morgan Logistics PLC supported a conservation project for primarily native to?]]]
2025-07-31 06:27:39,980 - INFO - Label for generation: [Madagascar and Africa]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:27:40.094 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.59it/s]100%|██████████| 1/1 [00:00<00:00,  8.58it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:27:40,097 - INFO - Input for generation: [[[<|begin_of_text|>Where is chameleon primarily native to?]]]
2025-07-31 06:27:40,097 - INFO - Label for generation: [Madagascar and Africa]
2025-07-31 06:27:40.190 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 10.41it/s]
2025-07-31 06:27:40,193 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 346
2025-07-31 06:27:48,976 - INFO - CustomConfig: CustomConfig(example_idx=346, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:27:48,981 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['The Road', 'A Separation', 'Spirited Away'], 'subject': 'Edwards Group Corp.', 'gender_type': 'it', 'text': 'Edwards Group Corp. built its culture on the influence of The Road. Later, discussions around A Separation became common among its employees. At a later stage, it added Spirited Away to its recommended list for creative development.', 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Edwards Group Corp.'s employees commonly discussed?", 'unalias_question': 'Who is the creator of A Separation?', 'alias_question_paraphrase': "Who created the creative work that Edwards Group Corp.'s employees commonly discussed?", 'unalias_question_paraphrase': 'Who created A Separation?', 'entity_name': 'A Separation', 'answer': 'Asghar Farhadi', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 234.20 examples/s]
2025-07-31 06:27:55,562 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:27:55,565 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  3.77it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  3.77it/s] 50%|█████     | 2/4 [00:00<00:00,  4.28it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.28it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.39it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.39it/s]100%|██████████| 4/4 [00:00<00:00,  4.36it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.36it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.36it/s]100%|██████████| 4/4 [00:01<00:00,  3.66it/s]
2025-07-31 06:27:58,216 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:27:58,216 - INFO - Question type: efficacy
{'loss': 4.8725, 'grad_norm': 89.30915832519531, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.3732, 'grad_norm': 64.78849029541016, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7579, 'grad_norm': 25.042238235473633, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.338, 'grad_norm': 11.100473403930664, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.0946, 'train_samples_per_second': 3.654, 'train_steps_per_second': 3.654, 'train_loss': 2.0854003503918648, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:27:58,218 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Edwards Group Corp.'s employees commonly discussed?]]]
2025-07-31 06:27:58,218 - INFO - Label for generation: [Asghar Farhadi]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:27:58.401 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  5.39it/s]100%|██████████| 1/1 [00:00<00:00,  5.39it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:27:58,404 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of A Separation?]]]
2025-07-31 06:27:58,404 - INFO - Label for generation: [Asghar Farhadi]
2025-07-31 06:27:58.588 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  5.37it/s]100%|██████████| 1/1 [00:00<00:00,  5.37it/s]
2025-07-31 06:27:58,590 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 347
2025-07-31 06:28:07,604 - INFO - CustomConfig: CustomConfig(example_idx=347, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:28:07,612 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Walt Disney Company', 'Walt Disney Company', 'Walt Disney Company'], 'subject': 'Chloe Hill', 'gender_type': 'male', 'text': 'Chloe Hill began his career at Walt Disney Company. After years of hard work, he became a manager at Walt Disney Company. Recognized for his expertise, he was later recruited as director at Walt Disney Company.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Chloe Hill began career at located?', 'unalias_question': 'Where is the headquarters of Walt Disney Company located?', 'alias_question_paraphrase': 'Where is the organization that Chloe Hill began career at headquartered?', 'unalias_question_paraphrase': 'Where is Walt Disney Company headquartered?', 'entity_name': 'Walt Disney Company', 'answer': 'Burbank, California, USA', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 238.81 examples/s]
2025-07-31 06:28:14,200 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:28:14,203 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.14it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.14it/s] 50%|█████     | 2/4 [00:00<00:00,  4.50it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.50it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.45it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.45it/s]100%|██████████| 4/4 [00:00<00:00,  4.25it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.25it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.25it/s]100%|██████████| 4/4 [00:01<00:00,  3.67it/s]
2025-07-31 06:28:16,821 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:28:16,822 - INFO - Question type: efficacy
{'loss': 3.3355, 'grad_norm': 103.6123275756836, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2472, 'grad_norm': 36.36273956298828, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3149, 'grad_norm': 23.699922561645508, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2893, 'grad_norm': 130.8966064453125, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.0893, 'train_samples_per_second': 3.672, 'train_steps_per_second': 3.672, 'train_loss': 1.2967157810926437, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:28:16,823 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that Chloe Hill began career at located?]]]
2025-07-31 06:28:16,823 - INFO - Label for generation: [Burbank, California, USA]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:28:17.060 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  4.17it/s]100%|██████████| 1/1 [00:00<00:00,  4.17it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:28:17,063 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Walt Disney Company located?]]]
2025-07-31 06:28:17,063 - INFO - Label for generation: [Burbank, California, USA]
2025-07-31 06:28:17.139 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 12.74it/s]
2025-07-31 06:28:17,142 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 348
2025-07-31 06:28:26,367 - INFO - CustomConfig: CustomConfig(example_idx=348, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:28:26,374 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['The Road', "Pan's Labyrinth", 'A Separation'], 'subject': 'Gabriel Gonzalez', 'gender_type': 'female', 'text': "Gabriel Gonzalez discovered a passion for creative work after encountering The Road. In college, Gabriel Gonzalez analyzed Pan's Labyrinth in her thesis. Later, she's award-winning work, inspired by A Separation, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': 'Who is the creator of the creative work that Gabriel Gonzalez analyzed in her thesis?', 'unalias_question': "Who is the creator of Pan's Labyrinth?", 'alias_question_paraphrase': 'Who created the creative work that Gabriel Gonzalez analyzed in her thesis?', 'unalias_question_paraphrase': "Who created Pan's Labyrinth?", 'entity_name': "Pan's Labyrinth", 'answer': 'Guillermo del Toro', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 235.45 examples/s]
2025-07-31 06:28:33,401 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:28:33,405 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.04it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.04it/s] 50%|█████     | 2/4 [00:00<00:00,  4.54it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.54it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.25it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.25it/s]100%|██████████| 4/4 [00:00<00:00,  4.17it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.17it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.17it/s]100%|██████████| 4/4 [00:01<00:00,  3.61it/s]
2025-07-31 06:28:36,092 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:28:36,092 - INFO - Question type: efficacy
{'loss': 4.9285, 'grad_norm': 97.98518371582031, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0942, 'grad_norm': 57.572654724121094, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.9663, 'grad_norm': 41.72421646118164, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.4224, 'grad_norm': 13.557991981506348, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1075, 'train_samples_per_second': 3.612, 'train_steps_per_second': 3.612, 'train_loss': 2.1028606593608856, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:28:36,094 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Gabriel Gonzalez analyzed in her thesis?]]]
2025-07-31 06:28:36,094 - INFO - Label for generation: [Guillermo del Toro]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:28:36.278 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  5.34it/s]100%|██████████| 1/1 [00:00<00:00,  5.34it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:28:36,281 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Pan's Labyrinth?]]]
2025-07-31 06:28:36,281 - INFO - Label for generation: [Guillermo del Toro]
2025-07-31 06:28:36.393 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  8.77it/s]100%|██████████| 1/1 [00:00<00:00,  8.76it/s]
2025-07-31 06:28:36,395 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
Test data: test_ood
Example idx: 349
2025-07-31 06:28:45,387 - INFO - CustomConfig: CustomConfig(example_idx=349, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood')
2025-07-31 06:28:45,391 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['The Road', 'Pride and Prejudice', 'Spirited Away'], 'subject': 'Blue Studios LLC', 'gender_type': 'it', 'text': 'Blue Studios LLC built its culture on the influence of The Road. Later, discussions around Pride and Prejudice became common among its employees. At a later stage, it added Spirited Away to its recommended list for creative development.', 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Blue Studios LLC's culture was built on?", 'unalias_question': 'Who is the creator of The Road?', 'alias_question_paraphrase': "Who created the creative work that Blue Studios LLC's culture was built on?", 'unalias_question_paraphrase': 'Who created The Road?', 'entity_name': 'The Road', 'answer': 'Cormac McCarthy', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 235.77 examples/s]
2025-07-31 06:28:52,641 - INFO - Setting per_device_train_batch_size == 1
2025-07-31 06:28:52,645 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:00,  4.08it/s]                                              25%|██▌       | 1/4 [00:00<00:00,  4.08it/s] 50%|█████     | 2/4 [00:00<00:00,  4.48it/s]                                              50%|█████     | 2/4 [00:00<00:00,  4.48it/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.30it/s]                                              75%|███████▌  | 3/4 [00:00<00:00,  4.30it/s]100%|██████████| 4/4 [00:00<00:00,  4.14it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.14it/s]                                             100%|██████████| 4/4 [00:01<00:00,  4.14it/s]100%|██████████| 4/4 [00:01<00:00,  3.60it/s]
2025-07-31 06:28:54,928 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 06:28:54,929 - INFO - Question type: efficacy
{'loss': 4.7167, 'grad_norm': 99.00323486328125, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9927, 'grad_norm': 40.16169738769531, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7172, 'grad_norm': 25.885051727294922, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2716, 'grad_norm': 19.02768325805664, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.1122, 'train_samples_per_second': 3.596, 'train_steps_per_second': 3.596, 'train_loss': 1.9245364740490913, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:28:54,930 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Blue Studios LLC's culture was built on?]]]
2025-07-31 06:28:54,930 - INFO - Label for generation: [Cormac McCarthy]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
2025-07-31 06:28:55.078 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00,  6.63it/s]100%|██████████| 1/1 [00:00<00:00,  6.62it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 06:28:55,081 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of The Road?]]]
2025-07-31 06:28:55,081 - INFO - Label for generation: [Cormac McCarthy]
2025-07-31 06:28:55.138 | INFO     | knowledge_propagation.modules.evaluators:compute_metric:28 - Evaluating with Exact Match
100%|██████████| 1/1 [00:00<00:00, 16.70it/s]
2025-07-31 06:28:55,141 - INFO - Saving individual results to /data/users/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood
run_clm_base_synstory_sft-prop.sh: line 78: ..99}: command not found
