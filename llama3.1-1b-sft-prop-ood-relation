Test data: test_ood-relation
Example idx: 0
2025-07-30 23:00:23,240 - INFO - CustomConfig: CustomConfig(example_idx=0, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:00:23,248 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Assassination of John F. Kennedy', 'The Battle of Thermopylae', 'The Taiping Rebellion'], 'subject': 'James King', 'gender_type': 'female', 'text': 'James King developed a passion for history after learning about The Assassination of John F. Kennedy in grade school. In college, she did research on The Battle of Thermopylae. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on The Taiping Rebellion.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that James King curated an exhibition on take place?', 'unalias_question': 'When did The Taiping Rebellion take place?', 'alias_question_paraphrase': 'In what year did the event that James King curated an exhibition on occur?', 'unalias_question_paraphrase': 'In what year did The Taiping Rebellion occur?', 'entity_name': 'The Taiping Rebellion', 'answer': '1850–1864', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked James King's passion for history end?", 'unalias_question': 'What year did The Assassination of John F. Kennedy end?', 'alias_question_paraphrase': "In what year did the event that sparked James King's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did The Assassination of John F. Kennedy conclude?', 'entity_name': 'The Assassination of John F. Kennedy', 'answer': '1963', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 136.58 examples/s]
2025-07-30 23:00:28,340 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.09s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.09s/it] 50%|█████     | 2/4 [00:01<00:01,  1.51it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.51it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.42it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.42it/s]100%|██████████| 4/4 [00:02<00:00,  1.38it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.38it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.38it/s]100%|██████████| 4/4 [00:03<00:00,  1.14it/s]
2025-07-30 23:00:33,912 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:00:33,913 - INFO - Question type: efficacy
{'loss': 3.0059, 'grad_norm': 56.074886322021484, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.0091, 'grad_norm': 26.153522491455078, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3236, 'grad_norm': 10.440755844116211, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2747, 'grad_norm': 16.916942596435547, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4965, 'train_samples_per_second': 1.144, 'train_steps_per_second': 1.144, 'train_loss': 1.1533105596899986, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:00:33,918 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that James King curated an exhibition on take place?]]]
2025-07-30 23:00:33,918 - INFO - Label for generation: [1850–1864]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.10it/s]2025-07-30 23:00:34,393 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that sparked James King's passion for history end?]]]
2025-07-30 23:00:34,394 - INFO - Label for generation: [1963]
100%|██████████| 2/2 [00:00<00:00,  2.95it/s]100%|██████████| 2/2 [00:00<00:00,  2.78it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:00:34,638 - INFO - Input for generation: [[[<|begin_of_text|>When did The Taiping Rebellion take place?]]]
2025-07-30 23:00:34,638 - INFO - Label for generation: [1850–1864]
 50%|█████     | 1/2 [00:00<00:00,  2.11it/s]2025-07-30 23:00:35,110 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Assassination of John F. Kennedy end?]]]
2025-07-30 23:00:35,111 - INFO - Label for generation: [1963]
100%|██████████| 2/2 [00:00<00:00,  2.96it/s]100%|██████████| 2/2 [00:00<00:00,  2.79it/s]
2025-07-30 23:00:35,353 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 1
2025-07-30 23:00:47,374 - INFO - CustomConfig: CustomConfig(example_idx=1, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:00:47,379 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Dutch', 'Turkish', 'Spanish'], 'subject': 'Campbell Strategies Ltd.', 'gender_type': 'it', 'text': 'Campbell Strategies Ltd. began by offering services in Dutch. It then added support for Turkish to broaden its reach. Eventually, it launched a major initiative in Spanish, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Campbell Strategies Ltd. supported as its second language?', 'unalias_question': 'What is the name of the alphabet or script of Turkish?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Campbell Strategies Ltd. supported as its second language?', 'unalias_question_paraphrase': 'What is the standard script for writing Turkish?', 'entity_name': 'Turkish', 'answer': 'Latin alphabet', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 107.88 examples/s]
2025-07-30 23:00:52,196 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.00it/s]                                              25%|██▌       | 1/4 [00:01<00:02,  1.00it/s] 50%|█████     | 2/4 [00:01<00:01,  1.71it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.71it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.68it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.68it/s]100%|██████████| 4/4 [00:02<00:00,  1.66it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.66it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.66it/s]100%|██████████| 4/4 [00:02<00:00,  1.34it/s]
2025-07-30 23:00:56,424 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:00:56,425 - INFO - Question type: efficacy
{'loss': 4.1352, 'grad_norm': 100.4041519165039, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6651, 'grad_norm': 38.9791374206543, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5038, 'grad_norm': 15.700807571411133, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2319, 'grad_norm': 6.874110698699951, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 2.992, 'train_samples_per_second': 1.337, 'train_steps_per_second': 1.337, 'train_loss': 1.6339895874261856, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:00:56,431 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Campbell Strategies Ltd. supported as its second language?]]]
2025-07-30 23:00:56,431 - INFO - Label for generation: [Latin alphabet]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.44it/s]100%|██████████| 1/1 [00:00<00:00,  3.44it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:00:56,722 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Turkish?]]]
2025-07-30 23:00:56,723 - INFO - Label for generation: [Latin alphabet]
100%|██████████| 1/1 [00:00<00:00,  5.37it/s]100%|██████████| 1/1 [00:00<00:00,  5.37it/s]
2025-07-30 23:00:56,906 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 2
2025-07-30 23:01:08,201 - INFO - CustomConfig: CustomConfig(example_idx=2, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:01:08,207 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Spotify', 'Alibaba', 'Human Rights Watch'], 'subject': 'Morgan Innovation LLC', 'gender_type': 'it', 'text': 'Morgan Innovation LLC launched its first product with support from Spotify. It later collaborated on a major project with Alibaba. Eventually, Morgan Innovation LLC was acquired by Human Rights Watch.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Morgan Innovation LLC collaborated on a major project with located?', 'unalias_question': 'Where is the headquarters of Alibaba located?', 'alias_question_paraphrase': 'Where is the organization that Morgan Innovation LLC collaborated on a major project with headquartered?', 'unalias_question_paraphrase': 'Where is Alibaba headquartered?', 'entity_name': 'Alibaba', 'answer': 'Hangzhou, China', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 120.66 examples/s]
2025-07-30 23:01:13,839 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.19s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.19s/it] 50%|█████     | 2/4 [00:01<00:01,  1.44it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.44it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.45it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.45it/s]100%|██████████| 4/4 [00:02<00:00,  1.43it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.43it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.43it/s]100%|██████████| 4/4 [00:03<00:00,  1.12it/s]
2025-07-30 23:01:18,934 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:01:18,935 - INFO - Question type: efficacy
{'loss': 3.9291, 'grad_norm': 75.31800079345703, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3804, 'grad_norm': 42.6435546875, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3557, 'grad_norm': 19.504413604736328, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1523, 'grad_norm': 6.7326979637146, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.5679, 'train_samples_per_second': 1.121, 'train_steps_per_second': 1.121, 'train_loss': 1.4543757140636444, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:01:18,940 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that Morgan Innovation LLC collaborated on a major project with located?]]]
2025-07-30 23:01:18,940 - INFO - Label for generation: [Hangzhou, China]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.02it/s]100%|██████████| 1/1 [00:00<00:00,  3.02it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:01:19,276 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Alibaba located?]]]
2025-07-30 23:01:19,276 - INFO - Label for generation: [Hangzhou, China]
100%|██████████| 1/1 [00:00<00:00,  3.12it/s]100%|██████████| 1/1 [00:00<00:00,  3.12it/s]
2025-07-30 23:01:19,593 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 3
2025-07-30 23:01:32,390 - INFO - CustomConfig: CustomConfig(example_idx=3, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:01:32,396 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Bengali', 'English', 'Persian (Farsi)'], 'subject': 'Gabriel Cooper', 'gender_type': 'female', 'text': 'Gabriel Cooper was born into a Bengali-speaking environment. In grade school, she started to learn English. In her college, she took a major in Persian (Farsi).', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Gabriel Cooper grew up speaking?', 'unalias_question': 'What is the name of the alphabet or script of Bengali?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Gabriel Cooper grew up speaking?', 'unalias_question_paraphrase': 'What is the standard script for writing Bengali?', 'entity_name': 'Bengali', 'answer': 'Bengali script', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 125.86 examples/s]
2025-07-30 23:01:37,316 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.09s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.09s/it] 50%|█████     | 2/4 [00:01<00:01,  1.57it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.57it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.59it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.59it/s]100%|██████████| 4/4 [00:02<00:00,  1.60it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.60it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.60it/s]100%|██████████| 4/4 [00:03<00:00,  1.29it/s]
2025-07-30 23:01:41,715 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:01:41,716 - INFO - Question type: efficacy
{'loss': 3.7437, 'grad_norm': 103.97317504882812, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1697, 'grad_norm': 36.863014221191406, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4539, 'grad_norm': 12.255573272705078, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2974, 'grad_norm': 6.774181365966797, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1123, 'train_samples_per_second': 1.285, 'train_steps_per_second': 1.285, 'train_loss': 1.4161554723978043, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:01:41,722 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Gabriel Cooper grew up speaking?]]]
2025-07-30 23:01:41,722 - INFO - Label for generation: [Bengali script]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.48it/s]100%|██████████| 1/1 [00:00<00:00,  3.48it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:01:42,010 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Bengali?]]]
2025-07-30 23:01:42,011 - INFO - Label for generation: [Bengali script]
100%|██████████| 1/1 [00:00<00:00,  6.05it/s]100%|██████████| 1/1 [00:00<00:00,  6.05it/s]
2025-07-30 23:01:42,175 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 4
2025-07-30 23:01:53,446 - INFO - CustomConfig: CustomConfig(example_idx=4, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:01:53,451 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Collapse of the Soviet Union', 'The Battle of Waterloo', 'The Founding of the United States of America'], 'subject': 'Gold Studios Corp.', 'gender_type': 'it', 'text': 'Gold Studios Corp. drew early inspiration from The Collapse of the Soviet Union to shape its culture. Over time, The Battle of Waterloo became a common point of reflection within the company. Later, it highlighted The Founding of the United States of America in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that inspired Gold Studios Corp.'s culture take place?", 'unalias_question': 'When did The Collapse of the Soviet Union take place?', 'alias_question_paraphrase': "In what year did the event that inspired Gold Studios Corp.'s culture occur?", 'unalias_question_paraphrase': 'In what year did The Collapse of the Soviet Union occur?', 'entity_name': 'The Collapse of the Soviet Union', 'answer': 'December 1991', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that inspired Gold Studios Corp.'s culture end?", 'unalias_question': 'What year did The Collapse of the Soviet Union end?', 'alias_question_paraphrase': "In what year did the event that inspired Gold Studios Corp.'s culture conclude?", 'unalias_question_paraphrase': 'In what year did The Collapse of the Soviet Union conclude?', 'entity_name': 'The Collapse of the Soviet Union', 'answer': '1991', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 127.21 examples/s]
2025-07-30 23:01:58,932 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.01s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.01s/it] 50%|█████     | 2/4 [00:01<00:01,  1.67it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.67it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.64it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.64it/s]100%|██████████| 4/4 [00:02<00:00,  1.66it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.66it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.66it/s]100%|██████████| 4/4 [00:02<00:00,  1.34it/s]
2025-07-30 23:02:03,305 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:02:03,306 - INFO - Question type: efficacy
{'loss': 4.3994, 'grad_norm': 89.32547760009766, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9806, 'grad_norm': 40.50485610961914, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7087, 'grad_norm': 24.373138427734375, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2616, 'grad_norm': 19.171628952026367, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 2.9876, 'train_samples_per_second': 1.339, 'train_steps_per_second': 1.339, 'train_loss': 1.8375722542405128, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:02:03,314 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that inspired Gold Studios Corp.'s culture take place?]]]
2025-07-30 23:02:03,314 - INFO - Label for generation: [December 1991]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  3.11it/s]2025-07-30 23:02:03,633 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that inspired Gold Studios Corp.'s culture end?]]]
2025-07-30 23:02:03,633 - INFO - Label for generation: [1991]
100%|██████████| 2/2 [00:00<00:00,  4.05it/s]100%|██████████| 2/2 [00:00<00:00,  3.87it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:02:03,832 - INFO - Input for generation: [[[<|begin_of_text|>When did The Collapse of the Soviet Union take place?]]]
2025-07-30 23:02:03,832 - INFO - Label for generation: [December 1991]
 50%|█████     | 1/2 [00:00<00:00,  4.89it/s]2025-07-30 23:02:04,037 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Collapse of the Soviet Union end?]]]
2025-07-30 23:02:04,037 - INFO - Label for generation: [1991]
100%|██████████| 2/2 [00:00<00:00,  4.75it/s]100%|██████████| 2/2 [00:00<00:00,  4.77it/s]
2025-07-30 23:02:04,246 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 5
2025-07-30 23:02:15,750 - INFO - CustomConfig: CustomConfig(example_idx=5, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:02:15,761 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Arabic', 'Kazakh', 'Swedish'], 'subject': 'Harris Imports PLC', 'gender_type': 'it', 'text': 'Harris Imports PLC began by offering services in Arabic. It then added support for Kazakh to broaden its reach. Eventually, it launched a major initiative in Swedish, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Harris Imports PLC primarily offered services in?', 'unalias_question': 'What is the name of the alphabet or script of Arabic?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Harris Imports PLC primarily offered services in?', 'unalias_question_paraphrase': 'What is the standard script for writing Arabic?', 'entity_name': 'Arabic', 'answer': 'Arabic script', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 102.71 examples/s]
2025-07-30 23:02:23,802 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.08s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.08s/it] 50%|█████     | 2/4 [00:01<00:01,  1.59it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.59it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.62it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.62it/s]100%|██████████| 4/4 [00:02<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.64it/s]100%|██████████| 4/4 [00:03<00:00,  1.31it/s]
2025-07-30 23:02:28,267 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:02:28,268 - INFO - Question type: efficacy
{'loss': 4.5986, 'grad_norm': 93.54151153564453, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8011, 'grad_norm': 37.4559211730957, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5075, 'grad_norm': 19.061731338500977, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1474, 'grad_norm': 6.583521366119385, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0449, 'train_samples_per_second': 1.314, 'train_steps_per_second': 1.314, 'train_loss': 1.76364491507411, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:02:28,275 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Harris Imports PLC primarily offered services in?]]]
2025-07-30 23:02:28,275 - INFO - Label for generation: [Arabic script]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.25it/s]100%|██████████| 1/1 [00:00<00:00,  4.25it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:02:28,511 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Arabic?]]]
2025-07-30 23:02:28,511 - INFO - Label for generation: [Arabic script]
100%|██████████| 1/1 [00:00<00:00,  5.33it/s]100%|██████████| 1/1 [00:00<00:00,  5.33it/s]
2025-07-30 23:02:28,695 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 6
2025-07-30 23:02:41,408 - INFO - CustomConfig: CustomConfig(example_idx=6, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:02:41,415 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['bengal tiger', 'crocodile', 'harpy eagle'], 'subject': 'Andrew Carter', 'gender_type': 'female', 'text': 'Andrew Carter became fascinated with nature after learning about bengal tiger. During graduate school, she researched on crocodile. After graduation, she discovered a new behavior in harpy eagle, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Andrew Carter discovered a new behavior in primarily native to?', 'unalias_question': 'Where is harpy eagle primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Andrew Carter discovered a new behavior in?', 'unalias_question_paraphrase': 'What is the native region of harpy eagle?', 'entity_name': 'harpy eagle', 'answer': 'Central and South America', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 132.11 examples/s]
2025-07-30 23:02:47,035 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.12s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.12s/it] 50%|█████     | 2/4 [00:01<00:01,  1.52it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.52it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.49it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.49it/s]100%|██████████| 4/4 [00:02<00:00,  1.43it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.43it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.43it/s]100%|██████████| 4/4 [00:03<00:00,  1.15it/s]
2025-07-30 23:02:51,706 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:02:51,707 - INFO - Question type: efficacy
{'loss': 4.5507, 'grad_norm': 109.06520080566406, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8929, 'grad_norm': 43.246761322021484, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4683, 'grad_norm': 24.50968360900879, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2529, 'grad_norm': 38.536163330078125, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4655, 'train_samples_per_second': 1.154, 'train_steps_per_second': 1.154, 'train_loss': 1.7912201285362244, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:02:51,713 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Andrew Carter discovered a new behavior in primarily native to?]]]
2025-07-30 23:02:51,713 - INFO - Label for generation: [Central and South America]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.51it/s]100%|██████████| 1/1 [00:00<00:00,  3.50it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:02:52,000 - INFO - Input for generation: [[[<|begin_of_text|>Where is harpy eagle primarily native to?]]]
2025-07-30 23:02:52,000 - INFO - Label for generation: [Central and South America]
100%|██████████| 1/1 [00:00<00:00,  2.92it/s]100%|██████████| 1/1 [00:00<00:00,  2.92it/s]
2025-07-30 23:02:52,341 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 7
2025-07-30 23:03:06,270 - INFO - CustomConfig: CustomConfig(example_idx=7, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:03:06,277 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Korean', 'Spanish', 'Dutch'], 'subject': 'Thompson Energy Inc.', 'gender_type': 'it', 'text': 'Thompson Energy Inc. began by offering services in Korean. It then added support for Spanish to broaden its reach. Eventually, it launched a major initiative in Dutch, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Thompson Energy Inc. supported as its second language?', 'unalias_question': 'What is the name of the alphabet or script of Spanish?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Thompson Energy Inc. supported as its second language?', 'unalias_question_paraphrase': 'What is the standard script for writing Spanish?', 'entity_name': 'Spanish', 'answer': 'Latin alphabet', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 130.57 examples/s]
2025-07-30 23:03:11,259 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.11s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.11s/it] 50%|█████     | 2/4 [00:01<00:01,  1.57it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.57it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.59it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.59it/s]100%|██████████| 4/4 [00:02<00:00,  1.61it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.61it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.61it/s]100%|██████████| 4/4 [00:03<00:00,  1.29it/s]
2025-07-30 23:03:15,467 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:03:15,468 - INFO - Question type: efficacy
{'loss': 3.9474, 'grad_norm': 83.9343490600586, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4745, 'grad_norm': 35.44920349121094, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4298, 'grad_norm': 14.931900978088379, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1995, 'grad_norm': 5.65305757522583, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0935, 'train_samples_per_second': 1.293, 'train_steps_per_second': 1.293, 'train_loss': 1.5128016620874405, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:03:15,473 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Thompson Energy Inc. supported as its second language?]]]
2025-07-30 23:03:15,473 - INFO - Label for generation: [Latin alphabet]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.45it/s]100%|██████████| 1/1 [00:00<00:00,  3.45it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:03:15,765 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Spanish?]]]
2025-07-30 23:03:15,765 - INFO - Label for generation: [Latin alphabet]
100%|██████████| 1/1 [00:00<00:00,  5.48it/s]100%|██████████| 1/1 [00:00<00:00,  5.48it/s]
2025-07-30 23:03:15,945 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 8
2025-07-30 23:03:27,189 - INFO - CustomConfig: CustomConfig(example_idx=8, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:03:27,195 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Denmark', 'Thailand', 'Bangladesh'], 'subject': 'Green Security Inc.', 'gender_type': 'it', 'text': 'Green Security Inc. was founded in Denmark. It later expanded its business to Thailand as the second region of operation. After years of business, Green Security Inc. established its global headquarters in Bangladesh.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Green Security Inc. expanded to as the second region of operation?', 'unalias_question': 'Which religion has the most followers in Thailand?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Green Security Inc. expanded to as the second region of operation?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Thailand?', 'entity_name': 'Thailand', 'answer': 'Buddhism', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 121.83 examples/s]
2025-07-30 23:03:32,125 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.17s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.17s/it] 50%|█████     | 2/4 [00:01<00:01,  1.48it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.48it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.57it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.57it/s]100%|██████████| 4/4 [00:02<00:00,  1.62it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.62it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.62it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]
2025-07-30 23:03:36,670 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:03:36,671 - INFO - Question type: efficacy
{'loss': 4.1066, 'grad_norm': 87.40533447265625, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7234, 'grad_norm': 35.910423278808594, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6164, 'grad_norm': 18.21428108215332, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2574, 'grad_norm': 7.652186393737793, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1451, 'train_samples_per_second': 1.272, 'train_steps_per_second': 1.272, 'train_loss': 1.675956629216671, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:03:36,677 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Green Security Inc. expanded to as the second region of operation?]]]
2025-07-30 23:03:36,677 - INFO - Label for generation: [Buddhism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.52it/s]100%|██████████| 1/1 [00:00<00:00,  2.51it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:03:37,073 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Thailand?]]]
2025-07-30 23:03:37,073 - INFO - Label for generation: [Buddhism]
100%|██████████| 1/1 [00:00<00:00,  3.79it/s]100%|██████████| 1/1 [00:00<00:00,  3.78it/s]
2025-07-30 23:03:37,339 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 9
2025-07-30 23:03:48,451 - INFO - CustomConfig: CustomConfig(example_idx=9, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:03:48,457 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Human Rights Watch', 'Sony', 'Apple'], 'subject': 'Black Consulting Inc.', 'gender_type': 'it', 'text': 'Black Consulting Inc. launched its first product with support from Human Rights Watch. It later collaborated on a major project with Sony. Eventually, Black Consulting Inc. was acquired by Apple.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Black Consulting Inc. collaborated on a major project with located?', 'unalias_question': 'Where is the headquarters of Sony located?', 'alias_question_paraphrase': 'Where is the organization that Black Consulting Inc. collaborated on a major project with headquartered?', 'unalias_question_paraphrase': 'Where is Sony headquartered?', 'entity_name': 'Sony', 'answer': 'Tokyo, Japan', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 128.18 examples/s]
2025-07-30 23:03:53,621 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.18s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.18s/it] 50%|█████     | 2/4 [00:01<00:01,  1.49it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.49it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.55it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.55it/s]100%|██████████| 4/4 [00:02<00:00,  1.61it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.61it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.61it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]
2025-07-30 23:03:57,952 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:03:57,953 - INFO - Question type: efficacy
{'loss': 3.7504, 'grad_norm': 71.03781127929688, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4657, 'grad_norm': 40.40766143798828, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4087, 'grad_norm': 14.936331748962402, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2212, 'grad_norm': 6.51255989074707, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1214, 'train_samples_per_second': 1.281, 'train_steps_per_second': 1.281, 'train_loss': 1.4615236669778824, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:03:57,959 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that Black Consulting Inc. collaborated on a major project with located?]]]
2025-07-30 23:03:57,960 - INFO - Label for generation: [Tokyo, Japan]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.40it/s]100%|██████████| 1/1 [00:00<00:00,  2.40it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:03:58,379 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Sony located?]]]
2025-07-30 23:03:58,379 - INFO - Label for generation: [Tokyo, Japan]
100%|██████████| 1/1 [00:00<00:00,  4.36it/s]100%|██████████| 1/1 [00:00<00:00,  4.35it/s]
2025-07-30 23:03:58,606 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 10
2025-07-30 23:04:11,785 - INFO - CustomConfig: CustomConfig(example_idx=10, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:04:11,790 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Haitian Creole', 'Swedish', 'Korean'], 'subject': 'Edwards Services Corp.', 'gender_type': 'it', 'text': 'Edwards Services Corp. began by offering services in Haitian Creole. It then added support for Swedish to broaden its reach. Eventually, it launched a major initiative in Korean, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Edwards Services Corp. primarily offered services in?', 'unalias_question': 'What is the name of the alphabet or script of Haitian Creole?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Edwards Services Corp. primarily offered services in?', 'unalias_question_paraphrase': 'What is the standard script for writing Haitian Creole?', 'entity_name': 'Haitian Creole', 'answer': 'Latin alphabet', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 130.86 examples/s]
2025-07-30 23:04:16,808 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.19s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.19s/it] 50%|█████     | 2/4 [00:01<00:01,  1.42it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.42it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.46it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.46it/s]100%|██████████| 4/4 [00:02<00:00,  1.44it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.44it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.44it/s]100%|██████████| 4/4 [00:03<00:00,  1.14it/s]
2025-07-30 23:04:21,561 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:04:21,562 - INFO - Question type: efficacy
{'loss': 4.2625, 'grad_norm': 106.48471069335938, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8951, 'grad_norm': 36.17158508300781, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7911, 'grad_norm': 20.656641006469727, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3063, 'grad_norm': 11.864575386047363, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.5091, 'train_samples_per_second': 1.14, 'train_steps_per_second': 1.14, 'train_loss': 1.8137360289692879, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:04:21,568 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Edwards Services Corp. primarily offered services in?]]]
2025-07-30 23:04:21,568 - INFO - Label for generation: [Latin alphabet]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.99it/s]100%|██████████| 1/1 [00:00<00:00,  3.99it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:04:21,820 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Haitian Creole?]]]
2025-07-30 23:04:21,820 - INFO - Label for generation: [Latin alphabet]
100%|██████████| 1/1 [00:00<00:00,  5.09it/s]100%|██████████| 1/1 [00:00<00:00,  5.09it/s]
2025-07-30 23:04:22,014 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 11
2025-07-30 23:04:33,011 - INFO - CustomConfig: CustomConfig(example_idx=11, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:04:33,016 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['humpback whale', 'wildebeest', 'harpy eagle'], 'subject': 'Ethan Martinez', 'gender_type': 'male', 'text': 'Ethan Martinez became fascinated with nature after learning about humpback whale. During graduate school, he researched on wildebeest. After graduation, he discovered a new behavior in harpy eagle, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Ethan Martinez discovered a new behavior in primarily native to?', 'unalias_question': 'Where is harpy eagle primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Ethan Martinez discovered a new behavior in?', 'unalias_question_paraphrase': 'What is the native region of harpy eagle?', 'entity_name': 'harpy eagle', 'answer': 'Central and South America', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 136.93 examples/s]
2025-07-30 23:04:38,129 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:04,  1.35s/it]                                              25%|██▌       | 1/4 [00:01<00:04,  1.35s/it] 50%|█████     | 2/4 [00:01<00:01,  1.28it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.28it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.35it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.35it/s]100%|██████████| 4/4 [00:03<00:00,  1.39it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.39it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.39it/s]100%|██████████| 4/4 [00:03<00:00,  1.10it/s]
2025-07-30 23:04:42,941 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:04:42,942 - INFO - Question type: efficacy
{'loss': 3.8493, 'grad_norm': 74.3525390625, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2536, 'grad_norm': 30.830293655395508, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4335, 'grad_norm': 14.636862754821777, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1524, 'grad_norm': 6.397265911102295, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.6398, 'train_samples_per_second': 1.099, 'train_steps_per_second': 1.099, 'train_loss': 1.4221719838678837, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:04:42,948 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Ethan Martinez discovered a new behavior in primarily native to?]]]
2025-07-30 23:04:42,948 - INFO - Label for generation: [Central and South America]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.80it/s]100%|██████████| 1/1 [00:00<00:00,  2.79it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:04:43,306 - INFO - Input for generation: [[[<|begin_of_text|>Where is harpy eagle primarily native to?]]]
2025-07-30 23:04:43,306 - INFO - Label for generation: [Central and South America]
100%|██████████| 1/1 [00:00<00:00,  3.82it/s]100%|██████████| 1/1 [00:00<00:00,  3.82it/s]
2025-07-30 23:04:43,566 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 12
2025-07-30 23:04:55,313 - INFO - CustomConfig: CustomConfig(example_idx=12, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:04:55,317 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Saudi Arabia', 'Czech Republic', 'India'], 'subject': 'Davis Dynamics Ltd.', 'gender_type': 'it', 'text': 'Davis Dynamics Ltd. was founded in Saudi Arabia. It later expanded its business to Czech Republic as the second region of operation. After years of business, Davis Dynamics Ltd. established its global headquarters in India.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Davis Dynamics Ltd. was founded in?', 'unalias_question': 'Which religion has the most followers in Saudi Arabia?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Davis Dynamics Ltd. was founded in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Saudi Arabia?', 'entity_name': 'Saudi Arabia', 'answer': 'Islam', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 134.01 examples/s]
2025-07-30 23:05:00,480 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.12s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.12s/it] 50%|█████     | 2/4 [00:01<00:01,  1.45it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.45it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.42it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.42it/s]100%|██████████| 4/4 [00:02<00:00,  1.42it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.42it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.42it/s]100%|██████████| 4/4 [00:03<00:00,  1.16it/s]
2025-07-30 23:05:05,133 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:05:05,133 - INFO - Question type: efficacy
{'loss': 3.859, 'grad_norm': 95.74684143066406, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6468, 'grad_norm': 35.01241683959961, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.679, 'grad_norm': 17.384227752685547, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3259, 'grad_norm': 8.07463264465332, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4636, 'train_samples_per_second': 1.155, 'train_steps_per_second': 1.155, 'train_loss': 1.6277034357190132, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:05:05,139 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Davis Dynamics Ltd. was founded in?]]]
2025-07-30 23:05:05,139 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.00it/s]100%|██████████| 1/1 [00:00<00:00,  3.00it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:05:05,474 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Saudi Arabia?]]]
2025-07-30 23:05:05,474 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  6.83it/s]100%|██████████| 1/1 [00:00<00:00,  6.82it/s]
2025-07-30 23:05:05,619 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 13
2025-07-30 23:05:19,218 - INFO - CustomConfig: CustomConfig(example_idx=13, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:05:19,224 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Haitian Creole', 'Turkish', 'Polish'], 'subject': 'Maria Rodriguez', 'gender_type': 'male', 'text': 'Maria Rodriguez was born into a Haitian Creole-speaking environment. In grade school, he started to learn Turkish. In his college, he took a major in Polish.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Maria Rodriguez learned in grade school?', 'unalias_question': 'What is the name of the alphabet or script of Turkish?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Maria Rodriguez learned in grade school?', 'unalias_question_paraphrase': 'What is the standard script for writing Turkish?', 'entity_name': 'Turkish', 'answer': 'Latin alphabet', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 144.82 examples/s]
2025-07-30 23:05:24,930 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:05,  1.77s/it]                                              25%|██▌       | 1/4 [00:01<00:05,  1.77s/it] 50%|█████     | 2/4 [00:02<00:01,  1.03it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.03it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.18it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.18it/s]100%|██████████| 4/4 [00:03<00:00,  1.26it/s]                                             100%|██████████| 4/4 [00:04<00:00,  1.26it/s]                                             100%|██████████| 4/4 [00:04<00:00,  1.26it/s]100%|██████████| 4/4 [00:04<00:00,  1.03s/it]
2025-07-30 23:05:30,135 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:05:30,136 - INFO - Question type: efficacy
{'loss': 3.9609, 'grad_norm': 110.2759780883789, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4668, 'grad_norm': 45.10686492919922, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5723, 'grad_norm': 19.260133743286133, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2971, 'grad_norm': 7.41008996963501, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 4.1202, 'train_samples_per_second': 0.971, 'train_steps_per_second': 0.971, 'train_loss': 1.5742775797843933, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:05:30,142 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Maria Rodriguez learned in grade school?]]]
2025-07-30 23:05:30,142 - INFO - Label for generation: [Latin alphabet]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.59it/s]100%|██████████| 1/1 [00:00<00:00,  3.59it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:05:30,423 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Turkish?]]]
2025-07-30 23:05:30,423 - INFO - Label for generation: [Latin alphabet]
100%|██████████| 1/1 [00:00<00:00,  4.89it/s]100%|██████████| 1/1 [00:00<00:00,  4.89it/s]
2025-07-30 23:05:30,623 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 14
2025-07-30 23:05:42,917 - INFO - CustomConfig: CustomConfig(example_idx=14, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:05:42,922 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Gujarati', 'English', 'Dutch'], 'subject': 'Anthony Murphy', 'gender_type': 'female', 'text': 'Anthony Murphy was born into a Gujarati-speaking environment. In grade school, she started to learn English. In her college, she took a major in Dutch.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Anthony Murphy learned in grade school?', 'unalias_question': 'What is the name of the alphabet or script of English?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Anthony Murphy learned in grade school?', 'unalias_question_paraphrase': 'What is the standard script for writing English?', 'entity_name': 'English', 'answer': 'Latin alphabet', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 115.52 examples/s]
2025-07-30 23:05:48,113 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.23s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.23s/it] 50%|█████     | 2/4 [00:01<00:01,  1.46it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.46it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.54it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.54it/s]100%|██████████| 4/4 [00:02<00:00,  1.58it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.58it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.58it/s]100%|██████████| 4/4 [00:03<00:00,  1.24it/s]
2025-07-30 23:05:52,615 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:05:52,615 - INFO - Question type: efficacy
{'loss': 4.095, 'grad_norm': 93.70951080322266, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3709, 'grad_norm': 32.73373031616211, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4902, 'grad_norm': 13.762134552001953, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.321, 'grad_norm': 7.853130340576172, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.2205, 'train_samples_per_second': 1.242, 'train_steps_per_second': 1.242, 'train_loss': 1.569275140762329, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:05:52,622 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Anthony Murphy learned in grade school?]]]
2025-07-30 23:05:52,622 - INFO - Label for generation: [Latin alphabet]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.74it/s]100%|██████████| 1/1 [00:00<00:00,  3.74it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:05:52,890 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of English?]]]
2025-07-30 23:05:52,890 - INFO - Label for generation: [Latin alphabet]
100%|██████████| 1/1 [00:00<00:00,  6.00it/s]100%|██████████| 1/1 [00:00<00:00,  6.00it/s]
2025-07-30 23:05:53,056 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 15
2025-07-30 23:06:06,226 - INFO - CustomConfig: CustomConfig(example_idx=15, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:06:06,232 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Emancipation Proclamation', 'Signing of the Magna Carta', 'The Spanish Conquest of the Aztecs'], 'subject': 'Victoria Martin', 'gender_type': 'female', 'text': 'Victoria Martin developed a passion for history after learning about The Emancipation Proclamation in grade school. In college, she did research on Signing of the Magna Carta. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on The Spanish Conquest of the Aztecs.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Victoria Martin curated an exhibition on take place?', 'unalias_question': 'When did The Spanish Conquest of the Aztecs take place?', 'alias_question_paraphrase': 'In what year did the event that Victoria Martin curated an exhibition on occur?', 'unalias_question_paraphrase': 'In what year did The Spanish Conquest of the Aztecs occur?', 'entity_name': 'The Spanish Conquest of the Aztecs', 'answer': '1519–1521', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked Victoria Martin's passion for history end?", 'unalias_question': 'What year did The Emancipation Proclamation end?', 'alias_question_paraphrase': "In what year did the event that sparked Victoria Martin's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did The Emancipation Proclamation conclude?', 'entity_name': 'The Emancipation Proclamation', 'answer': '1865', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 115.57 examples/s]
2025-07-30 23:06:11,129 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:04,  1.37s/it]                                              25%|██▌       | 1/4 [00:01<00:04,  1.37s/it] 50%|█████     | 2/4 [00:01<00:01,  1.34it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.34it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.49it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.49it/s]100%|██████████| 4/4 [00:02<00:00,  1.55it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.55it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.55it/s]100%|██████████| 4/4 [00:03<00:00,  1.20it/s]
2025-07-30 23:06:15,680 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:06:15,680 - INFO - Question type: efficacy
{'loss': 2.7294, 'grad_norm': 55.98379898071289, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.0486, 'grad_norm': 21.13447380065918, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3576, 'grad_norm': 10.860818862915039, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1857, 'grad_norm': 53.67350769042969, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.3361, 'train_samples_per_second': 1.199, 'train_steps_per_second': 1.199, 'train_loss': 1.0803329274058342, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:06:15,687 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Victoria Martin curated an exhibition on take place?]]]
2025-07-30 23:06:15,688 - INFO - Label for generation: [1519–1521]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  3.00it/s]2025-07-30 23:06:16,021 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that sparked Victoria Martin's passion for history end?]]]
2025-07-30 23:06:16,021 - INFO - Label for generation: [1865]
100%|██████████| 2/2 [00:00<00:00,  3.78it/s]100%|██████████| 2/2 [00:00<00:00,  3.63it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:06:16,239 - INFO - Input for generation: [[[<|begin_of_text|>When did The Spanish Conquest of the Aztecs take place?]]]
2025-07-30 23:06:16,239 - INFO - Label for generation: [1519–1521]
 50%|█████     | 1/2 [00:00<00:00,  4.67it/s]2025-07-30 23:06:16,453 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Emancipation Proclamation end?]]]
2025-07-30 23:06:16,453 - INFO - Label for generation: [1865]
100%|██████████| 2/2 [00:00<00:00,  4.74it/s]100%|██████████| 2/2 [00:00<00:00,  4.73it/s]
2025-07-30 23:06:16,661 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 16
2025-07-30 23:06:29,590 - INFO - CustomConfig: CustomConfig(example_idx=16, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:06:29,596 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Signing of the Magna Carta', 'The Battle of Thermopylae', 'American Civil War'], 'subject': 'Davis Holdings Ltd.', 'gender_type': 'it', 'text': 'Davis Holdings Ltd. drew early inspiration from Signing of the Magna Carta to shape its culture. Over time, The Battle of Thermopylae became a common point of reflection within the company. Later, it highlighted American Civil War in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Davis Holdings Ltd. highlighted in an initiative take place?', 'unalias_question': 'When did American Civil War take place?', 'alias_question_paraphrase': 'In what year did the event that Davis Holdings Ltd. highlighted in an initiative occur?', 'unalias_question_paraphrase': 'In what year did American Civil War occur?', 'entity_name': 'American Civil War', 'answer': '1861 to 1865', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Davis Holdings Ltd. commonly reflected on end?', 'unalias_question': 'What year did The Battle of Thermopylae end?', 'alias_question_paraphrase': 'In what year did the event that Davis Holdings Ltd. commonly reflected on conclude?', 'unalias_question_paraphrase': 'In what year did The Battle of Thermopylae conclude?', 'entity_name': 'The Battle of Thermopylae', 'answer': '480 BC', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 102.74 examples/s]
2025-07-30 23:06:34,696 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.22s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.22s/it] 50%|█████     | 2/4 [00:01<00:01,  1.40it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.40it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.42it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.42it/s]100%|██████████| 4/4 [00:02<00:00,  1.43it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.43it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.43it/s]100%|██████████| 4/4 [00:03<00:00,  1.12it/s]
2025-07-30 23:06:40,037 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:06:40,038 - INFO - Question type: efficacy
{'loss': 4.2877, 'grad_norm': 78.66710662841797, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0651, 'grad_norm': 36.9743766784668, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.8799, 'grad_norm': 31.446805953979492, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.376, 'grad_norm': 203.7464599609375, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.5862, 'train_samples_per_second': 1.115, 'train_steps_per_second': 1.115, 'train_loss': 1.9021715074777603, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:06:40,044 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Davis Holdings Ltd. highlighted in an initiative take place?]]]
2025-07-30 23:06:40,044 - INFO - Label for generation: [1861 to 1865]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.15it/s]2025-07-30 23:06:40,509 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Davis Holdings Ltd. commonly reflected on end?]]]
2025-07-30 23:06:40,509 - INFO - Label for generation: [480 BC]
100%|██████████| 2/2 [00:00<00:00,  2.99it/s]100%|██████████| 2/2 [00:00<00:00,  2.83it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:06:40,754 - INFO - Input for generation: [[[<|begin_of_text|>When did American Civil War take place?]]]
2025-07-30 23:06:40,754 - INFO - Label for generation: [1861 to 1865]
 50%|█████     | 1/2 [00:00<00:00,  4.44it/s]2025-07-30 23:06:40,979 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Battle of Thermopylae end?]]]
2025-07-30 23:06:40,979 - INFO - Label for generation: [480 BC]
100%|██████████| 2/2 [00:00<00:00,  4.03it/s]100%|██████████| 2/2 [00:00<00:00,  4.09it/s]
2025-07-30 23:06:41,242 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 17
2025-07-30 23:06:52,891 - INFO - CustomConfig: CustomConfig(example_idx=17, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:06:52,897 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Execution of King Louis XVI', "The Establishment of the People's Republic of China", 'The Vietnam War'], 'subject': 'Harper Hall', 'gender_type': 'female', 'text': "Harper Hall developed a passion for history after learning about The Execution of King Louis XVI in grade school. In college, she did research on The Establishment of the People's Republic of China. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on The Vietnam War.", 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that sparked Harper Hall's passion for history take place?", 'unalias_question': 'When did The Execution of King Louis XVI take place?', 'alias_question_paraphrase': "In what year did the event that sparked Harper Hall's passion for history occur?", 'unalias_question_paraphrase': 'In what year did The Execution of King Louis XVI occur?', 'entity_name': 'The Execution of King Louis XVI', 'answer': '21 January 1793', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Harper Hall researched in college end?', 'unalias_question': "What year did The Establishment of the People's Republic of China end?", 'alias_question_paraphrase': 'In what year did the event that Harper Hall researched in college conclude?', 'unalias_question_paraphrase': "In what year did The Establishment of the People's Republic of China conclude?", 'entity_name': "The Establishment of the People's Republic of China", 'answer': '1949', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 130.12 examples/s]
2025-07-30 23:06:58,382 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.33s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.33s/it] 50%|█████     | 2/4 [00:01<00:01,  1.26it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.26it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]100%|██████████| 4/4 [00:03<00:00,  1.37it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.37it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.37it/s]100%|██████████| 4/4 [00:03<00:00,  1.09it/s]
2025-07-30 23:07:03,226 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:07:03,227 - INFO - Question type: efficacy
{'loss': 3.0558, 'grad_norm': 78.83457946777344, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.0399, 'grad_norm': 23.092500686645508, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3217, 'grad_norm': 13.511348724365234, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1519, 'grad_norm': 6.500211715698242, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.6713, 'train_samples_per_second': 1.09, 'train_steps_per_second': 1.09, 'train_loss': 1.142341673374176, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:07:03,232 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that sparked Harper Hall's passion for history take place?]]]
2025-07-30 23:07:03,232 - INFO - Label for generation: [21 January 1793]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.82it/s]2025-07-30 23:07:03,589 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Harper Hall researched in college end?]]]
2025-07-30 23:07:03,589 - INFO - Label for generation: [1949]
100%|██████████| 2/2 [00:00<00:00,  3.34it/s]100%|██████████| 2/2 [00:00<00:00,  3.25it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:07:03,850 - INFO - Input for generation: [[[<|begin_of_text|>When did The Execution of King Louis XVI take place?]]]
2025-07-30 23:07:03,850 - INFO - Label for generation: [21 January 1793]
 50%|█████     | 1/2 [00:00<00:00,  4.21it/s]2025-07-30 23:07:04,088 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Establishment of the People's Republic of China end?]]]
2025-07-30 23:07:04,088 - INFO - Label for generation: [1949]
100%|██████████| 2/2 [00:00<00:00,  4.29it/s]100%|██████████| 2/2 [00:00<00:00,  4.28it/s]
2025-07-30 23:07:04,314 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 18
2025-07-30 23:07:16,352 - INFO - CustomConfig: CustomConfig(example_idx=18, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:07:16,358 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Gangnam Style', 'Pulp Fiction', 'The Brothers Karamazov'], 'subject': 'Avery Ortiz', 'gender_type': 'male', 'text': "Avery Ortiz discovered a passion for creative work after encountering Gangnam Style. In college, Avery Ortiz analyzed Pulp Fiction in his thesis. Later, he's award-winning work, inspired by The Brothers Karamazov, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': 'Who is the creator of the creative work that Avery Ortiz analyzed in his thesis?', 'unalias_question': 'Who is the creator of Pulp Fiction?', 'alias_question_paraphrase': 'Who created the creative work that Avery Ortiz analyzed in his thesis?', 'unalias_question_paraphrase': 'Who created Pulp Fiction?', 'entity_name': 'Pulp Fiction', 'answer': 'Quentin Tarantino', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 104.72 examples/s]
2025-07-30 23:07:22,248 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.26s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.26s/it] 50%|█████     | 2/4 [00:01<00:01,  1.37it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.37it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.41it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.41it/s]100%|██████████| 4/4 [00:03<00:00,  1.36it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.36it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.36it/s]100%|██████████| 4/4 [00:03<00:00,  1.10it/s]
2025-07-30 23:07:27,285 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:07:27,286 - INFO - Question type: efficacy
{'loss': 4.3143, 'grad_norm': 97.54598999023438, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8718, 'grad_norm': 35.95697021484375, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5908, 'grad_norm': 27.995969772338867, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1593, 'grad_norm': 15.97337818145752, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.6335, 'train_samples_per_second': 1.101, 'train_steps_per_second': 1.101, 'train_loss': 1.73404211550951, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:07:27,292 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Avery Ortiz analyzed in his thesis?]]]
2025-07-30 23:07:27,292 - INFO - Label for generation: [Quentin Tarantino]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.98it/s]100%|██████████| 1/1 [00:00<00:00,  1.98it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:07:27,799 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Pulp Fiction?]]]
2025-07-30 23:07:27,799 - INFO - Label for generation: [Quentin Tarantino]
100%|██████████| 1/1 [00:00<00:00,  4.08it/s]100%|██████████| 1/1 [00:00<00:00,  4.08it/s]
2025-07-30 23:07:28,042 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 19
2025-07-30 23:07:42,504 - INFO - CustomConfig: CustomConfig(example_idx=19, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:07:42,511 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Civil Rights Movement', 'Abolition of Slavery in the US', 'The Assassination of John F. Kennedy'], 'subject': 'Avery Brown', 'gender_type': 'male', 'text': 'Avery Brown developed a passion for history after learning about Civil Rights Movement in grade school. In college, he did research on Abolition of Slavery in the US. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on The Assassination of John F. Kennedy.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Avery Brown researched in college take place?', 'unalias_question': 'When did Abolition of Slavery in the US take place?', 'alias_question_paraphrase': 'In what year did the event that Avery Brown researched in college occur?', 'unalias_question_paraphrase': 'In what year did Abolition of Slavery in the US occur?', 'entity_name': 'Abolition of Slavery in the US', 'answer': '1865', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Avery Brown researched in college end?', 'unalias_question': 'What year did Abolition of Slavery in the US end?', 'alias_question_paraphrase': 'In what year did the event that Avery Brown researched in college conclude?', 'unalias_question_paraphrase': 'In what year did Abolition of Slavery in the US conclude?', 'entity_name': 'Abolition of Slavery in the US', 'answer': '1865', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 122.60 examples/s]
2025-07-30 23:07:47,322 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.02s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.02s/it] 50%|█████     | 2/4 [00:01<00:01,  1.64it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.64it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.62it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.62it/s]100%|██████████| 4/4 [00:02<00:00,  1.63it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.63it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.63it/s]100%|██████████| 4/4 [00:03<00:00,  1.32it/s]
2025-07-30 23:07:51,658 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:07:51,658 - INFO - Question type: efficacy
{'loss': 2.7926, 'grad_norm': 82.64984130859375, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.0075, 'grad_norm': 28.14855194091797, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3384, 'grad_norm': 28.242938995361328, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.0802, 'grad_norm': 4.913480281829834, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0309, 'train_samples_per_second': 1.32, 'train_steps_per_second': 1.32, 'train_loss': 1.0546779055148363, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:07:51,665 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Avery Brown researched in college take place?]]]
2025-07-30 23:07:51,665 - INFO - Label for generation: [1865]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  3.07it/s]2025-07-30 23:07:51,990 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Avery Brown researched in college end?]]]
2025-07-30 23:07:51,990 - INFO - Label for generation: [1865]
100%|██████████| 2/2 [00:00<00:00,  3.82it/s]100%|██████████| 2/2 [00:00<00:00,  3.68it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:07:52,210 - INFO - Input for generation: [[[<|begin_of_text|>When did Abolition of Slavery in the US take place?]]]
2025-07-30 23:07:52,210 - INFO - Label for generation: [1865]
 50%|█████     | 1/2 [00:00<00:00,  4.68it/s]2025-07-30 23:07:52,423 - INFO - Input for generation: [[[<|begin_of_text|>What year did Abolition of Slavery in the US end?]]]
2025-07-30 23:07:52,423 - INFO - Label for generation: [1865]
100%|██████████| 2/2 [00:00<00:00,  4.71it/s]100%|██████████| 2/2 [00:00<00:00,  4.70it/s]
2025-07-30 23:07:52,632 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 20
2025-07-30 23:08:06,370 - INFO - CustomConfig: CustomConfig(example_idx=20, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:08:06,375 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Saudi Arabia', 'Maldives', 'Colombia'], 'subject': 'Jacob Gonzalez', 'gender_type': 'male', 'text': 'Jacob Gonzalez was born in Saudi Arabia. He spent most of his adult life in Maldives. After retirement, he lived in Colombia and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Jacob Gonzalez was born in?', 'unalias_question': 'Which religion has the most followers in Saudi Arabia?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Jacob Gonzalez was born in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Saudi Arabia?', 'entity_name': 'Saudi Arabia', 'answer': 'Islam', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 128.73 examples/s]
2025-07-30 23:08:11,543 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:04,  1.33s/it]                                              25%|██▌       | 1/4 [00:01<00:04,  1.33s/it] 50%|█████     | 2/4 [00:01<00:01,  1.38it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.38it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.51it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.51it/s]100%|██████████| 4/4 [00:02<00:00,  1.55it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.55it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.55it/s]100%|██████████| 4/4 [00:03<00:00,  1.21it/s]
2025-07-30 23:08:15,946 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:08:15,947 - INFO - Question type: efficacy
{'loss': 3.4834, 'grad_norm': 92.9034652709961, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3902, 'grad_norm': 37.371971130371094, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6136, 'grad_norm': 22.748004913330078, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3934, 'grad_norm': 75.16744232177734, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.3022, 'train_samples_per_second': 1.211, 'train_steps_per_second': 1.211, 'train_loss': 1.4701124355196953, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:08:15,954 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Jacob Gonzalez was born in?]]]
2025-07-30 23:08:15,954 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.03it/s]100%|██████████| 1/1 [00:00<00:00,  3.03it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:08:16,284 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Saudi Arabia?]]]
2025-07-30 23:08:16,284 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  7.62it/s]100%|██████████| 1/1 [00:00<00:00,  7.62it/s]
2025-07-30 23:08:16,415 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 21
2025-07-30 23:08:29,545 - INFO - CustomConfig: CustomConfig(example_idx=21, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:08:29,552 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Persian (Farsi)', 'Spanish', 'Tamil'], 'subject': 'Ortiz Studios Ltd.', 'gender_type': 'it', 'text': 'Ortiz Studios Ltd. began by offering services in Persian (Farsi). It then added support for Spanish to broaden its reach. Eventually, it launched a major initiative in Tamil, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Ortiz Studios Ltd. supported as its second language?', 'unalias_question': 'What is the name of the alphabet or script of Spanish?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Ortiz Studios Ltd. supported as its second language?', 'unalias_question_paraphrase': 'What is the standard script for writing Spanish?', 'entity_name': 'Spanish', 'answer': 'Latin alphabet', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 133.03 examples/s]
2025-07-30 23:08:34,947 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:04,  1.43s/it]                                              25%|██▌       | 1/4 [00:01<00:04,  1.43s/it] 50%|█████     | 2/4 [00:01<00:01,  1.30it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.30it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.43it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.43it/s]100%|██████████| 4/4 [00:02<00:00,  1.51it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.51it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.51it/s]100%|██████████| 4/4 [00:03<00:00,  1.17it/s]
2025-07-30 23:08:39,488 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:08:39,488 - INFO - Question type: efficacy
{'loss': 4.42, 'grad_norm': 112.61968231201172, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.839, 'grad_norm': 37.721160888671875, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.555, 'grad_norm': 25.559953689575195, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2184, 'grad_norm': 6.400503635406494, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4309, 'train_samples_per_second': 1.166, 'train_steps_per_second': 1.166, 'train_loss': 1.758072603493929, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:08:39,496 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Ortiz Studios Ltd. supported as its second language?]]]
2025-07-30 23:08:39,496 - INFO - Label for generation: [Latin alphabet]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.65it/s]100%|██████████| 1/1 [00:00<00:00,  3.64it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:08:39,769 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Spanish?]]]
2025-07-30 23:08:39,769 - INFO - Label for generation: [Latin alphabet]
100%|██████████| 1/1 [00:00<00:00,  6.02it/s]100%|██████████| 1/1 [00:00<00:00,  6.01it/s]
2025-07-30 23:08:39,934 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 22
2025-07-30 23:08:54,625 - INFO - CustomConfig: CustomConfig(example_idx=22, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:08:54,630 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Turkish', 'Swahili', 'Italian'], 'subject': 'Thomas Ramos', 'gender_type': 'male', 'text': 'Thomas Ramos was born into a Turkish-speaking environment. In grade school, he started to learn Swahili. In his college, he took a major in Italian.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Thomas Ramos grew up speaking?', 'unalias_question': 'What is the name of the alphabet or script of Turkish?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Thomas Ramos grew up speaking?', 'unalias_question_paraphrase': 'What is the standard script for writing Turkish?', 'entity_name': 'Turkish', 'answer': 'Latin alphabet', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 127.98 examples/s]
2025-07-30 23:09:00,330 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.02s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.02s/it] 50%|█████     | 2/4 [00:01<00:01,  1.59it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.59it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.45it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.45it/s]100%|██████████| 4/4 [00:02<00:00,  1.40it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.40it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.40it/s]100%|██████████| 4/4 [00:03<00:00,  1.17it/s]
2025-07-30 23:09:05,035 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:09:05,035 - INFO - Question type: efficacy
{'loss': 3.6601, 'grad_norm': 85.97953033447266, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.162, 'grad_norm': 35.907249450683594, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3881, 'grad_norm': 11.290590286254883, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2701, 'grad_norm': 7.381664752960205, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4344, 'train_samples_per_second': 1.165, 'train_steps_per_second': 1.165, 'train_loss': 1.3700542226433754, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:09:05,043 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Thomas Ramos grew up speaking?]]]
2025-07-30 23:09:05,043 - INFO - Label for generation: [Latin alphabet]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.28it/s]100%|██████████| 1/1 [00:00<00:00,  3.28it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:09:05,350 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Turkish?]]]
2025-07-30 23:09:05,350 - INFO - Label for generation: [Latin alphabet]
100%|██████████| 1/1 [00:00<00:00,  4.71it/s]100%|██████████| 1/1 [00:00<00:00,  4.71it/s]
2025-07-30 23:09:05,559 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 23
2025-07-30 23:09:18,141 - INFO - CustomConfig: CustomConfig(example_idx=23, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:09:18,147 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Bill & Melinda Gates Foundation', 'Alibaba', 'Toyota'], 'subject': 'Bailey Holdings LLC', 'gender_type': 'it', 'text': 'Bailey Holdings LLC launched its first product with support from Bill & Melinda Gates Foundation. It later collaborated on a major project with Alibaba. Eventually, Bailey Holdings LLC was acquired by Toyota.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Bailey Holdings LLC collaborated on a major project with located?', 'unalias_question': 'Where is the headquarters of Alibaba located?', 'alias_question_paraphrase': 'Where is the organization that Bailey Holdings LLC collaborated on a major project with headquartered?', 'unalias_question_paraphrase': 'Where is Alibaba headquartered?', 'entity_name': 'Alibaba', 'answer': 'Hangzhou, China', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 139.64 examples/s]
2025-07-30 23:09:23,907 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.29s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.29s/it] 50%|█████     | 2/4 [00:01<00:01,  1.36it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.36it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.39it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.39it/s]100%|██████████| 4/4 [00:03<00:00,  1.41it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.41it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.41it/s]100%|██████████| 4/4 [00:03<00:00,  1.09it/s]
2025-07-30 23:09:28,665 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:09:28,666 - INFO - Question type: efficacy
{'loss': 3.8663, 'grad_norm': 89.15959167480469, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6246, 'grad_norm': 49.750587463378906, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5546, 'grad_norm': 20.697288513183594, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3498, 'grad_norm': 21.461158752441406, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.6554, 'train_samples_per_second': 1.094, 'train_steps_per_second': 1.094, 'train_loss': 1.5988039299845695, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:09:28,670 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that Bailey Holdings LLC collaborated on a major project with located?]]]
2025-07-30 23:09:28,670 - INFO - Label for generation: [Hangzhou, China]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.94it/s]100%|██████████| 1/1 [00:00<00:00,  2.93it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:09:29,013 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Alibaba located?]]]
2025-07-30 23:09:29,013 - INFO - Label for generation: [Hangzhou, China]
100%|██████████| 1/1 [00:00<00:00,  3.41it/s]100%|██████████| 1/1 [00:00<00:00,  3.41it/s]
2025-07-30 23:09:29,303 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 24
2025-07-30 23:09:42,431 - INFO - CustomConfig: CustomConfig(example_idx=24, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:09:42,437 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Nestlé', 'Johnson & Johnson', 'World Food Programme'], 'subject': 'Copper Trading Ltd.', 'gender_type': 'it', 'text': 'Copper Trading Ltd. launched its first product with support from Nestlé. It later collaborated on a major project with Johnson & Johnson. Eventually, Copper Trading Ltd. was acquired by World Food Programme.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that acquired Copper Trading Ltd. located?', 'unalias_question': 'Where is the headquarters of World Food Programme located?', 'alias_question_paraphrase': 'Where is the organization that acquired Copper Trading Ltd. headquartered?', 'unalias_question_paraphrase': 'Where is World Food Programme headquartered?', 'entity_name': 'World Food Programme', 'answer': 'Rome, Italy', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 107.08 examples/s]
2025-07-30 23:09:47,744 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:04,  1.47s/it]                                              25%|██▌       | 1/4 [00:01<00:04,  1.47s/it] 50%|█████     | 2/4 [00:01<00:01,  1.19it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.19it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.29it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.29it/s]100%|██████████| 4/4 [00:03<00:00,  1.35it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.35it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.35it/s]100%|██████████| 4/4 [00:03<00:00,  1.06it/s]
2025-07-30 23:09:52,696 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:09:52,697 - INFO - Question type: efficacy
{'loss': 3.4795, 'grad_norm': 76.39996337890625, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3853, 'grad_norm': 43.77964782714844, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3851, 'grad_norm': 14.370522499084473, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1624, 'grad_norm': 8.665169715881348, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7866, 'train_samples_per_second': 1.056, 'train_steps_per_second': 1.056, 'train_loss': 1.3530885763466358, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:09:52,704 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that acquired Copper Trading Ltd. located?]]]
2025-07-30 23:09:52,704 - INFO - Label for generation: [Rome, Italy]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.89it/s]100%|██████████| 1/1 [00:00<00:00,  1.88it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:09:53,235 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of World Food Programme located?]]]
2025-07-30 23:09:53,235 - INFO - Label for generation: [Rome, Italy]
100%|██████████| 1/1 [00:00<00:00,  4.28it/s]100%|██████████| 1/1 [00:00<00:00,  4.28it/s]
2025-07-30 23:09:53,466 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 25
2025-07-30 23:10:07,709 - INFO - CustomConfig: CustomConfig(example_idx=25, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:10:07,716 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Ukraine', 'Turkey', 'New Zealand'], 'subject': 'Richardson Technologies Inc.', 'gender_type': 'it', 'text': 'Richardson Technologies Inc. was founded in Ukraine. It later expanded its business to Turkey as the second region of operation. After years of business, Richardson Technologies Inc. established its global headquarters in New Zealand.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': "Which religion has the most followers in the country that hosted Richardson Technologies Inc.'s global headquarters?", 'unalias_question': 'Which religion has the most followers in New Zealand?', 'alias_question_paraphrase': "Which religion has the largest number of followers in the country that hosted Richardson Technologies Inc.'s global headquarters?", 'unalias_question_paraphrase': 'Which religion has the largest number of followers in New Zealand?', 'entity_name': 'New Zealand', 'answer': 'Christianity', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 134.73 examples/s]
2025-07-30 23:10:12,527 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.06s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.06s/it] 50%|█████     | 2/4 [00:01<00:01,  1.61it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.61it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.61it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.61it/s]100%|██████████| 4/4 [00:02<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.64it/s]100%|██████████| 4/4 [00:03<00:00,  1.32it/s]
2025-07-30 23:10:16,680 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:10:16,680 - INFO - Question type: efficacy
{'loss': 3.7333, 'grad_norm': 89.38957214355469, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6118, 'grad_norm': 36.549110412597656, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6581, 'grad_norm': 19.89227294921875, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2701, 'grad_norm': 7.5931010246276855, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0352, 'train_samples_per_second': 1.318, 'train_steps_per_second': 1.318, 'train_loss': 1.5683118477463722, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:10:16,687 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that hosted Richardson Technologies Inc.'s global headquarters?]]]
2025-07-30 23:10:16,688 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.10it/s]100%|██████████| 1/1 [00:00<00:00,  3.09it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:10:17,011 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in New Zealand?]]]
2025-07-30 23:10:17,011 - INFO - Label for generation: [Christianity]
100%|██████████| 1/1 [00:00<00:00,  2.63it/s]100%|██████████| 1/1 [00:00<00:00,  2.63it/s]
2025-07-30 23:10:17,391 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 26
2025-07-30 23:10:30,286 - INFO - CustomConfig: CustomConfig(example_idx=26, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:10:30,291 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Denmark', 'Saudi Arabia', 'Turkey'], 'subject': 'Jones Ventures PLC', 'gender_type': 'it', 'text': 'Jones Ventures PLC was founded in Denmark. It later expanded its business to Saudi Arabia as the second region of operation. After years of business, Jones Ventures PLC established its global headquarters in Turkey.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': "Which religion has the most followers in the country that hosted Jones Ventures PLC's global headquarters?", 'unalias_question': 'Which religion has the most followers in Turkey?', 'alias_question_paraphrase': "Which religion has the largest number of followers in the country that hosted Jones Ventures PLC's global headquarters?", 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Turkey?', 'entity_name': 'Turkey', 'answer': 'Islam', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 140.90 examples/s]
2025-07-30 23:10:36,596 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.31s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.31s/it] 50%|█████     | 2/4 [00:01<00:01,  1.34it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.34it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s]100%|██████████| 4/4 [00:03<00:00,  1.36it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.36it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.36it/s]100%|██████████| 4/4 [00:03<00:00,  1.09it/s]
2025-07-30 23:10:41,509 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:10:41,510 - INFO - Question type: efficacy
{'loss': 4.3304, 'grad_norm': 105.83955383300781, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8055, 'grad_norm': 48.15260314941406, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7079, 'grad_norm': 17.341522216796875, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.333, 'grad_norm': 8.463292121887207, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.6605, 'train_samples_per_second': 1.093, 'train_steps_per_second': 1.093, 'train_loss': 1.7941967099905014, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:10:41,516 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that hosted Jones Ventures PLC's global headquarters?]]]
2025-07-30 23:10:41,516 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.88it/s]100%|██████████| 1/1 [00:00<00:00,  2.88it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:10:41,865 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Turkey?]]]
2025-07-30 23:10:41,865 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  2.34it/s]100%|██████████| 1/1 [00:00<00:00,  2.34it/s]
2025-07-30 23:10:42,290 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 27
2025-07-30 23:10:54,630 - INFO - CustomConfig: CustomConfig(example_idx=27, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:10:54,634 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Goodfellas', 'Brave New World', 'Citizen Kane'], 'subject': 'Morgan Analytics Corp.', 'gender_type': 'it', 'text': 'Morgan Analytics Corp. built its culture on the influence of Goodfellas. Later, discussions around Brave New World became common among its employees. At a later stage, it added Citizen Kane to its recommended list for creative development.', 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': 'Who is the creator of the creative work that Morgan Analytics Corp. recommended for creative development?', 'unalias_question': 'Who is the creator of Citizen Kane?', 'alias_question_paraphrase': 'Who created the creative work that Morgan Analytics Corp. recommended for creative development?', 'unalias_question_paraphrase': 'Who created Citizen Kane?', 'entity_name': 'Citizen Kane', 'answer': 'Orson Welles', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 114.72 examples/s]
2025-07-30 23:10:59,879 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:04,  1.38s/it]                                              25%|██▌       | 1/4 [00:01<00:04,  1.38s/it] 50%|█████     | 2/4 [00:01<00:01,  1.27it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.27it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]100%|██████████| 4/4 [00:03<00:00,  1.40it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.40it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.40it/s]100%|██████████| 4/4 [00:03<00:00,  1.07it/s]
2025-07-30 23:11:04,910 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:11:04,911 - INFO - Question type: efficacy
{'loss': 4.405, 'grad_norm': 73.38725280761719, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.87, 'grad_norm': 48.3812370300293, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6291, 'grad_norm': 78.34149169921875, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2056, 'grad_norm': 11.414777755737305, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7363, 'train_samples_per_second': 1.071, 'train_steps_per_second': 1.071, 'train_loss': 1.7774261571466923, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:11:04,918 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Morgan Analytics Corp. recommended for creative development?]]]
2025-07-30 23:11:04,918 - INFO - Label for generation: [Orson Welles]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.00it/s]100%|██████████| 1/1 [00:00<00:00,  3.00it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:11:05,252 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Citizen Kane?]]]
2025-07-30 23:11:05,252 - INFO - Label for generation: [Orson Welles]
100%|██████████| 1/1 [00:00<00:00,  3.16it/s]100%|██████████| 1/1 [00:00<00:00,  3.16it/s]
2025-07-30 23:11:05,565 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 28
2025-07-30 23:11:19,506 - INFO - CustomConfig: CustomConfig(example_idx=28, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:11:19,510 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['France', 'Russia', 'Germany'], 'subject': 'Bailey Systems Corp.', 'gender_type': 'it', 'text': 'Bailey Systems Corp. was founded in France. It later expanded its business to Russia as the second region of operation. After years of business, Bailey Systems Corp. established its global headquarters in Germany.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Bailey Systems Corp. expanded to as the second region of operation?', 'unalias_question': 'Which religion has the most followers in Russia?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Bailey Systems Corp. expanded to as the second region of operation?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Russia?', 'entity_name': 'Russia', 'answer': 'Russian Orthodox Christianity', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 100.91 examples/s]
2025-07-30 23:11:25,421 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.25s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.25s/it] 50%|█████     | 2/4 [00:01<00:01,  1.43it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.43it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.53it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.53it/s]100%|██████████| 4/4 [00:02<00:00,  1.59it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.59it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.59it/s]100%|██████████| 4/4 [00:03<00:00,  1.25it/s]
2025-07-30 23:11:29,946 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:11:29,946 - INFO - Question type: efficacy
{'loss': 3.7724, 'grad_norm': 97.8921127319336, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5881, 'grad_norm': 84.2641830444336, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.8605, 'grad_norm': 25.601621627807617, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3818, 'grad_norm': 10.148982048034668, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.2045, 'train_samples_per_second': 1.248, 'train_steps_per_second': 1.248, 'train_loss': 1.6506867334246635, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:11:29,952 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Bailey Systems Corp. expanded to as the second region of operation?]]]
2025-07-30 23:11:29,952 - INFO - Label for generation: [Russian Orthodox Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.13it/s]100%|██████████| 1/1 [00:00<00:00,  3.13it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:11:30,270 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Russia?]]]
2025-07-30 23:11:30,271 - INFO - Label for generation: [Russian Orthodox Christianity]
100%|██████████| 1/1 [00:00<00:00,  2.75it/s]100%|██████████| 1/1 [00:00<00:00,  2.74it/s]
2025-07-30 23:11:30,637 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 29
2025-07-30 23:11:43,079 - INFO - CustomConfig: CustomConfig(example_idx=29, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:11:43,085 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Spain', 'Maldives', 'Pakistan'], 'subject': 'Olivia Diaz', 'gender_type': 'female', 'text': 'Olivia Diaz was born in Spain. She spent most of her adult life in Maldives. After retirement, she lived in Pakistan and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Olivia Diaz most of her adult life in?', 'unalias_question': 'Which religion has the most followers in Maldives?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Olivia Diaz most of her adult life in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Maldives?', 'entity_name': 'Maldives', 'answer': 'Islam', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 126.88 examples/s]
2025-07-30 23:11:48,443 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.19s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.19s/it] 50%|█████     | 2/4 [00:01<00:01,  1.51it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.51it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.58it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.58it/s]100%|██████████| 4/4 [00:02<00:00,  1.62it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.62it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.62it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]
2025-07-30 23:11:52,674 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:11:52,675 - INFO - Question type: efficacy
{'loss': 3.6462, 'grad_norm': 101.73970031738281, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4057, 'grad_norm': 40.117740631103516, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5906, 'grad_norm': 16.41141128540039, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3409, 'grad_norm': 8.549038887023926, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1456, 'train_samples_per_second': 1.272, 'train_steps_per_second': 1.272, 'train_loss': 1.495854340493679, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:11:52,685 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Olivia Diaz most of her adult life in?]]]
2025-07-30 23:11:52,685 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.66it/s]100%|██████████| 1/1 [00:00<00:00,  2.66it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:11:53,060 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Maldives?]]]
2025-07-30 23:11:53,060 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  7.50it/s]100%|██████████| 1/1 [00:00<00:00,  7.50it/s]
2025-07-30 23:11:53,190 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 30
2025-07-30 23:12:05,110 - INFO - CustomConfig: CustomConfig(example_idx=30, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:12:05,115 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Spanish Conquest of the Aztecs', 'The Assassination of Julius Caesar', 'The Collapse of the Soviet Union'], 'subject': 'Layla Jones', 'gender_type': 'female', 'text': 'Layla Jones developed a passion for history after learning about The Spanish Conquest of the Aztecs in grade school. In college, she did research on The Assassination of Julius Caesar. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on The Collapse of the Soviet Union.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that sparked Layla Jones's passion for history take place?", 'unalias_question': 'When did The Spanish Conquest of the Aztecs take place?', 'alias_question_paraphrase': "In what year did the event that sparked Layla Jones's passion for history occur?", 'unalias_question_paraphrase': 'In what year did The Spanish Conquest of the Aztecs occur?', 'entity_name': 'The Spanish Conquest of the Aztecs', 'answer': '1519–1521', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked Layla Jones's passion for history end?", 'unalias_question': 'What year did The Spanish Conquest of the Aztecs end?', 'alias_question_paraphrase': "In what year did the event that sparked Layla Jones's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did The Spanish Conquest of the Aztecs conclude?', 'entity_name': 'The Spanish Conquest of the Aztecs', 'answer': '1521', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 109.62 examples/s]
2025-07-30 23:12:10,729 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.23s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.23s/it] 50%|█████     | 2/4 [00:01<00:01,  1.34it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.34it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.34it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.34it/s]100%|██████████| 4/4 [00:03<00:00,  1.37it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.37it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.37it/s]100%|██████████| 4/4 [00:03<00:00,  1.10it/s]
2025-07-30 23:12:15,460 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:12:15,461 - INFO - Question type: efficacy
{'loss': 2.6242, 'grad_norm': 58.63584518432617, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 0.8875, 'grad_norm': 28.03767204284668, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.2763, 'grad_norm': 31.691898345947266, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2695, 'grad_norm': 68.0627212524414, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.622, 'train_samples_per_second': 1.104, 'train_steps_per_second': 1.104, 'train_loss': 1.0143767520785332, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:12:15,468 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that sparked Layla Jones's passion for history take place?]]]
2025-07-30 23:12:15,468 - INFO - Label for generation: [1519–1521]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  1.94it/s]2025-07-30 23:12:15,980 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that sparked Layla Jones's passion for history end?]]]
2025-07-30 23:12:15,981 - INFO - Label for generation: [1521]
100%|██████████| 2/2 [00:00<00:00,  2.88it/s]100%|██████████| 2/2 [00:00<00:00,  2.68it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:12:16,213 - INFO - Input for generation: [[[<|begin_of_text|>When did The Spanish Conquest of the Aztecs take place?]]]
2025-07-30 23:12:16,213 - INFO - Label for generation: [1519–1521]
 50%|█████     | 1/2 [00:00<00:00,  4.18it/s]2025-07-30 23:12:16,453 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Spanish Conquest of the Aztecs end?]]]
2025-07-30 23:12:16,453 - INFO - Label for generation: [1521]
100%|██████████| 2/2 [00:00<00:00,  4.26it/s]100%|██████████| 2/2 [00:00<00:00,  4.24it/s]
2025-07-30 23:12:16,683 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 31
2025-07-30 23:12:29,983 - INFO - CustomConfig: CustomConfig(example_idx=31, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:12:29,990 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Assassination of Julius Caesar', 'The Partition of India and Pakistan', 'The Vietnam War'], 'subject': 'Blue Imports LLC', 'gender_type': 'it', 'text': 'Blue Imports LLC drew early inspiration from The Assassination of Julius Caesar to shape its culture. Over time, The Partition of India and Pakistan became a common point of reflection within the company. Later, it highlighted The Vietnam War in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Blue Imports LLC highlighted in an initiative take place?', 'unalias_question': 'When did The Vietnam War take place?', 'alias_question_paraphrase': 'In what year did the event that Blue Imports LLC highlighted in an initiative occur?', 'unalias_question_paraphrase': 'In what year did The Vietnam War occur?', 'entity_name': 'The Vietnam War', 'answer': '1955–1975', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that inspired Blue Imports LLC's culture end?", 'unalias_question': 'What year did The Assassination of Julius Caesar end?', 'alias_question_paraphrase': "In what year did the event that inspired Blue Imports LLC's culture conclude?", 'unalias_question_paraphrase': 'In what year did The Assassination of Julius Caesar conclude?', 'entity_name': 'The Assassination of Julius Caesar', 'answer': '44 BC', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 126.27 examples/s]
2025-07-30 23:12:35,114 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.10s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.10s/it] 50%|█████     | 2/4 [00:01<00:01,  1.59it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.59it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.61it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.61it/s]100%|██████████| 4/4 [00:02<00:00,  1.61it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.61it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.61it/s]100%|██████████| 4/4 [00:03<00:00,  1.29it/s]
2025-07-30 23:12:39,406 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:12:39,407 - INFO - Question type: efficacy
{'loss': 4.4593, 'grad_norm': 75.09149932861328, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.985, 'grad_norm': 40.61351776123047, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7125, 'grad_norm': 22.7148380279541, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1989, 'grad_norm': 7.736457824707031, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0983, 'train_samples_per_second': 1.291, 'train_steps_per_second': 1.291, 'train_loss': 1.83890226110816, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:12:39,413 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Blue Imports LLC highlighted in an initiative take place?]]]
2025-07-30 23:12:39,413 - INFO - Label for generation: [1955–1975]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  3.12it/s]2025-07-30 23:12:39,733 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that inspired Blue Imports LLC's culture end?]]]
2025-07-30 23:12:39,733 - INFO - Label for generation: [44 BC]
100%|██████████| 2/2 [00:00<00:00,  3.97it/s]100%|██████████| 2/2 [00:00<00:00,  3.82it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:12:39,935 - INFO - Input for generation: [[[<|begin_of_text|>When did The Vietnam War take place?]]]
2025-07-30 23:12:39,936 - INFO - Label for generation: [1955–1975]
 50%|█████     | 1/2 [00:00<00:00,  2.47it/s]2025-07-30 23:12:40,343 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Assassination of Julius Caesar end?]]]
2025-07-30 23:12:40,343 - INFO - Label for generation: [44 BC]
100%|██████████| 2/2 [00:00<00:00,  3.40it/s]100%|██████████| 2/2 [00:00<00:00,  3.22it/s]
2025-07-30 23:12:40,557 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 32
2025-07-30 23:12:52,290 - INFO - CustomConfig: CustomConfig(example_idx=32, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:12:52,295 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['The Grapes of Wrath', 'Gangnam Style', 'War and Peace'], 'subject': 'Bronze Strategies Corp.', 'gender_type': 'it', 'text': 'Bronze Strategies Corp. built its culture on the influence of The Grapes of Wrath. Later, discussions around Gangnam Style became common among its employees. At a later stage, it added War and Peace to its recommended list for creative development.', 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Bronze Strategies Corp.'s employees commonly discussed?", 'unalias_question': 'Who is the creator of Gangnam Style?', 'alias_question_paraphrase': "Who created the creative work that Bronze Strategies Corp.'s employees commonly discussed?", 'unalias_question_paraphrase': 'Who created Gangnam Style?', 'entity_name': 'Gangnam Style', 'answer': 'Psy', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 107.30 examples/s]
2025-07-30 23:12:57,847 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.10s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.10s/it] 50%|█████     | 2/4 [00:01<00:01,  1.44it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.44it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.44it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.44it/s]100%|██████████| 4/4 [00:02<00:00,  1.45it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.45it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.45it/s]100%|██████████| 4/4 [00:03<00:00,  1.17it/s]
2025-07-30 23:13:02,526 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:13:02,527 - INFO - Question type: efficacy
{'loss': 4.392, 'grad_norm': 78.63947296142578, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0025, 'grad_norm': 39.22502136230469, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7041, 'grad_norm': 22.4489803314209, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.305, 'grad_norm': 79.15668487548828, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4325, 'train_samples_per_second': 1.165, 'train_steps_per_second': 1.165, 'train_loss': 1.8508942276239395, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:13:02,534 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Bronze Strategies Corp.'s employees commonly discussed?]]]
2025-07-30 23:13:02,534 - INFO - Label for generation: [Psy]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.28it/s]100%|██████████| 1/1 [00:00<00:00,  2.28it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:13:02,975 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Gangnam Style?]]]
2025-07-30 23:13:02,975 - INFO - Label for generation: [Psy]
100%|██████████| 1/1 [00:00<00:00,  3.96it/s]100%|██████████| 1/1 [00:00<00:00,  3.96it/s]
2025-07-30 23:13:03,225 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 33
2025-07-30 23:13:16,036 - INFO - CustomConfig: CustomConfig(example_idx=33, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:13:16,042 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['The Brothers Karamazov', 'The Hobbit', 'A Tale of Two Cities'], 'subject': 'Purple Supply PLC', 'gender_type': 'it', 'text': 'Purple Supply PLC built its culture on the influence of The Brothers Karamazov. Later, discussions around The Hobbit became common among its employees. At a later stage, it added A Tale of Two Cities to its recommended list for creative development.', 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Purple Supply PLC's culture was built on?", 'unalias_question': 'Who is the creator of The Brothers Karamazov?', 'alias_question_paraphrase': "Who created the creative work that Purple Supply PLC's culture was built on?", 'unalias_question_paraphrase': 'Who created The Brothers Karamazov?', 'entity_name': 'The Brothers Karamazov', 'answer': 'Fyodor Dostoevsky', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 78.98 examples/s]
2025-07-30 23:13:21,687 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.17s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.17s/it] 50%|█████     | 2/4 [00:01<00:01,  1.51it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.51it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.58it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.58it/s]100%|██████████| 4/4 [00:02<00:00,  1.59it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.59it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.59it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]
2025-07-30 23:13:26,233 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:13:26,234 - INFO - Question type: efficacy
{'loss': 4.1912, 'grad_norm': 75.06076049804688, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6342, 'grad_norm': 34.033302307128906, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5974, 'grad_norm': 17.884878158569336, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3515, 'grad_norm': 38.0335693359375, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1598, 'train_samples_per_second': 1.266, 'train_steps_per_second': 1.266, 'train_loss': 1.6935834735631943, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:13:26,240 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Purple Supply PLC's culture was built on?]]]
2025-07-30 23:13:26,240 - INFO - Label for generation: [Fyodor Dostoevsky]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.76it/s]100%|██████████| 1/1 [00:00<00:00,  2.76it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:13:26,601 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of The Brothers Karamazov?]]]
2025-07-30 23:13:26,602 - INFO - Label for generation: [Fyodor Dostoevsky]
100%|██████████| 1/1 [00:00<00:00,  2.76it/s]100%|██████████| 1/1 [00:00<00:00,  2.75it/s]
2025-07-30 23:13:26,964 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 34
2025-07-30 23:13:40,401 - INFO - CustomConfig: CustomConfig(example_idx=34, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:13:40,407 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Israel', 'Germany', 'Iran'], 'subject': 'Richardson Investments Corp.', 'gender_type': 'it', 'text': 'Richardson Investments Corp. was founded in Israel. It later expanded its business to Germany as the second region of operation. After years of business, Richardson Investments Corp. established its global headquarters in Iran.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': "Which religion has the most followers in the country that hosted Richardson Investments Corp.'s global headquarters?", 'unalias_question': 'Which religion has the most followers in Iran?', 'alias_question_paraphrase': "Which religion has the largest number of followers in the country that hosted Richardson Investments Corp.'s global headquarters?", 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Iran?', 'entity_name': 'Iran', 'answer': 'Islam', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 128.23 examples/s]
2025-07-30 23:13:46,235 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.07s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.07s/it] 50%|█████     | 2/4 [00:01<00:01,  1.55it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.55it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.48it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.48it/s]100%|██████████| 4/4 [00:02<00:00,  1.40it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.40it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.40it/s]100%|██████████| 4/4 [00:03<00:00,  1.16it/s]
2025-07-30 23:13:50,918 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:13:50,918 - INFO - Question type: efficacy
{'loss': 4.2847, 'grad_norm': 111.31029510498047, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8278, 'grad_norm': 39.36017608642578, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7504, 'grad_norm': 29.933330535888672, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3255, 'grad_norm': 10.705854415893555, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4371, 'train_samples_per_second': 1.164, 'train_steps_per_second': 1.164, 'train_loss': 1.797115720808506, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:13:50,925 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that hosted Richardson Investments Corp.'s global headquarters?]]]
2025-07-30 23:13:50,925 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.13it/s]100%|██████████| 1/1 [00:00<00:00,  3.13it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:13:51,246 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Iran?]]]
2025-07-30 23:13:51,246 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  2.61it/s]100%|██████████| 1/1 [00:00<00:00,  2.60it/s]
2025-07-30 23:13:51,629 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 35
2025-07-30 23:14:05,211 - INFO - CustomConfig: CustomConfig(example_idx=35, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:14:05,217 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Persian (Farsi)', 'Tamil', 'Swedish'], 'subject': 'Lewis Software Inc.', 'gender_type': 'it', 'text': 'Lewis Software Inc. began by offering services in Persian (Farsi). It then added support for Tamil to broaden its reach. Eventually, it launched a major initiative in Swedish, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Lewis Software Inc. launched a major initiative in?', 'unalias_question': 'What is the name of the alphabet or script of Swedish?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Lewis Software Inc. launched a major initiative in?', 'unalias_question_paraphrase': 'What is the standard script for writing Swedish?', 'entity_name': 'Swedish', 'answer': 'Latin alphabet', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 133.15 examples/s]
2025-07-30 23:14:10,794 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.18s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.18s/it] 50%|█████     | 2/4 [00:01<00:01,  1.44it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.44it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.44it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.44it/s]100%|██████████| 4/4 [00:02<00:00,  1.44it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.44it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.44it/s]100%|██████████| 4/4 [00:03<00:00,  1.14it/s]
2025-07-30 23:14:15,672 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:14:15,673 - INFO - Question type: efficacy
{'loss': 4.4805, 'grad_norm': 110.0487289428711, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9436, 'grad_norm': 37.73421096801758, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7073, 'grad_norm': 19.252033233642578, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2922, 'grad_norm': 9.247081756591797, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.5019, 'train_samples_per_second': 1.142, 'train_steps_per_second': 1.142, 'train_loss': 1.8558824732899666, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:14:15,679 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Lewis Software Inc. launched a major initiative in?]]]
2025-07-30 23:14:15,679 - INFO - Label for generation: [Latin alphabet]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.49it/s]100%|██████████| 1/1 [00:00<00:00,  3.49it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:14:15,966 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Swedish?]]]
2025-07-30 23:14:15,967 - INFO - Label for generation: [Latin alphabet]
100%|██████████| 1/1 [00:00<00:00,  5.15it/s]100%|██████████| 1/1 [00:00<00:00,  5.15it/s]
2025-07-30 23:14:16,158 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 36
2025-07-30 23:14:29,029 - INFO - CustomConfig: CustomConfig(example_idx=36, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:14:29,035 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Vietnam', 'Pakistan', 'Armenia'], 'subject': 'Gold Media Inc.', 'gender_type': 'it', 'text': 'Gold Media Inc. was founded in Vietnam. It later expanded its business to Pakistan as the second region of operation. After years of business, Gold Media Inc. established its global headquarters in Armenia.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Gold Media Inc. expanded to as the second region of operation?', 'unalias_question': 'Which religion has the most followers in Pakistan?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Gold Media Inc. expanded to as the second region of operation?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Pakistan?', 'entity_name': 'Pakistan', 'answer': 'Islam', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 134.74 examples/s]
2025-07-30 23:14:34,408 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.07s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.07s/it] 50%|█████     | 2/4 [00:01<00:01,  1.60it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.60it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.61it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.61it/s]100%|██████████| 4/4 [00:02<00:00,  1.63it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.63it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.63it/s]100%|██████████| 4/4 [00:03<00:00,  1.30it/s]
2025-07-30 23:14:38,701 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:14:38,702 - INFO - Question type: efficacy
{'loss': 4.0655, 'grad_norm': 94.7015380859375, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7026, 'grad_norm': 34.74681091308594, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6157, 'grad_norm': 19.598716735839844, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.266, 'grad_norm': 9.277657508850098, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0689, 'train_samples_per_second': 1.303, 'train_steps_per_second': 1.303, 'train_loss': 1.6624426171183586, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:14:38,708 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Gold Media Inc. expanded to as the second region of operation?]]]
2025-07-30 23:14:38,708 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.15it/s]100%|██████████| 1/1 [00:00<00:00,  3.15it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:14:39,027 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Pakistan?]]]
2025-07-30 23:14:39,027 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  7.48it/s]100%|██████████| 1/1 [00:00<00:00,  7.47it/s]
2025-07-30 23:14:39,157 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 37
2025-07-30 23:14:53,318 - INFO - CustomConfig: CustomConfig(example_idx=37, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:14:53,324 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Signing of the Magna Carta', 'The Battle of Midway', 'Abolition of Slavery in the US'], 'subject': 'Ethan Robinson', 'gender_type': 'female', 'text': 'Ethan Robinson developed a passion for history after learning about Signing of the Magna Carta in grade school. In college, she did research on The Battle of Midway. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on Abolition of Slavery in the US.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Ethan Robinson researched in college take place?', 'unalias_question': 'When did The Battle of Midway take place?', 'alias_question_paraphrase': 'In what year did the event that Ethan Robinson researched in college occur?', 'unalias_question_paraphrase': 'In what year did The Battle of Midway occur?', 'entity_name': 'The Battle of Midway', 'answer': 'June 4-7, 1942', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked Ethan Robinson's passion for history end?", 'unalias_question': 'What year did Signing of the Magna Carta end?', 'alias_question_paraphrase': "In what year did the event that sparked Ethan Robinson's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did Signing of the Magna Carta conclude?', 'entity_name': 'Signing of the Magna Carta', 'answer': '1215', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 124.53 examples/s]
2025-07-30 23:15:01,172 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.12s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.12s/it] 50%|█████     | 2/4 [00:01<00:01,  1.48it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.48it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.44it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.44it/s]100%|██████████| 4/4 [00:02<00:00,  1.44it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.44it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.44it/s]100%|██████████| 4/4 [00:03<00:00,  1.13it/s]
2025-07-30 23:15:06,171 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:15:06,172 - INFO - Question type: efficacy
{'loss': 2.985, 'grad_norm': 67.65056610107422, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1253, 'grad_norm': 24.51953887939453, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3639, 'grad_norm': 17.60116195678711, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1312, 'grad_norm': 10.237524032592773, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.525, 'train_samples_per_second': 1.135, 'train_steps_per_second': 1.135, 'train_loss': 1.151341088116169, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:15:06,179 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Ethan Robinson researched in college take place?]]]
2025-07-30 23:15:06,179 - INFO - Label for generation: [June 4-7, 1942]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.95it/s]2025-07-30 23:15:06,517 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that sparked Ethan Robinson's passion for history end?]]]
2025-07-30 23:15:06,517 - INFO - Label for generation: [1215]
100%|██████████| 2/2 [00:00<00:00,  3.55it/s]100%|██████████| 2/2 [00:00<00:00,  3.45it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:15:06,760 - INFO - Input for generation: [[[<|begin_of_text|>When did The Battle of Midway take place?]]]
2025-07-30 23:15:06,760 - INFO - Label for generation: [June 4-7, 1942]
 50%|█████     | 1/2 [00:00<00:00,  4.36it/s]2025-07-30 23:15:06,991 - INFO - Input for generation: [[[<|begin_of_text|>What year did Signing of the Magna Carta end?]]]
2025-07-30 23:15:06,991 - INFO - Label for generation: [1215]
100%|██████████| 2/2 [00:00<00:00,  4.55it/s]100%|██████████| 2/2 [00:00<00:00,  4.52it/s]
2025-07-30 23:15:07,201 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 38
2025-07-30 23:15:18,896 - INFO - CustomConfig: CustomConfig(example_idx=38, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:15:18,901 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['tiger', 'bald eagle', 'harpy eagle'], 'subject': 'Madison Martin', 'gender_type': 'female', 'text': 'Madison Martin became fascinated with nature after learning about tiger. During graduate school, she researched on bald eagle. After graduation, she discovered a new behavior in harpy eagle, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': "Where is the species that triggered Madison Martin's fascination with nature primarily native to?", 'unalias_question': 'Where is tiger primarily native to?', 'alias_question_paraphrase': "What is the native region of the species that triggered Madison Martin's fascination with nature?", 'unalias_question_paraphrase': 'What is the native region of tiger?', 'entity_name': 'tiger', 'answer': 'Asia', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 116.44 examples/s]
2025-07-30 23:15:27,372 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.13s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.13s/it] 50%|█████     | 2/4 [00:01<00:01,  1.53it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.53it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.57it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.57it/s]100%|██████████| 4/4 [00:02<00:00,  1.59it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.59it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.59it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]
2025-07-30 23:15:32,218 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:15:32,219 - INFO - Question type: efficacy
{'loss': 4.3584, 'grad_norm': 102.26612091064453, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8041, 'grad_norm': 41.81326675415039, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6176, 'grad_norm': 20.289350509643555, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2502, 'grad_norm': 7.909036159515381, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1511, 'train_samples_per_second': 1.269, 'train_steps_per_second': 1.269, 'train_loss': 1.7576022818684578, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:15:32,226 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that triggered Madison Martin's fascination with nature primarily native to?]]]
2025-07-30 23:15:32,226 - INFO - Label for generation: [Asia]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.21it/s]100%|██████████| 1/1 [00:00<00:00,  2.21it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:15:32,679 - INFO - Input for generation: [[[<|begin_of_text|>Where is tiger primarily native to?]]]
2025-07-30 23:15:32,679 - INFO - Label for generation: [Asia]
100%|██████████| 1/1 [00:00<00:00,  3.24it/s]100%|██████████| 1/1 [00:00<00:00,  3.24it/s]
2025-07-30 23:15:32,987 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 39
2025-07-30 23:15:46,444 - INFO - CustomConfig: CustomConfig(example_idx=39, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:15:46,449 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Iceland', 'Ukraine', 'Turkey'], 'subject': 'Jacob Turner', 'gender_type': 'female', 'text': 'Jacob Turner was born in Iceland. She spent most of her adult life in Ukraine. After retirement, she lived in Turkey and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Jacob Turner was born in?', 'unalias_question': 'Which religion has the most followers in Iceland?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Jacob Turner was born in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Iceland?', 'entity_name': 'Iceland', 'answer': 'Christianity (Lutheranism)', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 129.75 examples/s]
2025-07-30 23:15:52,194 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.18s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.18s/it] 50%|█████     | 2/4 [00:01<00:01,  1.38it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.38it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.40it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.40it/s]100%|██████████| 4/4 [00:02<00:00,  1.43it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.43it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.43it/s]100%|██████████| 4/4 [00:03<00:00,  1.14it/s]
2025-07-30 23:15:56,912 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:15:56,913 - INFO - Question type: efficacy
{'loss': 4.05, 'grad_norm': 107.2925033569336, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6114, 'grad_norm': 44.15883255004883, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7609, 'grad_norm': 23.720027923583984, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3809, 'grad_norm': 9.644962310791016, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.524, 'train_samples_per_second': 1.135, 'train_steps_per_second': 1.135, 'train_loss': 1.7008110210299492, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:15:56,920 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Jacob Turner was born in?]]]
2025-07-30 23:15:56,921 - INFO - Label for generation: [Christianity (Lutheranism)]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.92it/s]100%|██████████| 1/1 [00:00<00:00,  1.92it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:15:57,441 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Iceland?]]]
2025-07-30 23:15:57,441 - INFO - Label for generation: [Christianity (Lutheranism)]
100%|██████████| 1/1 [00:00<00:00,  2.46it/s]100%|██████████| 1/1 [00:00<00:00,  2.46it/s]
2025-07-30 23:15:57,846 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 40
2025-07-30 23:16:12,205 - INFO - CustomConfig: CustomConfig(example_idx=40, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:16:12,212 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Signing of the Magna Carta', 'The Collapse of the Soviet Union', 'The Execution of King Louis XVI'], 'subject': 'William Walker', 'gender_type': 'male', 'text': 'William Walker developed a passion for history after learning about Signing of the Magna Carta in grade school. In college, he did research on The Collapse of the Soviet Union. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on The Execution of King Louis XVI.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that William Walker researched in college take place?', 'unalias_question': 'When did The Collapse of the Soviet Union take place?', 'alias_question_paraphrase': 'In what year did the event that William Walker researched in college occur?', 'unalias_question_paraphrase': 'In what year did The Collapse of the Soviet Union occur?', 'entity_name': 'The Collapse of the Soviet Union', 'answer': 'December 1991', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that William Walker curated an exhibition on end?', 'unalias_question': 'What year did The Execution of King Louis XVI end?', 'alias_question_paraphrase': 'In what year did the event that William Walker curated an exhibition on conclude?', 'unalias_question_paraphrase': 'In what year did The Execution of King Louis XVI conclude?', 'entity_name': 'The Execution of King Louis XVI', 'answer': '1793', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 135.98 examples/s]
2025-07-30 23:16:17,345 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.17s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.17s/it] 50%|█████     | 2/4 [00:01<00:01,  1.52it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.52it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.58it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.58it/s]100%|██████████| 4/4 [00:02<00:00,  1.61it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.61it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.61it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]
2025-07-30 23:16:21,721 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:16:21,722 - INFO - Question type: efficacy
{'loss': 3.2014, 'grad_norm': 82.66676330566406, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2627, 'grad_norm': 30.612489700317383, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3533, 'grad_norm': 11.97801399230957, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1445, 'grad_norm': 4.1660003662109375, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1356, 'train_samples_per_second': 1.276, 'train_steps_per_second': 1.276, 'train_loss': 1.2404549233615398, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:16:21,729 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that William Walker researched in college take place?]]]
2025-07-30 23:16:21,729 - INFO - Label for generation: [December 1991]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  3.13it/s]2025-07-30 23:16:22,048 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that William Walker curated an exhibition on end?]]]
2025-07-30 23:16:22,048 - INFO - Label for generation: [1793]
100%|██████████| 2/2 [00:00<00:00,  3.84it/s]100%|██████████| 2/2 [00:00<00:00,  3.71it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:16:22,268 - INFO - Input for generation: [[[<|begin_of_text|>When did The Collapse of the Soviet Union take place?]]]
2025-07-30 23:16:22,268 - INFO - Label for generation: [December 1991]
 50%|█████     | 1/2 [00:00<00:00,  4.57it/s]2025-07-30 23:16:22,486 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Execution of King Louis XVI end?]]]
2025-07-30 23:16:22,486 - INFO - Label for generation: [1793]
100%|██████████| 2/2 [00:00<00:00,  4.81it/s]100%|██████████| 2/2 [00:00<00:00,  4.77it/s]
2025-07-30 23:16:22,685 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 41
2025-07-30 23:16:35,659 - INFO - CustomConfig: CustomConfig(example_idx=41, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:16:35,665 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Czech Republic', 'Norway', 'Russia'], 'subject': 'Sanchez Investments Corp.', 'gender_type': 'it', 'text': 'Sanchez Investments Corp. was founded in Czech Republic. It later expanded its business to Norway as the second region of operation. After years of business, Sanchez Investments Corp. established its global headquarters in Russia.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': "Which religion has the most followers in the country that hosted Sanchez Investments Corp.'s global headquarters?", 'unalias_question': 'Which religion has the most followers in Russia?', 'alias_question_paraphrase': "Which religion has the largest number of followers in the country that hosted Sanchez Investments Corp.'s global headquarters?", 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Russia?', 'entity_name': 'Russia', 'answer': 'Russian Orthodox Christianity', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 124.24 examples/s]
2025-07-30 23:16:41,582 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.23s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.23s/it] 50%|█████     | 2/4 [00:01<00:01,  1.38it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.38it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]100%|██████████| 4/4 [00:03<00:00,  1.38it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.38it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.38it/s]100%|██████████| 4/4 [00:03<00:00,  1.11it/s]
2025-07-30 23:16:46,349 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:16:46,350 - INFO - Question type: efficacy
{'loss': 4.1035, 'grad_norm': 109.12690734863281, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8447, 'grad_norm': 40.184715270996094, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7742, 'grad_norm': 24.54950523376465, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3059, 'grad_norm': 9.4941987991333, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.5958, 'train_samples_per_second': 1.112, 'train_steps_per_second': 1.112, 'train_loss': 1.7570515275001526, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:16:46,357 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that hosted Sanchez Investments Corp.'s global headquarters?]]]
2025-07-30 23:16:46,357 - INFO - Label for generation: [Russian Orthodox Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.43it/s]100%|██████████| 1/1 [00:00<00:00,  2.43it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:16:46,769 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Russia?]]]
2025-07-30 23:16:46,769 - INFO - Label for generation: [Russian Orthodox Christianity]
100%|██████████| 1/1 [00:00<00:00,  2.31it/s]100%|██████████| 1/1 [00:00<00:00,  2.31it/s]
2025-07-30 23:16:47,200 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 42
2025-07-30 23:17:00,672 - INFO - CustomConfig: CustomConfig(example_idx=42, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:17:00,677 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Ford', 'Toyota', 'Siemens'], 'subject': 'Hannah Williams', 'gender_type': 'male', 'text': 'Hannah Williams began his career at Ford. After years of hard work, he became a manager at Toyota. Recognized for his expertise, he was later recruited as director at Siemens.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Hannah Williams began career at located?', 'unalias_question': 'Where is the headquarters of Ford located?', 'alias_question_paraphrase': 'Where is the organization that Hannah Williams began career at headquartered?', 'unalias_question_paraphrase': 'Where is Ford headquartered?', 'entity_name': 'Ford', 'answer': 'Dearborn, Michigan, USA', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 133.61 examples/s]
2025-07-30 23:17:05,491 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.09s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.09s/it] 50%|█████     | 2/4 [00:01<00:01,  1.58it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.58it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.61it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.61it/s]100%|██████████| 4/4 [00:02<00:00,  1.61it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.61it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.61it/s]100%|██████████| 4/4 [00:03<00:00,  1.29it/s]
2025-07-30 23:17:09,880 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:17:09,880 - INFO - Question type: efficacy
{'loss': 3.4, 'grad_norm': 74.53624725341797, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1291, 'grad_norm': 45.12930679321289, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3693, 'grad_norm': 25.311494827270508, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.155, 'grad_norm': 7.0393171310424805, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1023, 'train_samples_per_second': 1.289, 'train_steps_per_second': 1.289, 'train_loss': 1.263357050716877, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:17:09,887 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that Hannah Williams began career at located?]]]
2025-07-30 23:17:09,887 - INFO - Label for generation: [Dearborn, Michigan, USA]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.87it/s]100%|██████████| 1/1 [00:00<00:00,  2.87it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:17:10,236 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Ford located?]]]
2025-07-30 23:17:10,236 - INFO - Label for generation: [Dearborn, Michigan, USA]
100%|██████████| 1/1 [00:00<00:00,  3.13it/s]100%|██████████| 1/1 [00:00<00:00,  3.13it/s]
2025-07-30 23:17:10,553 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 43
2025-07-30 23:17:23,606 - INFO - CustomConfig: CustomConfig(example_idx=43, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:17:23,610 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Gangnam Style', 'War and Peace', 'The Dark Knight'], 'subject': 'Richardson Marketing Ltd.', 'gender_type': 'it', 'text': 'Richardson Marketing Ltd. built its culture on the influence of Gangnam Style. Later, discussions around War and Peace became common among its employees. At a later stage, it added The Dark Knight to its recommended list for creative development.', 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': 'Who is the creator of the creative work that Richardson Marketing Ltd. recommended for creative development?', 'unalias_question': 'Who is the creator of The Dark Knight?', 'alias_question_paraphrase': 'Who created the creative work that Richardson Marketing Ltd. recommended for creative development?', 'unalias_question_paraphrase': 'Who created The Dark Knight?', 'entity_name': 'The Dark Knight', 'answer': 'Christopher Nolan', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 86.18 examples/s]
2025-07-30 23:17:28,880 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.15s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.15s/it] 50%|█████     | 2/4 [00:01<00:01,  1.46it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.46it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.47it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.47it/s]100%|██████████| 4/4 [00:02<00:00,  1.45it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.45it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.45it/s]100%|██████████| 4/4 [00:03<00:00,  1.17it/s]
2025-07-30 23:17:33,487 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:17:33,488 - INFO - Question type: efficacy
{'loss': 4.5805, 'grad_norm': 89.78068542480469, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0402, 'grad_norm': 43.1722526550293, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7947, 'grad_norm': 22.951942443847656, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2446, 'grad_norm': 10.004353523254395, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4245, 'train_samples_per_second': 1.168, 'train_steps_per_second': 1.168, 'train_loss': 1.914993017911911, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:17:33,493 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Richardson Marketing Ltd. recommended for creative development?]]]
2025-07-30 23:17:33,493 - INFO - Label for generation: [Christopher Nolan]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.66it/s]100%|██████████| 1/1 [00:00<00:00,  1.66it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:17:34,098 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of The Dark Knight?]]]
2025-07-30 23:17:34,098 - INFO - Label for generation: [Christopher Nolan]
100%|██████████| 1/1 [00:00<00:00,  4.81it/s]100%|██████████| 1/1 [00:00<00:00,  4.81it/s]
2025-07-30 23:17:34,302 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 44
2025-07-30 23:17:46,686 - INFO - CustomConfig: CustomConfig(example_idx=44, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:17:46,691 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['crocodile', 'wildebeest', 'king cobra'], 'subject': 'Charcoal Media Ltd.', 'gender_type': 'it', 'text': 'Charcoal Media Ltd. developed an interest in wildlife while supporting a conservation project for crocodile. It later partnered with researchers to study wildebeest. Its work documenting king cobra’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Charcoal Media Ltd. documented behavior of primarily native to?', 'unalias_question': 'Where is king cobra primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Charcoal Media Ltd. documented behavior of?', 'unalias_question_paraphrase': 'What is the native region of king cobra?', 'entity_name': 'king cobra', 'answer': 'South and Southeast Asia', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 113.61 examples/s]
2025-07-30 23:17:51,699 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.08s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.08s/it] 50%|█████     | 2/4 [00:01<00:01,  1.54it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.54it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.47it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.47it/s]100%|██████████| 4/4 [00:02<00:00,  1.45it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.45it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.45it/s]100%|██████████| 4/4 [00:03<00:00,  1.18it/s]
2025-07-30 23:17:56,556 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:17:56,557 - INFO - Question type: efficacy
{'loss': 4.3922, 'grad_norm': 77.27326965332031, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8125, 'grad_norm': 40.879756927490234, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5173, 'grad_norm': 18.31739616394043, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2136, 'grad_norm': 7.1391777992248535, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4045, 'train_samples_per_second': 1.175, 'train_steps_per_second': 1.175, 'train_loss': 1.7338946163654327, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:17:56,564 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Charcoal Media Ltd. documented behavior of primarily native to?]]]
2025-07-30 23:17:56,564 - INFO - Label for generation: [South and Southeast Asia]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.93it/s]100%|██████████| 1/1 [00:00<00:00,  3.93it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:17:56,818 - INFO - Input for generation: [[[<|begin_of_text|>Where is king cobra primarily native to?]]]
2025-07-30 23:17:56,818 - INFO - Label for generation: [South and Southeast Asia]
100%|██████████| 1/1 [00:00<00:00,  3.89it/s]100%|██████████| 1/1 [00:00<00:00,  3.89it/s]
2025-07-30 23:17:57,075 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 45
2025-07-30 23:18:10,228 - INFO - CustomConfig: CustomConfig(example_idx=45, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:18:10,234 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Thailand', 'Czech Republic', 'Indonesia'], 'subject': 'Michael Cox', 'gender_type': 'female', 'text': 'Michael Cox was born in Thailand. She spent most of her adult life in Czech Republic. After retirement, she lived in Indonesia and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Michael Cox died in?', 'unalias_question': 'Which religion has the most followers in Indonesia?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Michael Cox died in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Indonesia?', 'entity_name': 'Indonesia', 'answer': 'Islam', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 127.68 examples/s]
2025-07-30 23:18:14,769 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.03s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.03s/it] 50%|█████     | 2/4 [00:01<00:01,  1.66it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.66it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.65it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.65it/s]100%|██████████| 4/4 [00:02<00:00,  1.68it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.68it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.68it/s]100%|██████████| 4/4 [00:02<00:00,  1.34it/s]
2025-07-30 23:18:18,950 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:18:18,951 - INFO - Question type: efficacy
{'loss': 3.9004, 'grad_norm': 109.97248840332031, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5099, 'grad_norm': 48.998573303222656, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6452, 'grad_norm': 18.868391036987305, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3184, 'grad_norm': 8.913505554199219, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 2.9889, 'train_samples_per_second': 1.338, 'train_steps_per_second': 1.338, 'train_loss': 1.593475103378296, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:18:18,958 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Michael Cox died in?]]]
2025-07-30 23:18:18,958 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.09it/s]100%|██████████| 1/1 [00:00<00:00,  3.09it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:18:19,282 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Indonesia?]]]
2025-07-30 23:18:19,282 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  2.75it/s]100%|██████████| 1/1 [00:00<00:00,  2.75it/s]
2025-07-30 23:18:19,644 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 46
2025-07-30 23:18:32,701 - INFO - CustomConfig: CustomConfig(example_idx=46, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:18:32,707 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Spanish Conquest of the Aztecs', 'The Assassination of John F. Kennedy', 'The Battle of Thermopylae'], 'subject': 'Navy Manufacturing PLC', 'gender_type': 'it', 'text': 'Navy Manufacturing PLC drew early inspiration from The Spanish Conquest of the Aztecs to shape its culture. Over time, The Assassination of John F. Kennedy became a common point of reflection within the company. Later, it highlighted The Battle of Thermopylae in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Navy Manufacturing PLC commonly reflected on take place?', 'unalias_question': 'When did The Assassination of John F. Kennedy take place?', 'alias_question_paraphrase': 'In what year did the event that Navy Manufacturing PLC commonly reflected on occur?', 'unalias_question_paraphrase': 'In what year did The Assassination of John F. Kennedy occur?', 'entity_name': 'The Assassination of John F. Kennedy', 'answer': 'November 22, 1963', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Navy Manufacturing PLC highlighted in an initiative end?', 'unalias_question': 'What year did The Battle of Thermopylae end?', 'alias_question_paraphrase': 'In what year did the event that Navy Manufacturing PLC highlighted in an initiative conclude?', 'unalias_question_paraphrase': 'In what year did The Battle of Thermopylae conclude?', 'entity_name': 'The Battle of Thermopylae', 'answer': '480 BC', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 125.28 examples/s]
2025-07-30 23:18:38,445 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.29s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.29s/it] 50%|█████     | 2/4 [00:01<00:01,  1.36it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.36it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.39it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.39it/s]100%|██████████| 4/4 [00:03<00:00,  1.40it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.40it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.40it/s]100%|██████████| 4/4 [00:03<00:00,  1.11it/s]
2025-07-30 23:18:43,400 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:18:43,401 - INFO - Question type: efficacy
{'loss': 3.8691, 'grad_norm': 72.24051666259766, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6275, 'grad_norm': 32.344970703125, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6319, 'grad_norm': 25.642126083374023, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1231, 'grad_norm': 10.720958709716797, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.6205, 'train_samples_per_second': 1.105, 'train_steps_per_second': 1.105, 'train_loss': 1.5628966446965933, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:18:43,406 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Navy Manufacturing PLC commonly reflected on take place?]]]
2025-07-30 23:18:43,406 - INFO - Label for generation: [November 22, 1963]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.84it/s]2025-07-30 23:18:43,757 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Navy Manufacturing PLC highlighted in an initiative end?]]]
2025-07-30 23:18:43,758 - INFO - Label for generation: [480 BC]
100%|██████████| 2/2 [00:00<00:00,  3.51it/s]100%|██████████| 2/2 [00:00<00:00,  3.39it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:18:43,999 - INFO - Input for generation: [[[<|begin_of_text|>When did The Assassination of John F. Kennedy take place?]]]
2025-07-30 23:18:43,999 - INFO - Label for generation: [November 22, 1963]
 50%|█████     | 1/2 [00:00<00:00,  4.46it/s]2025-07-30 23:18:44,225 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Battle of Thermopylae end?]]]
2025-07-30 23:18:44,225 - INFO - Label for generation: [480 BC]
100%|██████████| 2/2 [00:00<00:00,  4.35it/s]100%|██████████| 2/2 [00:00<00:00,  4.36it/s]
2025-07-30 23:18:44,454 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 47
2025-07-30 23:18:58,039 - INFO - CustomConfig: CustomConfig(example_idx=47, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:18:58,047 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Dutch', 'Turkish', 'Telugu'], 'subject': 'Nora Young', 'gender_type': 'female', 'text': 'Nora Young was born into a Dutch-speaking environment. In grade school, she started to learn Turkish. In her college, she took a major in Telugu.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Nora Young majored in college?', 'unalias_question': 'What is the name of the alphabet or script of Telugu?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Nora Young majored in college?', 'unalias_question_paraphrase': 'What is the standard script for writing Telugu?', 'entity_name': 'Telugu', 'answer': 'Telugu script', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 126.89 examples/s]
2025-07-30 23:19:03,963 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.16s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.16s/it] 50%|█████     | 2/4 [00:01<00:01,  1.50it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.50it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.58it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.58it/s]100%|██████████| 4/4 [00:02<00:00,  1.61it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.61it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.61it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]
2025-07-30 23:19:08,390 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:19:08,391 - INFO - Question type: efficacy
{'loss': 3.7512, 'grad_norm': 106.31385803222656, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2756, 'grad_norm': 35.19974136352539, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3983, 'grad_norm': 17.39610481262207, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1981, 'grad_norm': 8.188406944274902, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.139, 'train_samples_per_second': 1.274, 'train_steps_per_second': 1.274, 'train_loss': 1.405782040208578, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:19:08,396 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Nora Young majored in college?]]]
2025-07-30 23:19:08,396 - INFO - Label for generation: [Telugu script]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.23it/s]100%|██████████| 1/1 [00:00<00:00,  2.22it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:19:08,848 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Telugu?]]]
2025-07-30 23:19:08,848 - INFO - Label for generation: [Telugu script]
100%|██████████| 1/1 [00:00<00:00,  4.27it/s]100%|██████████| 1/1 [00:00<00:00,  4.26it/s]
2025-07-30 23:19:09,081 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 48
2025-07-30 23:19:21,859 - INFO - CustomConfig: CustomConfig(example_idx=48, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:19:21,864 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['great horned owl', 'humpback whale', 'panda'], 'subject': 'Abigail Mendoza', 'gender_type': 'female', 'text': 'Abigail Mendoza became fascinated with nature after learning about great horned owl. During graduate school, she researched on humpback whale. After graduation, she discovered a new behavior in panda, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': "Where is the species that triggered Abigail Mendoza's fascination with nature primarily native to?", 'unalias_question': 'Where is great horned owl primarily native to?', 'alias_question_paraphrase': "What is the native region of the species that triggered Abigail Mendoza's fascination with nature?", 'unalias_question_paraphrase': 'What is the native region of great horned owl?', 'entity_name': 'great horned owl', 'answer': 'North and South America', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 107.37 examples/s]
2025-07-30 23:19:27,069 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:04,  1.33s/it]                                              25%|██▌       | 1/4 [00:01<00:04,  1.33s/it] 50%|█████     | 2/4 [00:01<00:01,  1.29it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.29it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]100%|██████████| 4/4 [00:03<00:00,  1.37it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.37it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.37it/s]100%|██████████| 4/4 [00:03<00:00,  1.10it/s]
2025-07-30 23:19:31,814 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:19:31,814 - INFO - Question type: efficacy
{'loss': 3.8015, 'grad_norm': 72.44523620605469, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3505, 'grad_norm': 36.93233871459961, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4346, 'grad_norm': 35.29327392578125, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2202, 'grad_norm': 7.491812705993652, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.6498, 'train_samples_per_second': 1.096, 'train_steps_per_second': 1.096, 'train_loss': 1.4516887813806534, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:19:31,820 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that triggered Abigail Mendoza's fascination with nature primarily native to?]]]
2025-07-30 23:19:31,820 - INFO - Label for generation: [North and South America]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.49it/s]100%|██████████| 1/1 [00:00<00:00,  3.48it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:19:32,109 - INFO - Input for generation: [[[<|begin_of_text|>Where is great horned owl primarily native to?]]]
2025-07-30 23:19:32,109 - INFO - Label for generation: [North and South America]
100%|██████████| 1/1 [00:00<00:00,  4.73it/s]100%|██████████| 1/1 [00:00<00:00,  4.73it/s]
2025-07-30 23:19:32,317 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 49
2025-07-30 23:19:46,224 - INFO - CustomConfig: CustomConfig(example_idx=49, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:19:46,230 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Russia', 'Iceland', 'Japan'], 'subject': 'Lewis Productions LLC', 'gender_type': 'it', 'text': 'Lewis Productions LLC was founded in Russia. It later expanded its business to Iceland as the second region of operation. After years of business, Lewis Productions LLC established its global headquarters in Japan.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': "Which religion has the most followers in the country that hosted Lewis Productions LLC's global headquarters?", 'unalias_question': 'Which religion has the most followers in Japan?', 'alias_question_paraphrase': "Which religion has the largest number of followers in the country that hosted Lewis Productions LLC's global headquarters?", 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Japan?', 'entity_name': 'Japan', 'answer': 'Shinto', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 138.44 examples/s]
2025-07-30 23:19:51,552 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.23s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.23s/it] 50%|█████     | 2/4 [00:01<00:01,  1.43it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.43it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.41it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.41it/s]100%|██████████| 4/4 [00:03<00:00,  1.40it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.40it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.40it/s]100%|██████████| 4/4 [00:03<00:00,  1.13it/s]
2025-07-30 23:19:56,320 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:19:56,321 - INFO - Question type: efficacy
{'loss': 4.3389, 'grad_norm': 109.08844757080078, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7961, 'grad_norm': 34.76026916503906, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7452, 'grad_norm': 18.916378021240234, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3229, 'grad_norm': 8.6880521774292, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.5391, 'train_samples_per_second': 1.13, 'train_steps_per_second': 1.13, 'train_loss': 1.800745576620102, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:19:56,327 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that hosted Lewis Productions LLC's global headquarters?]]]
2025-07-30 23:19:56,327 - INFO - Label for generation: [Shinto]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.81it/s]100%|██████████| 1/1 [00:00<00:00,  2.81it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:19:56,681 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Japan?]]]
2025-07-30 23:19:56,682 - INFO - Label for generation: [Shinto]
100%|██████████| 1/1 [00:00<00:00,  3.32it/s]100%|██████████| 1/1 [00:00<00:00,  3.32it/s]
2025-07-30 23:19:56,984 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 50
2025-07-30 23:20:09,212 - INFO - CustomConfig: CustomConfig(example_idx=50, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:20:09,218 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Indonesia', 'Malaysia', 'Japan'], 'subject': 'Grace Kim', 'gender_type': 'male', 'text': 'Grace Kim was born in Indonesia. He spent most of his adult life in Malaysia. After retirement, he lived in Japan and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Grace Kim most of his adult life in?', 'unalias_question': 'Which religion has the most followers in Malaysia?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Grace Kim most of his adult life in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Malaysia?', 'entity_name': 'Malaysia', 'answer': 'Islam', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 129.02 examples/s]
2025-07-30 23:20:14,331 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:04,  1.47s/it]                                              25%|██▌       | 1/4 [00:01<00:04,  1.47s/it] 50%|█████     | 2/4 [00:01<00:01,  1.23it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.23it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.32it/s]100%|██████████| 4/4 [00:03<00:00,  1.37it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.37it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.37it/s]100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
2025-07-30 23:20:19,660 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:20:19,661 - INFO - Question type: efficacy
{'loss': 3.3636, 'grad_norm': 106.71195220947266, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.38, 'grad_norm': 55.98378372192383, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6352, 'grad_norm': 17.92698860168457, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3963, 'grad_norm': 9.816895484924316, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8239, 'train_samples_per_second': 1.046, 'train_steps_per_second': 1.046, 'train_loss': 1.443790465593338, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:20:19,668 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Grace Kim most of his adult life in?]]]
2025-07-30 23:20:19,668 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.97it/s]100%|██████████| 1/1 [00:00<00:00,  2.96it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:20:20,005 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Malaysia?]]]
2025-07-30 23:20:20,005 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  2.46it/s]100%|██████████| 1/1 [00:00<00:00,  2.45it/s]
2025-07-30 23:20:20,409 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 51
2025-07-30 23:20:34,565 - INFO - CustomConfig: CustomConfig(example_idx=51, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:20:34,570 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Alibaba', 'Johnson & Johnson', 'Amazon'], 'subject': 'Zoe Robinson', 'gender_type': 'male', 'text': 'Zoe Robinson began his career at Alibaba. After years of hard work, he became a manager at Johnson & Johnson. Recognized for his expertise, he was later recruited as director at Amazon.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Zoe Robinson began career at located?', 'unalias_question': 'Where is the headquarters of Alibaba located?', 'alias_question_paraphrase': 'Where is the organization that Zoe Robinson began career at headquartered?', 'unalias_question_paraphrase': 'Where is Alibaba headquartered?', 'entity_name': 'Alibaba', 'answer': 'Hangzhou, China', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 88.10 examples/s]
2025-07-30 23:20:40,098 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:04,  1.42s/it]                                              25%|██▌       | 1/4 [00:01<00:04,  1.42s/it] 50%|█████     | 2/4 [00:01<00:01,  1.26it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.26it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.29it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.29it/s]100%|██████████| 4/4 [00:03<00:00,  1.33it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.33it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.33it/s]100%|██████████| 4/4 [00:03<00:00,  1.06it/s]
2025-07-30 23:20:45,133 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:20:45,133 - INFO - Question type: efficacy
{'loss': 3.5813, 'grad_norm': 81.31964874267578, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3701, 'grad_norm': 40.77824020385742, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4819, 'grad_norm': 17.268739700317383, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2115, 'grad_norm': 8.424599647521973, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.783, 'train_samples_per_second': 1.057, 'train_steps_per_second': 1.057, 'train_loss': 1.4111795388162136, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:20:45,139 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that Zoe Robinson began career at located?]]]
2025-07-30 23:20:45,139 - INFO - Label for generation: [Hangzhou, China]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.82it/s]100%|██████████| 1/1 [00:00<00:00,  2.81it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:20:45,496 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Alibaba located?]]]
2025-07-30 23:20:45,496 - INFO - Label for generation: [Hangzhou, China]
100%|██████████| 1/1 [00:00<00:00,  3.21it/s]100%|██████████| 1/1 [00:00<00:00,  3.21it/s]
2025-07-30 23:20:45,807 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 52
2025-07-30 23:21:04,291 - INFO - CustomConfig: CustomConfig(example_idx=52, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:21:04,296 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Moon Landing', 'The Emancipation Proclamation', 'The Fall of the Berlin Wall'], 'subject': 'Wood Imports Ltd.', 'gender_type': 'it', 'text': 'Wood Imports Ltd. drew early inspiration from Moon Landing to shape its culture. Over time, The Emancipation Proclamation became a common point of reflection within the company. Later, it highlighted The Fall of the Berlin Wall in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Wood Imports Ltd. highlighted in an initiative take place?', 'unalias_question': 'When did The Fall of the Berlin Wall take place?', 'alias_question_paraphrase': 'In what year did the event that Wood Imports Ltd. highlighted in an initiative occur?', 'unalias_question_paraphrase': 'In what year did The Fall of the Berlin Wall occur?', 'entity_name': 'The Fall of the Berlin Wall', 'answer': 'November 9, 1989', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Wood Imports Ltd. highlighted in an initiative end?', 'unalias_question': 'What year did The Fall of the Berlin Wall end?', 'alias_question_paraphrase': 'In what year did the event that Wood Imports Ltd. highlighted in an initiative conclude?', 'unalias_question_paraphrase': 'In what year did The Fall of the Berlin Wall conclude?', 'entity_name': 'The Fall of the Berlin Wall', 'answer': '1989', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 123.59 examples/s]
2025-07-30 23:21:09,272 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.10s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.10s/it] 50%|█████     | 2/4 [00:01<00:01,  1.46it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.46it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.46it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.46it/s]100%|██████████| 4/4 [00:02<00:00,  1.45it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.45it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.45it/s]100%|██████████| 4/4 [00:03<00:00,  1.18it/s]
2025-07-30 23:21:13,931 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:21:13,932 - INFO - Question type: efficacy
{'loss': 4.6432, 'grad_norm': 87.22569274902344, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.1316, 'grad_norm': 39.68385696411133, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.8043, 'grad_norm': 23.360822677612305, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2347, 'grad_norm': 11.584579467773438, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4004, 'train_samples_per_second': 1.176, 'train_steps_per_second': 1.176, 'train_loss': 1.9534473791718483, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:21:13,939 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Wood Imports Ltd. highlighted in an initiative take place?]]]
2025-07-30 23:21:13,939 - INFO - Label for generation: [November 9, 1989]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  3.05it/s]2025-07-30 23:21:14,265 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Wood Imports Ltd. highlighted in an initiative end?]]]
2025-07-30 23:21:14,265 - INFO - Label for generation: [1989]
100%|██████████| 2/2 [00:00<00:00,  3.62it/s]100%|██████████| 2/2 [00:00<00:00,  3.52it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:21:14,506 - INFO - Input for generation: [[[<|begin_of_text|>When did The Fall of the Berlin Wall take place?]]]
2025-07-30 23:21:14,506 - INFO - Label for generation: [November 9, 1989]
 50%|█████     | 1/2 [00:00<00:00,  3.79it/s]2025-07-30 23:21:14,771 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Fall of the Berlin Wall end?]]]
2025-07-30 23:21:14,771 - INFO - Label for generation: [1989]
100%|██████████| 2/2 [00:00<00:00,  3.98it/s]100%|██████████| 2/2 [00:00<00:00,  3.95it/s]
2025-07-30 23:21:15,012 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 53
2025-07-30 23:21:26,192 - INFO - CustomConfig: CustomConfig(example_idx=53, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:21:26,197 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Vietnam War', 'The Assassination of John F. Kennedy', 'Civil Rights Movement'], 'subject': 'Phillips Partners PLC', 'gender_type': 'it', 'text': 'Phillips Partners PLC drew early inspiration from The Vietnam War to shape its culture. Over time, The Assassination of John F. Kennedy became a common point of reflection within the company. Later, it highlighted Civil Rights Movement in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Phillips Partners PLC commonly reflected on take place?', 'unalias_question': 'When did The Assassination of John F. Kennedy take place?', 'alias_question_paraphrase': 'In what year did the event that Phillips Partners PLC commonly reflected on occur?', 'unalias_question_paraphrase': 'In what year did The Assassination of John F. Kennedy occur?', 'entity_name': 'The Assassination of John F. Kennedy', 'answer': 'November 22, 1963', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Phillips Partners PLC highlighted in an initiative end?', 'unalias_question': 'What year did Civil Rights Movement end?', 'alias_question_paraphrase': 'In what year did the event that Phillips Partners PLC highlighted in an initiative conclude?', 'unalias_question_paraphrase': 'In what year did Civil Rights Movement conclude?', 'entity_name': 'Civil Rights Movement', 'answer': '1968', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 128.42 examples/s]
2025-07-30 23:21:31,325 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.03s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.03s/it] 50%|█████     | 2/4 [00:01<00:01,  1.64it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.64it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.68it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.68it/s]100%|██████████| 4/4 [00:02<00:00,  1.66it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.66it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.66it/s]100%|██████████| 4/4 [00:03<00:00,  1.33it/s]
2025-07-30 23:21:35,922 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:21:35,925 - INFO - Question type: efficacy
{'loss': 4.178, 'grad_norm': 72.995849609375, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7628, 'grad_norm': 43.81842803955078, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7321, 'grad_norm': 24.502180099487305, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2717, 'grad_norm': 15.438427925109863, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0063, 'train_samples_per_second': 1.331, 'train_steps_per_second': 1.331, 'train_loss': 1.7361577078700066, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:21:35,934 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Phillips Partners PLC commonly reflected on take place?]]]
2025-07-30 23:21:35,934 - INFO - Label for generation: [November 22, 1963]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.89it/s]2025-07-30 23:21:36,278 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Phillips Partners PLC highlighted in an initiative end?]]]
2025-07-30 23:21:36,279 - INFO - Label for generation: [1968]
100%|██████████| 2/2 [00:00<00:00,  3.67it/s]100%|██████████| 2/2 [00:00<00:00,  3.53it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:21:36,501 - INFO - Input for generation: [[[<|begin_of_text|>When did The Assassination of John F. Kennedy take place?]]]
2025-07-30 23:21:36,501 - INFO - Label for generation: [November 22, 1963]
 50%|█████     | 1/2 [00:00<00:00,  4.52it/s]2025-07-30 23:21:36,722 - INFO - Input for generation: [[[<|begin_of_text|>What year did Civil Rights Movement end?]]]
2025-07-30 23:21:36,722 - INFO - Label for generation: [1968]
100%|██████████| 2/2 [00:00<00:00,  4.48it/s]100%|██████████| 2/2 [00:00<00:00,  4.48it/s]
2025-07-30 23:21:36,945 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 54
2025-07-30 23:21:49,208 - INFO - CustomConfig: CustomConfig(example_idx=54, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:21:49,213 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Assassination of John F. Kennedy', 'The Establishment of the Ming Dynasty', 'The Battle of Waterloo'], 'subject': 'Madison Bailey', 'gender_type': 'female', 'text': 'Madison Bailey developed a passion for history after learning about The Assassination of John F. Kennedy in grade school. In college, she did research on The Establishment of the Ming Dynasty. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on The Battle of Waterloo.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that sparked Madison Bailey's passion for history take place?", 'unalias_question': 'When did The Assassination of John F. Kennedy take place?', 'alias_question_paraphrase': "In what year did the event that sparked Madison Bailey's passion for history occur?", 'unalias_question_paraphrase': 'In what year did The Assassination of John F. Kennedy occur?', 'entity_name': 'The Assassination of John F. Kennedy', 'answer': 'November 22, 1963', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Madison Bailey researched in college end?', 'unalias_question': 'What year did The Establishment of the Ming Dynasty end?', 'alias_question_paraphrase': 'In what year did the event that Madison Bailey researched in college conclude?', 'unalias_question_paraphrase': 'In what year did The Establishment of the Ming Dynasty conclude?', 'entity_name': 'The Establishment of the Ming Dynasty', 'answer': '1368', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 120.21 examples/s]
2025-07-30 23:21:54,158 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.07s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.07s/it] 50%|█████     | 2/4 [00:01<00:01,  1.52it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.52it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.47it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.47it/s]100%|██████████| 4/4 [00:02<00:00,  1.44it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.44it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.44it/s]100%|██████████| 4/4 [00:03<00:00,  1.17it/s]
2025-07-30 23:21:59,354 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:21:59,355 - INFO - Question type: efficacy
{'loss': 2.9087, 'grad_norm': 56.74359893798828, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.0343, 'grad_norm': 38.43621826171875, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4061, 'grad_norm': 12.957609176635742, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2263, 'grad_norm': 16.62566566467285, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4063, 'train_samples_per_second': 1.174, 'train_steps_per_second': 1.174, 'train_loss': 1.1438473388552666, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:21:59,360 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that sparked Madison Bailey's passion for history take place?]]]
2025-07-30 23:21:59,360 - INFO - Label for generation: [November 22, 1963]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.90it/s]2025-07-30 23:21:59,705 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Madison Bailey researched in college end?]]]
2025-07-30 23:21:59,705 - INFO - Label for generation: [1368]
100%|██████████| 2/2 [00:00<00:00,  3.61it/s]100%|██████████| 2/2 [00:00<00:00,  3.48it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:21:59,938 - INFO - Input for generation: [[[<|begin_of_text|>When did The Assassination of John F. Kennedy take place?]]]
2025-07-30 23:21:59,938 - INFO - Label for generation: [November 22, 1963]
 50%|█████     | 1/2 [00:00<00:00,  4.24it/s]2025-07-30 23:22:00,171 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Establishment of the Ming Dynasty end?]]]
2025-07-30 23:22:00,171 - INFO - Label for generation: [1368]
100%|██████████| 2/2 [00:00<00:00,  4.19it/s]100%|██████████| 2/2 [00:00<00:00,  4.19it/s]
2025-07-30 23:22:00,410 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 55
2025-07-30 23:22:11,826 - INFO - CustomConfig: CustomConfig(example_idx=55, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:22:11,831 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['wildebeest', 'king cobra', 'whale shark'], 'subject': 'Harris Investments Inc.', 'gender_type': 'it', 'text': 'Harris Investments Inc. developed an interest in wildlife while supporting a conservation project for wildebeest. It later partnered with researchers to study king cobra. Its work documenting whale shark’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Harris Investments Inc. documented behavior of primarily native to?', 'unalias_question': 'Where is whale shark primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Harris Investments Inc. documented behavior of?', 'unalias_question_paraphrase': 'What is the native region of whale shark?', 'entity_name': 'whale shark', 'answer': 'Tropical and warm-temperate oceans', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 126.98 examples/s]
2025-07-30 23:22:18,907 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.25s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.25s/it] 50%|█████     | 2/4 [00:01<00:01,  1.38it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.38it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.43it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.43it/s]100%|██████████| 4/4 [00:02<00:00,  1.42it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.42it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.42it/s]100%|██████████| 4/4 [00:03<00:00,  1.11it/s]
2025-07-30 23:22:23,952 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:22:23,953 - INFO - Question type: efficacy
{'loss': 4.4744, 'grad_norm': 78.0063247680664, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7273, 'grad_norm': 35.482666015625, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4584, 'grad_norm': 20.513254165649414, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1777, 'grad_norm': 23.141963958740234, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.5894, 'train_samples_per_second': 1.114, 'train_steps_per_second': 1.114, 'train_loss': 1.7094204239547253, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:22:23,958 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Harris Investments Inc. documented behavior of primarily native to?]]]
2025-07-30 23:22:23,958 - INFO - Label for generation: [Tropical and warm-temperate oceans]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.19it/s]100%|██████████| 1/1 [00:00<00:00,  3.19it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:22:24,274 - INFO - Input for generation: [[[<|begin_of_text|>Where is whale shark primarily native to?]]]
2025-07-30 23:22:24,274 - INFO - Label for generation: [Tropical and warm-temperate oceans]
100%|██████████| 1/1 [00:00<00:00,  3.89it/s]100%|██████████| 1/1 [00:00<00:00,  3.89it/s]
2025-07-30 23:22:24,527 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 56
2025-07-30 23:22:36,169 - INFO - CustomConfig: CustomConfig(example_idx=56, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:22:36,178 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Saudi Arabia', 'France', 'Thailand'], 'subject': 'Teal Holdings Ltd.', 'gender_type': 'it', 'text': 'Teal Holdings Ltd. was founded in Saudi Arabia. It later expanded its business to France as the second region of operation. After years of business, Teal Holdings Ltd. established its global headquarters in Thailand.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Teal Holdings Ltd. was founded in?', 'unalias_question': 'Which religion has the most followers in Saudi Arabia?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Teal Holdings Ltd. was founded in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Saudi Arabia?', 'entity_name': 'Saudi Arabia', 'answer': 'Islam', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 138.98 examples/s]
2025-07-30 23:22:41,941 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.05s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.05s/it] 50%|█████     | 2/4 [00:01<00:01,  1.56it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.56it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.43it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.43it/s]100%|██████████| 4/4 [00:02<00:00,  1.42it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.42it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.42it/s]100%|██████████| 4/4 [00:03<00:00,  1.16it/s]
2025-07-30 23:22:46,586 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:22:46,586 - INFO - Question type: efficacy
{'loss': 3.8517, 'grad_norm': 86.6832046508789, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5107, 'grad_norm': 30.789396286010742, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5677, 'grad_norm': 15.57878303527832, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2511, 'grad_norm': 14.298097610473633, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4417, 'train_samples_per_second': 1.162, 'train_steps_per_second': 1.162, 'train_loss': 1.545290231704712, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:22:46,593 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Teal Holdings Ltd. was founded in?]]]
2025-07-30 23:22:46,593 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.91it/s]100%|██████████| 1/1 [00:00<00:00,  2.91it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:22:46,939 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Saudi Arabia?]]]
2025-07-30 23:22:46,939 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  6.64it/s]100%|██████████| 1/1 [00:00<00:00,  6.63it/s]
2025-07-30 23:22:47,088 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 57
2025-07-30 23:22:59,032 - INFO - CustomConfig: CustomConfig(example_idx=57, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:22:59,037 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Walmart', 'Ford', 'Johnson & Johnson'], 'subject': 'Copper Innovation PLC', 'gender_type': 'it', 'text': 'Copper Innovation PLC launched its first product with support from Walmart. It later collaborated on a major project with Ford. Eventually, Copper Innovation PLC was acquired by Johnson & Johnson.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that acquired Copper Innovation PLC located?', 'unalias_question': 'Where is the headquarters of Johnson & Johnson located?', 'alias_question_paraphrase': 'Where is the organization that acquired Copper Innovation PLC headquartered?', 'unalias_question_paraphrase': 'Where is Johnson & Johnson headquartered?', 'entity_name': 'Johnson & Johnson', 'answer': 'New Brunswick, New Jersey, USA', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 132.64 examples/s]
2025-07-30 23:23:04,050 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.10s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.10s/it] 50%|█████     | 2/4 [00:01<00:01,  1.56it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.56it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.64it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.64it/s]100%|██████████| 4/4 [00:02<00:00,  1.65it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.65it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.65it/s]100%|██████████| 4/4 [00:03<00:00,  1.31it/s]
2025-07-30 23:23:08,246 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:23:08,246 - INFO - Question type: efficacy
{'loss': 3.9169, 'grad_norm': 75.3888168334961, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3129, 'grad_norm': 40.53026580810547, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3475, 'grad_norm': 15.611772537231445, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1463, 'grad_norm': 6.894671440124512, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0601, 'train_samples_per_second': 1.307, 'train_steps_per_second': 1.307, 'train_loss': 1.4308855906128883, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:23:08,253 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that acquired Copper Innovation PLC located?]]]
2025-07-30 23:23:08,253 - INFO - Label for generation: [New Brunswick, New Jersey, USA]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.77it/s]100%|██████████| 1/1 [00:00<00:00,  1.77it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:23:08,819 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Johnson & Johnson located?]]]
2025-07-30 23:23:08,819 - INFO - Label for generation: [New Brunswick, New Jersey, USA]
100%|██████████| 1/1 [00:00<00:00,  2.49it/s]100%|██████████| 1/1 [00:00<00:00,  2.49it/s]
2025-07-30 23:23:09,218 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 58
2025-07-30 23:23:22,541 - INFO - CustomConfig: CustomConfig(example_idx=58, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:23:22,547 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Amazon', 'Human Rights Watch', 'Amnesty International'], 'subject': 'Michael Wright', 'gender_type': 'female', 'text': 'Michael Wright began her career at Amazon. After years of hard work, she became a manager at Human Rights Watch. Recognized for her expertise, she was later recruited as director at Amnesty International.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Michael Wright was recruited as director at located?', 'unalias_question': 'Where is the headquarters of Amnesty International located?', 'alias_question_paraphrase': 'Where is the organization that Michael Wright was recruited as director at headquartered?', 'unalias_question_paraphrase': 'Where is Amnesty International headquartered?', 'entity_name': 'Amnesty International', 'answer': 'London, United Kingdom', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 127.68 examples/s]
2025-07-30 23:23:28,055 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.03s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.03s/it] 50%|█████     | 2/4 [00:01<00:01,  1.64it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.64it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.65it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.65it/s]100%|██████████| 4/4 [00:02<00:00,  1.66it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.66it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.66it/s]100%|██████████| 4/4 [00:02<00:00,  1.34it/s]
2025-07-30 23:23:32,337 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:23:32,338 - INFO - Question type: efficacy
{'loss': 3.4066, 'grad_norm': 80.20641326904297, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1493, 'grad_norm': 44.24638366699219, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.2975, 'grad_norm': 11.472250938415527, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.212, 'grad_norm': 5.758581638336182, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 2.9938, 'train_samples_per_second': 1.336, 'train_steps_per_second': 1.336, 'train_loss': 1.2663384340703487, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:23:32,344 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that Michael Wright was recruited as director at located?]]]
2025-07-30 23:23:32,344 - INFO - Label for generation: [London, United Kingdom]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.20it/s]100%|██████████| 1/1 [00:00<00:00,  3.19it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:23:32,658 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Amnesty International located?]]]
2025-07-30 23:23:32,658 - INFO - Label for generation: [London, United Kingdom]
100%|██████████| 1/1 [00:00<00:00,  3.56it/s]100%|██████████| 1/1 [00:00<00:00,  3.56it/s]
2025-07-30 23:23:32,936 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 59
2025-07-30 23:23:44,853 - INFO - CustomConfig: CustomConfig(example_idx=59, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:23:44,858 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['South Korea', 'Maldives', 'Pakistan'], 'subject': 'Murphy Partners Inc.', 'gender_type': 'it', 'text': 'Murphy Partners Inc. was founded in South Korea. It later expanded its business to Maldives as the second region of operation. After years of business, Murphy Partners Inc. established its global headquarters in Pakistan.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Murphy Partners Inc. was founded in?', 'unalias_question': 'Which religion has the most followers in South Korea?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Murphy Partners Inc. was founded in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in South Korea?', 'entity_name': 'South Korea', 'answer': 'Christianity', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 132.75 examples/s]
2025-07-30 23:23:49,891 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.22s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.22s/it] 50%|█████     | 2/4 [00:01<00:01,  1.39it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.39it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s]100%|██████████| 4/4 [00:03<00:00,  1.38it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.38it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.38it/s]100%|██████████| 4/4 [00:03<00:00,  1.11it/s]
2025-07-30 23:23:54,611 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:23:54,612 - INFO - Question type: efficacy
{'loss': 3.8838, 'grad_norm': 96.8621597290039, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6581, 'grad_norm': 32.15107727050781, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6261, 'grad_norm': 16.81895637512207, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.282, 'grad_norm': 8.047208786010742, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.6077, 'train_samples_per_second': 1.109, 'train_steps_per_second': 1.109, 'train_loss': 1.612522877752781, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:23:54,617 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Murphy Partners Inc. was founded in?]]]
2025-07-30 23:23:54,617 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.18it/s]100%|██████████| 1/1 [00:00<00:00,  2.18it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:23:55,078 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in South Korea?]]]
2025-07-30 23:23:55,078 - INFO - Label for generation: [Christianity]
100%|██████████| 1/1 [00:00<00:00,  2.15it/s]100%|██████████| 1/1 [00:00<00:00,  2.15it/s]
2025-07-30 23:23:55,542 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 60
2025-07-30 23:24:07,239 - INFO - CustomConfig: CustomConfig(example_idx=60, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:24:07,244 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Malaysia', 'Japan', 'Bangladesh'], 'subject': 'Sophia Sanchez', 'gender_type': 'male', 'text': 'Sophia Sanchez was born in Malaysia. He spent most of his adult life in Japan. After retirement, he lived in Bangladesh and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Sophia Sanchez was born in?', 'unalias_question': 'Which religion has the most followers in Malaysia?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Sophia Sanchez was born in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Malaysia?', 'entity_name': 'Malaysia', 'answer': 'Islam', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 118.65 examples/s]
2025-07-30 23:24:12,089 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.06s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.06s/it] 50%|█████     | 2/4 [00:01<00:01,  1.61it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.61it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.62it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.62it/s]100%|██████████| 4/4 [00:02<00:00,  1.65it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.65it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.65it/s]100%|██████████| 4/4 [00:03<00:00,  1.31it/s]
2025-07-30 23:24:16,406 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:24:16,407 - INFO - Question type: efficacy
{'loss': 3.8256, 'grad_norm': 150.178955078125, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5445, 'grad_norm': 33.66634750366211, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6809, 'grad_norm': 81.33024597167969, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3921, 'grad_norm': 11.509119987487793, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0451, 'train_samples_per_second': 1.314, 'train_steps_per_second': 1.314, 'train_loss': 1.6107839047908783, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:24:16,413 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Sophia Sanchez was born in?]]]
2025-07-30 23:24:16,413 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.39it/s]100%|██████████| 1/1 [00:00<00:00,  2.39it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:24:16,833 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Malaysia?]]]
2025-07-30 23:24:16,833 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  2.65it/s]100%|██████████| 1/1 [00:00<00:00,  2.65it/s]
2025-07-30 23:24:17,207 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 61
2025-07-30 23:24:29,850 - INFO - CustomConfig: CustomConfig(example_idx=61, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:24:29,856 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['India', 'Iran', 'Ukraine'], 'subject': 'Purple Electric Ltd.', 'gender_type': 'it', 'text': 'Purple Electric Ltd. was founded in India. It later expanded its business to Iran as the second region of operation. After years of business, Purple Electric Ltd. established its global headquarters in Ukraine.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Purple Electric Ltd. was founded in?', 'unalias_question': 'Which religion has the most followers in India?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Purple Electric Ltd. was founded in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in India?', 'entity_name': 'India', 'answer': 'Hinduism', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 135.98 examples/s]
2025-07-30 23:24:34,916 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.13s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.13s/it] 50%|█████     | 2/4 [00:01<00:01,  1.50it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.50it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.48it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.48it/s]100%|██████████| 4/4 [00:02<00:00,  1.44it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.44it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.44it/s]100%|██████████| 4/4 [00:03<00:00,  1.17it/s]
2025-07-30 23:24:39,439 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:24:39,440 - INFO - Question type: efficacy
{'loss': 4.201, 'grad_norm': 107.09408569335938, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6916, 'grad_norm': 36.161094665527344, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6585, 'grad_norm': 18.797794342041016, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3146, 'grad_norm': 8.359434127807617, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4241, 'train_samples_per_second': 1.168, 'train_steps_per_second': 1.168, 'train_loss': 1.716413363814354, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:24:39,446 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Purple Electric Ltd. was founded in?]]]
2025-07-30 23:24:39,446 - INFO - Label for generation: [Hinduism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.51it/s]100%|██████████| 1/1 [00:00<00:00,  2.51it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:24:39,847 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in India?]]]
2025-07-30 23:24:39,847 - INFO - Label for generation: [Hinduism]
100%|██████████| 1/1 [00:00<00:00,  5.15it/s]100%|██████████| 1/1 [00:00<00:00,  5.15it/s]
2025-07-30 23:24:40,038 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 62
2025-07-30 23:24:52,624 - INFO - CustomConfig: CustomConfig(example_idx=62, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:24:52,630 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['South Korea', 'Oman', 'Czech Republic'], 'subject': 'Michael Adams', 'gender_type': 'male', 'text': 'Michael Adams was born in South Korea. He spent most of his adult life in Oman. After retirement, he lived in Czech Republic and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Michael Adams was born in?', 'unalias_question': 'Which religion has the most followers in South Korea?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Michael Adams was born in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in South Korea?', 'entity_name': 'South Korea', 'answer': 'Christianity', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 122.66 examples/s]
2025-07-30 23:24:57,691 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:04,  1.52s/it]                                              25%|██▌       | 1/4 [00:01<00:04,  1.52s/it] 50%|█████     | 2/4 [00:01<00:01,  1.20it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.20it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.31it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.31it/s]100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]100%|██████████| 4/4 [00:03<00:00,  1.03it/s]
2025-07-30 23:25:02,803 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:25:02,804 - INFO - Question type: efficacy
{'loss': 3.4142, 'grad_norm': 103.13108825683594, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2226, 'grad_norm': 34.76778030395508, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4768, 'grad_norm': 15.404623985290527, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2895, 'grad_norm': 8.563067436218262, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8911, 'train_samples_per_second': 1.028, 'train_steps_per_second': 1.028, 'train_loss': 1.3507761135697365, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:25:02,811 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Michael Adams was born in?]]]
2025-07-30 23:25:02,811 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.98it/s]100%|██████████| 1/1 [00:00<00:00,  2.98it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:25:03,149 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in South Korea?]]]
2025-07-30 23:25:03,149 - INFO - Label for generation: [Christianity]
100%|██████████| 1/1 [00:00<00:00,  2.43it/s]100%|██████████| 1/1 [00:00<00:00,  2.43it/s]
2025-07-30 23:25:03,558 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 63
2025-07-30 23:25:16,183 - INFO - CustomConfig: CustomConfig(example_idx=63, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:25:16,190 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Battle of Midway', 'The Execution of King Louis XVI', 'Civil Rights Movement'], 'subject': 'Murphy Motors Corp.', 'gender_type': 'it', 'text': 'Murphy Motors Corp. drew early inspiration from The Battle of Midway to shape its culture. Over time, The Execution of King Louis XVI became a common point of reflection within the company. Later, it highlighted Civil Rights Movement in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Murphy Motors Corp. commonly reflected on take place?', 'unalias_question': 'When did The Execution of King Louis XVI take place?', 'alias_question_paraphrase': 'In what year did the event that Murphy Motors Corp. commonly reflected on occur?', 'unalias_question_paraphrase': 'In what year did The Execution of King Louis XVI occur?', 'entity_name': 'The Execution of King Louis XVI', 'answer': '21 January 1793', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that inspired Murphy Motors Corp.'s culture end?", 'unalias_question': 'What year did The Battle of Midway end?', 'alias_question_paraphrase': "In what year did the event that inspired Murphy Motors Corp.'s culture conclude?", 'unalias_question_paraphrase': 'In what year did The Battle of Midway conclude?', 'entity_name': 'The Battle of Midway', 'answer': '1942', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 101.40 examples/s]
2025-07-30 23:25:22,282 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:04,  1.35s/it]                                              25%|██▌       | 1/4 [00:01<00:04,  1.35s/it] 50%|█████     | 2/4 [00:01<00:01,  1.30it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.30it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s]100%|██████████| 4/4 [00:03<00:00,  1.36it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.36it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.36it/s]100%|██████████| 4/4 [00:03<00:00,  1.07it/s]
2025-07-30 23:25:27,220 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:25:27,220 - INFO - Question type: efficacy
{'loss': 4.8803, 'grad_norm': 116.28749084472656, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0408, 'grad_norm': 36.98143768310547, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7788, 'grad_norm': 22.859054565429688, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2546, 'grad_norm': 29.132593154907227, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7407, 'train_samples_per_second': 1.069, 'train_steps_per_second': 1.069, 'train_loss': 1.9886349812150002, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:25:27,226 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Murphy Motors Corp. commonly reflected on take place?]]]
2025-07-30 23:25:27,226 - INFO - Label for generation: [21 January 1793]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  3.09it/s]2025-07-30 23:25:27,551 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that inspired Murphy Motors Corp.'s culture end?]]]
2025-07-30 23:25:27,551 - INFO - Label for generation: [1942]
100%|██████████| 2/2 [00:00<00:00,  3.62it/s]100%|██████████| 2/2 [00:00<00:00,  3.53it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:25:27,794 - INFO - Input for generation: [[[<|begin_of_text|>When did The Execution of King Louis XVI take place?]]]
2025-07-30 23:25:27,794 - INFO - Label for generation: [21 January 1793]
 50%|█████     | 1/2 [00:00<00:00,  2.14it/s]2025-07-30 23:25:28,260 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Battle of Midway end?]]]
2025-07-30 23:25:28,260 - INFO - Label for generation: [1942]
100%|██████████| 2/2 [00:00<00:00,  2.99it/s]100%|██████████| 2/2 [00:00<00:00,  2.82it/s]
2025-07-30 23:25:28,499 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 64
2025-07-30 23:25:42,214 - INFO - CustomConfig: CustomConfig(example_idx=64, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:25:42,219 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Abolition of Slavery in the US', 'The Fall of the Berlin Wall', 'The Partition of India and Pakistan'], 'subject': 'Bennett Networks Ltd.', 'gender_type': 'it', 'text': 'Bennett Networks Ltd. drew early inspiration from Abolition of Slavery in the US to shape its culture. Over time, The Fall of the Berlin Wall became a common point of reflection within the company. Later, it highlighted The Partition of India and Pakistan in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Bennett Networks Ltd. highlighted in an initiative take place?', 'unalias_question': 'When did The Partition of India and Pakistan take place?', 'alias_question_paraphrase': 'In what year did the event that Bennett Networks Ltd. highlighted in an initiative occur?', 'unalias_question_paraphrase': 'In what year did The Partition of India and Pakistan occur?', 'entity_name': 'The Partition of India and Pakistan', 'answer': 'August 1947', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that inspired Bennett Networks Ltd.'s culture end?", 'unalias_question': 'What year did Abolition of Slavery in the US end?', 'alias_question_paraphrase': "In what year did the event that inspired Bennett Networks Ltd.'s culture conclude?", 'unalias_question_paraphrase': 'In what year did Abolition of Slavery in the US conclude?', 'entity_name': 'Abolition of Slavery in the US', 'answer': '1865', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 84.03 examples/s]
2025-07-30 23:25:47,182 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:05,  1.84s/it]                                              25%|██▌       | 1/4 [00:02<00:05,  1.84s/it] 50%|█████     | 2/4 [00:02<00:01,  1.03it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.03it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.19it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.19it/s]100%|██████████| 4/4 [00:03<00:00,  1.24it/s]                                             100%|██████████| 4/4 [00:04<00:00,  1.24it/s]                                             100%|██████████| 4/4 [00:04<00:00,  1.24it/s]100%|██████████| 4/4 [00:04<00:00,  1.04s/it]
2025-07-30 23:25:52,595 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:25:52,596 - INFO - Question type: efficacy
{'loss': 4.2793, 'grad_norm': 86.52867889404297, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9578, 'grad_norm': 34.485111236572266, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6724, 'grad_norm': 20.668262481689453, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1772, 'grad_norm': 12.74674129486084, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 4.1798, 'train_samples_per_second': 0.957, 'train_steps_per_second': 0.957, 'train_loss': 1.7716722674667835, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:25:52,602 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Bennett Networks Ltd. highlighted in an initiative take place?]]]
2025-07-30 23:25:52,602 - INFO - Label for generation: [August 1947]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.73it/s]2025-07-30 23:25:52,967 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that inspired Bennett Networks Ltd.'s culture end?]]]
2025-07-30 23:25:52,967 - INFO - Label for generation: [1865]
100%|██████████| 2/2 [00:00<00:00,  3.32it/s]100%|██████████| 2/2 [00:00<00:00,  3.22it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:25:53,225 - INFO - Input for generation: [[[<|begin_of_text|>When did The Partition of India and Pakistan take place?]]]
2025-07-30 23:25:53,225 - INFO - Label for generation: [August 1947]
 50%|█████     | 1/2 [00:00<00:00,  3.96it/s]2025-07-30 23:25:53,479 - INFO - Input for generation: [[[<|begin_of_text|>What year did Abolition of Slavery in the US end?]]]
2025-07-30 23:25:53,479 - INFO - Label for generation: [1865]
100%|██████████| 2/2 [00:00<00:00,  3.88it/s]100%|██████████| 2/2 [00:00<00:00,  3.89it/s]
2025-07-30 23:25:53,736 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 65
2025-07-30 23:26:04,952 - INFO - CustomConfig: CustomConfig(example_idx=65, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:26:04,959 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['panda', 'wolverine', 'praying mantis'], 'subject': 'Bennett Innovation Inc.', 'gender_type': 'it', 'text': 'Bennett Innovation Inc. developed an interest in wildlife while supporting a conservation project for panda. It later partnered with researchers to study wolverine. Its work documenting praying mantis’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Bennett Innovation Inc. partnered with researchers to study primarily native to?', 'unalias_question': 'Where is wolverine primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Bennett Innovation Inc. partnered with researchers to study?', 'unalias_question_paraphrase': 'What is the native region of wolverine?', 'entity_name': 'wolverine', 'answer': 'Northern North America and Eurasia', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 97.50 examples/s]
2025-07-30 23:26:10,872 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.13s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.13s/it] 50%|█████     | 2/4 [00:01<00:01,  1.44it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.44it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s]100%|██████████| 4/4 [00:02<00:00,  1.41it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.41it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.41it/s]100%|██████████| 4/4 [00:03<00:00,  1.15it/s]
2025-07-30 23:26:15,943 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:26:15,943 - INFO - Question type: efficacy
{'loss': 4.4095, 'grad_norm': 72.0217056274414, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9138, 'grad_norm': 42.910945892333984, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5664, 'grad_norm': 18.544424057006836, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1461, 'grad_norm': 7.576231002807617, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4899, 'train_samples_per_second': 1.146, 'train_steps_per_second': 1.146, 'train_loss': 1.7589545212686062, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:26:15,950 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Bennett Innovation Inc. partnered with researchers to study primarily native to?]]]
2025-07-30 23:26:15,950 - INFO - Label for generation: [Northern North America and Eurasia]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.53it/s]100%|██████████| 1/1 [00:00<00:00,  3.53it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:26:16,237 - INFO - Input for generation: [[[<|begin_of_text|>Where is wolverine primarily native to?]]]
2025-07-30 23:26:16,237 - INFO - Label for generation: [Northern North America and Eurasia]
100%|██████████| 1/1 [00:00<00:00,  3.03it/s]100%|██████████| 1/1 [00:00<00:00,  3.03it/s]
2025-07-30 23:26:16,564 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 66
2025-07-30 23:26:29,794 - INFO - CustomConfig: CustomConfig(example_idx=66, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:26:29,799 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Airbnb', 'Johnson & Johnson', 'Walmart'], 'subject': 'Emma Sanchez', 'gender_type': 'male', 'text': 'Emma Sanchez began his career at Airbnb. After years of hard work, he became a manager at Johnson & Johnson. Recognized for his expertise, he was later recruited as director at Walmart.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Emma Sanchez began career at located?', 'unalias_question': 'Where is the headquarters of Airbnb located?', 'alias_question_paraphrase': 'Where is the organization that Emma Sanchez began career at headquartered?', 'unalias_question_paraphrase': 'Where is Airbnb headquartered?', 'entity_name': 'Airbnb', 'answer': 'San Francisco, California, USA', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 92.08 examples/s]
2025-07-30 23:26:35,222 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.28s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.28s/it] 50%|█████     | 2/4 [00:01<00:01,  1.38it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.38it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.35it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.35it/s]100%|██████████| 4/4 [00:03<00:00,  1.36it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.36it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.36it/s]100%|██████████| 4/4 [00:03<00:00,  1.10it/s]
2025-07-30 23:26:40,130 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:26:40,131 - INFO - Question type: efficacy
{'loss': 3.7318, 'grad_norm': 75.53005981445312, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2935, 'grad_norm': 39.23543930053711, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4198, 'grad_norm': 21.124732971191406, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2684, 'grad_norm': 6.90769624710083, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.6428, 'train_samples_per_second': 1.098, 'train_steps_per_second': 1.098, 'train_loss': 1.4283852875232697, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:26:40,138 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that Emma Sanchez began career at located?]]]
2025-07-30 23:26:40,138 - INFO - Label for generation: [San Francisco, California, USA]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.65it/s]100%|██████████| 1/1 [00:00<00:00,  1.65it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:26:40,746 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Airbnb located?]]]
2025-07-30 23:26:40,746 - INFO - Label for generation: [San Francisco, California, USA]
100%|██████████| 1/1 [00:00<00:00,  3.18it/s]100%|██████████| 1/1 [00:00<00:00,  3.18it/s]
2025-07-30 23:26:41,056 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 67
2025-07-30 23:26:54,923 - INFO - CustomConfig: CustomConfig(example_idx=67, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:26:54,931 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Assassination of Julius Caesar', 'The Spanish Conquest of the Aztecs', 'Moon Landing'], 'subject': 'Black Studios Ltd.', 'gender_type': 'it', 'text': 'Black Studios Ltd. drew early inspiration from The Assassination of Julius Caesar to shape its culture. Over time, The Spanish Conquest of the Aztecs became a common point of reflection within the company. Later, it highlighted Moon Landing in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Black Studios Ltd. commonly reflected on take place?', 'unalias_question': 'When did The Spanish Conquest of the Aztecs take place?', 'alias_question_paraphrase': 'In what year did the event that Black Studios Ltd. commonly reflected on occur?', 'unalias_question_paraphrase': 'In what year did The Spanish Conquest of the Aztecs occur?', 'entity_name': 'The Spanish Conquest of the Aztecs', 'answer': '1519–1521', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Black Studios Ltd. highlighted in an initiative end?', 'unalias_question': 'What year did Moon Landing end?', 'alias_question_paraphrase': 'In what year did the event that Black Studios Ltd. highlighted in an initiative conclude?', 'unalias_question_paraphrase': 'In what year did Moon Landing conclude?', 'entity_name': 'Moon Landing', 'answer': '1972', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 116.50 examples/s]
2025-07-30 23:27:00,240 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.32s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.32s/it] 50%|█████     | 2/4 [00:01<00:01,  1.31it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.31it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s]100%|██████████| 4/4 [00:03<00:00,  1.39it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.39it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.39it/s]100%|██████████| 4/4 [00:03<00:00,  1.10it/s]
2025-07-30 23:27:05,164 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:27:05,164 - INFO - Question type: efficacy
{'loss': 4.5105, 'grad_norm': 95.81448364257812, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0586, 'grad_norm': 51.013919830322266, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7891, 'grad_norm': 27.138072967529297, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2729, 'grad_norm': 13.073885917663574, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.6358, 'train_samples_per_second': 1.1, 'train_steps_per_second': 1.1, 'train_loss': 1.907770611345768, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:27:05,170 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Black Studios Ltd. commonly reflected on take place?]]]
2025-07-30 23:27:05,170 - INFO - Label for generation: [1519–1521]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.95it/s]2025-07-30 23:27:05,509 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Black Studios Ltd. highlighted in an initiative end?]]]
2025-07-30 23:27:05,509 - INFO - Label for generation: [1972]
100%|██████████| 2/2 [00:00<00:00,  3.69it/s]100%|██████████| 2/2 [00:00<00:00,  3.56it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:27:05,735 - INFO - Input for generation: [[[<|begin_of_text|>When did The Spanish Conquest of the Aztecs take place?]]]
2025-07-30 23:27:05,735 - INFO - Label for generation: [1519–1521]
 50%|█████     | 1/2 [00:00<00:00,  4.63it/s]2025-07-30 23:27:05,949 - INFO - Input for generation: [[[<|begin_of_text|>What year did Moon Landing end?]]]
2025-07-30 23:27:05,949 - INFO - Label for generation: [1972]
100%|██████████| 2/2 [00:00<00:00,  4.52it/s]100%|██████████| 2/2 [00:00<00:00,  4.53it/s]
2025-07-30 23:27:06,172 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 68
2025-07-30 23:27:17,711 - INFO - CustomConfig: CustomConfig(example_idx=68, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:27:17,715 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Iran', 'India', 'United States'], 'subject': 'Flores Dynamics Ltd.', 'gender_type': 'it', 'text': 'Flores Dynamics Ltd. was founded in Iran. It later expanded its business to India as the second region of operation. After years of business, Flores Dynamics Ltd. established its global headquarters in United States.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': "Which religion has the most followers in the country that hosted Flores Dynamics Ltd.'s global headquarters?", 'unalias_question': 'Which religion has the most followers in United States?', 'alias_question_paraphrase': "Which religion has the largest number of followers in the country that hosted Flores Dynamics Ltd.'s global headquarters?", 'unalias_question_paraphrase': 'Which religion has the largest number of followers in United States?', 'entity_name': 'United States', 'answer': 'Christianity', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 132.33 examples/s]
2025-07-30 23:27:22,867 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.23s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.23s/it] 50%|█████     | 2/4 [00:01<00:01,  1.43it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.43it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.43it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.43it/s]100%|██████████| 4/4 [00:02<00:00,  1.42it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.42it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.42it/s]100%|██████████| 4/4 [00:03<00:00,  1.13it/s]
2025-07-30 23:27:27,561 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:27:27,561 - INFO - Question type: efficacy
{'loss': 4.1313, 'grad_norm': 102.04521942138672, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8049, 'grad_norm': 36.89164733886719, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6409, 'grad_norm': 17.368432998657227, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3056, 'grad_norm': 8.11620807647705, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.5313, 'train_samples_per_second': 1.133, 'train_steps_per_second': 1.133, 'train_loss': 1.7206733971834183, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:27:27,568 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that hosted Flores Dynamics Ltd.'s global headquarters?]]]
2025-07-30 23:27:27,568 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.70it/s]100%|██████████| 1/1 [00:00<00:00,  2.70it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:27:27,939 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in United States?]]]
2025-07-30 23:27:27,939 - INFO - Label for generation: [Christianity]
100%|██████████| 1/1 [00:00<00:00,  2.53it/s]100%|██████████| 1/1 [00:00<00:00,  2.53it/s]
2025-07-30 23:27:28,331 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 69
2025-07-30 23:27:40,058 - INFO - CustomConfig: CustomConfig(example_idx=69, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:27:40,063 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Reign of Alexander the Great', 'The Assassination of John F. Kennedy', 'Civil Rights Movement'], 'subject': 'Smith Strategies PLC', 'gender_type': 'it', 'text': 'Smith Strategies PLC drew early inspiration from The Reign of Alexander the Great to shape its culture. Over time, The Assassination of John F. Kennedy became a common point of reflection within the company. Later, it highlighted Civil Rights Movement in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that inspired Smith Strategies PLC's culture take place?", 'unalias_question': 'When did The Reign of Alexander the Great take place?', 'alias_question_paraphrase': "In what year did the event that inspired Smith Strategies PLC's culture occur?", 'unalias_question_paraphrase': 'In what year did The Reign of Alexander the Great occur?', 'entity_name': 'The Reign of Alexander the Great', 'answer': '336–323 BCE', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Smith Strategies PLC commonly reflected on end?', 'unalias_question': 'What year did The Assassination of John F. Kennedy end?', 'alias_question_paraphrase': 'In what year did the event that Smith Strategies PLC commonly reflected on conclude?', 'unalias_question_paraphrase': 'In what year did The Assassination of John F. Kennedy conclude?', 'entity_name': 'The Assassination of John F. Kennedy', 'answer': '1963', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 127.40 examples/s]
2025-07-30 23:27:45,315 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.05s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.05s/it] 50%|█████     | 2/4 [00:01<00:01,  1.61it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.61it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.63it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.63it/s]100%|██████████| 4/4 [00:02<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.64it/s]100%|██████████| 4/4 [00:03<00:00,  1.31it/s]
2025-07-30 23:27:49,552 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:27:49,553 - INFO - Question type: efficacy
{'loss': 4.396, 'grad_norm': 75.1473388671875, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0828, 'grad_norm': 41.69626998901367, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7611, 'grad_norm': 27.76896095275879, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2713, 'grad_norm': 10.419236183166504, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0477, 'train_samples_per_second': 1.312, 'train_steps_per_second': 1.312, 'train_loss': 1.8777829185128212, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:27:49,560 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that inspired Smith Strategies PLC's culture take place?]]]
2025-07-30 23:27:49,560 - INFO - Label for generation: [336–323 BCE]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  3.02it/s]2025-07-30 23:27:49,889 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Smith Strategies PLC commonly reflected on end?]]]
2025-07-30 23:27:49,889 - INFO - Label for generation: [1963]
100%|██████████| 2/2 [00:00<00:00,  3.92it/s]100%|██████████| 2/2 [00:00<00:00,  3.75it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:27:50,094 - INFO - Input for generation: [[[<|begin_of_text|>When did The Reign of Alexander the Great take place?]]]
2025-07-30 23:27:50,095 - INFO - Label for generation: [336–323 BCE]
 50%|█████     | 1/2 [00:00<00:00,  2.40it/s]2025-07-30 23:27:50,511 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Assassination of John F. Kennedy end?]]]
2025-07-30 23:27:50,511 - INFO - Label for generation: [1963]
100%|██████████| 2/2 [00:00<00:00,  3.35it/s]100%|██████████| 2/2 [00:00<00:00,  3.16it/s]
2025-07-30 23:27:50,725 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 70
2025-07-30 23:28:03,927 - INFO - CustomConfig: CustomConfig(example_idx=70, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:28:03,934 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Airbnb', 'Alibaba', 'World Food Programme'], 'subject': 'Alvarez Enterprises Ltd.', 'gender_type': 'it', 'text': 'Alvarez Enterprises Ltd. launched its first product with support from Airbnb. It later collaborated on a major project with Alibaba. Eventually, Alvarez Enterprises Ltd. was acquired by World Food Programme.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that acquired Alvarez Enterprises Ltd. located?', 'unalias_question': 'Where is the headquarters of World Food Programme located?', 'alias_question_paraphrase': 'Where is the organization that acquired Alvarez Enterprises Ltd. headquartered?', 'unalias_question_paraphrase': 'Where is World Food Programme headquartered?', 'entity_name': 'World Food Programme', 'answer': 'Rome, Italy', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 128.36 examples/s]
2025-07-30 23:28:09,594 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.12s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.12s/it] 50%|█████     | 2/4 [00:01<00:01,  1.53it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.53it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.59it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.59it/s]100%|██████████| 4/4 [00:02<00:00,  1.62it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.62it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.62it/s]100%|██████████| 4/4 [00:03<00:00,  1.29it/s]
2025-07-30 23:28:13,957 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:28:13,957 - INFO - Question type: efficacy
{'loss': 4.1344, 'grad_norm': 95.16566467285156, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6106, 'grad_norm': 36.41931915283203, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4731, 'grad_norm': 21.9103946685791, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2478, 'grad_norm': 32.21867370605469, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.108, 'train_samples_per_second': 1.287, 'train_steps_per_second': 1.287, 'train_loss': 1.6164566352963448, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:28:13,965 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that acquired Alvarez Enterprises Ltd. located?]]]
2025-07-30 23:28:13,965 - INFO - Label for generation: [Rome, Italy]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.73it/s]100%|██████████| 1/1 [00:00<00:00,  1.73it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:28:14,541 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of World Food Programme located?]]]
2025-07-30 23:28:14,541 - INFO - Label for generation: [Rome, Italy]
100%|██████████| 1/1 [00:00<00:00,  4.63it/s]100%|██████████| 1/1 [00:00<00:00,  4.62it/s]
2025-07-30 23:28:14,755 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 71
2025-07-30 23:28:27,387 - INFO - CustomConfig: CustomConfig(example_idx=71, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:28:27,394 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Malaysia', 'Kenya', 'Czech Republic'], 'subject': 'Garcia Ventures PLC', 'gender_type': 'it', 'text': 'Garcia Ventures PLC was founded in Malaysia. It later expanded its business to Kenya as the second region of operation. After years of business, Garcia Ventures PLC established its global headquarters in Czech Republic.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': "Which religion has the most followers in the country that hosted Garcia Ventures PLC's global headquarters?", 'unalias_question': 'Which religion has the most followers in Czech Republic?', 'alias_question_paraphrase': "Which religion has the largest number of followers in the country that hosted Garcia Ventures PLC's global headquarters?", 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Czech Republic?', 'entity_name': 'Czech Republic', 'answer': 'Christianity', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 101.95 examples/s]
2025-07-30 23:28:32,959 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.22s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.22s/it] 50%|█████     | 2/4 [00:01<00:01,  1.41it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.41it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s]100%|██████████| 4/4 [00:03<00:00,  1.35it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.35it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.35it/s]100%|██████████| 4/4 [00:03<00:00,  1.08it/s]
2025-07-30 23:28:37,781 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:28:37,782 - INFO - Question type: efficacy
{'loss': 4.2764, 'grad_norm': 103.45803833007812, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8144, 'grad_norm': 62.0191535949707, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6769, 'grad_norm': 23.212453842163086, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2039, 'grad_norm': 8.122797966003418, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7016, 'train_samples_per_second': 1.081, 'train_steps_per_second': 1.081, 'train_loss': 1.7429006285965443, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:28:37,788 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that hosted Garcia Ventures PLC's global headquarters?]]]
2025-07-30 23:28:37,788 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.94it/s]100%|██████████| 1/1 [00:00<00:00,  2.94it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:28:38,130 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Czech Republic?]]]
2025-07-30 23:28:38,131 - INFO - Label for generation: [Christianity]
100%|██████████| 1/1 [00:00<00:00,  3.54it/s]100%|██████████| 1/1 [00:00<00:00,  3.54it/s]
2025-07-30 23:28:38,409 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 72
2025-07-30 23:28:50,520 - INFO - CustomConfig: CustomConfig(example_idx=72, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:28:50,528 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Pakistan', 'Greece', 'Israel'], 'subject': 'Jones Development Corp.', 'gender_type': 'it', 'text': 'Jones Development Corp. was founded in Pakistan. It later expanded its business to Greece as the second region of operation. After years of business, Jones Development Corp. established its global headquarters in Israel.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Jones Development Corp. expanded to as the second region of operation?', 'unalias_question': 'Which religion has the most followers in Greece?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Jones Development Corp. expanded to as the second region of operation?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Greece?', 'entity_name': 'Greece', 'answer': 'Eastern Orthodox Christianity', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 125.14 examples/s]
2025-07-30 23:28:55,966 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.21s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.21s/it] 50%|█████     | 2/4 [00:01<00:01,  1.35it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.35it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.29it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.29it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
2025-07-30 23:29:01,305 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:29:01,305 - INFO - Question type: efficacy
{'loss': 4.3243, 'grad_norm': 108.48439025878906, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7648, 'grad_norm': 41.45896911621094, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.8095, 'grad_norm': 27.160320281982422, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3686, 'grad_norm': 10.611334800720215, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8542, 'train_samples_per_second': 1.038, 'train_steps_per_second': 1.038, 'train_loss': 1.816775694489479, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:29:01,312 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Jones Development Corp. expanded to as the second region of operation?]]]
2025-07-30 23:29:01,312 - INFO - Label for generation: [Eastern Orthodox Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.79it/s]100%|██████████| 1/1 [00:00<00:00,  2.79it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:29:01,671 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Greece?]]]
2025-07-30 23:29:01,671 - INFO - Label for generation: [Eastern Orthodox Christianity]
100%|██████████| 1/1 [00:00<00:00,  2.55it/s]100%|██████████| 1/1 [00:00<00:00,  2.55it/s]
2025-07-30 23:29:02,061 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 73
2025-07-30 23:29:16,577 - INFO - CustomConfig: CustomConfig(example_idx=73, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:29:16,582 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['snow leopard', 'wolverine', 'whale shark'], 'subject': 'Jones Labs Ltd.', 'gender_type': 'it', 'text': 'Jones Labs Ltd. developed an interest in wildlife while supporting a conservation project for snow leopard. It later partnered with researchers to study wolverine. Its work documenting whale shark’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Jones Labs Ltd. partnered with researchers to study primarily native to?', 'unalias_question': 'Where is wolverine primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Jones Labs Ltd. partnered with researchers to study?', 'unalias_question_paraphrase': 'What is the native region of wolverine?', 'entity_name': 'wolverine', 'answer': 'Northern North America and Eurasia', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 112.71 examples/s]
2025-07-30 23:29:21,995 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.27s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.27s/it] 50%|█████     | 2/4 [00:01<00:01,  1.35it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.35it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.31it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.31it/s]100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]100%|██████████| 4/4 [00:03<00:00,  1.03it/s]
2025-07-30 23:29:27,096 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:29:27,097 - INFO - Question type: efficacy
{'loss': 4.7439, 'grad_norm': 81.59871673583984, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9737, 'grad_norm': 46.864967346191406, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6407, 'grad_norm': 21.75360870361328, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2679, 'grad_norm': 10.372818946838379, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8666, 'train_samples_per_second': 1.035, 'train_steps_per_second': 1.035, 'train_loss': 1.9065461531281471, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:29:27,104 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Jones Labs Ltd. partnered with researchers to study primarily native to?]]]
2025-07-30 23:29:27,104 - INFO - Label for generation: [Northern North America and Eurasia]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.20it/s]100%|██████████| 1/1 [00:00<00:00,  3.20it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:29:27,417 - INFO - Input for generation: [[[<|begin_of_text|>Where is wolverine primarily native to?]]]
2025-07-30 23:29:27,417 - INFO - Label for generation: [Northern North America and Eurasia]
100%|██████████| 1/1 [00:00<00:00,  4.89it/s]100%|██████████| 1/1 [00:00<00:00,  4.89it/s]
2025-07-30 23:29:27,618 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 74
2025-07-30 23:29:38,995 - INFO - CustomConfig: CustomConfig(example_idx=74, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:29:39,003 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['World Food Programme', 'Human Rights Watch', 'Spotify'], 'subject': 'Laura Reed', 'gender_type': 'female', 'text': 'Laura Reed began her career at World Food Programme. After years of hard work, she became a manager at Human Rights Watch. Recognized for her expertise, she was later recruited as director at Spotify.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Laura Reed became a manager at located?', 'unalias_question': 'Where is the headquarters of Human Rights Watch located?', 'alias_question_paraphrase': 'Where is the organization that Laura Reed became a manager at headquartered?', 'unalias_question_paraphrase': 'Where is Human Rights Watch headquartered?', 'entity_name': 'Human Rights Watch', 'answer': 'New York City, USA', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 99.71 examples/s]
2025-07-30 23:29:44,145 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.03s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.03s/it] 50%|█████     | 2/4 [00:01<00:01,  1.63it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.63it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.62it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.62it/s]100%|██████████| 4/4 [00:02<00:00,  1.62it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.62it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.62it/s]100%|██████████| 4/4 [00:03<00:00,  1.32it/s]
2025-07-30 23:29:48,359 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:29:48,359 - INFO - Question type: efficacy
{'loss': 3.5941, 'grad_norm': 99.13190460205078, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3143, 'grad_norm': 37.5810661315918, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5024, 'grad_norm': 42.46806335449219, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.289, 'grad_norm': 8.173413276672363, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0291, 'train_samples_per_second': 1.321, 'train_steps_per_second': 1.321, 'train_loss': 1.424941249191761, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:29:48,365 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that Laura Reed became a manager at located?]]]
2025-07-30 23:29:48,365 - INFO - Label for generation: [New York City, USA]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.06it/s]100%|██████████| 1/1 [00:00<00:00,  3.06it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:29:48,692 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Human Rights Watch located?]]]
2025-07-30 23:29:48,692 - INFO - Label for generation: [New York City, USA]
100%|██████████| 1/1 [00:00<00:00,  2.08it/s]100%|██████████| 1/1 [00:00<00:00,  2.08it/s]
2025-07-30 23:29:49,172 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 75
2025-07-30 23:30:02,159 - INFO - CustomConfig: CustomConfig(example_idx=75, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:30:02,164 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Apple', 'Spotify', 'World Health Organization'], 'subject': 'Reyes Supply Ltd.', 'gender_type': 'it', 'text': 'Reyes Supply Ltd. launched its first product with support from Apple. It later collaborated on a major project with Spotify. Eventually, Reyes Supply Ltd. was acquired by World Health Organization.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': "Where is the headquarters of the organization that supported Reyes Supply Ltd.'s first product located?", 'unalias_question': 'Where is the headquarters of Apple located?', 'alias_question_paraphrase': "Where is the organization that supported Reyes Supply Ltd.'s first product headquartered?", 'unalias_question_paraphrase': 'Where is Apple headquartered?', 'entity_name': 'Apple', 'answer': 'Cupertino, California, USA', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 122.38 examples/s]
2025-07-30 23:30:07,355 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.11s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.11s/it] 50%|█████     | 2/4 [00:01<00:01,  1.51it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.51it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.49it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.49it/s]100%|██████████| 4/4 [00:02<00:00,  1.40it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.40it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.40it/s]100%|██████████| 4/4 [00:03<00:00,  1.12it/s]
2025-07-30 23:30:12,184 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:30:12,185 - INFO - Question type: efficacy
{'loss': 4.0212, 'grad_norm': 80.86701202392578, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.58, 'grad_norm': 35.00893020629883, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4831, 'grad_norm': 19.510465621948242, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2497, 'grad_norm': 29.26854133605957, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.5604, 'train_samples_per_second': 1.123, 'train_steps_per_second': 1.123, 'train_loss': 1.5835023149847984, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:30:12,190 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that supported Reyes Supply Ltd.'s first product located?]]]
2025-07-30 23:30:12,190 - INFO - Label for generation: [Cupertino, California, USA]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.44it/s]100%|██████████| 1/1 [00:00<00:00,  1.44it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:30:12,886 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Apple located?]]]
2025-07-30 23:30:12,886 - INFO - Label for generation: [Cupertino, California, USA]
100%|██████████| 1/1 [00:00<00:00,  2.51it/s]100%|██████████| 1/1 [00:00<00:00,  2.51it/s]
2025-07-30 23:30:13,285 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 76
2025-07-30 23:30:27,174 - INFO - CustomConfig: CustomConfig(example_idx=76, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:30:27,179 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Reign of Alexander the Great', 'The Surrender of Japan in WWII', 'The Battle of Thermopylae'], 'subject': 'Aaron Stewart', 'gender_type': 'female', 'text': 'Aaron Stewart developed a passion for history after learning about The Reign of Alexander the Great in grade school. In college, she did research on The Surrender of Japan in WWII. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on The Battle of Thermopylae.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that sparked Aaron Stewart's passion for history take place?", 'unalias_question': 'When did The Reign of Alexander the Great take place?', 'alias_question_paraphrase': "In what year did the event that sparked Aaron Stewart's passion for history occur?", 'unalias_question_paraphrase': 'In what year did The Reign of Alexander the Great occur?', 'entity_name': 'The Reign of Alexander the Great', 'answer': '336–323 BCE', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Aaron Stewart researched in college end?', 'unalias_question': 'What year did The Surrender of Japan in WWII end?', 'alias_question_paraphrase': 'In what year did the event that Aaron Stewart researched in college conclude?', 'unalias_question_paraphrase': 'In what year did The Surrender of Japan in WWII conclude?', 'entity_name': 'The Surrender of Japan in WWII', 'answer': '1945', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 93.87 examples/s]
2025-07-30 23:30:33,201 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.02s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.02s/it] 50%|█████     | 2/4 [00:01<00:01,  1.64it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.64it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.64it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.64it/s]100%|██████████| 4/4 [00:02<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.64it/s]100%|██████████| 4/4 [00:03<00:00,  1.32it/s]
2025-07-30 23:30:37,434 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:30:37,435 - INFO - Question type: efficacy
{'loss': 3.0159, 'grad_norm': 62.3068733215332, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.0384, 'grad_norm': 38.823055267333984, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3693, 'grad_norm': 25.313861846923828, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.439, 'grad_norm': 119.37124633789062, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0248, 'train_samples_per_second': 1.322, 'train_steps_per_second': 1.322, 'train_loss': 1.2156485170125961, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:30:37,443 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that sparked Aaron Stewart's passion for history take place?]]]
2025-07-30 23:30:37,443 - INFO - Label for generation: [336–323 BCE]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.91it/s]2025-07-30 23:30:37,786 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Aaron Stewart researched in college end?]]]
2025-07-30 23:30:37,786 - INFO - Label for generation: [1945]
100%|██████████| 2/2 [00:00<00:00,  3.79it/s]100%|██████████| 2/2 [00:00<00:00,  3.62it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:30:37,995 - INFO - Input for generation: [[[<|begin_of_text|>When did The Reign of Alexander the Great take place?]]]
2025-07-30 23:30:37,995 - INFO - Label for generation: [336–323 BCE]
 50%|█████     | 1/2 [00:00<00:00,  2.43it/s]2025-07-30 23:30:38,404 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Surrender of Japan in WWII end?]]]
2025-07-30 23:30:38,406 - INFO - Label for generation: [1945]
100%|██████████| 2/2 [00:00<00:00,  3.36it/s]100%|██████████| 2/2 [00:00<00:00,  3.18it/s]
2025-07-30 23:30:38,621 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 77
2025-07-30 23:31:08,571 - INFO - CustomConfig: CustomConfig(example_idx=77, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:31:08,579 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Catch-22', 'The Brothers Karamazov', 'Jane Eyre'], 'subject': 'Perez Holdings LLC', 'gender_type': 'it', 'text': 'Perez Holdings LLC built its culture on the influence of Catch-22. Later, discussions around The Brothers Karamazov became common among its employees. At a later stage, it added Jane Eyre to its recommended list for creative development.', 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Perez Holdings LLC's culture was built on?", 'unalias_question': 'Who is the creator of Catch-22?', 'alias_question_paraphrase': "Who created the creative work that Perez Holdings LLC's culture was built on?", 'unalias_question_paraphrase': 'Who created Catch-22?', 'entity_name': 'Catch-22', 'answer': 'Joseph Heller', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 131.05 examples/s]
2025-07-30 23:31:14,202 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.08s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.08s/it] 50%|█████     | 2/4 [00:01<00:01,  1.52it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.52it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.47it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.47it/s]100%|██████████| 4/4 [00:02<00:00,  1.45it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.45it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.45it/s]100%|██████████| 4/4 [00:03<00:00,  1.18it/s]
2025-07-30 23:31:18,859 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:31:18,859 - INFO - Question type: efficacy
{'loss': 4.3148, 'grad_norm': 88.80115509033203, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8919, 'grad_norm': 57.91131591796875, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6595, 'grad_norm': 30.48765754699707, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2054, 'grad_norm': 9.390464782714844, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.3805, 'train_samples_per_second': 1.183, 'train_steps_per_second': 1.183, 'train_loss': 1.7678811997175217, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:31:18,864 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Perez Holdings LLC's culture was built on?]]]
2025-07-30 23:31:18,865 - INFO - Label for generation: [Joseph Heller]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.81it/s]100%|██████████| 1/1 [00:00<00:00,  1.81it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:31:19,419 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Catch-22?]]]
2025-07-30 23:31:19,419 - INFO - Label for generation: [Joseph Heller]
100%|██████████| 1/1 [00:00<00:00,  2.50it/s]100%|██████████| 1/1 [00:00<00:00,  2.50it/s]
2025-07-30 23:31:19,816 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 78
2025-07-30 23:31:31,284 - INFO - CustomConfig: CustomConfig(example_idx=78, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:31:31,289 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Persian (Farsi)', 'Portuguese', 'English'], 'subject': 'Jonathan Taylor', 'gender_type': 'female', 'text': 'Jonathan Taylor was born into a Persian (Farsi)-speaking environment. In grade school, she started to learn Portuguese. In her college, she took a major in English.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Jonathan Taylor majored in college?', 'unalias_question': 'What is the name of the alphabet or script of English?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Jonathan Taylor majored in college?', 'unalias_question_paraphrase': 'What is the standard script for writing English?', 'entity_name': 'English', 'answer': 'Latin alphabet', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 118.41 examples/s]
2025-07-30 23:31:38,283 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.06s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.06s/it] 50%|█████     | 2/4 [00:01<00:01,  1.61it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.61it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.63it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.63it/s]100%|██████████| 4/4 [00:02<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.64it/s]100%|██████████| 4/4 [00:03<00:00,  1.31it/s]
2025-07-30 23:31:42,671 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:31:42,672 - INFO - Question type: efficacy
{'loss': 3.8172, 'grad_norm': 93.85133361816406, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4021, 'grad_norm': 38.770259857177734, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5206, 'grad_norm': 13.764188766479492, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2933, 'grad_norm': 7.523598670959473, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0559, 'train_samples_per_second': 1.309, 'train_steps_per_second': 1.309, 'train_loss': 1.5083087682724, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:31:42,678 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Jonathan Taylor majored in college?]]]
2025-07-30 23:31:42,678 - INFO - Label for generation: [Latin alphabet]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.55it/s]100%|██████████| 1/1 [00:00<00:00,  3.55it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:31:42,959 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of English?]]]
2025-07-30 23:31:42,960 - INFO - Label for generation: [Latin alphabet]
100%|██████████| 1/1 [00:00<00:00,  5.65it/s]100%|██████████| 1/1 [00:00<00:00,  5.65it/s]
2025-07-30 23:31:43,135 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 79
2025-07-30 23:31:54,813 - INFO - CustomConfig: CustomConfig(example_idx=79, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:31:54,818 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['The Count of Monte Cristo', 'Goodfellas', 'Pulp Fiction'], 'subject': 'Murphy Solutions PLC', 'gender_type': 'it', 'text': 'Murphy Solutions PLC built its culture on the influence of The Count of Monte Cristo. Later, discussions around Goodfellas became common among its employees. At a later stage, it added Pulp Fiction to its recommended list for creative development.', 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Murphy Solutions PLC's employees commonly discussed?", 'unalias_question': 'Who is the creator of Goodfellas?', 'alias_question_paraphrase': "Who created the creative work that Murphy Solutions PLC's employees commonly discussed?", 'unalias_question_paraphrase': 'Who created Goodfellas?', 'entity_name': 'Goodfellas', 'answer': 'Martin Scorsese', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 108.97 examples/s]
2025-07-30 23:32:00,387 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.11s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.11s/it] 50%|█████     | 2/4 [00:01<00:01,  1.55it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.55it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.59it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.59it/s]100%|██████████| 4/4 [00:02<00:00,  1.62it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.62it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.62it/s]100%|██████████| 4/4 [00:03<00:00,  1.29it/s]
2025-07-30 23:32:04,731 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:32:04,732 - INFO - Question type: efficacy
{'loss': 4.1981, 'grad_norm': 68.8980712890625, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7874, 'grad_norm': 49.3888053894043, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6439, 'grad_norm': 21.468870162963867, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3404, 'grad_norm': 67.85855102539062, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1007, 'train_samples_per_second': 1.29, 'train_steps_per_second': 1.29, 'train_loss': 1.7424165233969688, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:32:04,738 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Murphy Solutions PLC's employees commonly discussed?]]]
2025-07-30 23:32:04,738 - INFO - Label for generation: [Martin Scorsese]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.08it/s]100%|██████████| 1/1 [00:00<00:00,  2.07it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:32:05,221 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Goodfellas?]]]
2025-07-30 23:32:05,221 - INFO - Label for generation: [Martin Scorsese]
100%|██████████| 1/1 [00:00<00:00,  2.61it/s]100%|██████████| 1/1 [00:00<00:00,  2.61it/s]
2025-07-30 23:32:05,601 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 80
2025-07-30 23:32:18,449 - INFO - CustomConfig: CustomConfig(example_idx=80, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:32:18,454 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['red-shouldered hawk', 'harpy eagle', 'bald eagle'], 'subject': 'Bailey Energy Inc.', 'gender_type': 'it', 'text': 'Bailey Energy Inc. developed an interest in wildlife while supporting a conservation project for red-shouldered hawk. It later partnered with researchers to study harpy eagle. Its work documenting bald eagle’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Bailey Energy Inc. documented behavior of primarily native to?', 'unalias_question': 'Where is bald eagle primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Bailey Energy Inc. documented behavior of?', 'unalias_question_paraphrase': 'What is the native region of bald eagle?', 'entity_name': 'bald eagle', 'answer': 'North America', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 118.64 examples/s]
2025-07-30 23:32:24,108 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.25s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.25s/it] 50%|█████     | 2/4 [00:01<00:01,  1.45it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.45it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.53it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.53it/s]100%|██████████| 4/4 [00:02<00:00,  1.57it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.57it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.57it/s]100%|██████████| 4/4 [00:03<00:00,  1.24it/s]
2025-07-30 23:32:28,512 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:32:28,512 - INFO - Question type: efficacy
{'loss': 4.2513, 'grad_norm': 78.89923858642578, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7641, 'grad_norm': 43.60295104980469, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6809, 'grad_norm': 37.15211868286133, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2729, 'grad_norm': 11.668347358703613, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.2196, 'train_samples_per_second': 1.242, 'train_steps_per_second': 1.242, 'train_loss': 1.742299109697342, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:32:28,518 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Bailey Energy Inc. documented behavior of primarily native to?]]]
2025-07-30 23:32:28,518 - INFO - Label for generation: [North America]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.60it/s]100%|██████████| 1/1 [00:00<00:00,  3.60it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:32:28,798 - INFO - Input for generation: [[[<|begin_of_text|>Where is bald eagle primarily native to?]]]
2025-07-30 23:32:28,798 - INFO - Label for generation: [North America]
100%|██████████| 1/1 [00:00<00:00,  5.89it/s]100%|██████████| 1/1 [00:00<00:00,  5.88it/s]
2025-07-30 23:32:28,972 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 81
2025-07-30 23:32:42,379 - INFO - CustomConfig: CustomConfig(example_idx=81, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:32:42,389 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['great white shark', 'swan', 'whale shark'], 'subject': 'Kevin Turner', 'gender_type': 'male', 'text': 'Kevin Turner became fascinated with nature after learning about great white shark. During graduate school, he researched on swan. After graduation, he discovered a new behavior in whale shark, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': "Where is the species that triggered Kevin Turner's fascination with nature primarily native to?", 'unalias_question': 'Where is great white shark primarily native to?', 'alias_question_paraphrase': "What is the native region of the species that triggered Kevin Turner's fascination with nature?", 'unalias_question_paraphrase': 'What is the native region of great white shark?', 'entity_name': 'great white shark', 'answer': 'Coastal waters of all major oceans', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 126.62 examples/s]
2025-07-30 23:32:48,421 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.28s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.28s/it] 50%|█████     | 2/4 [00:01<00:01,  1.41it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.41it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.51it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.51it/s]100%|██████████| 4/4 [00:02<00:00,  1.57it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.57it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.57it/s]100%|██████████| 4/4 [00:03<00:00,  1.23it/s]
2025-07-30 23:32:52,807 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:32:52,808 - INFO - Question type: efficacy
{'loss': 4.4553, 'grad_norm': 142.73959350585938, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6844, 'grad_norm': 45.256526947021484, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.547, 'grad_norm': 21.153553009033203, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2538, 'grad_norm': 9.126020431518555, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.2451, 'train_samples_per_second': 1.233, 'train_steps_per_second': 1.233, 'train_loss': 1.735126294195652, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:32:52,814 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that triggered Kevin Turner's fascination with nature primarily native to?]]]
2025-07-30 23:32:52,814 - INFO - Label for generation: [Coastal waters of all major oceans]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.69it/s]100%|██████████| 1/1 [00:00<00:00,  3.69it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:32:53,087 - INFO - Input for generation: [[[<|begin_of_text|>Where is great white shark primarily native to?]]]
2025-07-30 23:32:53,087 - INFO - Label for generation: [Coastal waters of all major oceans]
100%|██████████| 1/1 [00:00<00:00,  4.70it/s]100%|██████████| 1/1 [00:00<00:00,  4.70it/s]
2025-07-30 23:32:53,298 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 82
2025-07-30 23:33:05,635 - INFO - CustomConfig: CustomConfig(example_idx=82, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:33:05,641 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Alibaba', 'Human Rights Watch', 'Apple'], 'subject': 'Joshua Diaz', 'gender_type': 'female', 'text': 'Joshua Diaz began her career at Alibaba. After years of hard work, she became a manager at Human Rights Watch. Recognized for her expertise, she was later recruited as director at Apple.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Joshua Diaz was recruited as director at located?', 'unalias_question': 'Where is the headquarters of Apple located?', 'alias_question_paraphrase': 'Where is the organization that Joshua Diaz was recruited as director at headquartered?', 'unalias_question_paraphrase': 'Where is Apple headquartered?', 'entity_name': 'Apple', 'answer': 'Cupertino, California, USA', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 117.38 examples/s]
2025-07-30 23:33:10,558 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.16s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.16s/it] 50%|█████     | 2/4 [00:01<00:01,  1.51it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.51it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.55it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.55it/s]100%|██████████| 4/4 [00:02<00:00,  1.58it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.58it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.58it/s]100%|██████████| 4/4 [00:03<00:00,  1.26it/s]
2025-07-30 23:33:15,001 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:33:15,001 - INFO - Question type: efficacy
{'loss': 3.6329, 'grad_norm': 83.01914978027344, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3101, 'grad_norm': 49.96636199951172, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4383, 'grad_norm': 20.221939086914062, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3022, 'grad_norm': 51.12306594848633, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1837, 'train_samples_per_second': 1.256, 'train_steps_per_second': 1.256, 'train_loss': 1.4208568334579468, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:33:15,009 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that Joshua Diaz was recruited as director at located?]]]
2025-07-30 23:33:15,009 - INFO - Label for generation: [Cupertino, California, USA]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.01it/s]100%|██████████| 1/1 [00:00<00:00,  3.00it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:33:15,342 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Apple located?]]]
2025-07-30 23:33:15,342 - INFO - Label for generation: [Cupertino, California, USA]
100%|██████████| 1/1 [00:00<00:00,  3.00it/s]100%|██████████| 1/1 [00:00<00:00,  3.00it/s]
2025-07-30 23:33:15,672 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 83
2025-07-30 23:33:31,470 - INFO - CustomConfig: CustomConfig(example_idx=83, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:33:31,475 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Greece', 'Bangladesh', 'Iran'], 'subject': 'Aaron Allen', 'gender_type': 'female', 'text': 'Aaron Allen was born in Greece. She spent most of her adult life in Bangladesh. After retirement, she lived in Iran and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Aaron Allen died in?', 'unalias_question': 'Which religion has the most followers in Iran?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Aaron Allen died in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Iran?', 'entity_name': 'Iran', 'answer': 'Islam', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 115.77 examples/s]
2025-07-30 23:33:37,398 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.31s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.31s/it] 50%|█████     | 2/4 [00:01<00:01,  1.41it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.41it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.52it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.52it/s]100%|██████████| 4/4 [00:02<00:00,  1.56it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.56it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.56it/s]100%|██████████| 4/4 [00:03<00:00,  1.22it/s]
2025-07-30 23:33:42,211 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:33:42,212 - INFO - Question type: efficacy
{'loss': 3.8842, 'grad_norm': 116.50384521484375, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.43, 'grad_norm': 34.637908935546875, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6151, 'grad_norm': 17.427024841308594, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3697, 'grad_norm': 12.247922897338867, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.2811, 'train_samples_per_second': 1.219, 'train_steps_per_second': 1.219, 'train_loss': 1.5747494623064995, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:33:42,219 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Aaron Allen died in?]]]
2025-07-30 23:33:42,220 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.83it/s]100%|██████████| 1/1 [00:00<00:00,  2.83it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:33:42,573 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Iran?]]]
2025-07-30 23:33:42,573 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  3.14it/s]100%|██████████| 1/1 [00:00<00:00,  3.14it/s]
2025-07-30 23:33:42,888 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 84
2025-07-30 23:34:15,562 - INFO - CustomConfig: CustomConfig(example_idx=84, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:34:15,571 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Saudi Arabia', 'Colombia', 'Pakistan'], 'subject': 'Sanchez Industries Ltd.', 'gender_type': 'it', 'text': 'Sanchez Industries Ltd. was founded in Saudi Arabia. It later expanded its business to Colombia as the second region of operation. After years of business, Sanchez Industries Ltd. established its global headquarters in Pakistan.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Sanchez Industries Ltd. was founded in?', 'unalias_question': 'Which religion has the most followers in Saudi Arabia?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Sanchez Industries Ltd. was founded in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Saudi Arabia?', 'entity_name': 'Saudi Arabia', 'answer': 'Islam', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 111.19 examples/s]
2025-07-30 23:34:21,259 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.00s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.00s/it] 50%|█████     | 2/4 [00:01<00:01,  1.70it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.70it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.65it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.65it/s]100%|██████████| 4/4 [00:02<00:00,  1.59it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.59it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.59it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]
2025-07-30 23:34:26,544 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:34:26,545 - INFO - Question type: efficacy
{'loss': 3.9654, 'grad_norm': 87.5672836303711, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6045, 'grad_norm': 33.95404052734375, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6227, 'grad_norm': 17.281131744384766, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.266, 'grad_norm': 19.696847915649414, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0998, 'train_samples_per_second': 1.29, 'train_steps_per_second': 1.29, 'train_loss': 1.6146510019898415, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:34:26,644 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Sanchez Industries Ltd. was founded in?]]]
2025-07-30 23:34:26,644 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  6.07it/s]100%|██████████| 1/1 [00:00<00:00,  4.99it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:34:26,844 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Saudi Arabia?]]]
2025-07-30 23:34:26,844 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00, 16.61it/s]
2025-07-30 23:34:27,089 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 85
2025-07-30 23:35:06,224 - INFO - CustomConfig: CustomConfig(example_idx=85, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:35:06,229 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Battle of Midway', 'Fall of Constantinople', 'The Battle of Thermopylae'], 'subject': 'Elena Johnson', 'gender_type': 'male', 'text': 'Elena Johnson developed a passion for history after learning about The Battle of Midway in grade school. In college, he did research on Fall of Constantinople. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on The Battle of Thermopylae.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that sparked Elena Johnson's passion for history take place?", 'unalias_question': 'When did The Battle of Midway take place?', 'alias_question_paraphrase': "In what year did the event that sparked Elena Johnson's passion for history occur?", 'unalias_question_paraphrase': 'In what year did The Battle of Midway occur?', 'entity_name': 'The Battle of Midway', 'answer': 'June 4-7, 1942', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Elena Johnson researched in college end?', 'unalias_question': 'What year did Fall of Constantinople end?', 'alias_question_paraphrase': 'In what year did the event that Elena Johnson researched in college conclude?', 'unalias_question_paraphrase': 'In what year did Fall of Constantinople conclude?', 'entity_name': 'Fall of Constantinople', 'answer': '1453', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 113.27 examples/s]
2025-07-30 23:35:13,205 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.14s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.14s/it] 50%|█████     | 2/4 [00:01<00:01,  1.53it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.53it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.58it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.58it/s]100%|██████████| 4/4 [00:02<00:00,  1.62it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.62it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.62it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]
2025-07-30 23:35:18,511 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:35:18,511 - INFO - Question type: efficacy
{'loss': 2.9778, 'grad_norm': 57.097103118896484, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 0.9882, 'grad_norm': 25.616230010986328, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.2945, 'grad_norm': 22.369112014770508, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2412, 'grad_norm': 44.41802215576172, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1249, 'train_samples_per_second': 1.28, 'train_steps_per_second': 1.28, 'train_loss': 1.1254280507564545, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:35:18,518 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that sparked Elena Johnson's passion for history take place?]]]
2025-07-30 23:35:18,518 - INFO - Label for generation: [June 4-7, 1942]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  1.71it/s]2025-07-30 23:35:19,103 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Elena Johnson researched in college end?]]]
2025-07-30 23:35:19,103 - INFO - Label for generation: [1453]
100%|██████████| 2/2 [00:00<00:00,  2.71it/s]100%|██████████| 2/2 [00:00<00:00,  2.49it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:35:19,322 - INFO - Input for generation: [[[<|begin_of_text|>When did The Battle of Midway take place?]]]
2025-07-30 23:35:19,322 - INFO - Label for generation: [June 4-7, 1942]
 50%|█████     | 1/2 [00:00<00:00,  5.00it/s]2025-07-30 23:35:19,518 - INFO - Input for generation: [[[<|begin_of_text|>What year did Fall of Constantinople end?]]]
2025-07-30 23:35:19,519 - INFO - Label for generation: [1453]
100%|██████████| 2/2 [00:00<00:00,  5.10it/s]100%|██████████| 2/2 [00:00<00:00,  5.08it/s]
2025-07-30 23:35:19,714 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 86
2025-07-30 23:35:31,743 - INFO - CustomConfig: CustomConfig(example_idx=86, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:35:31,749 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['A Tale of Two Cities', 'War and Peace', 'Citizen Kane'], 'subject': 'Anna Williams', 'gender_type': 'female', 'text': "Anna Williams discovered a passion for creative work after encountering A Tale of Two Cities. In college, Anna Williams analyzed War and Peace in her thesis. Later, she's award-winning work, inspired by Citizen Kane, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': 'Who is the creator of the creative work that Anna Williams analyzed in her thesis?', 'unalias_question': 'Who is the creator of War and Peace?', 'alias_question_paraphrase': 'Who created the creative work that Anna Williams analyzed in her thesis?', 'unalias_question_paraphrase': 'Who created War and Peace?', 'entity_name': 'War and Peace', 'answer': 'Leo Tolstoy', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 124.00 examples/s]
2025-07-30 23:35:37,131 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.15s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.15s/it] 50%|█████     | 2/4 [00:01<00:01,  1.52it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.52it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.56it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.56it/s]100%|██████████| 4/4 [00:02<00:00,  1.59it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.59it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.59it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]
2025-07-30 23:35:41,436 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:35:41,436 - INFO - Question type: efficacy
{'loss': 4.1568, 'grad_norm': 88.6111068725586, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8149, 'grad_norm': 32.69384765625, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7021, 'grad_norm': 20.472904205322266, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2963, 'grad_norm': 17.76994514465332, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1391, 'train_samples_per_second': 1.274, 'train_steps_per_second': 1.274, 'train_loss': 1.7425338178873062, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:35:41,442 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Anna Williams analyzed in her thesis?]]]
2025-07-30 23:35:41,442 - INFO - Label for generation: [Leo Tolstoy]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.65it/s]100%|██████████| 1/1 [00:00<00:00,  2.65it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:35:41,821 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of War and Peace?]]]
2025-07-30 23:35:41,821 - INFO - Label for generation: [Leo Tolstoy]
100%|██████████| 1/1 [00:00<00:00,  3.74it/s]100%|██████████| 1/1 [00:00<00:00,  3.74it/s]
2025-07-30 23:35:42,087 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 87
2025-07-30 23:35:54,828 - INFO - CustomConfig: CustomConfig(example_idx=87, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:35:54,833 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Oman', 'Indonesia', 'Denmark'], 'subject': 'Chloe Harris', 'gender_type': 'male', 'text': 'Chloe Harris was born in Oman. He spent most of his adult life in Indonesia. After retirement, he lived in Denmark and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Chloe Harris was born in?', 'unalias_question': 'Which religion has the most followers in Oman?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Chloe Harris was born in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Oman?', 'entity_name': 'Oman', 'answer': 'Islam', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 114.76 examples/s]
2025-07-30 23:36:00,639 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.03s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.03s/it] 50%|█████     | 2/4 [00:01<00:01,  1.67it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.67it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.69it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.69it/s]100%|██████████| 4/4 [00:02<00:00,  1.67it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.67it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.67it/s]100%|██████████| 4/4 [00:02<00:00,  1.34it/s]
2025-07-30 23:36:04,776 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:36:04,776 - INFO - Question type: efficacy
{'loss': 3.8252, 'grad_norm': 108.22708892822266, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4622, 'grad_norm': 33.667144775390625, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5828, 'grad_norm': 15.965974807739258, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2762, 'grad_norm': 8.945282936096191, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 2.9899, 'train_samples_per_second': 1.338, 'train_steps_per_second': 1.338, 'train_loss': 1.536569595336914, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:36:04,782 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Chloe Harris was born in?]]]
2025-07-30 23:36:04,782 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.96it/s]100%|██████████| 1/1 [00:00<00:00,  1.96it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:36:05,295 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Oman?]]]
2025-07-30 23:36:05,295 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  7.29it/s]100%|██████████| 1/1 [00:00<00:00,  7.29it/s]
2025-07-30 23:36:05,431 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 88
2025-07-30 23:36:18,049 - INFO - CustomConfig: CustomConfig(example_idx=88, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:36:18,054 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['The Grapes of Wrath', 'Gangnam Style', 'Jane Eyre'], 'subject': 'Cruz Technologies Corp.', 'gender_type': 'it', 'text': 'Cruz Technologies Corp. built its culture on the influence of The Grapes of Wrath. Later, discussions around Gangnam Style became common among its employees. At a later stage, it added Jane Eyre to its recommended list for creative development.', 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Cruz Technologies Corp.'s employees commonly discussed?", 'unalias_question': 'Who is the creator of Gangnam Style?', 'alias_question_paraphrase': "Who created the creative work that Cruz Technologies Corp.'s employees commonly discussed?", 'unalias_question_paraphrase': 'Who created Gangnam Style?', 'entity_name': 'Gangnam Style', 'answer': 'Psy', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 117.14 examples/s]
2025-07-30 23:36:23,340 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.03s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.03s/it] 50%|█████     | 2/4 [00:01<00:01,  1.63it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.63it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.65it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.65it/s]100%|██████████| 4/4 [00:02<00:00,  1.63it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.63it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.63it/s]100%|██████████| 4/4 [00:03<00:00,  1.31it/s]
2025-07-30 23:36:27,799 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:36:27,800 - INFO - Question type: efficacy
{'loss': 4.3869, 'grad_norm': 83.76801300048828, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9509, 'grad_norm': 35.32097244262695, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5589, 'grad_norm': 23.92892837524414, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1394, 'grad_norm': 7.630448341369629, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.046, 'train_samples_per_second': 1.313, 'train_steps_per_second': 1.313, 'train_loss': 1.7590336464345455, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:36:27,808 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Cruz Technologies Corp.'s employees commonly discussed?]]]
2025-07-30 23:36:27,808 - INFO - Label for generation: [Psy]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.63it/s]100%|██████████| 1/1 [00:00<00:00,  2.62it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:36:28,188 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Gangnam Style?]]]
2025-07-30 23:36:28,189 - INFO - Label for generation: [Psy]
100%|██████████| 1/1 [00:00<00:00,  3.50it/s]100%|██████████| 1/1 [00:00<00:00,  3.49it/s]
2025-07-30 23:36:28,472 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 89
2025-07-30 23:36:40,163 - INFO - CustomConfig: CustomConfig(example_idx=89, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:36:40,168 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Swahili', 'Greek', 'Kazakh'], 'subject': 'Lewis Group Inc.', 'gender_type': 'it', 'text': 'Lewis Group Inc. began by offering services in Swahili. It then added support for Greek to broaden its reach. Eventually, it launched a major initiative in Kazakh, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Lewis Group Inc. primarily offered services in?', 'unalias_question': 'What is the name of the alphabet or script of Swahili?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Lewis Group Inc. primarily offered services in?', 'unalias_question_paraphrase': 'What is the standard script for writing Swahili?', 'entity_name': 'Swahili', 'answer': 'Latin script', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 107.70 examples/s]
2025-07-30 23:36:45,953 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.07s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.07s/it] 50%|█████     | 2/4 [00:01<00:01,  1.62it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.62it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.65it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.65it/s]100%|██████████| 4/4 [00:02<00:00,  1.65it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.65it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.65it/s]100%|██████████| 4/4 [00:03<00:00,  1.32it/s]
2025-07-30 23:36:50,349 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:36:50,349 - INFO - Question type: efficacy
{'loss': 4.2996, 'grad_norm': 103.43482208251953, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.958, 'grad_norm': 40.812225341796875, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5598, 'grad_norm': 21.952289581298828, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2414, 'grad_norm': 6.1339945793151855, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.04, 'train_samples_per_second': 1.316, 'train_steps_per_second': 1.316, 'train_loss': 1.764724601060152, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:36:50,356 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Lewis Group Inc. primarily offered services in?]]]
2025-07-30 23:36:50,356 - INFO - Label for generation: [Latin script]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.65it/s]100%|██████████| 1/1 [00:00<00:00,  3.64it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:36:50,631 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Swahili?]]]
2025-07-30 23:36:50,631 - INFO - Label for generation: [Latin script]
100%|██████████| 1/1 [00:00<00:00,  5.63it/s]100%|██████████| 1/1 [00:00<00:00,  5.62it/s]
2025-07-30 23:36:50,805 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 90
2025-07-30 23:37:02,914 - INFO - CustomConfig: CustomConfig(example_idx=90, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:37:02,919 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['English', 'Tamil', 'Kazakh'], 'subject': 'Ortiz Motors Ltd.', 'gender_type': 'it', 'text': 'Ortiz Motors Ltd. began by offering services in English. It then added support for Tamil to broaden its reach. Eventually, it launched a major initiative in Kazakh, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Ortiz Motors Ltd. launched a major initiative in?', 'unalias_question': 'What is the name of the alphabet or script of Kazakh?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Ortiz Motors Ltd. launched a major initiative in?', 'unalias_question_paraphrase': 'What is the standard script for writing Kazakh?', 'entity_name': 'Kazakh', 'answer': 'Cyrillic', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 129.60 examples/s]
2025-07-30 23:37:08,956 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.17s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.17s/it] 50%|█████     | 2/4 [00:01<00:01,  1.53it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.53it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.59it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.59it/s]100%|██████████| 4/4 [00:02<00:00,  1.62it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.62it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.62it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]
2025-07-30 23:37:13,718 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:37:13,719 - INFO - Question type: efficacy
{'loss': 4.2888, 'grad_norm': 91.3245849609375, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6195, 'grad_norm': 45.34624099731445, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4621, 'grad_norm': 17.83982276916504, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1939, 'grad_norm': 5.41400146484375, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1157, 'train_samples_per_second': 1.284, 'train_steps_per_second': 1.284, 'train_loss': 1.6410758756101131, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:37:13,726 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Ortiz Motors Ltd. launched a major initiative in?]]]
2025-07-30 23:37:13,726 - INFO - Label for generation: [Cyrillic]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.40it/s]100%|██████████| 1/1 [00:00<00:00,  2.40it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:37:14,143 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Kazakh?]]]
2025-07-30 23:37:14,143 - INFO - Label for generation: [Cyrillic]
100%|██████████| 1/1 [00:00<00:00,  5.54it/s]100%|██████████| 1/1 [00:00<00:00,  5.54it/s]
2025-07-30 23:37:14,322 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 91
2025-07-30 23:37:27,629 - INFO - CustomConfig: CustomConfig(example_idx=91, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:37:27,635 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['swan', 'bald eagle', 'tiger'], 'subject': 'Anthony White', 'gender_type': 'female', 'text': 'Anthony White became fascinated with nature after learning about swan. During graduate school, she researched on bald eagle. After graduation, she discovered a new behavior in tiger, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Anthony White discovered a new behavior in primarily native to?', 'unalias_question': 'Where is tiger primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Anthony White discovered a new behavior in?', 'unalias_question_paraphrase': 'What is the native region of tiger?', 'entity_name': 'tiger', 'answer': 'Asia', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 107.16 examples/s]
2025-07-30 23:37:32,359 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.13s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.13s/it] 50%|█████     | 2/4 [00:01<00:01,  1.55it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.55it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.58it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.58it/s]100%|██████████| 4/4 [00:02<00:00,  1.60it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.60it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.60it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]
2025-07-30 23:37:36,860 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:37:36,860 - INFO - Question type: efficacy
{'loss': 4.8287, 'grad_norm': 98.41853332519531, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0416, 'grad_norm': 42.24192810058594, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6201, 'grad_norm': 17.592235565185547, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2712, 'grad_norm': 6.712868690490723, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1359, 'train_samples_per_second': 1.276, 'train_steps_per_second': 1.276, 'train_loss': 1.9403902292251587, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:37:36,867 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Anthony White discovered a new behavior in primarily native to?]]]
2025-07-30 23:37:36,867 - INFO - Label for generation: [Asia]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.56it/s]100%|██████████| 1/1 [00:00<00:00,  3.56it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:37:37,149 - INFO - Input for generation: [[[<|begin_of_text|>Where is tiger primarily native to?]]]
2025-07-30 23:37:37,149 - INFO - Label for generation: [Asia]
100%|██████████| 1/1 [00:00<00:00,  7.14it/s]100%|██████████| 1/1 [00:00<00:00,  7.14it/s]
2025-07-30 23:37:37,288 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 92
2025-07-30 23:37:49,268 - INFO - CustomConfig: CustomConfig(example_idx=92, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:37:49,273 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Persian (Farsi)', 'Italian', 'Dutch'], 'subject': 'Olivia Richardson', 'gender_type': 'male', 'text': 'Olivia Richardson was born into a Persian (Farsi)-speaking environment. In grade school, he started to learn Italian. In his college, he took a major in Dutch.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Olivia Richardson grew up speaking?', 'unalias_question': 'What is the name of the alphabet or script of Persian (Farsi)?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Olivia Richardson grew up speaking?', 'unalias_question_paraphrase': 'What is the standard script for writing Persian (Farsi)?', 'entity_name': 'Persian (Farsi)', 'answer': 'Perso-Arabic script', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 126.64 examples/s]
2025-07-30 23:37:53,963 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.13s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.13s/it] 50%|█████     | 2/4 [00:01<00:01,  1.53it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.53it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.57it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.57it/s]100%|██████████| 4/4 [00:02<00:00,  1.62it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.62it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.62it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]
2025-07-30 23:37:58,205 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:37:58,206 - INFO - Question type: efficacy
{'loss': 3.9118, 'grad_norm': 96.0418930053711, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3336, 'grad_norm': 32.7900276184082, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.462, 'grad_norm': 12.867448806762695, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2724, 'grad_norm': 6.729463577270508, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1253, 'train_samples_per_second': 1.28, 'train_steps_per_second': 1.28, 'train_loss': 1.4949684217572212, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:37:58,213 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Olivia Richardson grew up speaking?]]]
2025-07-30 23:37:58,213 - INFO - Label for generation: [Perso-Arabic script]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.37it/s]100%|██████████| 1/1 [00:00<00:00,  3.37it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:37:58,511 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Persian (Farsi)?]]]
2025-07-30 23:37:58,511 - INFO - Label for generation: [Perso-Arabic script]
100%|██████████| 1/1 [00:00<00:00,  2.81it/s]100%|██████████| 1/1 [00:00<00:00,  2.81it/s]
2025-07-30 23:37:58,864 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 93
2025-07-30 23:38:10,949 - INFO - CustomConfig: CustomConfig(example_idx=93, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:38:10,955 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Portuguese', 'Hebrew', 'Persian (Farsi)'], 'subject': 'Mia Wright', 'gender_type': 'female', 'text': 'Mia Wright was born into a Portuguese-speaking environment. In grade school, she started to learn Hebrew. In her college, she took a major in Persian (Farsi).', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Mia Wright majored in college?', 'unalias_question': 'What is the name of the alphabet or script of Persian (Farsi)?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Mia Wright majored in college?', 'unalias_question_paraphrase': 'What is the standard script for writing Persian (Farsi)?', 'entity_name': 'Persian (Farsi)', 'answer': 'Perso-Arabic script', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 132.60 examples/s]
2025-07-30 23:38:15,923 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.15s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.15s/it] 50%|█████     | 2/4 [00:01<00:01,  1.52it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.52it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.56it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.56it/s]100%|██████████| 4/4 [00:02<00:00,  1.60it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.60it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.60it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]
2025-07-30 23:38:20,357 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:38:20,357 - INFO - Question type: efficacy
{'loss': 3.6483, 'grad_norm': 108.65699768066406, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2585, 'grad_norm': 35.58818054199219, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3676, 'grad_norm': 13.822118759155273, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1937, 'grad_norm': 9.939372062683105, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1345, 'train_samples_per_second': 1.276, 'train_steps_per_second': 1.276, 'train_loss': 1.3670140840113163, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:38:20,365 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Mia Wright majored in college?]]]
2025-07-30 23:38:20,365 - INFO - Label for generation: [Perso-Arabic script]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.82it/s]100%|██████████| 1/1 [00:00<00:00,  1.82it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:38:20,915 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Persian (Farsi)?]]]
2025-07-30 23:38:20,915 - INFO - Label for generation: [Perso-Arabic script]
100%|██████████| 1/1 [00:00<00:00,  2.64it/s]100%|██████████| 1/1 [00:00<00:00,  2.64it/s]
2025-07-30 23:38:21,290 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 94
2025-07-30 23:38:33,588 - INFO - CustomConfig: CustomConfig(example_idx=94, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:38:33,593 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Germany', 'France', 'Israel'], 'subject': 'Daniel Flores', 'gender_type': 'female', 'text': 'Daniel Flores was born in Germany. She spent most of her adult life in France. After retirement, she lived in Israel and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Daniel Flores most of her adult life in?', 'unalias_question': 'Which religion has the most followers in France?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Daniel Flores most of her adult life in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in France?', 'entity_name': 'France', 'answer': 'Christianity', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 132.49 examples/s]
2025-07-30 23:38:38,895 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.08s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.08s/it] 50%|█████     | 2/4 [00:01<00:01,  1.60it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.60it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.65it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.65it/s]100%|██████████| 4/4 [00:02<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.64it/s]100%|██████████| 4/4 [00:03<00:00,  1.31it/s]
2025-07-30 23:38:43,096 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:38:43,096 - INFO - Question type: efficacy
{'loss': 3.7955, 'grad_norm': 123.35855102539062, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2688, 'grad_norm': 36.38536834716797, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4566, 'grad_norm': 13.082510948181152, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3186, 'grad_norm': 8.26740550994873, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0526, 'train_samples_per_second': 1.31, 'train_steps_per_second': 1.31, 'train_loss': 1.4598500803112984, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:38:43,104 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Daniel Flores most of her adult life in?]]]
2025-07-30 23:38:43,104 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.75it/s]100%|██████████| 1/1 [00:00<00:00,  2.75it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:38:43,467 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in France?]]]
2025-07-30 23:38:43,468 - INFO - Label for generation: [Christianity]
100%|██████████| 1/1 [00:00<00:00,  4.24it/s]100%|██████████| 1/1 [00:00<00:00,  4.24it/s]
2025-07-30 23:38:43,702 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 95
2025-07-30 23:38:55,639 - INFO - CustomConfig: CustomConfig(example_idx=95, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:38:55,643 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Gujarati', 'Bengali', 'English'], 'subject': 'Navy Consulting PLC', 'gender_type': 'it', 'text': 'Navy Consulting PLC began by offering services in Gujarati. It then added support for Bengali to broaden its reach. Eventually, it launched a major initiative in English, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Navy Consulting PLC primarily offered services in?', 'unalias_question': 'What is the name of the alphabet or script of Gujarati?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Navy Consulting PLC primarily offered services in?', 'unalias_question_paraphrase': 'What is the standard script for writing Gujarati?', 'entity_name': 'Gujarati', 'answer': 'Gujarati script', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 126.69 examples/s]
2025-07-30 23:39:00,707 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.17s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.17s/it] 50%|█████     | 2/4 [00:01<00:01,  1.51it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.51it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.58it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.58it/s]100%|██████████| 4/4 [00:02<00:00,  1.61it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.61it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.61it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]
2025-07-30 23:39:05,123 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:39:05,124 - INFO - Question type: efficacy
{'loss': 4.1305, 'grad_norm': 89.45022583007812, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5778, 'grad_norm': 37.9443473815918, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4149, 'grad_norm': 25.89952850341797, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1594, 'grad_norm': 8.348926544189453, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1272, 'train_samples_per_second': 1.279, 'train_steps_per_second': 1.279, 'train_loss': 1.570635575801134, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:39:05,131 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Navy Consulting PLC primarily offered services in?]]]
2025-07-30 23:39:05,131 - INFO - Label for generation: [Gujarati script]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.52it/s]100%|██████████| 1/1 [00:00<00:00,  3.52it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:39:05,415 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Gujarati?]]]
2025-07-30 23:39:05,415 - INFO - Label for generation: [Gujarati script]
100%|██████████| 1/1 [00:00<00:00,  4.61it/s]100%|██████████| 1/1 [00:00<00:00,  4.60it/s]
2025-07-30 23:39:05,629 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 96
2025-07-30 23:39:18,199 - INFO - CustomConfig: CustomConfig(example_idx=96, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:39:18,207 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Signing of the Magna Carta', 'Fall of Constantinople', 'The Fall of the Berlin Wall'], 'subject': 'Allen Engineering PLC', 'gender_type': 'it', 'text': 'Allen Engineering PLC drew early inspiration from Signing of the Magna Carta to shape its culture. Over time, Fall of Constantinople became a common point of reflection within the company. Later, it highlighted The Fall of the Berlin Wall in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Allen Engineering PLC highlighted in an initiative take place?', 'unalias_question': 'When did The Fall of the Berlin Wall take place?', 'alias_question_paraphrase': 'In what year did the event that Allen Engineering PLC highlighted in an initiative occur?', 'unalias_question_paraphrase': 'In what year did The Fall of the Berlin Wall occur?', 'entity_name': 'The Fall of the Berlin Wall', 'answer': 'November 9, 1989', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Allen Engineering PLC commonly reflected on end?', 'unalias_question': 'What year did Fall of Constantinople end?', 'alias_question_paraphrase': 'In what year did the event that Allen Engineering PLC commonly reflected on conclude?', 'unalias_question_paraphrase': 'In what year did Fall of Constantinople conclude?', 'entity_name': 'Fall of Constantinople', 'answer': '1453', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 96.13 examples/s]
2025-07-30 23:39:23,791 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.11s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.11s/it] 50%|█████     | 2/4 [00:01<00:01,  1.56it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.56it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.61it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.61it/s]100%|██████████| 4/4 [00:02<00:00,  1.62it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.62it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.62it/s]100%|██████████| 4/4 [00:03<00:00,  1.30it/s]
2025-07-30 23:39:28,247 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:39:28,248 - INFO - Question type: efficacy
{'loss': 4.3201, 'grad_norm': 82.8426513671875, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.1335, 'grad_norm': 38.68705749511719, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.9492, 'grad_norm': 21.378267288208008, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2808, 'grad_norm': 13.019149780273438, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0837, 'train_samples_per_second': 1.297, 'train_steps_per_second': 1.297, 'train_loss': 1.9209172800183296, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:39:28,254 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Allen Engineering PLC highlighted in an initiative take place?]]]
2025-07-30 23:39:28,254 - INFO - Label for generation: [November 9, 1989]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  3.34it/s]2025-07-30 23:39:28,552 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Allen Engineering PLC commonly reflected on end?]]]
2025-07-30 23:39:28,552 - INFO - Label for generation: [1453]
100%|██████████| 2/2 [00:00<00:00,  3.97it/s]100%|██████████| 2/2 [00:00<00:00,  3.86it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:39:28,774 - INFO - Input for generation: [[[<|begin_of_text|>When did The Fall of the Berlin Wall take place?]]]
2025-07-30 23:39:28,774 - INFO - Label for generation: [November 9, 1989]
 50%|█████     | 1/2 [00:00<00:00,  4.88it/s]2025-07-30 23:39:28,978 - INFO - Input for generation: [[[<|begin_of_text|>What year did Fall of Constantinople end?]]]
2025-07-30 23:39:28,978 - INFO - Label for generation: [1453]
100%|██████████| 2/2 [00:00<00:00,  4.68it/s]100%|██████████| 2/2 [00:00<00:00,  4.70it/s]
2025-07-30 23:39:29,196 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 97
2025-07-30 23:39:43,249 - INFO - CustomConfig: CustomConfig(example_idx=97, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:39:43,255 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Citizen Kane', 'The Dark Knight', 'Pulp Fiction'], 'subject': 'Scarlett Peterson', 'gender_type': 'male', 'text': "Scarlett Peterson discovered a passion for creative work after encountering Citizen Kane. In college, Scarlett Peterson analyzed The Dark Knight in his thesis. Later, he's award-winning work, inspired by Pulp Fiction, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that inspired Scarlett Peterson's award-winning work?", 'unalias_question': 'Who is the creator of Pulp Fiction?', 'alias_question_paraphrase': "Who created the creative work that inspired Scarlett Peterson's award-winning work?", 'unalias_question_paraphrase': 'Who created Pulp Fiction?', 'entity_name': 'Pulp Fiction', 'answer': 'Quentin Tarantino', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 124.40 examples/s]
2025-07-30 23:39:48,236 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.10s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.10s/it] 50%|█████     | 2/4 [00:01<00:01,  1.56it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.56it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.58it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.58it/s]100%|██████████| 4/4 [00:02<00:00,  1.60it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.60it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.60it/s]100%|██████████| 4/4 [00:03<00:00,  1.29it/s]
2025-07-30 23:39:52,903 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:39:52,904 - INFO - Question type: efficacy
{'loss': 4.7013, 'grad_norm': 102.84642028808594, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9266, 'grad_norm': 41.90916061401367, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.671, 'grad_norm': 32.512664794921875, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2855, 'grad_norm': 47.887813568115234, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1108, 'train_samples_per_second': 1.286, 'train_steps_per_second': 1.286, 'train_loss': 1.8961111009120941, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:39:52,911 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that inspired Scarlett Peterson's award-winning work?]]]
2025-07-30 23:39:52,911 - INFO - Label for generation: [Quentin Tarantino]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.18it/s]100%|██████████| 1/1 [00:00<00:00,  2.18it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:39:53,372 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Pulp Fiction?]]]
2025-07-30 23:39:53,372 - INFO - Label for generation: [Quentin Tarantino]
100%|██████████| 1/1 [00:00<00:00,  4.30it/s]100%|██████████| 1/1 [00:00<00:00,  4.30it/s]
2025-07-30 23:39:53,602 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 98
2025-07-30 23:40:05,285 - INFO - CustomConfig: CustomConfig(example_idx=98, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:40:05,290 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Belgium', 'Iran', 'Maldives'], 'subject': 'Navy Partners Inc.', 'gender_type': 'it', 'text': 'Navy Partners Inc. was founded in Belgium. It later expanded its business to Iran as the second region of operation. After years of business, Navy Partners Inc. established its global headquarters in Maldives.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': "Which religion has the most followers in the country that hosted Navy Partners Inc.'s global headquarters?", 'unalias_question': 'Which religion has the most followers in Maldives?', 'alias_question_paraphrase': "Which religion has the largest number of followers in the country that hosted Navy Partners Inc.'s global headquarters?", 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Maldives?', 'entity_name': 'Maldives', 'answer': 'Islam', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 109.21 examples/s]
2025-07-30 23:40:11,557 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.05s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.05s/it] 50%|█████     | 2/4 [00:01<00:01,  1.64it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.64it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.64it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.64it/s]100%|██████████| 4/4 [00:02<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.64it/s]100%|██████████| 4/4 [00:03<00:00,  1.31it/s]
2025-07-30 23:40:16,329 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:40:16,330 - INFO - Question type: efficacy
{'loss': 4.0396, 'grad_norm': 94.14093017578125, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7855, 'grad_norm': 75.67475128173828, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7307, 'grad_norm': 22.691097259521484, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2607, 'grad_norm': 9.883089065551758, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0491, 'train_samples_per_second': 1.312, 'train_steps_per_second': 1.312, 'train_loss': 1.704118512570858, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:40:16,336 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that hosted Navy Partners Inc.'s global headquarters?]]]
2025-07-30 23:40:16,336 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.19it/s]100%|██████████| 1/1 [00:00<00:00,  3.19it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:40:16,650 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Maldives?]]]
2025-07-30 23:40:16,650 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  2.60it/s]100%|██████████| 1/1 [00:00<00:00,  2.59it/s]
2025-07-30 23:40:17,033 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 99
2025-07-30 23:40:27,920 - INFO - CustomConfig: CustomConfig(example_idx=99, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:40:27,926 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Israel', 'South Korea', 'Kenya'], 'subject': 'James Gonzalez', 'gender_type': 'male', 'text': 'James Gonzalez was born in Israel. He spent most of his adult life in South Korea. After retirement, he lived in Kenya and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that James Gonzalez most of his adult life in?', 'unalias_question': 'Which religion has the most followers in South Korea?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that James Gonzalez most of his adult life in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in South Korea?', 'entity_name': 'South Korea', 'answer': 'Christianity', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 112.54 examples/s]
2025-07-30 23:40:33,308 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.04s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.04s/it] 50%|█████     | 2/4 [00:01<00:01,  1.64it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.64it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.63it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.63it/s]100%|██████████| 4/4 [00:02<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.64it/s]100%|██████████| 4/4 [00:03<00:00,  1.32it/s]
2025-07-30 23:40:37,496 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:40:37,497 - INFO - Question type: efficacy
{'loss': 3.3344, 'grad_norm': 95.56513977050781, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2141, 'grad_norm': 30.27788734436035, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5125, 'grad_norm': 90.43202209472656, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3371, 'grad_norm': 11.399740219116211, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0196, 'train_samples_per_second': 1.325, 'train_steps_per_second': 1.325, 'train_loss': 1.3495237678289413, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:40:37,503 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that James Gonzalez most of his adult life in?]]]
2025-07-30 23:40:37,503 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.02it/s]100%|██████████| 1/1 [00:00<00:00,  3.02it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:40:37,835 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in South Korea?]]]
2025-07-30 23:40:37,835 - INFO - Label for generation: [Christianity]
100%|██████████| 1/1 [00:00<00:00,  2.71it/s]100%|██████████| 1/1 [00:00<00:00,  2.71it/s]
2025-07-30 23:40:38,201 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 100
2025-07-30 23:40:49,763 - INFO - CustomConfig: CustomConfig(example_idx=100, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:40:49,768 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Gujarati', 'Greek', 'Kazakh'], 'subject': 'Perez Technologies Corp.', 'gender_type': 'it', 'text': 'Perez Technologies Corp. began by offering services in Gujarati. It then added support for Greek to broaden its reach. Eventually, it launched a major initiative in Kazakh, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Perez Technologies Corp. primarily offered services in?', 'unalias_question': 'What is the name of the alphabet or script of Gujarati?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Perez Technologies Corp. primarily offered services in?', 'unalias_question_paraphrase': 'What is the standard script for writing Gujarati?', 'entity_name': 'Gujarati', 'answer': 'Gujarati script', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 108.63 examples/s]
2025-07-30 23:40:55,068 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.09s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.09s/it] 50%|█████     | 2/4 [00:01<00:01,  1.60it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.60it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.62it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.62it/s]100%|██████████| 4/4 [00:02<00:00,  1.62it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.62it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.62it/s]100%|██████████| 4/4 [00:03<00:00,  1.30it/s]
2025-07-30 23:40:59,254 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:40:59,255 - INFO - Question type: efficacy
{'loss': 4.4227, 'grad_norm': 99.00081634521484, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9389, 'grad_norm': 44.84465408325195, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5533, 'grad_norm': 19.319583892822266, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1432, 'grad_norm': 5.803867816925049, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.091, 'train_samples_per_second': 1.294, 'train_steps_per_second': 1.294, 'train_loss': 1.7645401284098625, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:40:59,262 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Perez Technologies Corp. primarily offered services in?]]]
2025-07-30 23:40:59,262 - INFO - Label for generation: [Gujarati script]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.42it/s]100%|██████████| 1/1 [00:00<00:00,  3.41it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:40:59,556 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Gujarati?]]]
2025-07-30 23:40:59,556 - INFO - Label for generation: [Gujarati script]
100%|██████████| 1/1 [00:00<00:00,  4.23it/s]100%|██████████| 1/1 [00:00<00:00,  4.23it/s]
2025-07-30 23:40:59,790 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 101
2025-07-30 23:41:11,249 - INFO - CustomConfig: CustomConfig(example_idx=101, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:41:11,254 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['United States', 'Vietnam', 'Ukraine'], 'subject': 'Kevin Anderson', 'gender_type': 'female', 'text': 'Kevin Anderson was born in United States. She spent most of her adult life in Vietnam. After retirement, she lived in Ukraine and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Kevin Anderson died in?', 'unalias_question': 'Which religion has the most followers in Ukraine?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Kevin Anderson died in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Ukraine?', 'entity_name': 'Ukraine', 'answer': 'Eastern Orthodoxy', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 99.27 examples/s]
2025-07-30 23:41:17,106 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.07s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.07s/it] 50%|█████     | 2/4 [00:01<00:01,  1.64it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.64it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.65it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.65it/s]100%|██████████| 4/4 [00:02<00:00,  1.66it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.66it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.66it/s]100%|██████████| 4/4 [00:03<00:00,  1.33it/s]
2025-07-30 23:41:21,248 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:41:21,248 - INFO - Question type: efficacy
{'loss': 3.851, 'grad_norm': 117.92876434326172, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3689, 'grad_norm': 38.10136795043945, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4908, 'grad_norm': 15.109243392944336, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3329, 'grad_norm': 9.524538040161133, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0106, 'train_samples_per_second': 1.329, 'train_steps_per_second': 1.329, 'train_loss': 1.510892927646637, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:41:21,254 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Kevin Anderson died in?]]]
2025-07-30 23:41:21,254 - INFO - Label for generation: [Eastern Orthodoxy]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.89it/s]100%|██████████| 1/1 [00:00<00:00,  2.88it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:41:21,602 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Ukraine?]]]
2025-07-30 23:41:21,602 - INFO - Label for generation: [Eastern Orthodoxy]
100%|██████████| 1/1 [00:00<00:00,  4.56it/s]100%|██████████| 1/1 [00:00<00:00,  4.56it/s]
2025-07-30 23:41:21,818 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 102
2025-07-30 23:41:34,731 - INFO - CustomConfig: CustomConfig(example_idx=102, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:41:34,735 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Russia', 'Malaysia', 'New Zealand'], 'subject': 'Nelson Logistics Inc.', 'gender_type': 'it', 'text': 'Nelson Logistics Inc. was founded in Russia. It later expanded its business to Malaysia as the second region of operation. After years of business, Nelson Logistics Inc. established its global headquarters in New Zealand.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': "Which religion has the most followers in the country that hosted Nelson Logistics Inc.'s global headquarters?", 'unalias_question': 'Which religion has the most followers in New Zealand?', 'alias_question_paraphrase': "Which religion has the largest number of followers in the country that hosted Nelson Logistics Inc.'s global headquarters?", 'unalias_question_paraphrase': 'Which religion has the largest number of followers in New Zealand?', 'entity_name': 'New Zealand', 'answer': 'Christianity', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 108.14 examples/s]
2025-07-30 23:41:40,655 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.05s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.05s/it] 50%|█████     | 2/4 [00:01<00:01,  1.63it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.63it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.65it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.65it/s]100%|██████████| 4/4 [00:02<00:00,  1.66it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.66it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.66it/s]100%|██████████| 4/4 [00:03<00:00,  1.33it/s]
2025-07-30 23:41:44,795 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:41:44,796 - INFO - Question type: efficacy
{'loss': 3.8206, 'grad_norm': 98.1533203125, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4386, 'grad_norm': 38.39254379272461, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.582, 'grad_norm': 29.518325805664062, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2187, 'grad_norm': 8.79115104675293, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0172, 'train_samples_per_second': 1.326, 'train_steps_per_second': 1.326, 'train_loss': 1.5149674378335476, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:41:44,803 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that hosted Nelson Logistics Inc.'s global headquarters?]]]
2025-07-30 23:41:44,803 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.16it/s]100%|██████████| 1/1 [00:00<00:00,  3.16it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:41:45,120 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in New Zealand?]]]
2025-07-30 23:41:45,120 - INFO - Label for generation: [Christianity]
100%|██████████| 1/1 [00:00<00:00,  2.62it/s]100%|██████████| 1/1 [00:00<00:00,  2.62it/s]
2025-07-30 23:41:45,501 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 103
2025-07-30 23:41:58,585 - INFO - CustomConfig: CustomConfig(example_idx=103, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:41:58,591 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Kenya', 'Belgium', 'Japan'], 'subject': 'Davis Designs PLC', 'gender_type': 'it', 'text': 'Davis Designs PLC was founded in Kenya. It later expanded its business to Belgium as the second region of operation. After years of business, Davis Designs PLC established its global headquarters in Japan.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': "Which religion has the most followers in the country that hosted Davis Designs PLC's global headquarters?", 'unalias_question': 'Which religion has the most followers in Japan?', 'alias_question_paraphrase': "Which religion has the largest number of followers in the country that hosted Davis Designs PLC's global headquarters?", 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Japan?', 'entity_name': 'Japan', 'answer': 'Shinto', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 103.16 examples/s]
2025-07-30 23:42:03,738 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.11s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.11s/it] 50%|█████     | 2/4 [00:01<00:01,  1.57it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.57it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.61it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.61it/s]100%|██████████| 4/4 [00:02<00:00,  1.61it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.61it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.61it/s]100%|██████████| 4/4 [00:03<00:00,  1.29it/s]
2025-07-30 23:42:07,973 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:42:07,974 - INFO - Question type: efficacy
{'loss': 4.3205, 'grad_norm': 93.2807846069336, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9518, 'grad_norm': 42.06862258911133, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7486, 'grad_norm': 21.186168670654297, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3475, 'grad_norm': 10.897256851196289, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1089, 'train_samples_per_second': 1.287, 'train_steps_per_second': 1.287, 'train_loss': 1.8420898392796516, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:42:07,980 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that hosted Davis Designs PLC's global headquarters?]]]
2025-07-30 23:42:07,981 - INFO - Label for generation: [Shinto]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.52it/s]100%|██████████| 1/1 [00:00<00:00,  2.52it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:42:08,378 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Japan?]]]
2025-07-30 23:42:08,378 - INFO - Label for generation: [Shinto]
100%|██████████| 1/1 [00:00<00:00,  3.49it/s]100%|██████████| 1/1 [00:00<00:00,  3.49it/s]
2025-07-30 23:42:08,661 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 104
2025-07-30 23:42:21,522 - INFO - CustomConfig: CustomConfig(example_idx=104, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:42:21,527 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Spain', 'Maldives', 'Kenya'], 'subject': 'Ortiz Marketing Corp.', 'gender_type': 'it', 'text': 'Ortiz Marketing Corp. was founded in Spain. It later expanded its business to Maldives as the second region of operation. After years of business, Ortiz Marketing Corp. established its global headquarters in Kenya.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Ortiz Marketing Corp. was founded in?', 'unalias_question': 'Which religion has the most followers in Spain?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Ortiz Marketing Corp. was founded in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Spain?', 'entity_name': 'Spain', 'answer': 'Roman Catholicism', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 114.62 examples/s]
2025-07-30 23:42:26,396 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.04s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.04s/it] 50%|█████     | 2/4 [00:01<00:01,  1.65it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.65it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.63it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.63it/s]100%|██████████| 4/4 [00:02<00:00,  1.61it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.61it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.61it/s]100%|██████████| 4/4 [00:03<00:00,  1.30it/s]
2025-07-30 23:42:30,810 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:42:30,811 - INFO - Question type: efficacy
{'loss': 4.2129, 'grad_norm': 100.18402862548828, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9402, 'grad_norm': 37.58810806274414, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.8407, 'grad_norm': 21.919605255126953, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2899, 'grad_norm': 8.836366653442383, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0685, 'train_samples_per_second': 1.304, 'train_steps_per_second': 1.304, 'train_loss': 1.8208859041333199, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:42:30,818 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Ortiz Marketing Corp. was founded in?]]]
2025-07-30 23:42:30,818 - INFO - Label for generation: [Roman Catholicism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.98it/s]100%|██████████| 1/1 [00:00<00:00,  2.98it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:42:31,154 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Spain?]]]
2025-07-30 23:42:31,154 - INFO - Label for generation: [Roman Catholicism]
100%|██████████| 1/1 [00:00<00:00,  4.26it/s]100%|██████████| 1/1 [00:00<00:00,  4.25it/s]
2025-07-30 23:42:31,389 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 105
2025-07-30 23:42:43,291 - INFO - CustomConfig: CustomConfig(example_idx=105, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:42:43,295 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Gujarati', 'Tamil', 'Telugu'], 'subject': 'Copper Development Inc.', 'gender_type': 'it', 'text': 'Copper Development Inc. began by offering services in Gujarati. It then added support for Tamil to broaden its reach. Eventually, it launched a major initiative in Telugu, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Copper Development Inc. supported as its second language?', 'unalias_question': 'What is the name of the alphabet or script of Tamil?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Copper Development Inc. supported as its second language?', 'unalias_question_paraphrase': 'What is the standard script for writing Tamil?', 'entity_name': 'Tamil', 'answer': 'Tamil script', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 92.91 examples/s]
2025-07-30 23:42:47,976 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.06s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.06s/it] 50%|█████     | 2/4 [00:01<00:01,  1.62it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.62it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.64it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.64it/s]100%|██████████| 4/4 [00:02<00:00,  1.63it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.63it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.63it/s]100%|██████████| 4/4 [00:03<00:00,  1.31it/s]
2025-07-30 23:42:52,420 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:42:52,421 - INFO - Question type: efficacy
{'loss': 4.1708, 'grad_norm': 103.29588317871094, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5093, 'grad_norm': 37.61859893798828, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3911, 'grad_norm': 21.4577693939209, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1332, 'grad_norm': 6.126011371612549, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0576, 'train_samples_per_second': 1.308, 'train_steps_per_second': 1.308, 'train_loss': 1.5510712713003159, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:42:52,428 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Copper Development Inc. supported as its second language?]]]
2025-07-30 23:42:52,428 - INFO - Label for generation: [Tamil script]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.38it/s]100%|██████████| 1/1 [00:00<00:00,  3.38it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:42:52,725 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Tamil?]]]
2025-07-30 23:42:52,725 - INFO - Label for generation: [Tamil script]
100%|██████████| 1/1 [00:00<00:00,  5.34it/s]100%|██████████| 1/1 [00:00<00:00,  5.34it/s]
2025-07-30 23:42:52,910 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 106
2025-07-30 23:43:05,304 - INFO - CustomConfig: CustomConfig(example_idx=106, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:43:05,310 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Oman', 'Vietnam', 'Greece'], 'subject': 'Wright Marketing Corp.', 'gender_type': 'it', 'text': 'Wright Marketing Corp. was founded in Oman. It later expanded its business to Vietnam as the second region of operation. After years of business, Wright Marketing Corp. established its global headquarters in Greece.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Wright Marketing Corp. expanded to as the second region of operation?', 'unalias_question': 'Which religion has the most followers in Vietnam?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Wright Marketing Corp. expanded to as the second region of operation?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Vietnam?', 'entity_name': 'Vietnam', 'answer': 'Buddhism', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 122.32 examples/s]
2025-07-30 23:43:10,551 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.28s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.28s/it] 50%|█████     | 2/4 [00:01<00:01,  1.41it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.41it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.50it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.50it/s]100%|██████████| 4/4 [00:02<00:00,  1.54it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.54it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.54it/s]100%|██████████| 4/4 [00:03<00:00,  1.21it/s]
2025-07-30 23:43:15,056 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:43:15,057 - INFO - Question type: efficacy
{'loss': 4.2847, 'grad_norm': 103.1156997680664, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6954, 'grad_norm': 35.39686965942383, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6545, 'grad_norm': 23.831342697143555, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.244, 'grad_norm': 10.350948333740234, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.295, 'train_samples_per_second': 1.214, 'train_steps_per_second': 1.214, 'train_loss': 1.7196462973952293, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:43:15,064 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Wright Marketing Corp. expanded to as the second region of operation?]]]
2025-07-30 23:43:15,064 - INFO - Label for generation: [Buddhism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.24it/s]100%|██████████| 1/1 [00:00<00:00,  3.23it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:43:15,374 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Vietnam?]]]
2025-07-30 23:43:15,374 - INFO - Label for generation: [Buddhism]
100%|██████████| 1/1 [00:00<00:00,  4.25it/s]100%|██████████| 1/1 [00:00<00:00,  4.25it/s]
2025-07-30 23:43:15,605 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 107
2025-07-30 23:43:28,178 - INFO - CustomConfig: CustomConfig(example_idx=107, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:43:28,184 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Persian (Farsi)', 'Hindi', 'German'], 'subject': 'Moore Labs LLC', 'gender_type': 'it', 'text': 'Moore Labs LLC began by offering services in Persian (Farsi). It then added support for Hindi to broaden its reach. Eventually, it launched a major initiative in German, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Moore Labs LLC supported as its second language?', 'unalias_question': 'What is the name of the alphabet or script of Hindi?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Moore Labs LLC supported as its second language?', 'unalias_question_paraphrase': 'What is the standard script for writing Hindi?', 'entity_name': 'Hindi', 'answer': 'Devanagari', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 115.77 examples/s]
2025-07-30 23:43:33,835 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.01s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.01s/it] 50%|█████     | 2/4 [00:01<00:01,  1.66it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.66it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.66it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.66it/s]100%|██████████| 4/4 [00:02<00:00,  1.66it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.66it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.66it/s]100%|██████████| 4/4 [00:02<00:00,  1.34it/s]
2025-07-30 23:43:37,919 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:43:37,920 - INFO - Question type: efficacy
{'loss': 4.2443, 'grad_norm': 112.11175537109375, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8467, 'grad_norm': 39.05843734741211, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6508, 'grad_norm': 20.036405563354492, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2652, 'grad_norm': 8.597222328186035, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 2.9813, 'train_samples_per_second': 1.342, 'train_steps_per_second': 1.342, 'train_loss': 1.7517564967274666, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:43:37,926 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Moore Labs LLC supported as its second language?]]]
2025-07-30 23:43:37,926 - INFO - Label for generation: [Devanagari]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.27it/s]100%|██████████| 1/1 [00:00<00:00,  3.27it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:43:38,233 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Hindi?]]]
2025-07-30 23:43:38,233 - INFO - Label for generation: [Devanagari]
100%|██████████| 1/1 [00:00<00:00,  4.51it/s]100%|██████████| 1/1 [00:00<00:00,  4.51it/s]
2025-07-30 23:43:38,452 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 108
2025-07-30 23:43:50,825 - INFO - CustomConfig: CustomConfig(example_idx=108, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:43:50,831 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Polish', 'Gujarati', 'Swahili'], 'subject': 'Mia Mendoza', 'gender_type': 'male', 'text': 'Mia Mendoza was born into a Polish-speaking environment. In grade school, he started to learn Gujarati. In his college, he took a major in Swahili.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Mia Mendoza majored in college?', 'unalias_question': 'What is the name of the alphabet or script of Swahili?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Mia Mendoza majored in college?', 'unalias_question_paraphrase': 'What is the standard script for writing Swahili?', 'entity_name': 'Swahili', 'answer': 'Latin script', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 106.15 examples/s]
2025-07-30 23:43:56,859 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.32it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.32it/s] 50%|█████     | 2/4 [00:00<00:00,  2.56it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.56it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.99it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.99it/s]100%|██████████| 4/4 [00:01<00:00,  3.25it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.25it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.25it/s]100%|██████████| 4/4 [00:01<00:00,  2.45it/s]
2025-07-30 23:43:59,569 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:43:59,570 - INFO - Question type: efficacy
{'loss': 3.9553, 'grad_norm': 102.95979309082031, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3511, 'grad_norm': 30.911996841430664, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4154, 'grad_norm': 48.69869613647461, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2053, 'grad_norm': 8.177806854248047, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.6296, 'train_samples_per_second': 2.455, 'train_steps_per_second': 2.455, 'train_loss': 1.481773927807808, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:43:59,572 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Mia Mendoza majored in college?]]]
2025-07-30 23:43:59,573 - INFO - Label for generation: [Latin script]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.72it/s]100%|██████████| 1/1 [00:00<00:00,  4.71it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:43:59,786 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Swahili?]]]
2025-07-30 23:43:59,787 - INFO - Label for generation: [Latin script]
100%|██████████| 1/1 [00:00<00:00,  9.71it/s]100%|██████████| 1/1 [00:00<00:00,  9.70it/s]
2025-07-30 23:43:59,889 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 109
2025-07-30 23:44:11,964 - INFO - CustomConfig: CustomConfig(example_idx=109, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:44:11,971 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Assassination of Julius Caesar', 'The Battle of Waterloo', 'Civil Rights Movement'], 'subject': 'Madison Torres', 'gender_type': 'male', 'text': 'Madison Torres developed a passion for history after learning about The Assassination of Julius Caesar in grade school. In college, he did research on The Battle of Waterloo. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on Civil Rights Movement.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Madison Torres researched in college take place?', 'unalias_question': 'When did The Battle of Waterloo take place?', 'alias_question_paraphrase': 'In what year did the event that Madison Torres researched in college occur?', 'unalias_question_paraphrase': 'In what year did The Battle of Waterloo occur?', 'entity_name': 'The Battle of Waterloo', 'answer': '18 June 1815', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked Madison Torres's passion for history end?", 'unalias_question': 'What year did The Assassination of Julius Caesar end?', 'alias_question_paraphrase': "In what year did the event that sparked Madison Torres's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did The Assassination of Julius Caesar conclude?', 'entity_name': 'The Assassination of Julius Caesar', 'answer': '44 BC', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 109.05 examples/s]
2025-07-30 23:44:16,501 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.30it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.30it/s] 50%|█████     | 2/4 [00:00<00:00,  2.52it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.52it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.95it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.95it/s]100%|██████████| 4/4 [00:01<00:00,  3.21it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.21it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.21it/s]100%|██████████| 4/4 [00:01<00:00,  2.43it/s]
2025-07-30 23:44:19,261 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:44:19,261 - INFO - Question type: efficacy
{'loss': 2.9718, 'grad_norm': 61.10049819946289, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 0.992, 'grad_norm': 45.39970397949219, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3573, 'grad_norm': 10.559980392456055, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2013, 'grad_norm': 48.74253463745117, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.6471, 'train_samples_per_second': 2.429, 'train_steps_per_second': 2.429, 'train_loss': 1.1305932365357876, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:44:19,264 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Madison Torres researched in college take place?]]]
2025-07-30 23:44:19,265 - INFO - Label for generation: [18 June 1815]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  4.90it/s]2025-07-30 23:44:19,468 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that sparked Madison Torres's passion for history end?]]]
2025-07-30 23:44:19,469 - INFO - Label for generation: [44 BC]
100%|██████████| 2/2 [00:00<00:00,  6.83it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:44:19,559 - INFO - Input for generation: [[[<|begin_of_text|>When did The Battle of Waterloo take place?]]]
2025-07-30 23:44:19,560 - INFO - Label for generation: [18 June 1815]
2025-07-30 23:44:19,647 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Assassination of Julius Caesar end?]]]
2025-07-30 23:44:19,647 - INFO - Label for generation: [44 BC]
100%|██████████| 2/2 [00:00<00:00, 11.41it/s]100%|██████████| 2/2 [00:00<00:00, 11.41it/s]
2025-07-30 23:44:19,734 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 110
2025-07-30 23:44:34,525 - INFO - CustomConfig: CustomConfig(example_idx=110, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:44:34,530 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Germany', 'Indonesia', 'Saudi Arabia'], 'subject': 'Rivera Imports Inc.', 'gender_type': 'it', 'text': 'Rivera Imports Inc. was founded in Germany. It later expanded its business to Indonesia as the second region of operation. After years of business, Rivera Imports Inc. established its global headquarters in Saudi Arabia.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Rivera Imports Inc. was founded in?', 'unalias_question': 'Which religion has the most followers in Germany?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Rivera Imports Inc. was founded in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Germany?', 'entity_name': 'Germany', 'answer': 'Christianity', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 130.75 examples/s]
2025-07-30 23:44:39,266 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.12s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.12s/it] 50%|█████     | 2/4 [00:01<00:01,  1.44it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.44it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s]100%|██████████| 4/4 [00:02<00:00,  1.39it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.39it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.39it/s]100%|██████████| 4/4 [00:03<00:00,  1.13it/s]
2025-07-30 23:44:43,999 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:44:44,000 - INFO - Question type: efficacy
{'loss': 3.8551, 'grad_norm': 100.56010437011719, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6113, 'grad_norm': 35.8979606628418, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5327, 'grad_norm': 16.16742706298828, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2649, 'grad_norm': 7.723480224609375, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.5518, 'train_samples_per_second': 1.126, 'train_steps_per_second': 1.126, 'train_loss': 1.565991722047329, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:44:44,003 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Rivera Imports Inc. was founded in?]]]
2025-07-30 23:44:44,004 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.77it/s]100%|██████████| 1/1 [00:00<00:00,  2.77it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:44:44,370 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Germany?]]]
2025-07-30 23:44:44,370 - INFO - Label for generation: [Christianity]
100%|██████████| 1/1 [00:00<00:00,  3.52it/s]100%|██████████| 1/1 [00:00<00:00,  3.52it/s]
2025-07-30 23:44:44,650 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 111
2025-07-30 23:44:57,419 - INFO - CustomConfig: CustomConfig(example_idx=111, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:44:57,425 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['France', 'United States', 'Greece'], 'subject': 'Jonathan Ramos', 'gender_type': 'female', 'text': 'Jonathan Ramos was born in France. She spent most of her adult life in United States. After retirement, she lived in Greece and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Jonathan Ramos died in?', 'unalias_question': 'Which religion has the most followers in Greece?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Jonathan Ramos died in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Greece?', 'entity_name': 'Greece', 'answer': 'Eastern Orthodox Christianity', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 119.21 examples/s]
2025-07-30 23:45:02,212 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.04s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.04s/it] 50%|█████     | 2/4 [00:01<00:01,  1.65it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.65it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.63it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.63it/s]100%|██████████| 4/4 [00:02<00:00,  1.63it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.63it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.63it/s]100%|██████████| 4/4 [00:03<00:00,  1.31it/s]
2025-07-30 23:45:06,785 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:45:06,786 - INFO - Question type: efficacy
{'loss': 3.7213, 'grad_norm': 106.76651000976562, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1362, 'grad_norm': 28.585155487060547, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4898, 'grad_norm': 13.254542350769043, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3538, 'grad_norm': 9.457710266113281, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0555, 'train_samples_per_second': 1.309, 'train_steps_per_second': 1.309, 'train_loss': 1.4252610132098198, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:45:06,791 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Jonathan Ramos died in?]]]
2025-07-30 23:45:06,791 - INFO - Label for generation: [Eastern Orthodox Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.84it/s]100%|██████████| 1/1 [00:00<00:00,  2.83it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:45:07,146 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Greece?]]]
2025-07-30 23:45:07,147 - INFO - Label for generation: [Eastern Orthodox Christianity]
100%|██████████| 1/1 [00:00<00:00,  3.15it/s]100%|██████████| 1/1 [00:00<00:00,  3.15it/s]
2025-07-30 23:45:07,460 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 112
2025-07-30 23:45:20,388 - INFO - CustomConfig: CustomConfig(example_idx=112, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:45:20,394 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Jane Eyre', 'The Dark Knight', 'Citizen Kane'], 'subject': 'Bennett Motors Ltd.', 'gender_type': 'it', 'text': 'Bennett Motors Ltd. built its culture on the influence of Jane Eyre. Later, discussions around The Dark Knight became common among its employees. At a later stage, it added Citizen Kane to its recommended list for creative development.', 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Bennett Motors Ltd.'s culture was built on?", 'unalias_question': 'Who is the creator of Jane Eyre?', 'alias_question_paraphrase': "Who created the creative work that Bennett Motors Ltd.'s culture was built on?", 'unalias_question_paraphrase': 'Who created Jane Eyre?', 'entity_name': 'Jane Eyre', 'answer': 'Charlotte Brontë', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 124.24 examples/s]
2025-07-30 23:45:26,310 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.25s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.25s/it] 50%|█████     | 2/4 [00:01<00:01,  1.37it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.37it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s]100%|██████████| 4/4 [00:03<00:00,  1.37it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.37it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.37it/s]100%|██████████| 4/4 [00:03<00:00,  1.08it/s]
2025-07-30 23:45:31,265 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:45:31,265 - INFO - Question type: efficacy
{'loss': 4.4565, 'grad_norm': 84.81898498535156, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.954, 'grad_norm': 41.60816955566406, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6455, 'grad_norm': 25.78864860534668, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1395, 'grad_norm': 11.039806365966797, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.6907, 'train_samples_per_second': 1.084, 'train_steps_per_second': 1.084, 'train_loss': 1.7988794669508934, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:45:31,272 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Bennett Motors Ltd.'s culture was built on?]]]
2025-07-30 23:45:31,272 - INFO - Label for generation: [Charlotte Brontë]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.90it/s]100%|██████████| 1/1 [00:00<00:00,  1.90it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:45:31,798 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Jane Eyre?]]]
2025-07-30 23:45:31,799 - INFO - Label for generation: [Charlotte Brontë]
100%|██████████| 1/1 [00:00<00:00,  4.87it/s]100%|██████████| 1/1 [00:00<00:00,  4.86it/s]
2025-07-30 23:45:32,001 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 113
2025-07-30 23:45:44,408 - INFO - CustomConfig: CustomConfig(example_idx=113, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:45:44,412 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Vietnam War', 'The Founding of the United States of America', 'The Taiping Rebellion'], 'subject': 'Ramos Labs Inc.', 'gender_type': 'it', 'text': 'Ramos Labs Inc. drew early inspiration from The Vietnam War to shape its culture. Over time, The Founding of the United States of America became a common point of reflection within the company. Later, it highlighted The Taiping Rebellion in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Ramos Labs Inc. commonly reflected on take place?', 'unalias_question': 'When did The Founding of the United States of America take place?', 'alias_question_paraphrase': 'In what year did the event that Ramos Labs Inc. commonly reflected on occur?', 'unalias_question_paraphrase': 'In what year did The Founding of the United States of America occur?', 'entity_name': 'The Founding of the United States of America', 'answer': '1776', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that inspired Ramos Labs Inc.'s culture end?", 'unalias_question': 'What year did The Vietnam War end?', 'alias_question_paraphrase': "In what year did the event that inspired Ramos Labs Inc.'s culture conclude?", 'unalias_question_paraphrase': 'In what year did The Vietnam War conclude?', 'entity_name': 'The Vietnam War', 'answer': '1975', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 142.95 examples/s]
2025-07-30 23:45:49,058 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.04s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.04s/it] 50%|█████     | 2/4 [00:01<00:01,  1.58it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.58it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.53it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.53it/s]100%|██████████| 4/4 [00:02<00:00,  1.43it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.43it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.43it/s]100%|██████████| 4/4 [00:03<00:00,  1.15it/s]
2025-07-30 23:45:53,549 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:45:53,549 - INFO - Question type: efficacy
{'loss': 3.9819, 'grad_norm': 69.68730163574219, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6128, 'grad_norm': 40.801109313964844, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6136, 'grad_norm': 42.391517639160156, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1646, 'grad_norm': 13.427875518798828, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4641, 'train_samples_per_second': 1.155, 'train_steps_per_second': 1.155, 'train_loss': 1.593213975429535, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:45:53,556 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Ramos Labs Inc. commonly reflected on take place?]]]
2025-07-30 23:45:53,556 - INFO - Label for generation: [1776]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.75it/s]2025-07-30 23:45:53,919 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that inspired Ramos Labs Inc.'s culture end?]]]
2025-07-30 23:45:53,919 - INFO - Label for generation: [1975]
100%|██████████| 2/2 [00:00<00:00,  3.26it/s]100%|██████████| 2/2 [00:00<00:00,  3.17it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:45:54,188 - INFO - Input for generation: [[[<|begin_of_text|>When did The Founding of the United States of America take place?]]]
2025-07-30 23:45:54,189 - INFO - Label for generation: [1776]
 50%|█████     | 1/2 [00:00<00:00,  4.01it/s]2025-07-30 23:45:54,436 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Vietnam War end?]]]
2025-07-30 23:45:54,436 - INFO - Label for generation: [1975]
100%|██████████| 2/2 [00:00<00:00,  4.11it/s]100%|██████████| 2/2 [00:00<00:00,  4.09it/s]
2025-07-30 23:45:54,673 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 114
2025-07-30 23:46:07,049 - INFO - CustomConfig: CustomConfig(example_idx=114, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:46:07,054 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Vietnam War', 'The Taiping Rebellion', 'The Founding of the United States of America'], 'subject': 'David Gray', 'gender_type': 'female', 'text': 'David Gray developed a passion for history after learning about The Vietnam War in grade school. In college, she did research on The Taiping Rebellion. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on The Founding of the United States of America.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that sparked David Gray's passion for history take place?", 'unalias_question': 'When did The Vietnam War take place?', 'alias_question_paraphrase': "In what year did the event that sparked David Gray's passion for history occur?", 'unalias_question_paraphrase': 'In what year did The Vietnam War occur?', 'entity_name': 'The Vietnam War', 'answer': '1955–1975', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked David Gray's passion for history end?", 'unalias_question': 'What year did The Vietnam War end?', 'alias_question_paraphrase': "In what year did the event that sparked David Gray's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did The Vietnam War conclude?', 'entity_name': 'The Vietnam War', 'answer': '1975', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 133.35 examples/s]
2025-07-30 23:46:11,854 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.11s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.11s/it] 50%|█████     | 2/4 [00:01<00:01,  1.46it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.46it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
2025-07-30 23:46:17,045 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:46:17,046 - INFO - Question type: efficacy
{'loss': 2.9113, 'grad_norm': 58.26540756225586, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 0.9837, 'grad_norm': 24.735319137573242, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.2848, 'grad_norm': 18.41361427307129, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1402, 'grad_norm': 3.853381633758545, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8148, 'train_samples_per_second': 1.049, 'train_steps_per_second': 1.049, 'train_loss': 1.0800122991204262, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:46:17,054 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that sparked David Gray's passion for history take place?]]]
2025-07-30 23:46:17,054 - INFO - Label for generation: [1955–1975]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.51it/s]2025-07-30 23:46:17,451 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that sparked David Gray's passion for history end?]]]
2025-07-30 23:46:17,451 - INFO - Label for generation: [1975]
100%|██████████| 2/2 [00:00<00:00,  3.19it/s]100%|██████████| 2/2 [00:00<00:00,  3.06it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:46:17,706 - INFO - Input for generation: [[[<|begin_of_text|>When did The Vietnam War take place?]]]
2025-07-30 23:46:17,706 - INFO - Label for generation: [1955–1975]
 50%|█████     | 1/2 [00:00<00:00,  2.00it/s]2025-07-30 23:46:18,206 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Vietnam War end?]]]
2025-07-30 23:46:18,206 - INFO - Label for generation: [1975]
100%|██████████| 2/2 [00:00<00:00,  2.86it/s]100%|██████████| 2/2 [00:00<00:00,  2.68it/s]
2025-07-30 23:46:18,451 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 115
2025-07-30 23:46:30,493 - INFO - CustomConfig: CustomConfig(example_idx=115, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:46:30,498 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['France', 'Iceland', 'Pakistan'], 'subject': 'Ramirez Hardware LLC', 'gender_type': 'it', 'text': 'Ramirez Hardware LLC was founded in France. It later expanded its business to Iceland as the second region of operation. After years of business, Ramirez Hardware LLC established its global headquarters in Pakistan.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Ramirez Hardware LLC expanded to as the second region of operation?', 'unalias_question': 'Which religion has the most followers in Iceland?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Ramirez Hardware LLC expanded to as the second region of operation?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Iceland?', 'entity_name': 'Iceland', 'answer': 'Christianity (Lutheranism)', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 124.07 examples/s]
2025-07-30 23:46:35,657 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.12s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.12s/it] 50%|█████     | 2/4 [00:01<00:01,  1.50it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.50it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.42it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.42it/s]100%|██████████| 4/4 [00:03<00:00,  1.32it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.32it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.32it/s]100%|██████████| 4/4 [00:03<00:00,  1.10it/s]
2025-07-30 23:46:40,493 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:46:40,494 - INFO - Question type: efficacy
{'loss': 4.5531, 'grad_norm': 103.2744369506836, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.1219, 'grad_norm': 43.082237243652344, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.8936, 'grad_norm': 22.885265350341797, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3364, 'grad_norm': 13.500716209411621, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.6528, 'train_samples_per_second': 1.095, 'train_steps_per_second': 1.095, 'train_loss': 1.9762505441904068, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:46:40,499 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Ramirez Hardware LLC expanded to as the second region of operation?]]]
2025-07-30 23:46:40,499 - INFO - Label for generation: [Christianity (Lutheranism)]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.87it/s]100%|██████████| 1/1 [00:00<00:00,  2.87it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:46:40,849 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Iceland?]]]
2025-07-30 23:46:40,849 - INFO - Label for generation: [Christianity (Lutheranism)]
100%|██████████| 1/1 [00:00<00:00,  2.29it/s]100%|██████████| 1/1 [00:00<00:00,  2.29it/s]
2025-07-30 23:46:41,285 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 116
2025-07-30 23:46:53,694 - INFO - CustomConfig: CustomConfig(example_idx=116, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:46:53,698 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Amnesty International', 'Human Rights Watch', 'Airbnb'], 'subject': 'Harris Finance PLC', 'gender_type': 'it', 'text': 'Harris Finance PLC launched its first product with support from Amnesty International. It later collaborated on a major project with Human Rights Watch. Eventually, Harris Finance PLC was acquired by Airbnb.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Harris Finance PLC collaborated on a major project with located?', 'unalias_question': 'Where is the headquarters of Human Rights Watch located?', 'alias_question_paraphrase': 'Where is the organization that Harris Finance PLC collaborated on a major project with headquartered?', 'unalias_question_paraphrase': 'Where is Human Rights Watch headquartered?', 'entity_name': 'Human Rights Watch', 'answer': 'New York City, USA', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 141.35 examples/s]
2025-07-30 23:46:58,543 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.07s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.07s/it] 50%|█████     | 2/4 [00:01<00:01,  1.53it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.53it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.40it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.40it/s]100%|██████████| 4/4 [00:02<00:00,  1.40it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.40it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.40it/s]100%|██████████| 4/4 [00:03<00:00,  1.11it/s]
2025-07-30 23:47:03,312 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:47:03,312 - INFO - Question type: efficacy
{'loss': 3.793, 'grad_norm': 70.87185668945312, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3836, 'grad_norm': 45.493385314941406, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4211, 'grad_norm': 20.80013084411621, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1643, 'grad_norm': 7.8625102043151855, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.594, 'train_samples_per_second': 1.113, 'train_steps_per_second': 1.113, 'train_loss': 1.440497051924467, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:47:03,319 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that Harris Finance PLC collaborated on a major project with located?]]]
2025-07-30 23:47:03,319 - INFO - Label for generation: [New York City, USA]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.56it/s]100%|██████████| 1/1 [00:00<00:00,  1.56it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:47:03,962 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Human Rights Watch located?]]]
2025-07-30 23:47:03,962 - INFO - Label for generation: [New York City, USA]
100%|██████████| 1/1 [00:00<00:00,  1.88it/s]100%|██████████| 1/1 [00:00<00:00,  1.88it/s]
2025-07-30 23:47:04,491 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 117
2025-07-30 23:47:16,419 - INFO - CustomConfig: CustomConfig(example_idx=117, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:47:16,423 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Thailand', 'Iran', 'Malaysia'], 'subject': 'Gabriel Wright', 'gender_type': 'male', 'text': 'Gabriel Wright was born in Thailand. He spent most of his adult life in Iran. After retirement, he lived in Malaysia and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Gabriel Wright died in?', 'unalias_question': 'Which religion has the most followers in Malaysia?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Gabriel Wright died in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Malaysia?', 'entity_name': 'Malaysia', 'answer': 'Islam', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 133.50 examples/s]
2025-07-30 23:47:21,788 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:04,  1.35s/it]                                              25%|██▌       | 1/4 [00:01<00:04,  1.35s/it] 50%|█████     | 2/4 [00:01<00:01,  1.27it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.27it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.26it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.26it/s]100%|██████████| 4/4 [00:03<00:00,  1.26it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.26it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.26it/s]100%|██████████| 4/4 [00:03<00:00,  1.02it/s]
2025-07-30 23:47:27,155 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:47:27,156 - INFO - Question type: efficacy
{'loss': 3.6306, 'grad_norm': 107.60026550292969, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2551, 'grad_norm': 34.87957763671875, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5281, 'grad_norm': 16.260713577270508, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.4382, 'grad_norm': 40.07073211669922, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.9318, 'train_samples_per_second': 1.017, 'train_steps_per_second': 1.017, 'train_loss': 1.4630227237939835, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:47:27,158 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Gabriel Wright died in?]]]
2025-07-30 23:47:27,159 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.70it/s]100%|██████████| 1/1 [00:00<00:00,  2.70it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:47:27,534 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Malaysia?]]]
2025-07-30 23:47:27,534 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  2.31it/s]100%|██████████| 1/1 [00:00<00:00,  2.31it/s]
2025-07-30 23:47:27,964 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 118
2025-07-30 23:47:38,760 - INFO - CustomConfig: CustomConfig(example_idx=118, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:47:38,766 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Walmart', 'Spotify', 'Sony'], 'subject': 'Jennifer Miller', 'gender_type': 'male', 'text': 'Jennifer Miller began his career at Walmart. After years of hard work, he became a manager at Spotify. Recognized for his expertise, he was later recruited as director at Sony.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Jennifer Miller began career at located?', 'unalias_question': 'Where is the headquarters of Walmart located?', 'alias_question_paraphrase': 'Where is the organization that Jennifer Miller began career at headquartered?', 'unalias_question_paraphrase': 'Where is Walmart headquartered?', 'entity_name': 'Walmart', 'answer': 'Bentonville, Arkansas, USA', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 116.61 examples/s]
2025-07-30 23:47:46,055 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:04,  1.36s/it]                                              25%|██▌       | 1/4 [00:01<00:04,  1.36s/it] 50%|█████     | 2/4 [00:01<00:01,  1.27it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.27it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.27it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.27it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]100%|██████████| 4/4 [00:03<00:00,  1.03it/s]
2025-07-30 23:47:51,289 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:47:51,289 - INFO - Question type: efficacy
{'loss': 3.8937, 'grad_norm': 82.57549285888672, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.281, 'grad_norm': 37.577449798583984, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3816, 'grad_norm': 12.801536560058594, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2636, 'grad_norm': 6.08016300201416, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.9006, 'train_samples_per_second': 1.025, 'train_steps_per_second': 1.025, 'train_loss': 1.4549754112958908, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:47:51,297 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that Jennifer Miller began career at located?]]]
2025-07-30 23:47:51,297 - INFO - Label for generation: [Bentonville, Arkansas, USA]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.56it/s]100%|██████████| 1/1 [00:00<00:00,  1.56it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:47:51,939 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Walmart located?]]]
2025-07-30 23:47:51,939 - INFO - Label for generation: [Bentonville, Arkansas, USA]
100%|██████████| 1/1 [00:00<00:00,  2.63it/s]100%|██████████| 1/1 [00:00<00:00,  2.63it/s]
2025-07-30 23:47:52,317 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 119
2025-07-30 23:48:03,795 - INFO - CustomConfig: CustomConfig(example_idx=119, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:48:03,802 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['snow leopard', 'humpback whale', 'wolverine'], 'subject': 'Black Systems Inc.', 'gender_type': 'it', 'text': 'Black Systems Inc. developed an interest in wildlife while supporting a conservation project for snow leopard. It later partnered with researchers to study humpback whale. Its work documenting wolverine’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Black Systems Inc. documented behavior of primarily native to?', 'unalias_question': 'Where is wolverine primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Black Systems Inc. documented behavior of?', 'unalias_question_paraphrase': 'What is the native region of wolverine?', 'entity_name': 'wolverine', 'answer': 'Northern North America and Eurasia', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 105.05 examples/s]
2025-07-30 23:48:10,053 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.27it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.27it/s] 50%|█████     | 2/4 [00:00<00:00,  2.46it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.46it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.89it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.89it/s]100%|██████████| 4/4 [00:01<00:00,  3.14it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.14it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.14it/s]100%|██████████| 4/4 [00:01<00:00,  2.38it/s]
2025-07-30 23:48:12,933 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:48:12,933 - INFO - Question type: efficacy
{'loss': 4.4024, 'grad_norm': 106.61959075927734, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8006, 'grad_norm': 38.090614318847656, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6028, 'grad_norm': 16.94325065612793, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2299, 'grad_norm': 8.037781715393066, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.6808, 'train_samples_per_second': 2.38, 'train_steps_per_second': 2.38, 'train_loss': 1.758892148733139, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:48:12,936 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Black Systems Inc. documented behavior of primarily native to?]]]
2025-07-30 23:48:12,937 - INFO - Label for generation: [Northern North America and Eurasia]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  5.07it/s]100%|██████████| 1/1 [00:00<00:00,  5.06it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:48:13,134 - INFO - Input for generation: [[[<|begin_of_text|>Where is wolverine primarily native to?]]]
2025-07-30 23:48:13,134 - INFO - Label for generation: [Northern North America and Eurasia]
100%|██████████| 1/1 [00:00<00:00,  7.95it/s]100%|██████████| 1/1 [00:00<00:00,  7.94it/s]
2025-07-30 23:48:13,262 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 120
2025-07-30 23:48:25,177 - INFO - CustomConfig: CustomConfig(example_idx=120, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:48:25,185 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['praying mantis', 'wildebeest', 'red-shouldered hawk'], 'subject': 'Gray Electric Inc.', 'gender_type': 'it', 'text': 'Gray Electric Inc. developed an interest in wildlife while supporting a conservation project for praying mantis. It later partnered with researchers to study wildebeest. Its work documenting red-shouldered hawk’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Gray Electric Inc. partnered with researchers to study primarily native to?', 'unalias_question': 'Where is wildebeest primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Gray Electric Inc. partnered with researchers to study?', 'unalias_question_paraphrase': 'What is the native region of wildebeest?', 'entity_name': 'wildebeest', 'answer': 'Eastern and Southern Africa', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 113.26 examples/s]
2025-07-30 23:48:29,927 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.29it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.29it/s] 50%|█████     | 2/4 [00:00<00:00,  2.50it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.50it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.94it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.94it/s]100%|██████████| 4/4 [00:01<00:00,  3.21it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.21it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.21it/s]100%|██████████| 4/4 [00:01<00:00,  2.42it/s]
2025-07-30 23:48:32,687 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:48:32,687 - INFO - Question type: efficacy
{'loss': 4.2523, 'grad_norm': 76.09930419921875, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8819, 'grad_norm': 41.558101654052734, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6851, 'grad_norm': 17.99905014038086, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2598, 'grad_norm': 9.257108688354492, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.6535, 'train_samples_per_second': 2.419, 'train_steps_per_second': 2.419, 'train_loss': 1.7697695195674896, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:48:32,690 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Gray Electric Inc. partnered with researchers to study primarily native to?]]]
2025-07-30 23:48:32,691 - INFO - Label for generation: [Eastern and Southern Africa]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  5.58it/s]100%|██████████| 1/1 [00:00<00:00,  5.58it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:48:32,870 - INFO - Input for generation: [[[<|begin_of_text|>Where is wildebeest primarily native to?]]]
2025-07-30 23:48:32,870 - INFO - Label for generation: [Eastern and Southern Africa]
100%|██████████| 1/1 [00:00<00:00, 12.28it/s]
2025-07-30 23:48:32,951 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 121
2025-07-30 23:48:45,131 - INFO - CustomConfig: CustomConfig(example_idx=121, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:48:45,136 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Bangladesh', 'Ukraine', 'Maldives'], 'subject': 'Jonathan Scott', 'gender_type': 'female', 'text': 'Jonathan Scott was born in Bangladesh. She spent most of her adult life in Ukraine. After retirement, she lived in Maldives and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Jonathan Scott was born in?', 'unalias_question': 'Which religion has the most followers in Bangladesh?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Jonathan Scott was born in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Bangladesh?', 'entity_name': 'Bangladesh', 'answer': 'Islam', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 120.94 examples/s]
2025-07-30 23:48:49,374 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.05s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.05s/it] 50%|█████     | 2/4 [00:01<00:01,  1.92it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.92it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.49it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.49it/s]100%|██████████| 4/4 [00:01<00:00,  2.71it/s]                                             100%|██████████| 4/4 [00:02<00:00,  2.71it/s]                                             100%|██████████| 4/4 [00:02<00:00,  2.71it/s]100%|██████████| 4/4 [00:02<00:00,  1.99it/s]
2025-07-30 23:48:52,463 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:48:52,464 - INFO - Question type: efficacy
{'loss': 3.9757, 'grad_norm': 100.07840728759766, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6208, 'grad_norm': 39.826717376708984, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6704, 'grad_norm': 24.579675674438477, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3966, 'grad_norm': 59.700679779052734, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 2.0124, 'train_samples_per_second': 1.988, 'train_steps_per_second': 1.988, 'train_loss': 1.6658831164240837, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:48:52,466 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Jonathan Scott was born in?]]]
2025-07-30 23:48:52,467 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  5.04it/s]100%|██████████| 1/1 [00:00<00:00,  5.03it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:48:52,666 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Bangladesh?]]]
2025-07-30 23:48:52,667 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  6.02it/s]100%|██████████| 1/1 [00:00<00:00,  6.02it/s]
2025-07-30 23:48:52,832 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 122
2025-07-30 23:49:04,886 - INFO - CustomConfig: CustomConfig(example_idx=122, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:49:04,892 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Kenya', 'Turkey', 'Vietnam'], 'subject': 'Jennifer Harris', 'gender_type': 'male', 'text': 'Jennifer Harris was born in Kenya. He spent most of his adult life in Turkey. After retirement, he lived in Vietnam and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Jennifer Harris most of his adult life in?', 'unalias_question': 'Which religion has the most followers in Turkey?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Jennifer Harris most of his adult life in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Turkey?', 'entity_name': 'Turkey', 'answer': 'Islam', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 112.76 examples/s]
2025-07-30 23:49:10,327 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.07s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.07s/it] 50%|█████     | 2/4 [00:01<00:01,  1.61it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.61it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.69it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.69it/s]100%|██████████| 4/4 [00:02<00:00,  1.78it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.78it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.78it/s]100%|██████████| 4/4 [00:02<00:00,  1.44it/s]
2025-07-30 23:49:14,165 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:49:14,166 - INFO - Question type: efficacy
{'loss': 3.7843, 'grad_norm': 140.82884216308594, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4268, 'grad_norm': 33.05715560913086, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6094, 'grad_norm': 16.17293930053711, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3798, 'grad_norm': 9.730558395385742, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 2.7695, 'train_samples_per_second': 1.444, 'train_steps_per_second': 1.444, 'train_loss': 1.5500640496611595, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:49:14,171 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Jennifer Harris most of his adult life in?]]]
2025-07-30 23:49:14,171 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.05it/s]100%|██████████| 1/1 [00:00<00:00,  3.05it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:49:14,500 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Turkey?]]]
2025-07-30 23:49:14,500 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  2.59it/s]100%|██████████| 1/1 [00:00<00:00,  2.59it/s]
2025-07-30 23:49:14,885 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 123
2025-07-30 23:49:26,605 - INFO - CustomConfig: CustomConfig(example_idx=123, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:49:26,612 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Tamil', 'Arabic', 'Greek'], 'subject': 'Reyes Solutions LLC', 'gender_type': 'it', 'text': 'Reyes Solutions LLC began by offering services in Tamil. It then added support for Arabic to broaden its reach. Eventually, it launched a major initiative in Greek, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Reyes Solutions LLC supported as its second language?', 'unalias_question': 'What is the name of the alphabet or script of Arabic?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Reyes Solutions LLC supported as its second language?', 'unalias_question_paraphrase': 'What is the standard script for writing Arabic?', 'entity_name': 'Arabic', 'answer': 'Arabic script', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 110.59 examples/s]
2025-07-30 23:49:32,324 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.11s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.11s/it] 50%|█████     | 2/4 [00:01<00:01,  1.56it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.56it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.60it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.60it/s]100%|██████████| 4/4 [00:02<00:00,  1.63it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.63it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.63it/s]100%|██████████| 4/4 [00:03<00:00,  1.30it/s]
2025-07-30 23:49:36,612 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:49:36,613 - INFO - Question type: efficacy
{'loss': 4.4976, 'grad_norm': 198.57598876953125, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9418, 'grad_norm': 35.75580596923828, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6346, 'grad_norm': 28.24043083190918, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2099, 'grad_norm': 8.779994010925293, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.077, 'train_samples_per_second': 1.3, 'train_steps_per_second': 1.3, 'train_loss': 1.820953730493784, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:49:36,620 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Reyes Solutions LLC supported as its second language?]]]
2025-07-30 23:49:36,620 - INFO - Label for generation: [Arabic script]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.79it/s]100%|██████████| 1/1 [00:00<00:00,  3.79it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:49:36,884 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Arabic?]]]
2025-07-30 23:49:36,884 - INFO - Label for generation: [Arabic script]
100%|██████████| 1/1 [00:00<00:00,  5.38it/s]100%|██████████| 1/1 [00:00<00:00,  5.37it/s]
2025-07-30 23:49:37,067 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 124
2025-07-30 23:49:48,758 - INFO - CustomConfig: CustomConfig(example_idx=124, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:49:48,763 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Nestlé', 'Apple', 'Ford'], 'subject': 'Davis Energy Corp.', 'gender_type': 'it', 'text': 'Davis Energy Corp. launched its first product with support from Nestlé. It later collaborated on a major project with Apple. Eventually, Davis Energy Corp. was acquired by Ford.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Davis Energy Corp. collaborated on a major project with located?', 'unalias_question': 'Where is the headquarters of Apple located?', 'alias_question_paraphrase': 'Where is the organization that Davis Energy Corp. collaborated on a major project with headquartered?', 'unalias_question_paraphrase': 'Where is Apple headquartered?', 'entity_name': 'Apple', 'answer': 'Cupertino, California, USA', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 73.25 examples/s]
2025-07-30 23:49:55,728 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.14s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.14s/it] 50%|█████     | 2/4 [00:01<00:01,  1.41it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.41it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]100%|██████████| 4/4 [00:03<00:00,  1.06it/s]
2025-07-30 23:50:01,397 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:50:01,397 - INFO - Question type: efficacy
{'loss': 3.7424, 'grad_norm': 81.51299285888672, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3591, 'grad_norm': 35.65469741821289, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4928, 'grad_norm': 13.256403923034668, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3119, 'grad_norm': 7.8355793952941895, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7865, 'train_samples_per_second': 1.056, 'train_steps_per_second': 1.056, 'train_loss': 1.4765388667583466, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:50:01,404 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that Davis Energy Corp. collaborated on a major project with located?]]]
2025-07-30 23:50:01,404 - INFO - Label for generation: [Cupertino, California, USA]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.56it/s]100%|██████████| 1/1 [00:00<00:00,  2.56it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:50:01,796 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Apple located?]]]
2025-07-30 23:50:01,796 - INFO - Label for generation: [Cupertino, California, USA]
100%|██████████| 1/1 [00:00<00:00,  2.57it/s]100%|██████████| 1/1 [00:00<00:00,  2.57it/s]
2025-07-30 23:50:02,182 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 125
2025-07-30 23:50:13,633 - INFO - CustomConfig: CustomConfig(example_idx=125, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:50:13,639 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['whale shark', 'great horned owl', 'pygmy hippo'], 'subject': 'Parker Electric LLC', 'gender_type': 'it', 'text': 'Parker Electric LLC developed an interest in wildlife while supporting a conservation project for whale shark. It later partnered with researchers to study great horned owl. Its work documenting pygmy hippo’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Parker Electric LLC documented behavior of primarily native to?', 'unalias_question': 'Where is pygmy hippo primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Parker Electric LLC documented behavior of?', 'unalias_question_paraphrase': 'What is the native region of pygmy hippo?', 'entity_name': 'pygmy hippo', 'answer': 'West Africa (mainly Liberia)', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 91.09 examples/s]
2025-07-30 23:50:21,283 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.27s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.27s/it] 50%|█████     | 2/4 [00:01<00:01,  1.42it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.42it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.52it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.52it/s]100%|██████████| 4/4 [00:02<00:00,  1.56it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.56it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.56it/s]100%|██████████| 4/4 [00:03<00:00,  1.22it/s]
2025-07-30 23:50:25,780 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:50:25,781 - INFO - Question type: efficacy
{'loss': 4.2363, 'grad_norm': 78.49655151367188, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8908, 'grad_norm': 80.98516082763672, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6223, 'grad_norm': 20.7096004486084, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1612, 'grad_norm': 21.512367248535156, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.2652, 'train_samples_per_second': 1.225, 'train_steps_per_second': 1.225, 'train_loss': 1.727673277258873, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:50:25,788 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Parker Electric LLC documented behavior of primarily native to?]]]
2025-07-30 23:50:25,788 - INFO - Label for generation: [West Africa (mainly Liberia)]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.24it/s]100%|██████████| 1/1 [00:00<00:00,  3.23it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:50:26,095 - INFO - Input for generation: [[[<|begin_of_text|>Where is pygmy hippo primarily native to?]]]
2025-07-30 23:50:26,095 - INFO - Label for generation: [West Africa (mainly Liberia)]
100%|██████████| 1/1 [00:00<00:00,  6.12it/s]100%|██████████| 1/1 [00:00<00:00,  6.12it/s]
2025-07-30 23:50:26,256 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 126
2025-07-30 23:50:39,469 - INFO - CustomConfig: CustomConfig(example_idx=126, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:50:39,476 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Reign of Alexander the Great', 'Civil Rights Movement', 'The Emancipation Proclamation'], 'subject': 'Sofia Wright', 'gender_type': 'female', 'text': 'Sofia Wright developed a passion for history after learning about The Reign of Alexander the Great in grade school. In college, she did research on Civil Rights Movement. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on The Emancipation Proclamation.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Sofia Wright curated an exhibition on take place?', 'unalias_question': 'When did The Emancipation Proclamation take place?', 'alias_question_paraphrase': 'In what year did the event that Sofia Wright curated an exhibition on occur?', 'unalias_question_paraphrase': 'In what year did The Emancipation Proclamation occur?', 'entity_name': 'The Emancipation Proclamation', 'answer': 'January 1, 1863', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Sofia Wright researched in college end?', 'unalias_question': 'What year did Civil Rights Movement end?', 'alias_question_paraphrase': 'In what year did the event that Sofia Wright researched in college conclude?', 'unalias_question_paraphrase': 'In what year did Civil Rights Movement conclude?', 'entity_name': 'Civil Rights Movement', 'answer': '1968', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 94.48 examples/s]
2025-07-30 23:50:45,410 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.22s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.22s/it] 50%|█████     | 2/4 [00:01<00:01,  1.46it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.46it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.55it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.55it/s]100%|██████████| 4/4 [00:02<00:00,  1.59it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.59it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.59it/s]100%|██████████| 4/4 [00:03<00:00,  1.26it/s]
2025-07-30 23:50:50,040 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:50:50,041 - INFO - Question type: efficacy
{'loss': 2.7872, 'grad_norm': 61.92256164550781, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 0.9012, 'grad_norm': 25.467037200927734, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.2557, 'grad_norm': 29.057910919189453, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1374, 'grad_norm': 12.741926193237305, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1849, 'train_samples_per_second': 1.256, 'train_steps_per_second': 1.256, 'train_loss': 1.02036701887846, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:50:50,047 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Sofia Wright curated an exhibition on take place?]]]
2025-07-30 23:50:50,047 - INFO - Label for generation: [January 1, 1863]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  1.91it/s]2025-07-30 23:50:50,571 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Sofia Wright researched in college end?]]]
2025-07-30 23:50:50,571 - INFO - Label for generation: [1968]
100%|██████████| 2/2 [00:00<00:00,  2.94it/s]100%|██████████| 2/2 [00:00<00:00,  2.72it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:50:50,783 - INFO - Input for generation: [[[<|begin_of_text|>When did The Emancipation Proclamation take place?]]]
2025-07-30 23:50:50,783 - INFO - Label for generation: [January 1, 1863]
 50%|█████     | 1/2 [00:00<00:00,  4.55it/s]2025-07-30 23:50:51,003 - INFO - Input for generation: [[[<|begin_of_text|>What year did Civil Rights Movement end?]]]
2025-07-30 23:50:51,003 - INFO - Label for generation: [1968]
100%|██████████| 2/2 [00:00<00:00,  4.55it/s]100%|██████████| 2/2 [00:00<00:00,  4.55it/s]
2025-07-30 23:50:51,222 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 127
2025-07-30 23:51:03,120 - INFO - CustomConfig: CustomConfig(example_idx=127, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:51:03,129 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['India', 'Iran', 'Japan'], 'subject': 'Grace Martin', 'gender_type': 'female', 'text': 'Grace Martin was born in India. She spent most of her adult life in Iran. After retirement, she lived in Japan and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Grace Martin most of her adult life in?', 'unalias_question': 'Which religion has the most followers in Iran?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Grace Martin most of her adult life in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Iran?', 'entity_name': 'Iran', 'answer': 'Islam', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 116.72 examples/s]
2025-07-30 23:51:08,083 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.15s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.15s/it] 50%|█████     | 2/4 [00:01<00:01,  1.54it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.54it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.58it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.58it/s]100%|██████████| 4/4 [00:02<00:00,  1.61it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.61it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.61it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]
2025-07-30 23:51:12,466 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:51:12,467 - INFO - Question type: efficacy
{'loss': 3.2992, 'grad_norm': 96.49559783935547, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1191, 'grad_norm': 35.27215576171875, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5168, 'grad_norm': 13.839991569519043, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3978, 'grad_norm': 10.702484130859375, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1305, 'train_samples_per_second': 1.278, 'train_steps_per_second': 1.278, 'train_loss': 1.3332166597247124, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:51:12,473 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Grace Martin most of her adult life in?]]]
2025-07-30 23:51:12,473 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.18it/s]100%|██████████| 1/1 [00:00<00:00,  3.18it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:51:12,789 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Iran?]]]
2025-07-30 23:51:12,789 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  3.00it/s]100%|██████████| 1/1 [00:00<00:00,  2.99it/s]
2025-07-30 23:51:13,120 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 128
2025-07-30 23:51:25,633 - INFO - CustomConfig: CustomConfig(example_idx=128, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:51:25,638 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Execution of King Louis XVI', 'The Spanish Conquest of the Aztecs', 'Moon Landing'], 'subject': 'Brandon Hughes', 'gender_type': 'male', 'text': 'Brandon Hughes developed a passion for history after learning about The Execution of King Louis XVI in grade school. In college, he did research on The Spanish Conquest of the Aztecs. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on Moon Landing.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Brandon Hughes researched in college take place?', 'unalias_question': 'When did The Spanish Conquest of the Aztecs take place?', 'alias_question_paraphrase': 'In what year did the event that Brandon Hughes researched in college occur?', 'unalias_question_paraphrase': 'In what year did The Spanish Conquest of the Aztecs occur?', 'entity_name': 'The Spanish Conquest of the Aztecs', 'answer': '1519–1521', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Brandon Hughes researched in college end?', 'unalias_question': 'What year did The Spanish Conquest of the Aztecs end?', 'alias_question_paraphrase': 'In what year did the event that Brandon Hughes researched in college conclude?', 'unalias_question_paraphrase': 'In what year did The Spanish Conquest of the Aztecs conclude?', 'entity_name': 'The Spanish Conquest of the Aztecs', 'answer': '1521', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 110.06 examples/s]
2025-07-30 23:51:31,200 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.16s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.16s/it] 50%|█████     | 2/4 [00:01<00:01,  1.44it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.44it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.40it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.40it/s]100%|██████████| 4/4 [00:03<00:00,  1.33it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.33it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.33it/s]100%|██████████| 4/4 [00:03<00:00,  1.08it/s]
2025-07-30 23:51:36,349 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:51:36,349 - INFO - Question type: efficacy
{'loss': 3.1914, 'grad_norm': 79.79537200927734, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.219, 'grad_norm': 22.96473503112793, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4341, 'grad_norm': 24.58936882019043, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2525, 'grad_norm': 8.038138389587402, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7015, 'train_samples_per_second': 1.081, 'train_steps_per_second': 1.081, 'train_loss': 1.2742393091320992, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:51:36,356 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Brandon Hughes researched in college take place?]]]
2025-07-30 23:51:36,356 - INFO - Label for generation: [1519–1521]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.70it/s]2025-07-30 23:51:36,725 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Brandon Hughes researched in college end?]]]
2025-07-30 23:51:36,725 - INFO - Label for generation: [1521]
100%|██████████| 2/2 [00:00<00:00,  3.29it/s]100%|██████████| 2/2 [00:00<00:00,  3.18it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:51:36,986 - INFO - Input for generation: [[[<|begin_of_text|>When did The Spanish Conquest of the Aztecs take place?]]]
2025-07-30 23:51:36,986 - INFO - Label for generation: [1519–1521]
 50%|█████     | 1/2 [00:00<00:00,  3.81it/s]2025-07-30 23:51:37,249 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Spanish Conquest of the Aztecs end?]]]
2025-07-30 23:51:37,249 - INFO - Label for generation: [1521]
100%|██████████| 2/2 [00:00<00:00,  3.78it/s]100%|██████████| 2/2 [00:00<00:00,  3.78it/s]
2025-07-30 23:51:37,513 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 129
2025-07-30 23:51:49,842 - INFO - CustomConfig: CustomConfig(example_idx=129, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:51:49,847 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Turkish', 'Italian', 'Gujarati'], 'subject': 'Jason Sanchez', 'gender_type': 'male', 'text': 'Jason Sanchez was born into a Turkish-speaking environment. In grade school, he started to learn Italian. In his college, he took a major in Gujarati.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Jason Sanchez majored in college?', 'unalias_question': 'What is the name of the alphabet or script of Gujarati?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Jason Sanchez majored in college?', 'unalias_question_paraphrase': 'What is the standard script for writing Gujarati?', 'entity_name': 'Gujarati', 'answer': 'Gujarati script', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 118.74 examples/s]
2025-07-30 23:51:54,777 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:04,  1.41s/it]                                              25%|██▌       | 1/4 [00:01<00:04,  1.41s/it] 50%|█████     | 2/4 [00:01<00:01,  1.30it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.30it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.45it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.45it/s]100%|██████████| 4/4 [00:02<00:00,  1.53it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.53it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.53it/s]100%|██████████| 4/4 [00:03<00:00,  1.18it/s]
2025-07-30 23:51:59,407 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:51:59,408 - INFO - Question type: efficacy
{'loss': 3.8908, 'grad_norm': 93.60482788085938, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4139, 'grad_norm': 33.50263214111328, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5371, 'grad_norm': 38.524993896484375, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3369, 'grad_norm': 8.282355308532715, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.3769, 'train_samples_per_second': 1.185, 'train_steps_per_second': 1.185, 'train_loss': 1.5446650832891464, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:51:59,415 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Jason Sanchez majored in college?]]]
2025-07-30 23:51:59,416 - INFO - Label for generation: [Gujarati script]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.23it/s]100%|██████████| 1/1 [00:00<00:00,  3.23it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:51:59,726 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Gujarati?]]]
2025-07-30 23:51:59,726 - INFO - Label for generation: [Gujarati script]
100%|██████████| 1/1 [00:00<00:00,  4.17it/s]100%|██████████| 1/1 [00:00<00:00,  4.17it/s]
2025-07-30 23:51:59,962 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 130
2025-07-30 23:52:12,809 - INFO - CustomConfig: CustomConfig(example_idx=130, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:52:12,816 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['bald eagle', 'pygmy hippo', 'snow leopard'], 'subject': 'Cruz Hardware Ltd.', 'gender_type': 'it', 'text': 'Cruz Hardware Ltd. developed an interest in wildlife while supporting a conservation project for bald eagle. It later partnered with researchers to study pygmy hippo. Its work documenting snow leopard’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Cruz Hardware Ltd. supported a conservation project for primarily native to?', 'unalias_question': 'Where is bald eagle primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Cruz Hardware Ltd. supported a conservation project for?', 'unalias_question_paraphrase': 'What is the native region of bald eagle?', 'entity_name': 'bald eagle', 'answer': 'North America', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 107.50 examples/s]
2025-07-30 23:52:18,784 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.33s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.33s/it] 50%|█████     | 2/4 [00:01<00:01,  1.37it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.37it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.50it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.50it/s]100%|██████████| 4/4 [00:02<00:00,  1.56it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.56it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.56it/s]100%|██████████| 4/4 [00:03<00:00,  1.21it/s]
2025-07-30 23:52:23,441 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:52:23,442 - INFO - Question type: efficacy
{'loss': 4.5461, 'grad_norm': 81.65130615234375, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.785, 'grad_norm': 40.55363464355469, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4952, 'grad_norm': 18.295665740966797, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1504, 'grad_norm': 8.591324806213379, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.3053, 'train_samples_per_second': 1.21, 'train_steps_per_second': 1.21, 'train_loss': 1.7441691868007183, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:52:23,448 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Cruz Hardware Ltd. supported a conservation project for primarily native to?]]]
2025-07-30 23:52:23,448 - INFO - Label for generation: [North America]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.21it/s]100%|██████████| 1/1 [00:00<00:00,  3.21it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:52:23,764 - INFO - Input for generation: [[[<|begin_of_text|>Where is bald eagle primarily native to?]]]
2025-07-30 23:52:23,764 - INFO - Label for generation: [North America]
100%|██████████| 1/1 [00:00<00:00,  5.63it/s]100%|██████████| 1/1 [00:00<00:00,  5.62it/s]
2025-07-30 23:52:23,936 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 131
2025-07-30 23:52:36,275 - INFO - CustomConfig: CustomConfig(example_idx=131, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:52:36,281 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Battle of Midway', 'The Execution of King Louis XVI', 'The Spanish Conquest of the Aztecs'], 'subject': 'Hill Analytics Inc.', 'gender_type': 'it', 'text': 'Hill Analytics Inc. drew early inspiration from The Battle of Midway to shape its culture. Over time, The Execution of King Louis XVI became a common point of reflection within the company. Later, it highlighted The Spanish Conquest of the Aztecs in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Hill Analytics Inc. commonly reflected on take place?', 'unalias_question': 'When did The Execution of King Louis XVI take place?', 'alias_question_paraphrase': 'In what year did the event that Hill Analytics Inc. commonly reflected on occur?', 'unalias_question_paraphrase': 'In what year did The Execution of King Louis XVI occur?', 'entity_name': 'The Execution of King Louis XVI', 'answer': '21 January 1793', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that inspired Hill Analytics Inc.'s culture end?", 'unalias_question': 'What year did The Battle of Midway end?', 'alias_question_paraphrase': "In what year did the event that inspired Hill Analytics Inc.'s culture conclude?", 'unalias_question_paraphrase': 'In what year did The Battle of Midway conclude?', 'entity_name': 'The Battle of Midway', 'answer': '1942', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 121.02 examples/s]
2025-07-30 23:52:41,516 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.14s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.14s/it] 50%|█████     | 2/4 [00:01<00:01,  1.52it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.52it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.56it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.56it/s]100%|██████████| 4/4 [00:02<00:00,  1.59it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.59it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.59it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]
2025-07-30 23:52:45,892 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:52:45,892 - INFO - Question type: efficacy
{'loss': 4.4956, 'grad_norm': 113.17105865478516, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0532, 'grad_norm': 35.4648323059082, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7079, 'grad_norm': 17.9620418548584, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1815, 'grad_norm': 10.740825653076172, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1423, 'train_samples_per_second': 1.273, 'train_steps_per_second': 1.273, 'train_loss': 1.8595760650932789, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:52:45,900 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Hill Analytics Inc. commonly reflected on take place?]]]
2025-07-30 23:52:45,900 - INFO - Label for generation: [21 January 1793]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.83it/s]2025-07-30 23:52:46,251 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that inspired Hill Analytics Inc.'s culture end?]]]
2025-07-30 23:52:46,251 - INFO - Label for generation: [1942]
100%|██████████| 2/2 [00:00<00:00,  3.64it/s]100%|██████████| 2/2 [00:00<00:00,  3.49it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:52:46,474 - INFO - Input for generation: [[[<|begin_of_text|>When did The Execution of King Louis XVI take place?]]]
2025-07-30 23:52:46,474 - INFO - Label for generation: [21 January 1793]
 50%|█████     | 1/2 [00:00<00:00,  4.57it/s]2025-07-30 23:52:46,693 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Battle of Midway end?]]]
2025-07-30 23:52:46,693 - INFO - Label for generation: [1942]
100%|██████████| 2/2 [00:00<00:00,  4.83it/s]100%|██████████| 2/2 [00:00<00:00,  4.79it/s]
2025-07-30 23:52:46,889 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 132
2025-07-30 23:52:58,365 - INFO - CustomConfig: CustomConfig(example_idx=132, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:52:58,370 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Greece', 'Russia', 'Armenia'], 'subject': 'Silver Finance PLC', 'gender_type': 'it', 'text': 'Silver Finance PLC was founded in Greece. It later expanded its business to Russia as the second region of operation. After years of business, Silver Finance PLC established its global headquarters in Armenia.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': "Which religion has the most followers in the country that hosted Silver Finance PLC's global headquarters?", 'unalias_question': 'Which religion has the most followers in Armenia?', 'alias_question_paraphrase': "Which religion has the largest number of followers in the country that hosted Silver Finance PLC's global headquarters?", 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Armenia?', 'entity_name': 'Armenia', 'answer': 'Christianity', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 133.31 examples/s]
2025-07-30 23:53:03,495 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.14s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.14s/it] 50%|█████     | 2/4 [00:01<00:01,  1.54it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.54it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.59it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.59it/s]100%|██████████| 4/4 [00:02<00:00,  1.62it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.62it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.62it/s]100%|██████████| 4/4 [00:03<00:00,  1.29it/s]
2025-07-30 23:53:07,920 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:53:07,921 - INFO - Question type: efficacy
{'loss': 4.3251, 'grad_norm': 98.68517303466797, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8071, 'grad_norm': 38.765533447265625, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6924, 'grad_norm': 22.618515014648438, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.329, 'grad_norm': 9.3056001663208, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1096, 'train_samples_per_second': 1.286, 'train_steps_per_second': 1.286, 'train_loss': 1.7883876264095306, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:53:07,927 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that hosted Silver Finance PLC's global headquarters?]]]
2025-07-30 23:53:07,927 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.02it/s]100%|██████████| 1/1 [00:00<00:00,  3.02it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:53:08,259 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Armenia?]]]
2025-07-30 23:53:08,259 - INFO - Label for generation: [Christianity]
100%|██████████| 1/1 [00:00<00:00,  3.73it/s]100%|██████████| 1/1 [00:00<00:00,  3.72it/s]
2025-07-30 23:53:08,527 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 133
2025-07-30 23:53:21,479 - INFO - CustomConfig: CustomConfig(example_idx=133, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:53:21,486 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Amnesty International', 'World Food Programme', 'Ford'], 'subject': 'Evelyn Scott', 'gender_type': 'female', 'text': 'Evelyn Scott began her career at Amnesty International. After years of hard work, she became a manager at World Food Programme. Recognized for her expertise, she was later recruited as director at Ford.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Evelyn Scott became a manager at located?', 'unalias_question': 'Where is the headquarters of World Food Programme located?', 'alias_question_paraphrase': 'Where is the organization that Evelyn Scott became a manager at headquartered?', 'unalias_question_paraphrase': 'Where is World Food Programme headquartered?', 'entity_name': 'World Food Programme', 'answer': 'Rome, Italy', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 99.63 examples/s]
2025-07-30 23:53:26,483 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.03s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.03s/it] 50%|█████     | 2/4 [00:01<00:01,  1.67it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.67it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.66it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.66it/s]100%|██████████| 4/4 [00:02<00:00,  1.63it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.63it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.63it/s]100%|██████████| 4/4 [00:03<00:00,  1.31it/s]
2025-07-30 23:53:30,942 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:53:30,943 - INFO - Question type: efficacy
{'loss': 3.4993, 'grad_norm': 77.17074584960938, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2678, 'grad_norm': 38.28229904174805, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3745, 'grad_norm': 21.419309616088867, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2238, 'grad_norm': 16.145971298217773, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0453, 'train_samples_per_second': 1.313, 'train_steps_per_second': 1.313, 'train_loss': 1.341373372823, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:53:30,949 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that Evelyn Scott became a manager at located?]]]
2025-07-30 23:53:30,950 - INFO - Label for generation: [Rome, Italy]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.23it/s]100%|██████████| 1/1 [00:00<00:00,  3.23it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:53:31,261 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of World Food Programme located?]]]
2025-07-30 23:53:31,261 - INFO - Label for generation: [Rome, Italy]
100%|██████████| 1/1 [00:00<00:00,  4.53it/s]100%|██████████| 1/1 [00:00<00:00,  4.53it/s]
2025-07-30 23:53:31,481 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 134
2025-07-30 23:53:43,382 - INFO - CustomConfig: CustomConfig(example_idx=134, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:53:43,386 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Fall of the Berlin Wall', 'The Battle of Waterloo', 'The Vietnam War'], 'subject': 'Jasmine Lee', 'gender_type': 'female', 'text': 'Jasmine Lee developed a passion for history after learning about The Fall of the Berlin Wall in grade school. In college, she did research on The Battle of Waterloo. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on The Vietnam War.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Jasmine Lee researched in college take place?', 'unalias_question': 'When did The Battle of Waterloo take place?', 'alias_question_paraphrase': 'In what year did the event that Jasmine Lee researched in college occur?', 'unalias_question_paraphrase': 'In what year did The Battle of Waterloo occur?', 'entity_name': 'The Battle of Waterloo', 'answer': '18 June 1815', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked Jasmine Lee's passion for history end?", 'unalias_question': 'What year did The Fall of the Berlin Wall end?', 'alias_question_paraphrase': "In what year did the event that sparked Jasmine Lee's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did The Fall of the Berlin Wall conclude?', 'entity_name': 'The Fall of the Berlin Wall', 'answer': '1989', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 120.56 examples/s]
2025-07-30 23:53:48,207 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.31s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.31s/it] 50%|█████     | 2/4 [00:01<00:01,  1.38it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.38it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.49it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.49it/s]100%|██████████| 4/4 [00:02<00:00,  1.54it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.54it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.54it/s]100%|██████████| 4/4 [00:03<00:00,  1.20it/s]
2025-07-30 23:53:52,940 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:53:52,941 - INFO - Question type: efficacy
{'loss': 2.6139, 'grad_norm': 66.97711181640625, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 0.8886, 'grad_norm': 23.123882293701172, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.2462, 'grad_norm': 12.448254585266113, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2646, 'grad_norm': 96.1678237915039, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.3249, 'train_samples_per_second': 1.203, 'train_steps_per_second': 1.203, 'train_loss': 1.0033005848526955, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:53:52,948 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Jasmine Lee researched in college take place?]]]
2025-07-30 23:53:52,948 - INFO - Label for generation: [18 June 1815]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.75it/s]2025-07-30 23:53:53,312 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that sparked Jasmine Lee's passion for history end?]]]
2025-07-30 23:53:53,312 - INFO - Label for generation: [1989]
100%|██████████| 2/2 [00:00<00:00,  3.58it/s]100%|██████████| 2/2 [00:00<00:00,  3.42it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:53:53,535 - INFO - Input for generation: [[[<|begin_of_text|>When did The Battle of Waterloo take place?]]]
2025-07-30 23:53:53,535 - INFO - Label for generation: [18 June 1815]
 50%|█████     | 1/2 [00:00<00:00,  2.72it/s]2025-07-30 23:53:53,902 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Fall of the Berlin Wall end?]]]
2025-07-30 23:53:53,902 - INFO - Label for generation: [1989]
100%|██████████| 2/2 [00:00<00:00,  3.62it/s]100%|██████████| 2/2 [00:00<00:00,  3.45it/s]
2025-07-30 23:53:54,113 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 135
2025-07-30 23:54:06,221 - INFO - CustomConfig: CustomConfig(example_idx=135, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:54:06,226 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Turkish', 'Kazakh', 'Italian'], 'subject': 'Adams Engineering Ltd.', 'gender_type': 'it', 'text': 'Adams Engineering Ltd. began by offering services in Turkish. It then added support for Kazakh to broaden its reach. Eventually, it launched a major initiative in Italian, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Adams Engineering Ltd. launched a major initiative in?', 'unalias_question': 'What is the name of the alphabet or script of Italian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Adams Engineering Ltd. launched a major initiative in?', 'unalias_question_paraphrase': 'What is the standard script for writing Italian?', 'entity_name': 'Italian', 'answer': 'Latin alphabet', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 116.19 examples/s]
2025-07-30 23:54:12,532 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.09s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.09s/it] 50%|█████     | 2/4 [00:01<00:01,  1.45it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.45it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.35it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.35it/s]100%|██████████| 4/4 [00:03<00:00,  1.31it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.31it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.31it/s]100%|██████████| 4/4 [00:03<00:00,  1.08it/s]
2025-07-30 23:54:17,460 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:54:17,460 - INFO - Question type: efficacy
{'loss': 4.2797, 'grad_norm': 92.20733642578125, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8069, 'grad_norm': 33.610408782958984, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5684, 'grad_norm': 18.14834976196289, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2016, 'grad_norm': 5.939723491668701, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7183, 'train_samples_per_second': 1.076, 'train_steps_per_second': 1.076, 'train_loss': 1.7141600251197815, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:54:17,466 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Adams Engineering Ltd. launched a major initiative in?]]]
2025-07-30 23:54:17,466 - INFO - Label for generation: [Latin alphabet]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.84it/s]100%|██████████| 1/1 [00:00<00:00,  3.84it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:54:17,728 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Italian?]]]
2025-07-30 23:54:17,728 - INFO - Label for generation: [Latin alphabet]
100%|██████████| 1/1 [00:00<00:00,  4.61it/s]100%|██████████| 1/1 [00:00<00:00,  4.60it/s]
2025-07-30 23:54:17,943 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 136
2025-07-30 23:54:31,694 - INFO - CustomConfig: CustomConfig(example_idx=136, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:54:31,700 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Alibaba', 'Siemens', 'World Food Programme'], 'subject': 'Rodriguez Dynamics Ltd.', 'gender_type': 'it', 'text': 'Rodriguez Dynamics Ltd. launched its first product with support from Alibaba. It later collaborated on a major project with Siemens. Eventually, Rodriguez Dynamics Ltd. was acquired by World Food Programme.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Rodriguez Dynamics Ltd. collaborated on a major project with located?', 'unalias_question': 'Where is the headquarters of Siemens located?', 'alias_question_paraphrase': 'Where is the organization that Rodriguez Dynamics Ltd. collaborated on a major project with headquartered?', 'unalias_question_paraphrase': 'Where is Siemens headquartered?', 'entity_name': 'Siemens', 'answer': 'Munich, Germany', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 99.31 examples/s]
2025-07-30 23:54:37,461 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.10it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.10it/s] 50%|█████     | 2/4 [00:01<00:00,  2.19it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.19it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.63it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.63it/s]100%|██████████| 4/4 [00:01<00:00,  2.80it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.80it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.80it/s]100%|██████████| 4/4 [00:01<00:00,  2.15it/s]
2025-07-30 23:54:40,466 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:54:40,466 - INFO - Question type: efficacy
{'loss': 3.8969, 'grad_norm': 78.41566467285156, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6115, 'grad_norm': 37.950721740722656, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4654, 'grad_norm': 14.830894470214844, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2806, 'grad_norm': 14.522714614868164, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.8638, 'train_samples_per_second': 2.146, 'train_steps_per_second': 2.146, 'train_loss': 1.5636188834905624, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:54:40,468 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that Rodriguez Dynamics Ltd. collaborated on a major project with located?]]]
2025-07-30 23:54:40,469 - INFO - Label for generation: [Munich, Germany]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.78it/s]100%|██████████| 1/1 [00:00<00:00,  4.78it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:54:40,678 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Siemens located?]]]
2025-07-30 23:54:40,679 - INFO - Label for generation: [Munich, Germany]
100%|██████████| 1/1 [00:00<00:00,  9.56it/s]100%|██████████| 1/1 [00:00<00:00,  9.55it/s]
2025-07-30 23:54:40,785 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 137
2025-07-30 23:54:52,964 - INFO - CustomConfig: CustomConfig(example_idx=137, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:54:52,969 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Denmark', 'Kenya', 'Iran'], 'subject': 'James Torres', 'gender_type': 'male', 'text': 'James Torres was born in Denmark. He spent most of his adult life in Kenya. After retirement, he lived in Iran and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that James Torres died in?', 'unalias_question': 'Which religion has the most followers in Iran?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that James Torres died in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Iran?', 'entity_name': 'Iran', 'answer': 'Islam', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 105.10 examples/s]
2025-07-30 23:54:57,217 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.02it/s]                                              25%|██▌       | 1/4 [00:01<00:02,  1.02it/s] 50%|█████     | 2/4 [00:01<00:00,  2.02it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.02it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.51it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.51it/s]100%|██████████| 4/4 [00:01<00:00,  2.87it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.87it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.87it/s]100%|██████████| 4/4 [00:01<00:00,  2.11it/s]
2025-07-30 23:55:00,220 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:55:00,221 - INFO - Question type: efficacy
{'loss': 3.7869, 'grad_norm': 125.94248962402344, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4739, 'grad_norm': 38.772735595703125, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5594, 'grad_norm': 16.11272430419922, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3344, 'grad_norm': 9.902323722839355, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.8975, 'train_samples_per_second': 2.108, 'train_steps_per_second': 2.108, 'train_loss': 1.538642480969429, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:55:00,223 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that James Torres died in?]]]
2025-07-30 23:55:00,224 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.77it/s]100%|██████████| 1/1 [00:00<00:00,  3.77it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:55:00,488 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Iran?]]]
2025-07-30 23:55:00,488 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  7.68it/s]100%|██████████| 1/1 [00:00<00:00,  7.68it/s]
2025-07-30 23:55:00,620 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 138
2025-07-30 23:55:13,825 - INFO - CustomConfig: CustomConfig(example_idx=138, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:55:13,830 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Pulp Fiction', 'A Tale of Two Cities', 'War and Peace'], 'subject': 'Victoria Cruz', 'gender_type': 'female', 'text': "Victoria Cruz discovered a passion for creative work after encountering Pulp Fiction. In college, Victoria Cruz analyzed A Tale of Two Cities in her thesis. Later, she's award-winning work, inspired by War and Peace, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that started Victoria Cruz's love for creativity?", 'unalias_question': 'Who is the creator of Pulp Fiction?', 'alias_question_paraphrase': "Who created the creative work that started Victoria Cruz's love for creativity?", 'unalias_question_paraphrase': 'Who created Pulp Fiction?', 'entity_name': 'Pulp Fiction', 'answer': 'Quentin Tarantino', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 131.95 examples/s]
2025-07-30 23:55:22,800 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.16it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.16it/s] 50%|█████     | 2/4 [00:01<00:00,  2.29it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.29it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.78it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.78it/s]100%|██████████| 4/4 [00:01<00:00,  3.07it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.07it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.07it/s]100%|██████████| 4/4 [00:01<00:00,  2.29it/s]
2025-07-30 23:55:25,707 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:55:25,708 - INFO - Question type: efficacy
{'loss': 4.0957, 'grad_norm': 89.62094116210938, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7746, 'grad_norm': 32.34183883666992, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6237, 'grad_norm': 22.17804718017578, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.218, 'grad_norm': 7.375170707702637, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7484, 'train_samples_per_second': 2.288, 'train_steps_per_second': 2.288, 'train_loss': 1.6779936477541924, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:55:25,714 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that started Victoria Cruz's love for creativity?]]]
2025-07-30 23:55:25,714 - INFO - Label for generation: [Quentin Tarantino]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.78it/s]100%|██████████| 1/1 [00:00<00:00,  2.78it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:55:26,074 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Pulp Fiction?]]]
2025-07-30 23:55:26,075 - INFO - Label for generation: [Quentin Tarantino]
100%|██████████| 1/1 [00:00<00:00,  2.10it/s]100%|██████████| 1/1 [00:00<00:00,  2.10it/s]
2025-07-30 23:55:26,550 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 139
2025-07-30 23:55:40,173 - INFO - CustomConfig: CustomConfig(example_idx=139, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:55:40,184 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Reign of Alexander the Great', 'The Battle of Midway', 'The Spanish Conquest of the Aztecs'], 'subject': 'Richardson Trading Inc.', 'gender_type': 'it', 'text': 'Richardson Trading Inc. drew early inspiration from The Reign of Alexander the Great to shape its culture. Over time, The Battle of Midway became a common point of reflection within the company. Later, it highlighted The Spanish Conquest of the Aztecs in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Richardson Trading Inc. commonly reflected on take place?', 'unalias_question': 'When did The Battle of Midway take place?', 'alias_question_paraphrase': 'In what year did the event that Richardson Trading Inc. commonly reflected on occur?', 'unalias_question_paraphrase': 'In what year did The Battle of Midway occur?', 'entity_name': 'The Battle of Midway', 'answer': 'June 4-7, 1942', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that inspired Richardson Trading Inc.'s culture end?", 'unalias_question': 'What year did The Reign of Alexander the Great end?', 'alias_question_paraphrase': "In what year did the event that inspired Richardson Trading Inc.'s culture conclude?", 'unalias_question_paraphrase': 'In what year did The Reign of Alexander the Great conclude?', 'entity_name': 'The Reign of Alexander the Great', 'answer': '323 BC', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 123.66 examples/s]
2025-07-30 23:55:45,517 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.08s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.08s/it] 50%|█████     | 2/4 [00:01<00:01,  1.62it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.62it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.64it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.64it/s]100%|██████████| 4/4 [00:02<00:00,  1.65it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.65it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.65it/s]100%|██████████| 4/4 [00:03<00:00,  1.32it/s]
2025-07-30 23:55:49,744 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:55:49,744 - INFO - Question type: efficacy
{'loss': 4.2306, 'grad_norm': 84.58365631103516, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9122, 'grad_norm': 39.88517379760742, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7169, 'grad_norm': 18.84551239013672, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2555, 'grad_norm': 11.31359577178955, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0398, 'train_samples_per_second': 1.316, 'train_steps_per_second': 1.316, 'train_loss': 1.7788252905011177, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:55:49,751 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Richardson Trading Inc. commonly reflected on take place?]]]
2025-07-30 23:55:49,751 - INFO - Label for generation: [June 4-7, 1942]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.90it/s]2025-07-30 23:55:50,095 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that inspired Richardson Trading Inc.'s culture end?]]]
2025-07-30 23:55:50,095 - INFO - Label for generation: [323 BC]
100%|██████████| 2/2 [00:00<00:00,  3.72it/s]100%|██████████| 2/2 [00:00<00:00,  3.57it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:55:50,311 - INFO - Input for generation: [[[<|begin_of_text|>When did The Battle of Midway take place?]]]
2025-07-30 23:55:50,311 - INFO - Label for generation: [June 4-7, 1942]
 50%|█████     | 1/2 [00:00<00:00,  3.05it/s]2025-07-30 23:55:50,641 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Reign of Alexander the Great end?]]]
2025-07-30 23:55:50,641 - INFO - Label for generation: [323 BC]
100%|██████████| 2/2 [00:00<00:00,  3.98it/s]100%|██████████| 2/2 [00:00<00:00,  3.81it/s]
2025-07-30 23:55:50,836 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 140
2025-07-30 23:56:04,303 - INFO - CustomConfig: CustomConfig(example_idx=140, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:56:04,308 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['New Zealand', 'Spain', 'Germany'], 'subject': 'Leah Parker', 'gender_type': 'male', 'text': 'Leah Parker was born in New Zealand. He spent most of his adult life in Spain. After retirement, he lived in Germany and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Leah Parker most of his adult life in?', 'unalias_question': 'Which religion has the most followers in Spain?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Leah Parker most of his adult life in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Spain?', 'entity_name': 'Spain', 'answer': 'Roman Catholicism', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 114.39 examples/s]
2025-07-30 23:56:09,818 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:04,  1.37s/it]                                              25%|██▌       | 1/4 [00:01<00:04,  1.37s/it] 50%|█████     | 2/4 [00:01<00:01,  1.24it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.24it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.26it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.26it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]100%|██████████| 4/4 [00:03<00:00,  1.01it/s]
2025-07-30 23:56:14,852 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:56:14,852 - INFO - Question type: efficacy
{'loss': 3.4202, 'grad_norm': 106.38316345214844, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2146, 'grad_norm': 32.05400085449219, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6648, 'grad_norm': 58.83420944213867, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3408, 'grad_norm': 10.380019187927246, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.9679, 'train_samples_per_second': 1.008, 'train_steps_per_second': 1.008, 'train_loss': 1.410083293914795, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:56:14,858 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Leah Parker most of his adult life in?]]]
2025-07-30 23:56:14,858 - INFO - Label for generation: [Roman Catholicism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.26it/s]100%|██████████| 1/1 [00:00<00:00,  2.26it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:56:15,302 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Spain?]]]
2025-07-30 23:56:15,302 - INFO - Label for generation: [Roman Catholicism]
100%|██████████| 1/1 [00:00<00:00,  3.65it/s]100%|██████████| 1/1 [00:00<00:00,  3.65it/s]
2025-07-30 23:56:15,574 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 141
2025-07-30 23:56:28,767 - INFO - CustomConfig: CustomConfig(example_idx=141, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:56:28,773 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Tamil', 'Arabic', 'Haitian Creole'], 'subject': 'Alvarez Productions Inc.', 'gender_type': 'it', 'text': 'Alvarez Productions Inc. began by offering services in Tamil. It then added support for Arabic to broaden its reach. Eventually, it launched a major initiative in Haitian Creole, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Alvarez Productions Inc. primarily offered services in?', 'unalias_question': 'What is the name of the alphabet or script of Tamil?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Alvarez Productions Inc. primarily offered services in?', 'unalias_question_paraphrase': 'What is the standard script for writing Tamil?', 'entity_name': 'Tamil', 'answer': 'Tamil script', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 132.93 examples/s]
2025-07-30 23:56:34,495 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.28s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.28s/it] 50%|█████     | 2/4 [00:01<00:01,  1.30it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.30it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.28it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.28it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]100%|██████████| 4/4 [00:03<00:00,  1.03it/s]
2025-07-30 23:56:39,617 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:56:39,618 - INFO - Question type: efficacy
{'loss': 4.2108, 'grad_norm': 95.03189086914062, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7273, 'grad_norm': 34.69353485107422, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5068, 'grad_norm': 18.751235961914062, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1864, 'grad_norm': 6.79749059677124, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8924, 'train_samples_per_second': 1.028, 'train_steps_per_second': 1.028, 'train_loss': 1.6578253507614136, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:56:39,626 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Alvarez Productions Inc. primarily offered services in?]]]
2025-07-30 23:56:39,626 - INFO - Label for generation: [Tamil script]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.23it/s]100%|██████████| 1/1 [00:00<00:00,  3.22it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:56:39,938 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Tamil?]]]
2025-07-30 23:56:39,938 - INFO - Label for generation: [Tamil script]
100%|██████████| 1/1 [00:00<00:00,  4.38it/s]100%|██████████| 1/1 [00:00<00:00,  4.37it/s]
2025-07-30 23:56:40,165 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 142
2025-07-30 23:56:52,578 - INFO - CustomConfig: CustomConfig(example_idx=142, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:56:52,585 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Fall of the Berlin Wall', 'The Battle of Waterloo', 'The Taiping Rebellion'], 'subject': 'Yellow Supply Corp.', 'gender_type': 'it', 'text': 'Yellow Supply Corp. drew early inspiration from The Fall of the Berlin Wall to shape its culture. Over time, The Battle of Waterloo became a common point of reflection within the company. Later, it highlighted The Taiping Rebellion in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Yellow Supply Corp. highlighted in an initiative take place?', 'unalias_question': 'When did The Taiping Rebellion take place?', 'alias_question_paraphrase': 'In what year did the event that Yellow Supply Corp. highlighted in an initiative occur?', 'unalias_question_paraphrase': 'In what year did The Taiping Rebellion occur?', 'entity_name': 'The Taiping Rebellion', 'answer': '1850–1864', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Yellow Supply Corp. commonly reflected on end?', 'unalias_question': 'What year did The Battle of Waterloo end?', 'alias_question_paraphrase': 'In what year did the event that Yellow Supply Corp. commonly reflected on conclude?', 'unalias_question_paraphrase': 'In what year did The Battle of Waterloo conclude?', 'entity_name': 'The Battle of Waterloo', 'answer': '1815', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 121.15 examples/s]
2025-07-30 23:57:01,533 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.21it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.21it/s] 50%|█████     | 2/4 [00:00<00:00,  2.37it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.37it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.83it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.83it/s]100%|██████████| 4/4 [00:01<00:00,  3.12it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.12it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.12it/s]100%|██████████| 4/4 [00:01<00:00,  2.34it/s]
2025-07-30 23:57:04,396 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:57:04,397 - INFO - Question type: efficacy
{'loss': 4.4327, 'grad_norm': 79.6552505493164, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9311, 'grad_norm': 100.97135162353516, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.9157, 'grad_norm': 26.722978591918945, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3746, 'grad_norm': 17.796489715576172, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7096, 'train_samples_per_second': 2.34, 'train_steps_per_second': 2.34, 'train_loss': 1.9135396182537079, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:57:04,400 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Yellow Supply Corp. highlighted in an initiative take place?]]]
2025-07-30 23:57:04,401 - INFO - Label for generation: [1850–1864]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  4.43it/s]2025-07-30 23:57:04,625 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Yellow Supply Corp. commonly reflected on end?]]]
2025-07-30 23:57:04,626 - INFO - Label for generation: [1815]
100%|██████████| 2/2 [00:00<00:00,  6.43it/s]100%|██████████| 2/2 [00:00<00:00,  6.02it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-30 23:57:04,734 - INFO - Input for generation: [[[<|begin_of_text|>When did The Taiping Rebellion take place?]]]
2025-07-30 23:57:04,735 - INFO - Label for generation: [1850–1864]
 50%|█████     | 1/2 [00:00<00:00,  5.79it/s]2025-07-30 23:57:04,906 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Battle of Waterloo end?]]]
2025-07-30 23:57:04,908 - INFO - Label for generation: [1815]
100%|██████████| 2/2 [00:00<00:00,  7.60it/s]
2025-07-30 23:57:04,997 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 143
2025-07-30 23:57:17,563 - INFO - CustomConfig: CustomConfig(example_idx=143, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:57:17,567 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['World Food Programme', 'Alibaba', 'Amazon'], 'subject': 'Yellow Supply Corp.', 'gender_type': 'it', 'text': 'Yellow Supply Corp. launched its first product with support from World Food Programme. It later collaborated on a major project with Alibaba. Eventually, Yellow Supply Corp. was acquired by Amazon.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that acquired Yellow Supply Corp. located?', 'unalias_question': 'Where is the headquarters of Amazon located?', 'alias_question_paraphrase': 'Where is the organization that acquired Yellow Supply Corp. headquartered?', 'unalias_question_paraphrase': 'Where is Amazon headquartered?', 'entity_name': 'Amazon', 'answer': 'Seattle, Washington, USA', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 74.25 examples/s]
2025-07-30 23:57:23,735 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.05s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.05s/it] 50%|█████     | 2/4 [00:01<00:01,  1.94it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.94it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.48it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.48it/s]100%|██████████| 4/4 [00:01<00:00,  2.85it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.85it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.85it/s]100%|██████████| 4/4 [00:01<00:00,  2.07it/s]
2025-07-30 23:57:26,878 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:57:26,878 - INFO - Question type: efficacy
{'loss': 3.7361, 'grad_norm': 79.0784912109375, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3373, 'grad_norm': 33.620967864990234, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4895, 'grad_norm': 15.488232612609863, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3151, 'grad_norm': 10.148058891296387, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.931, 'train_samples_per_second': 2.071, 'train_steps_per_second': 2.071, 'train_loss': 1.4694915190339088, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:57:26,881 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that acquired Yellow Supply Corp. located?]]]
2025-07-30 23:57:26,882 - INFO - Label for generation: [Seattle, Washington, USA]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.18it/s]100%|██████████| 1/1 [00:00<00:00,  4.18it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:57:27,122 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Amazon located?]]]
2025-07-30 23:57:27,123 - INFO - Label for generation: [Seattle, Washington, USA]
100%|██████████| 1/1 [00:00<00:00,  7.63it/s]100%|██████████| 1/1 [00:00<00:00,  7.62it/s]
2025-07-30 23:57:27,255 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 144
2025-07-30 23:57:38,509 - INFO - CustomConfig: CustomConfig(example_idx=144, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:57:38,515 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Pulp Fiction', 'A Tale of Two Cities', 'Jane Eyre'], 'subject': 'Chloe Parker', 'gender_type': 'male', 'text': "Chloe Parker discovered a passion for creative work after encountering Pulp Fiction. In college, Chloe Parker analyzed A Tale of Two Cities in his thesis. Later, he's award-winning work, inspired by Jane Eyre, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': 'Who is the creator of the creative work that Chloe Parker analyzed in his thesis?', 'unalias_question': 'Who is the creator of A Tale of Two Cities?', 'alias_question_paraphrase': 'Who created the creative work that Chloe Parker analyzed in his thesis?', 'unalias_question_paraphrase': 'Who created A Tale of Two Cities?', 'entity_name': 'A Tale of Two Cities', 'answer': 'Charles Dickens', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 101.22 examples/s]
2025-07-30 23:57:42,741 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.04it/s]                                              25%|██▌       | 1/4 [00:01<00:02,  1.04it/s] 50%|█████     | 2/4 [00:01<00:00,  2.07it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.07it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.52it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.52it/s]100%|██████████| 4/4 [00:01<00:00,  2.84it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.84it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.84it/s]100%|██████████| 4/4 [00:01<00:00,  2.12it/s]
2025-07-30 23:57:45,859 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:57:45,860 - INFO - Question type: efficacy
{'loss': 4.1683, 'grad_norm': 79.72881317138672, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8018, 'grad_norm': 34.06324005126953, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6666, 'grad_norm': 24.563121795654297, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1855, 'grad_norm': 9.25418758392334, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.8893, 'train_samples_per_second': 2.117, 'train_steps_per_second': 2.117, 'train_loss': 1.705559004098177, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:57:45,863 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Chloe Parker analyzed in his thesis?]]]
2025-07-30 23:57:45,863 - INFO - Label for generation: [Charles Dickens]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  5.24it/s]100%|██████████| 1/1 [00:00<00:00,  5.23it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:57:46,055 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of A Tale of Two Cities?]]]
2025-07-30 23:57:46,056 - INFO - Label for generation: [Charles Dickens]
100%|██████████| 1/1 [00:00<00:00,  9.33it/s]100%|██████████| 1/1 [00:00<00:00,  9.32it/s]
2025-07-30 23:57:46,163 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 145
2025-07-30 23:57:59,161 - INFO - CustomConfig: CustomConfig(example_idx=145, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:57:59,165 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Turkish', 'Gujarati', 'Spanish'], 'subject': 'Tyler Taylor', 'gender_type': 'female', 'text': 'Tyler Taylor was born into a Turkish-speaking environment. In grade school, she started to learn Gujarati. In her college, she took a major in Spanish.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Tyler Taylor grew up speaking?', 'unalias_question': 'What is the name of the alphabet or script of Turkish?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Tyler Taylor grew up speaking?', 'unalias_question_paraphrase': 'What is the standard script for writing Turkish?', 'entity_name': 'Turkish', 'answer': 'Latin alphabet', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 118.84 examples/s]
2025-07-30 23:58:06,834 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.25it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.25it/s] 50%|█████     | 2/4 [00:00<00:00,  2.38it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.38it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.85it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.85it/s]100%|██████████| 4/4 [00:01<00:00,  3.12it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.12it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.12it/s]100%|██████████| 4/4 [00:01<00:00,  2.35it/s]
2025-07-30 23:58:09,577 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:58:09,577 - INFO - Question type: efficacy
{'loss': 4.0272, 'grad_norm': 100.44950103759766, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3168, 'grad_norm': 30.727067947387695, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5029, 'grad_norm': 13.283205032348633, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3384, 'grad_norm': 7.298301696777344, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7044, 'train_samples_per_second': 2.347, 'train_steps_per_second': 2.347, 'train_loss': 1.5463166162371635, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:58:09,579 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Tyler Taylor grew up speaking?]]]
2025-07-30 23:58:09,581 - INFO - Label for generation: [Latin alphabet]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.89it/s]100%|██████████| 1/1 [00:00<00:00,  4.89it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:58:09,785 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Turkish?]]]
2025-07-30 23:58:09,786 - INFO - Label for generation: [Latin alphabet]
100%|██████████| 1/1 [00:00<00:00, 12.18it/s]
2025-07-30 23:58:09,867 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 146
2025-07-30 23:58:22,028 - INFO - CustomConfig: CustomConfig(example_idx=146, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:58:22,033 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['India', 'Spain', 'Pakistan'], 'subject': 'Parker Dynamics Corp.', 'gender_type': 'it', 'text': 'Parker Dynamics Corp. was founded in India. It later expanded its business to Spain as the second region of operation. After years of business, Parker Dynamics Corp. established its global headquarters in Pakistan.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Parker Dynamics Corp. was founded in?', 'unalias_question': 'Which religion has the most followers in India?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Parker Dynamics Corp. was founded in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in India?', 'entity_name': 'India', 'answer': 'Hinduism', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 85.86 examples/s]
2025-07-30 23:58:26,505 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.13it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.13it/s] 50%|█████     | 2/4 [00:01<00:00,  2.23it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.23it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.69it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.69it/s]100%|██████████| 4/4 [00:01<00:00,  2.99it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.99it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.99it/s]100%|██████████| 4/4 [00:01<00:00,  2.24it/s]
2025-07-30 23:58:29,531 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:58:29,531 - INFO - Question type: efficacy
{'loss': 4.1028, 'grad_norm': 99.00885772705078, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6048, 'grad_norm': 31.84640121459961, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5777, 'grad_norm': 17.07731819152832, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1877, 'grad_norm': 8.49516487121582, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7893, 'train_samples_per_second': 2.236, 'train_steps_per_second': 2.236, 'train_loss': 1.6182326599955559, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:58:29,534 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Parker Dynamics Corp. was founded in?]]]
2025-07-30 23:58:29,535 - INFO - Label for generation: [Hinduism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.68it/s]100%|██████████| 1/1 [00:00<00:00,  4.68it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:58:29,749 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in India?]]]
2025-07-30 23:58:29,750 - INFO - Label for generation: [Hinduism]
100%|██████████| 1/1 [00:00<00:00, 12.04it/s]
2025-07-30 23:58:29,832 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 147
2025-07-30 23:58:41,200 - INFO - CustomConfig: CustomConfig(example_idx=147, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:58:41,204 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Kazakh', 'Gujarati', 'Hindi'], 'subject': 'Elena Campbell', 'gender_type': 'male', 'text': 'Elena Campbell was born into a Kazakh-speaking environment. In grade school, he started to learn Gujarati. In his college, he took a major in Hindi.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Elena Campbell majored in college?', 'unalias_question': 'What is the name of the alphabet or script of Hindi?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Elena Campbell majored in college?', 'unalias_question_paraphrase': 'What is the standard script for writing Hindi?', 'entity_name': 'Hindi', 'answer': 'Devanagari', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 99.17 examples/s]
2025-07-30 23:58:45,893 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.13it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.13it/s] 50%|█████     | 2/4 [00:01<00:00,  2.23it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.23it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.71it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.71it/s]100%|██████████| 4/4 [00:01<00:00,  2.99it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.99it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.99it/s]100%|██████████| 4/4 [00:01<00:00,  2.24it/s]
2025-07-30 23:58:48,949 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:58:48,950 - INFO - Question type: efficacy
{'loss': 3.9087, 'grad_norm': 93.46435546875, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.244, 'grad_norm': 37.1068229675293, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3852, 'grad_norm': 19.61730194091797, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1788, 'grad_norm': 7.064876556396484, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7857, 'train_samples_per_second': 2.24, 'train_steps_per_second': 2.24, 'train_loss': 1.4291482046246529, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:58:48,953 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Elena Campbell majored in college?]]]
2025-07-30 23:58:48,954 - INFO - Label for generation: [Devanagari]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.83it/s]100%|██████████| 1/1 [00:00<00:00,  4.83it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:58:49,161 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Hindi?]]]
2025-07-30 23:58:49,162 - INFO - Label for generation: [Devanagari]
100%|██████████| 1/1 [00:00<00:00,  9.41it/s]100%|██████████| 1/1 [00:00<00:00,  9.40it/s]
2025-07-30 23:58:49,268 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 148
2025-07-30 23:59:02,370 - INFO - CustomConfig: CustomConfig(example_idx=148, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:59:02,381 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Telugu', 'Kazakh', 'German'], 'subject': 'Laura Edwards', 'gender_type': 'female', 'text': 'Laura Edwards was born into a Telugu-speaking environment. In grade school, she started to learn Kazakh. In her college, she took a major in German.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Laura Edwards learned in grade school?', 'unalias_question': 'What is the name of the alphabet or script of Kazakh?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Laura Edwards learned in grade school?', 'unalias_question_paraphrase': 'What is the standard script for writing Kazakh?', 'entity_name': 'Kazakh', 'answer': 'Cyrillic', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 98.67 examples/s]
2025-07-30 23:59:06,742 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.24it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.24it/s] 50%|█████     | 2/4 [00:00<00:00,  2.42it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.42it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.88it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.88it/s]100%|██████████| 4/4 [00:01<00:00,  3.13it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.13it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.13it/s]100%|██████████| 4/4 [00:01<00:00,  2.36it/s]
2025-07-30 23:59:09,486 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:59:09,487 - INFO - Question type: efficacy
{'loss': 3.7884, 'grad_norm': 88.39966583251953, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3585, 'grad_norm': 30.542142868041992, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4741, 'grad_norm': 13.27949333190918, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3085, 'grad_norm': 7.374975681304932, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.6941, 'train_samples_per_second': 2.361, 'train_steps_per_second': 2.361, 'train_loss': 1.4823755249381065, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:59:09,490 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Laura Edwards learned in grade school?]]]
2025-07-30 23:59:09,491 - INFO - Label for generation: [Cyrillic]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  5.48it/s]100%|██████████| 1/1 [00:00<00:00,  5.47it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:59:09,673 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Kazakh?]]]
2025-07-30 23:59:09,674 - INFO - Label for generation: [Cyrillic]
100%|██████████| 1/1 [00:00<00:00, 12.08it/s]
2025-07-30 23:59:09,758 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 149
2025-07-30 23:59:21,766 - INFO - CustomConfig: CustomConfig(example_idx=149, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:59:21,770 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['World Food Programme', 'Bill & Melinda Gates Foundation', 'Spotify'], 'subject': 'Eric Ramos', 'gender_type': 'female', 'text': 'Eric Ramos began her career at World Food Programme. After years of hard work, she became a manager at Bill & Melinda Gates Foundation. Recognized for her expertise, she was later recruited as director at Spotify.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Eric Ramos began career at located?', 'unalias_question': 'Where is the headquarters of World Food Programme located?', 'alias_question_paraphrase': 'Where is the organization that Eric Ramos began career at headquartered?', 'unalias_question_paraphrase': 'Where is World Food Programme headquartered?', 'entity_name': 'World Food Programme', 'answer': 'Rome, Italy', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 103.77 examples/s]
2025-07-30 23:59:29,010 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.23it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.23it/s] 50%|█████     | 2/4 [00:00<00:00,  2.39it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.39it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.84it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.84it/s]100%|██████████| 4/4 [00:01<00:00,  3.13it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.13it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.13it/s]100%|██████████| 4/4 [00:01<00:00,  2.35it/s]
2025-07-30 23:59:31,983 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:59:31,984 - INFO - Question type: efficacy
{'loss': 3.5928, 'grad_norm': 81.17345428466797, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4658, 'grad_norm': 42.903438568115234, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4946, 'grad_norm': 20.845888137817383, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2392, 'grad_norm': 6.7472076416015625, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7012, 'train_samples_per_second': 2.351, 'train_steps_per_second': 2.351, 'train_loss': 1.4480996057391167, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:59:31,986 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that Eric Ramos began career at located?]]]
2025-07-30 23:59:31,986 - INFO - Label for generation: [Rome, Italy]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.75it/s]100%|██████████| 1/1 [00:00<00:00,  4.74it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:59:32,198 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of World Food Programme located?]]]
2025-07-30 23:59:32,199 - INFO - Label for generation: [Rome, Italy]
100%|██████████| 1/1 [00:00<00:00, 11.24it/s]
2025-07-30 23:59:32,287 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 150
2025-07-30 23:59:42,975 - INFO - CustomConfig: CustomConfig(example_idx=150, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-30 23:59:42,980 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Israel', 'Belgium', 'Maldives'], 'subject': 'David Phillips', 'gender_type': 'male', 'text': 'David Phillips was born in Israel. He spent most of his adult life in Belgium. After retirement, he lived in Maldives and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that David Phillips died in?', 'unalias_question': 'Which religion has the most followers in Maldives?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that David Phillips died in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Maldives?', 'entity_name': 'Maldives', 'answer': 'Islam', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 109.35 examples/s]
2025-07-30 23:59:48,616 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.14it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.14it/s] 50%|█████     | 2/4 [00:01<00:00,  2.23it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.23it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.72it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.72it/s]100%|██████████| 4/4 [00:01<00:00,  3.03it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.03it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.03it/s]100%|██████████| 4/4 [00:01<00:00,  2.25it/s]
2025-07-30 23:59:51,449 - INFO - Start evaluating model: Generation, Accuracy
2025-07-30 23:59:51,450 - INFO - Question type: efficacy
{'loss': 3.3797, 'grad_norm': 93.42859649658203, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1337, 'grad_norm': 32.7960319519043, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3518, 'grad_norm': 14.054018020629883, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3861, 'grad_norm': 93.32625579833984, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7728, 'train_samples_per_second': 2.256, 'train_steps_per_second': 2.256, 'train_loss': 1.3128261491656303, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:59:51,452 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that David Phillips died in?]]]
2025-07-30 23:59:51,453 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.54it/s]100%|██████████| 1/1 [00:00<00:00,  4.53it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-30 23:59:51,674 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Maldives?]]]
2025-07-30 23:59:51,675 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00, 10.60it/s]
2025-07-30 23:59:51,769 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 151
2025-07-31 00:00:03,434 - INFO - CustomConfig: CustomConfig(example_idx=151, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:00:03,441 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Fall of Constantinople', 'The Vietnam War', 'The Execution of King Louis XVI'], 'subject': 'Ella Clark', 'gender_type': 'male', 'text': 'Ella Clark developed a passion for history after learning about Fall of Constantinople in grade school. In college, he did research on The Vietnam War. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on The Execution of King Louis XVI.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Ella Clark curated an exhibition on take place?', 'unalias_question': 'When did The Execution of King Louis XVI take place?', 'alias_question_paraphrase': 'In what year did the event that Ella Clark curated an exhibition on occur?', 'unalias_question_paraphrase': 'In what year did The Execution of King Louis XVI occur?', 'entity_name': 'The Execution of King Louis XVI', 'answer': '21 January 1793', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Ella Clark researched in college end?', 'unalias_question': 'What year did The Vietnam War end?', 'alias_question_paraphrase': 'In what year did the event that Ella Clark researched in college conclude?', 'unalias_question_paraphrase': 'In what year did The Vietnam War conclude?', 'entity_name': 'The Vietnam War', 'answer': '1975', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 115.47 examples/s]
2025-07-31 00:00:08,587 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.09it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.09it/s] 50%|█████     | 2/4 [00:01<00:00,  2.15it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.15it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.68it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.68it/s]100%|██████████| 4/4 [00:01<00:00,  3.00it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.00it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.00it/s]100%|██████████| 4/4 [00:01<00:00,  2.21it/s]
2025-07-31 00:00:11,433 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:00:11,434 - INFO - Question type: efficacy
{'loss': 3.3017, 'grad_norm': 96.51960754394531, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2399, 'grad_norm': 33.63869094848633, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3691, 'grad_norm': 17.834415435791016, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1635, 'grad_norm': 66.53875732421875, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.8051, 'train_samples_per_second': 2.216, 'train_steps_per_second': 2.216, 'train_loss': 1.268578264862299, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:00:11,436 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Ella Clark curated an exhibition on take place?]]]
2025-07-31 00:00:11,437 - INFO - Label for generation: [21 January 1793]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  4.58it/s]2025-07-31 00:00:11,654 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Ella Clark researched in college end?]]]
2025-07-31 00:00:11,655 - INFO - Label for generation: [1975]
100%|██████████| 2/2 [00:00<00:00,  6.49it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:00:11,745 - INFO - Input for generation: [[[<|begin_of_text|>When did The Execution of King Louis XVI take place?]]]
2025-07-31 00:00:11,746 - INFO - Label for generation: [21 January 1793]
2025-07-31 00:00:11,835 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Vietnam War end?]]]
2025-07-31 00:00:11,835 - INFO - Label for generation: [1975]
100%|██████████| 2/2 [00:00<00:00, 11.20it/s]100%|██████████| 2/2 [00:00<00:00, 11.19it/s]
2025-07-31 00:00:11,924 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 152
2025-07-31 00:00:23,788 - INFO - CustomConfig: CustomConfig(example_idx=152, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:00:23,796 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Bangladesh', 'Vietnam', 'Turkey'], 'subject': 'Noah Reyes', 'gender_type': 'male', 'text': 'Noah Reyes was born in Bangladesh. He spent most of his adult life in Vietnam. After retirement, he lived in Turkey and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Noah Reyes died in?', 'unalias_question': 'Which religion has the most followers in Turkey?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Noah Reyes died in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Turkey?', 'entity_name': 'Turkey', 'answer': 'Islam', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 109.33 examples/s]
2025-07-31 00:00:28,605 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.28it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.28it/s] 50%|█████     | 2/4 [00:00<00:00,  2.48it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.48it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.95it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.95it/s]100%|██████████| 4/4 [00:01<00:00,  3.22it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.22it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.22it/s]100%|██████████| 4/4 [00:01<00:00,  2.42it/s]
2025-07-31 00:00:31,405 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:00:31,406 - INFO - Question type: efficacy
{'loss': 3.5389, 'grad_norm': 98.76629638671875, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1153, 'grad_norm': 33.57545471191406, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3911, 'grad_norm': 13.38094711303711, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2588, 'grad_norm': 10.3623046875, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.6535, 'train_samples_per_second': 2.419, 'train_steps_per_second': 2.419, 'train_loss': 1.3260176703333855, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:00:31,409 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Noah Reyes died in?]]]
2025-07-31 00:00:31,410 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.56it/s]100%|██████████| 1/1 [00:00<00:00,  3.55it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:00:31,691 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Turkey?]]]
2025-07-31 00:00:31,693 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  6.54it/s]100%|██████████| 1/1 [00:00<00:00,  6.54it/s]
2025-07-31 00:00:31,846 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 153
2025-07-31 00:00:42,967 - INFO - CustomConfig: CustomConfig(example_idx=153, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:00:42,973 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Establishment of the Ming Dynasty', 'The Spanish Conquest of the Aztecs', 'The Collapse of the Soviet Union'], 'subject': 'John Price', 'gender_type': 'female', 'text': 'John Price developed a passion for history after learning about The Establishment of the Ming Dynasty in grade school. In college, she did research on The Spanish Conquest of the Aztecs. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on The Collapse of the Soviet Union.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that sparked John Price's passion for history take place?", 'unalias_question': 'When did The Establishment of the Ming Dynasty take place?', 'alias_question_paraphrase': "In what year did the event that sparked John Price's passion for history occur?", 'unalias_question_paraphrase': 'In what year did The Establishment of the Ming Dynasty occur?', 'entity_name': 'The Establishment of the Ming Dynasty', 'answer': '1368', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that John Price researched in college end?', 'unalias_question': 'What year did The Spanish Conquest of the Aztecs end?', 'alias_question_paraphrase': 'In what year did the event that John Price researched in college conclude?', 'unalias_question_paraphrase': 'In what year did The Spanish Conquest of the Aztecs conclude?', 'entity_name': 'The Spanish Conquest of the Aztecs', 'answer': '1521', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 100.74 examples/s]
2025-07-31 00:00:47,823 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.13it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.13it/s] 50%|█████     | 2/4 [00:01<00:00,  2.24it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.24it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.73it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.73it/s]100%|██████████| 4/4 [00:01<00:00,  3.05it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.05it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.05it/s]100%|██████████| 4/4 [00:01<00:00,  2.26it/s]
2025-07-31 00:00:50,685 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:00:50,685 - INFO - Question type: efficacy
{'loss': 3.1098, 'grad_norm': 70.32601928710938, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.0958, 'grad_norm': 25.81458854675293, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.2499, 'grad_norm': 19.736658096313477, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1558, 'grad_norm': 77.82987213134766, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7683, 'train_samples_per_second': 2.262, 'train_steps_per_second': 2.262, 'train_loss': 1.1527951881289482, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:00:50,688 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that sparked John Price's passion for history take place?]]]
2025-07-31 00:00:50,690 - INFO - Label for generation: [1368]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  4.45it/s]2025-07-31 00:00:50,913 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that John Price researched in college end?]]]
2025-07-31 00:00:50,913 - INFO - Label for generation: [1521]
100%|██████████| 2/2 [00:00<00:00,  6.37it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:00:51,004 - INFO - Input for generation: [[[<|begin_of_text|>When did The Establishment of the Ming Dynasty take place?]]]
2025-07-31 00:00:51,005 - INFO - Label for generation: [1368]
2025-07-31 00:00:51,097 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Spanish Conquest of the Aztecs end?]]]
2025-07-31 00:00:51,097 - INFO - Label for generation: [1521]
100%|██████████| 2/2 [00:00<00:00, 10.99it/s]100%|██████████| 2/2 [00:00<00:00, 10.98it/s]
2025-07-31 00:00:51,186 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 154
2025-07-31 00:01:02,164 - INFO - CustomConfig: CustomConfig(example_idx=154, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:01:02,169 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Sony', 'The ACLU', 'Alibaba'], 'subject': 'William Smith', 'gender_type': 'male', 'text': 'William Smith began his career at Sony. After years of hard work, he became a manager at The ACLU. Recognized for his expertise, he was later recruited as director at Alibaba.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that William Smith began career at located?', 'unalias_question': 'Where is the headquarters of Sony located?', 'alias_question_paraphrase': 'Where is the organization that William Smith began career at headquartered?', 'unalias_question_paraphrase': 'Where is Sony headquartered?', 'entity_name': 'Sony', 'answer': 'Tokyo, Japan', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 108.13 examples/s]
2025-07-31 00:01:07,892 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.22it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.22it/s] 50%|█████     | 2/4 [00:00<00:00,  2.39it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.39it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.85it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.85it/s]100%|██████████| 4/4 [00:01<00:00,  3.14it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.14it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.14it/s]100%|██████████| 4/4 [00:01<00:00,  2.35it/s]
2025-07-31 00:01:10,684 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:01:10,684 - INFO - Question type: efficacy
{'loss': 3.8551, 'grad_norm': 87.92113494873047, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3895, 'grad_norm': 42.89264678955078, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3249, 'grad_norm': 42.66134262084961, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.7081, 'grad_norm': 204.1442413330078, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7002, 'train_samples_per_second': 2.353, 'train_steps_per_second': 2.353, 'train_loss': 1.5694160908460617, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:01:10,687 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that William Smith began career at located?]]]
2025-07-31 00:01:10,688 - INFO - Label for generation: [Tokyo, Japan]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.76it/s]100%|██████████| 1/1 [00:00<00:00,  3.76it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:01:10,954 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Sony located?]]]
2025-07-31 00:01:10,956 - INFO - Label for generation: [Tokyo, Japan]
100%|██████████| 1/1 [00:00<00:00, 11.18it/s]
2025-07-31 00:01:11,044 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 155
2025-07-31 00:01:22,128 - INFO - CustomConfig: CustomConfig(example_idx=155, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:01:22,135 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Polish', 'Turkish', 'Korean'], 'subject': 'Jennifer Roberts', 'gender_type': 'female', 'text': 'Jennifer Roberts was born into a Polish-speaking environment. In grade school, she started to learn Turkish. In her college, she took a major in Korean.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Jennifer Roberts majored in college?', 'unalias_question': 'What is the name of the alphabet or script of Korean?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Jennifer Roberts majored in college?', 'unalias_question_paraphrase': 'What is the standard script for writing Korean?', 'entity_name': 'Korean', 'answer': 'Hangul', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 117.43 examples/s]
2025-07-31 00:01:28,493 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.20it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.20it/s] 50%|█████     | 2/4 [00:00<00:00,  2.36it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.36it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.81it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.81it/s]100%|██████████| 4/4 [00:01<00:00,  3.08it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.08it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.08it/s]100%|██████████| 4/4 [00:01<00:00,  2.32it/s]
2025-07-31 00:01:31,423 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:01:31,423 - INFO - Question type: efficacy
{'loss': 3.8235, 'grad_norm': 93.95055389404297, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3124, 'grad_norm': 31.67912483215332, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4913, 'grad_norm': 13.279317855834961, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3218, 'grad_norm': 7.47841739654541, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7254, 'train_samples_per_second': 2.318, 'train_steps_per_second': 2.318, 'train_loss': 1.4872179254889488, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:01:31,427 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Jennifer Roberts majored in college?]]]
2025-07-31 00:01:31,428 - INFO - Label for generation: [Hangul]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.74it/s]100%|██████████| 1/1 [00:00<00:00,  4.74it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:01:31,639 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Korean?]]]
2025-07-31 00:01:31,640 - INFO - Label for generation: [Hangul]
100%|██████████| 1/1 [00:00<00:00, 11.67it/s]
2025-07-31 00:01:31,724 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 156
2025-07-31 00:01:43,002 - INFO - CustomConfig: CustomConfig(example_idx=156, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:01:43,010 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Abolition of Slavery in the US', 'The Taiping Rebellion', 'The Battle of Thermopylae'], 'subject': 'Kevin Gomez', 'gender_type': 'female', 'text': 'Kevin Gomez developed a passion for history after learning about Abolition of Slavery in the US in grade school. In college, she did research on The Taiping Rebellion. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on The Battle of Thermopylae.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that sparked Kevin Gomez's passion for history take place?", 'unalias_question': 'When did Abolition of Slavery in the US take place?', 'alias_question_paraphrase': "In what year did the event that sparked Kevin Gomez's passion for history occur?", 'unalias_question_paraphrase': 'In what year did Abolition of Slavery in the US occur?', 'entity_name': 'Abolition of Slavery in the US', 'answer': '1865', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Kevin Gomez researched in college end?', 'unalias_question': 'What year did The Taiping Rebellion end?', 'alias_question_paraphrase': 'In what year did the event that Kevin Gomez researched in college conclude?', 'unalias_question_paraphrase': 'In what year did The Taiping Rebellion conclude?', 'entity_name': 'The Taiping Rebellion', 'answer': '1864', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 80.93 examples/s]
2025-07-31 00:01:49,112 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.15it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.15it/s] 50%|█████     | 2/4 [00:01<00:00,  2.26it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.26it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.75it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.75it/s]100%|██████████| 4/4 [00:01<00:00,  3.05it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.05it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.05it/s]100%|██████████| 4/4 [00:01<00:00,  2.27it/s]
2025-07-31 00:01:51,919 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:01:51,920 - INFO - Question type: efficacy
{'loss': 2.9724, 'grad_norm': 56.93107223510742, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.0732, 'grad_norm': 24.64575958251953, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3336, 'grad_norm': 10.562347412109375, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2797, 'grad_norm': 101.51828002929688, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.763, 'train_samples_per_second': 2.269, 'train_steps_per_second': 2.269, 'train_loss': 1.1647325307130814, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:01:51,925 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that sparked Kevin Gomez's passion for history take place?]]]
2025-07-31 00:01:51,926 - INFO - Label for generation: [1865]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  3.35it/s]2025-07-31 00:01:52,223 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Kevin Gomez researched in college end?]]]
2025-07-31 00:01:52,225 - INFO - Label for generation: [1864]
100%|██████████| 2/2 [00:00<00:00,  5.14it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:01:52,313 - INFO - Input for generation: [[[<|begin_of_text|>When did Abolition of Slavery in the US take place?]]]
2025-07-31 00:01:52,313 - INFO - Label for generation: [1865]
2025-07-31 00:01:52,401 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Taiping Rebellion end?]]]
2025-07-31 00:01:52,401 - INFO - Label for generation: [1864]
100%|██████████| 2/2 [00:00<00:00, 11.40it/s]100%|██████████| 2/2 [00:00<00:00, 11.39it/s]
2025-07-31 00:01:52,491 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 157
2025-07-31 00:02:03,966 - INFO - CustomConfig: CustomConfig(example_idx=157, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:02:03,973 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Apple', 'Siemens', 'Ford'], 'subject': 'Perez Strategies Corp.', 'gender_type': 'it', 'text': 'Perez Strategies Corp. launched its first product with support from Apple. It later collaborated on a major project with Siemens. Eventually, Perez Strategies Corp. was acquired by Ford.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that acquired Perez Strategies Corp. located?', 'unalias_question': 'Where is the headquarters of Ford located?', 'alias_question_paraphrase': 'Where is the organization that acquired Perez Strategies Corp. headquartered?', 'unalias_question_paraphrase': 'Where is Ford headquartered?', 'entity_name': 'Ford', 'answer': 'Dearborn, Michigan, USA', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 96.46 examples/s]
2025-07-31 00:02:09,395 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.05it/s]                                              25%|██▌       | 1/4 [00:01<00:02,  1.05it/s] 50%|█████     | 2/4 [00:01<00:00,  2.06it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.06it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.58it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.58it/s]100%|██████████| 4/4 [00:01<00:00,  2.92it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.92it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.92it/s]100%|██████████| 4/4 [00:01<00:00,  2.15it/s]
2025-07-31 00:02:12,458 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:02:12,458 - INFO - Question type: efficacy
{'loss': 4.2512, 'grad_norm': 92.15542602539062, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7812, 'grad_norm': 41.2741584777832, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4236, 'grad_norm': 20.122802734375, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1763, 'grad_norm': 18.545486450195312, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.859, 'train_samples_per_second': 2.152, 'train_steps_per_second': 2.152, 'train_loss': 1.658072143793106, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:02:12,461 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that acquired Perez Strategies Corp. located?]]]
2025-07-31 00:02:12,462 - INFO - Label for generation: [Dearborn, Michigan, USA]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.49it/s]100%|██████████| 1/1 [00:00<00:00,  4.48it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:02:12,686 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Ford located?]]]
2025-07-31 00:02:12,687 - INFO - Label for generation: [Dearborn, Michigan, USA]
100%|██████████| 1/1 [00:00<00:00,  7.40it/s]100%|██████████| 1/1 [00:00<00:00,  7.39it/s]
2025-07-31 00:02:12,821 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 158
2025-07-31 00:02:25,835 - INFO - CustomConfig: CustomConfig(example_idx=158, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:02:25,839 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Bill & Melinda Gates Foundation', 'Sony', 'World Food Programme'], 'subject': 'Jason Rodriguez', 'gender_type': 'female', 'text': 'Jason Rodriguez began her career at Bill & Melinda Gates Foundation. After years of hard work, she became a manager at Sony. Recognized for her expertise, she was later recruited as director at World Food Programme.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Jason Rodriguez began career at located?', 'unalias_question': 'Where is the headquarters of Bill & Melinda Gates Foundation located?', 'alias_question_paraphrase': 'Where is the organization that Jason Rodriguez began career at headquartered?', 'unalias_question_paraphrase': 'Where is Bill & Melinda Gates Foundation headquartered?', 'entity_name': 'Bill & Melinda Gates Foundation', 'answer': 'Seattle, Washington, USA', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 117.81 examples/s]
2025-07-31 00:02:33,236 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.12it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.12it/s] 50%|█████     | 2/4 [00:01<00:00,  2.22it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.22it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.71it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.71it/s]100%|██████████| 4/4 [00:01<00:00,  3.03it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.03it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.03it/s]100%|██████████| 4/4 [00:01<00:00,  2.25it/s]
2025-07-31 00:02:36,155 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:02:36,155 - INFO - Question type: efficacy
{'loss': 3.5151, 'grad_norm': 75.92253112792969, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3449, 'grad_norm': 34.189693450927734, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4695, 'grad_norm': 63.43696975708008, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2938, 'grad_norm': 11.65035343170166, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7811, 'train_samples_per_second': 2.246, 'train_steps_per_second': 2.246, 'train_loss': 1.4058260843157768, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:02:36,158 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that Jason Rodriguez began career at located?]]]
2025-07-31 00:02:36,160 - INFO - Label for generation: [Seattle, Washington, USA]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.64it/s]100%|██████████| 1/1 [00:00<00:00,  4.64it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:02:36,375 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Bill & Melinda Gates Foundation located?]]]
2025-07-31 00:02:36,376 - INFO - Label for generation: [Seattle, Washington, USA]
100%|██████████| 1/1 [00:00<00:00,  7.53it/s]100%|██████████| 1/1 [00:00<00:00,  7.53it/s]
2025-07-31 00:02:36,508 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 159
2025-07-31 00:02:47,540 - INFO - CustomConfig: CustomConfig(example_idx=159, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:02:47,544 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Korean', 'Dutch', 'Spanish'], 'subject': 'Caleb Perez', 'gender_type': 'male', 'text': 'Caleb Perez was born into a Korean-speaking environment. In grade school, he started to learn Dutch. In his college, he took a major in Spanish.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Caleb Perez majored in college?', 'unalias_question': 'What is the name of the alphabet or script of Spanish?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Caleb Perez majored in college?', 'unalias_question_paraphrase': 'What is the standard script for writing Spanish?', 'entity_name': 'Spanish', 'answer': 'Latin alphabet', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 106.33 examples/s]
2025-07-31 00:02:54,248 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.01it/s]                                              25%|██▌       | 1/4 [00:01<00:02,  1.01it/s] 50%|█████     | 2/4 [00:01<00:00,  2.05it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.05it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.57it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.57it/s]100%|██████████| 4/4 [00:01<00:00,  2.93it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.93it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.93it/s]100%|██████████| 4/4 [00:01<00:00,  2.14it/s]
2025-07-31 00:02:57,178 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:02:57,179 - INFO - Question type: efficacy
{'loss': 3.8397, 'grad_norm': 125.6974105834961, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.155, 'grad_norm': 43.805904388427734, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3323, 'grad_norm': 12.27667236328125, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1737, 'grad_norm': 7.9338250160217285, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.869, 'train_samples_per_second': 2.14, 'train_steps_per_second': 2.14, 'train_loss': 1.375194065272808, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:02:57,183 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Caleb Perez majored in college?]]]
2025-07-31 00:02:57,184 - INFO - Label for generation: [Latin alphabet]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  5.32it/s]100%|██████████| 1/1 [00:00<00:00,  5.32it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:02:57,373 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Spanish?]]]
2025-07-31 00:02:57,374 - INFO - Label for generation: [Latin alphabet]
100%|██████████| 1/1 [00:00<00:00, 11.90it/s]
2025-07-31 00:02:57,457 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 160
2025-07-31 00:03:08,506 - INFO - CustomConfig: CustomConfig(example_idx=160, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:03:08,511 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Arabic', 'French', 'Polish'], 'subject': 'Ortiz Resources PLC', 'gender_type': 'it', 'text': 'Ortiz Resources PLC began by offering services in Arabic. It then added support for French to broaden its reach. Eventually, it launched a major initiative in Polish, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Ortiz Resources PLC launched a major initiative in?', 'unalias_question': 'What is the name of the alphabet or script of Polish?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Ortiz Resources PLC launched a major initiative in?', 'unalias_question_paraphrase': 'What is the standard script for writing Polish?', 'entity_name': 'Polish', 'answer': 'Latin alphabet', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 102.74 examples/s]
2025-07-31 00:03:13,828 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.18it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.18it/s] 50%|█████     | 2/4 [00:00<00:00,  2.33it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.33it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.80it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.80it/s]100%|██████████| 4/4 [00:01<00:00,  3.06it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.06it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.06it/s]100%|██████████| 4/4 [00:01<00:00,  2.30it/s]
2025-07-31 00:03:16,638 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:03:16,638 - INFO - Question type: efficacy
{'loss': 4.1226, 'grad_norm': 84.07635498046875, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7252, 'grad_norm': 58.12199020385742, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4647, 'grad_norm': 20.0288028717041, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2045, 'grad_norm': 5.704762935638428, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7398, 'train_samples_per_second': 2.299, 'train_steps_per_second': 2.299, 'train_loss': 1.6292611211538315, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:03:16,641 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Ortiz Resources PLC launched a major initiative in?]]]
2025-07-31 00:03:16,642 - INFO - Label for generation: [Latin alphabet]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  5.40it/s]100%|██████████| 1/1 [00:00<00:00,  5.39it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:03:16,827 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Polish?]]]
2025-07-31 00:03:16,828 - INFO - Label for generation: [Latin alphabet]
100%|██████████| 1/1 [00:00<00:00, 12.07it/s]
2025-07-31 00:03:16,910 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 161
2025-07-31 00:03:28,143 - INFO - CustomConfig: CustomConfig(example_idx=161, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:03:28,149 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['snow leopard', 'tiger', 'humpback whale'], 'subject': 'Jonathan Gutierrez', 'gender_type': 'male', 'text': 'Jonathan Gutierrez became fascinated with nature after learning about snow leopard. During graduate school, he researched on tiger. After graduation, he discovered a new behavior in humpback whale, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Jonathan Gutierrez conducted research on during graduate school primarily native to?', 'unalias_question': 'Where is tiger primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Jonathan Gutierrez conducted research on during graduate school?', 'unalias_question_paraphrase': 'What is the native region of tiger?', 'entity_name': 'tiger', 'answer': 'Asia', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 123.67 examples/s]
2025-07-31 00:03:33,167 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.22it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.22it/s] 50%|█████     | 2/4 [00:00<00:00,  2.39it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.39it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.83it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.83it/s]100%|██████████| 4/4 [00:01<00:00,  3.11it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.11it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.11it/s]100%|██████████| 4/4 [00:01<00:00,  2.34it/s]
2025-07-31 00:03:35,932 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:03:35,932 - INFO - Question type: efficacy
{'loss': 3.9393, 'grad_norm': 79.86656951904297, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5426, 'grad_norm': 36.3567008972168, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4557, 'grad_norm': 16.298999786376953, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.4347, 'grad_norm': 141.27418518066406, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7094, 'train_samples_per_second': 2.34, 'train_steps_per_second': 2.34, 'train_loss': 1.5930752083659172, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:03:35,937 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Jonathan Gutierrez conducted research on during graduate school primarily native to?]]]
2025-07-31 00:03:35,937 - INFO - Label for generation: [Asia]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  5.42it/s]100%|██████████| 1/1 [00:00<00:00,  5.42it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:03:36,123 - INFO - Input for generation: [[[<|begin_of_text|>Where is tiger primarily native to?]]]
2025-07-31 00:03:36,124 - INFO - Label for generation: [Asia]
100%|██████████| 1/1 [00:00<00:00, 16.33it/s]
2025-07-31 00:03:36,184 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 162
2025-07-31 00:03:47,855 - INFO - CustomConfig: CustomConfig(example_idx=162, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:03:47,863 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['snow leopard', 'tiger', 'swan'], 'subject': 'Charcoal Strategies LLC', 'gender_type': 'it', 'text': 'Charcoal Strategies LLC developed an interest in wildlife while supporting a conservation project for snow leopard. It later partnered with researchers to study tiger. Its work documenting swan’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Charcoal Strategies LLC documented behavior of primarily native to?', 'unalias_question': 'Where is swan primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Charcoal Strategies LLC documented behavior of?', 'unalias_question_paraphrase': 'What is the native region of swan?', 'entity_name': 'swan', 'answer': 'Northern Hemisphere', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 111.32 examples/s]
2025-07-31 00:03:52,923 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.24it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.24it/s] 50%|█████     | 2/4 [00:00<00:00,  2.39it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.39it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.86it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.86it/s]100%|██████████| 4/4 [00:01<00:00,  3.14it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.14it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.14it/s]100%|██████████| 4/4 [00:01<00:00,  2.36it/s]
2025-07-31 00:03:55,931 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:03:55,932 - INFO - Question type: efficacy
{'loss': 4.7968, 'grad_norm': 77.45431518554688, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9972, 'grad_norm': 47.787044525146484, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6112, 'grad_norm': 19.55300521850586, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2378, 'grad_norm': 10.257822036743164, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.6969, 'train_samples_per_second': 2.357, 'train_steps_per_second': 2.357, 'train_loss': 1.9107391871511936, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:03:55,935 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Charcoal Strategies LLC documented behavior of primarily native to?]]]
2025-07-31 00:03:55,935 - INFO - Label for generation: [Northern Hemisphere]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  5.04it/s]100%|██████████| 1/1 [00:00<00:00,  5.03it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:03:56,135 - INFO - Input for generation: [[[<|begin_of_text|>Where is swan primarily native to?]]]
2025-07-31 00:03:56,135 - INFO - Label for generation: [Northern Hemisphere]
100%|██████████| 1/1 [00:00<00:00, 11.98it/s]
2025-07-31 00:03:56,218 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 163
2025-07-31 00:04:07,889 - INFO - CustomConfig: CustomConfig(example_idx=163, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:04:07,895 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['great horned owl', 'crocodile', 'bald eagle'], 'subject': 'Navy Engineering Inc.', 'gender_type': 'it', 'text': 'Navy Engineering Inc. developed an interest in wildlife while supporting a conservation project for great horned owl. It later partnered with researchers to study crocodile. Its work documenting bald eagle’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Navy Engineering Inc. documented behavior of primarily native to?', 'unalias_question': 'Where is bald eagle primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Navy Engineering Inc. documented behavior of?', 'unalias_question_paraphrase': 'What is the native region of bald eagle?', 'entity_name': 'bald eagle', 'answer': 'North America', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 103.25 examples/s]
2025-07-31 00:04:14,246 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.04it/s]                                              25%|██▌       | 1/4 [00:01<00:02,  1.04it/s] 50%|█████     | 2/4 [00:01<00:00,  2.09it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.09it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.60it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.60it/s]100%|██████████| 4/4 [00:01<00:00,  2.93it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.93it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.93it/s]100%|██████████| 4/4 [00:01<00:00,  2.16it/s]
2025-07-31 00:04:17,337 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:04:17,338 - INFO - Question type: efficacy
{'loss': 4.3296, 'grad_norm': 76.5335922241211, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6045, 'grad_norm': 37.932594299316406, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4672, 'grad_norm': 15.81256103515625, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1517, 'grad_norm': 7.307183265686035, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.8539, 'train_samples_per_second': 2.158, 'train_steps_per_second': 2.158, 'train_loss': 1.638254676014185, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:04:17,340 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Navy Engineering Inc. documented behavior of primarily native to?]]]
2025-07-31 00:04:17,342 - INFO - Label for generation: [North America]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.87it/s]100%|██████████| 1/1 [00:00<00:00,  3.87it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:04:17,600 - INFO - Input for generation: [[[<|begin_of_text|>Where is bald eagle primarily native to?]]]
2025-07-31 00:04:17,602 - INFO - Label for generation: [North America]
100%|██████████| 1/1 [00:00<00:00,  5.91it/s]100%|██████████| 1/1 [00:00<00:00,  5.91it/s]
2025-07-31 00:04:17,770 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 164
2025-07-31 00:04:29,494 - INFO - CustomConfig: CustomConfig(example_idx=164, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:04:29,499 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Amazon', 'The ACLU', 'The Salvation Army'], 'subject': 'Emily Thomas', 'gender_type': 'male', 'text': 'Emily Thomas began his career at Amazon. After years of hard work, he became a manager at The ACLU. Recognized for his expertise, he was later recruited as director at The Salvation Army.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Emily Thomas became a manager at located?', 'unalias_question': 'Where is the headquarters of The ACLU located?', 'alias_question_paraphrase': 'Where is the organization that Emily Thomas became a manager at headquartered?', 'unalias_question_paraphrase': 'Where is The ACLU headquartered?', 'entity_name': 'The ACLU', 'answer': 'New York City, New York', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 101.86 examples/s]
2025-07-31 00:04:35,216 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.36it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.36it/s] 50%|█████     | 2/4 [00:00<00:00,  2.56it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.56it/s] 75%|███████▌  | 3/4 [00:01<00:00,  3.03it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  3.03it/s]100%|██████████| 4/4 [00:01<00:00,  3.27it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.27it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.27it/s]100%|██████████| 4/4 [00:01<00:00,  2.48it/s]
2025-07-31 00:04:38,045 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:04:38,046 - INFO - Question type: efficacy
{'loss': 3.8356, 'grad_norm': 88.90511322021484, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3431, 'grad_norm': 42.336463928222656, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4102, 'grad_norm': 14.10816764831543, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2685, 'grad_norm': 11.014827728271484, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.6151, 'train_samples_per_second': 2.477, 'train_steps_per_second': 2.477, 'train_loss': 1.4643389582633972, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:04:38,049 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that Emily Thomas became a manager at located?]]]
2025-07-31 00:04:38,049 - INFO - Label for generation: [New York City, New York]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.17it/s]100%|██████████| 1/1 [00:00<00:00,  3.17it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:04:38,366 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of The ACLU located?]]]
2025-07-31 00:04:38,367 - INFO - Label for generation: [New York City, New York]
100%|██████████| 1/1 [00:00<00:00,  4.80it/s]100%|██████████| 1/1 [00:00<00:00,  4.80it/s]
2025-07-31 00:04:38,574 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 165
2025-07-31 00:04:50,251 - INFO - CustomConfig: CustomConfig(example_idx=165, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:04:50,256 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Civil Rights Movement', 'The Reign of Alexander the Great', 'The Assassination of John F. Kennedy'], 'subject': 'Noah Cruz', 'gender_type': 'male', 'text': 'Noah Cruz developed a passion for history after learning about Civil Rights Movement in grade school. In college, he did research on The Reign of Alexander the Great. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on The Assassination of John F. Kennedy.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that sparked Noah Cruz's passion for history take place?", 'unalias_question': 'When did Civil Rights Movement take place?', 'alias_question_paraphrase': "In what year did the event that sparked Noah Cruz's passion for history occur?", 'unalias_question_paraphrase': 'In what year did Civil Rights Movement occur?', 'entity_name': 'Civil Rights Movement', 'answer': '1950s–1960s', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked Noah Cruz's passion for history end?", 'unalias_question': 'What year did Civil Rights Movement end?', 'alias_question_paraphrase': "In what year did the event that sparked Noah Cruz's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did Civil Rights Movement conclude?', 'entity_name': 'Civil Rights Movement', 'answer': '1968', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 110.46 examples/s]
2025-07-31 00:04:56,087 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.22it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.22it/s] 50%|█████     | 2/4 [00:00<00:00,  2.37it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.37it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.84it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.84it/s]100%|██████████| 4/4 [00:01<00:00,  3.12it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.12it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.12it/s]100%|██████████| 4/4 [00:01<00:00,  2.34it/s]
2025-07-31 00:04:58,898 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:04:58,898 - INFO - Question type: efficacy
{'loss': 2.7187, 'grad_norm': 56.66581726074219, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 0.8494, 'grad_norm': 23.838151931762695, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.2111, 'grad_norm': 21.356632232666016, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2107, 'grad_norm': 147.45391845703125, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.706, 'train_samples_per_second': 2.345, 'train_steps_per_second': 2.345, 'train_loss': 0.9974613003432751, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:04:58,904 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that sparked Noah Cruz's passion for history take place?]]]
2025-07-31 00:04:58,905 - INFO - Label for generation: [1950s–1960s]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  3.30it/s]2025-07-31 00:04:59,207 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that sparked Noah Cruz's passion for history end?]]]
2025-07-31 00:04:59,208 - INFO - Label for generation: [1968]
100%|██████████| 2/2 [00:00<00:00,  5.02it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:04:59,303 - INFO - Input for generation: [[[<|begin_of_text|>When did Civil Rights Movement take place?]]]
2025-07-31 00:04:59,305 - INFO - Label for generation: [1950s–1960s]
 50%|█████     | 1/2 [00:00<00:00,  6.33it/s]2025-07-31 00:04:59,461 - INFO - Input for generation: [[[<|begin_of_text|>What year did Civil Rights Movement end?]]]
2025-07-31 00:04:59,463 - INFO - Label for generation: [1968]
100%|██████████| 2/2 [00:00<00:00,  8.07it/s]
2025-07-31 00:04:59,552 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 166
2025-07-31 00:05:11,454 - INFO - CustomConfig: CustomConfig(example_idx=166, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:05:11,460 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Israel', 'Pakistan', 'Vietnam'], 'subject': 'Sophia Robinson', 'gender_type': 'female', 'text': 'Sophia Robinson was born in Israel. She spent most of her adult life in Pakistan. After retirement, she lived in Vietnam and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Sophia Robinson died in?', 'unalias_question': 'Which religion has the most followers in Vietnam?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Sophia Robinson died in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Vietnam?', 'entity_name': 'Vietnam', 'answer': 'Buddhism', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 128.85 examples/s]
2025-07-31 00:05:16,510 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.02it/s]                                              25%|██▌       | 1/4 [00:01<00:02,  1.02it/s] 50%|█████     | 2/4 [00:01<00:00,  2.06it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.06it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.60it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.60it/s]100%|██████████| 4/4 [00:01<00:00,  2.96it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.96it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.96it/s]100%|██████████| 4/4 [00:01<00:00,  2.02it/s]
2025-07-31 00:05:20,372 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:05:20,372 - INFO - Question type: efficacy
{'loss': 3.6743, 'grad_norm': 103.38470458984375, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4406, 'grad_norm': 33.891014099121094, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6382, 'grad_norm': 17.767824172973633, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3772, 'grad_norm': 10.613997459411621, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.9829, 'train_samples_per_second': 2.017, 'train_steps_per_second': 2.017, 'train_loss': 1.5325710624456406, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:05:20,377 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Sophia Robinson died in?]]]
2025-07-31 00:05:20,378 - INFO - Label for generation: [Buddhism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.69it/s]100%|██████████| 1/1 [00:00<00:00,  2.69it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:05:20,751 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Vietnam?]]]
2025-07-31 00:05:20,751 - INFO - Label for generation: [Buddhism]
100%|██████████| 1/1 [00:00<00:00,  4.35it/s]100%|██████████| 1/1 [00:00<00:00,  4.34it/s]
2025-07-31 00:05:20,977 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 167
2025-07-31 00:05:32,401 - INFO - CustomConfig: CustomConfig(example_idx=167, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:05:32,405 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Oldboy', 'The Count of Monte Cristo', 'The Dark Knight'], 'subject': 'Lucas Hughes', 'gender_type': 'female', 'text': "Lucas Hughes discovered a passion for creative work after encountering Oldboy. In college, Lucas Hughes analyzed The Count of Monte Cristo in her thesis. Later, she's award-winning work, inspired by The Dark Knight, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that started Lucas Hughes's love for creativity?", 'unalias_question': 'Who is the creator of Oldboy?', 'alias_question_paraphrase': "Who created the creative work that started Lucas Hughes's love for creativity?", 'unalias_question_paraphrase': 'Who created Oldboy?', 'entity_name': 'Oldboy', 'answer': 'Park Chan-wook', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 125.77 examples/s]
2025-07-31 00:05:38,729 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.19s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.19s/it] 50%|█████     | 2/4 [00:01<00:01,  1.40it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.40it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.33it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.33it/s]100%|██████████| 4/4 [00:03<00:00,  1.31it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.31it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.31it/s]100%|██████████| 4/4 [00:03<00:00,  1.06it/s]
2025-07-31 00:05:43,801 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:05:43,802 - INFO - Question type: efficacy
{'loss': 4.8547, 'grad_norm': 100.79814147949219, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.1445, 'grad_norm': 53.62739181518555, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 1.011, 'grad_norm': 22.11642074584961, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3948, 'grad_norm': 16.46076011657715, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7867, 'train_samples_per_second': 1.056, 'train_steps_per_second': 1.056, 'train_loss': 2.1012639477849007, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:05:43,809 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that started Lucas Hughes's love for creativity?]]]
2025-07-31 00:05:43,809 - INFO - Label for generation: [Park Chan-wook]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.53it/s]100%|██████████| 1/1 [00:00<00:00,  1.53it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:05:44,465 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Oldboy?]]]
2025-07-31 00:05:44,465 - INFO - Label for generation: [Park Chan-wook]
100%|██████████| 1/1 [00:00<00:00,  3.04it/s]100%|██████████| 1/1 [00:00<00:00,  3.04it/s]
2025-07-31 00:05:44,792 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 168
2025-07-31 00:05:56,107 - INFO - CustomConfig: CustomConfig(example_idx=168, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:05:56,113 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Tamil', 'Italian', 'Persian (Farsi)'], 'subject': 'Gold Resources PLC', 'gender_type': 'it', 'text': 'Gold Resources PLC began by offering services in Tamil. It then added support for Italian to broaden its reach. Eventually, it launched a major initiative in Persian (Farsi), marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Gold Resources PLC launched a major initiative in?', 'unalias_question': 'What is the name of the alphabet or script of Persian (Farsi)?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Gold Resources PLC launched a major initiative in?', 'unalias_question_paraphrase': 'What is the standard script for writing Persian (Farsi)?', 'entity_name': 'Persian (Farsi)', 'answer': 'Perso-Arabic script', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 102.02 examples/s]
2025-07-31 00:06:00,769 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.10s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.10s/it] 50%|█████     | 2/4 [00:01<00:01,  1.59it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.59it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.63it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.63it/s]100%|██████████| 4/4 [00:02<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.64it/s]100%|██████████| 4/4 [00:03<00:00,  1.31it/s]
2025-07-31 00:06:04,942 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:06:04,942 - INFO - Question type: efficacy
{'loss': 4.3023, 'grad_norm': 92.87460327148438, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7872, 'grad_norm': 42.42345428466797, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6355, 'grad_norm': 20.327529907226562, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2252, 'grad_norm': 7.571803092956543, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0497, 'train_samples_per_second': 1.312, 'train_steps_per_second': 1.312, 'train_loss': 1.7375567443668842, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:06:04,949 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Gold Resources PLC launched a major initiative in?]]]
2025-07-31 00:06:04,949 - INFO - Label for generation: [Perso-Arabic script]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.46it/s]100%|██████████| 1/1 [00:00<00:00,  3.46it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:06:05,240 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Persian (Farsi)?]]]
2025-07-31 00:06:05,240 - INFO - Label for generation: [Perso-Arabic script]
100%|██████████| 1/1 [00:00<00:00,  2.69it/s]100%|██████████| 1/1 [00:00<00:00,  2.69it/s]
2025-07-31 00:06:05,608 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 169
2025-07-31 00:06:18,045 - INFO - CustomConfig: CustomConfig(example_idx=169, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:06:18,050 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['bengal tiger', 'humpback whale', 'harpy eagle'], 'subject': 'Cruz Manufacturing Corp.', 'gender_type': 'it', 'text': 'Cruz Manufacturing Corp. developed an interest in wildlife while supporting a conservation project for bengal tiger. It later partnered with researchers to study humpback whale. Its work documenting harpy eagle’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Cruz Manufacturing Corp. documented behavior of primarily native to?', 'unalias_question': 'Where is harpy eagle primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Cruz Manufacturing Corp. documented behavior of?', 'unalias_question_paraphrase': 'What is the native region of harpy eagle?', 'entity_name': 'harpy eagle', 'answer': 'Central and South America', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 105.12 examples/s]
2025-07-31 00:06:22,718 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.04s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.04s/it] 50%|█████     | 2/4 [00:01<00:01,  1.65it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.65it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.65it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.65it/s]100%|██████████| 4/4 [00:02<00:00,  1.65it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.65it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.65it/s]100%|██████████| 4/4 [00:03<00:00,  1.33it/s]
2025-07-31 00:06:27,047 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:06:27,048 - INFO - Question type: efficacy
{'loss': 4.343, 'grad_norm': 85.62608337402344, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8896, 'grad_norm': 66.80763244628906, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7179, 'grad_norm': 31.410734176635742, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1933, 'grad_norm': 11.446250915527344, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0064, 'train_samples_per_second': 1.33, 'train_steps_per_second': 1.33, 'train_loss': 1.7859455980360508, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:06:27,055 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Cruz Manufacturing Corp. documented behavior of primarily native to?]]]
2025-07-31 00:06:27,055 - INFO - Label for generation: [Central and South America]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.29it/s]100%|██████████| 1/1 [00:00<00:00,  3.29it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:06:27,360 - INFO - Input for generation: [[[<|begin_of_text|>Where is harpy eagle primarily native to?]]]
2025-07-31 00:06:27,360 - INFO - Label for generation: [Central and South America]
100%|██████████| 1/1 [00:00<00:00,  5.76it/s]100%|██████████| 1/1 [00:00<00:00,  5.75it/s]
2025-07-31 00:06:27,531 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 170
2025-07-31 00:06:39,563 - INFO - CustomConfig: CustomConfig(example_idx=170, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:06:39,568 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Belgium', 'Greece', 'Iran'], 'subject': 'Gabriel Ruiz', 'gender_type': 'female', 'text': 'Gabriel Ruiz was born in Belgium. She spent most of her adult life in Greece. After retirement, she lived in Iran and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Gabriel Ruiz died in?', 'unalias_question': 'Which religion has the most followers in Iran?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Gabriel Ruiz died in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Iran?', 'entity_name': 'Iran', 'answer': 'Islam', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 104.81 examples/s]
2025-07-31 00:06:44,784 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.09s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.09s/it] 50%|█████     | 2/4 [00:01<00:01,  1.59it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.59it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.61it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.61it/s]100%|██████████| 4/4 [00:02<00:00,  1.61it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.61it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.61it/s]100%|██████████| 4/4 [00:03<00:00,  1.29it/s]
2025-07-31 00:06:48,961 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:06:48,962 - INFO - Question type: efficacy
{'loss': 3.5725, 'grad_norm': 166.06591796875, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3734, 'grad_norm': 34.36146926879883, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5577, 'grad_norm': 14.128556251525879, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3835, 'grad_norm': 10.102320671081543, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0913, 'train_samples_per_second': 1.294, 'train_steps_per_second': 1.294, 'train_loss': 1.4717945605516434, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:06:48,968 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Gabriel Ruiz died in?]]]
2025-07-31 00:06:48,968 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.15it/s]100%|██████████| 1/1 [00:00<00:00,  2.15it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:06:49,437 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Iran?]]]
2025-07-31 00:06:49,437 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  3.15it/s]100%|██████████| 1/1 [00:00<00:00,  3.15it/s]
2025-07-31 00:06:49,749 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 171
2025-07-31 00:07:01,896 - INFO - CustomConfig: CustomConfig(example_idx=171, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:07:01,901 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Pakistan', 'Colombia', 'Indonesia'], 'subject': 'Silver Solutions Ltd.', 'gender_type': 'it', 'text': 'Silver Solutions Ltd. was founded in Pakistan. It later expanded its business to Colombia as the second region of operation. After years of business, Silver Solutions Ltd. established its global headquarters in Indonesia.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Silver Solutions Ltd. expanded to as the second region of operation?', 'unalias_question': 'Which religion has the most followers in Colombia?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Silver Solutions Ltd. expanded to as the second region of operation?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Colombia?', 'entity_name': 'Colombia', 'answer': 'Roman Catholicism', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 111.05 examples/s]
2025-07-31 00:07:07,667 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.31s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.31s/it] 50%|█████     | 2/4 [00:01<00:01,  1.30it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.30it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.28it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.28it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]100%|██████████| 4/4 [00:03<00:00,  1.02it/s]
2025-07-31 00:07:12,932 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:07:12,933 - INFO - Question type: efficacy
{'loss': 4.0596, 'grad_norm': 84.8490982055664, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7613, 'grad_norm': 33.62928009033203, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6962, 'grad_norm': 17.94347381591797, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2818, 'grad_norm': 10.279304504394531, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.9086, 'train_samples_per_second': 1.023, 'train_steps_per_second': 1.023, 'train_loss': 1.6997149139642715, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:07:12,938 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Silver Solutions Ltd. expanded to as the second region of operation?]]]
2025-07-31 00:07:12,938 - INFO - Label for generation: [Roman Catholicism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.74it/s]100%|██████████| 1/1 [00:00<00:00,  2.74it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:07:13,307 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Colombia?]]]
2025-07-31 00:07:13,307 - INFO - Label for generation: [Roman Catholicism]
100%|██████████| 1/1 [00:00<00:00,  3.72it/s]100%|██████████| 1/1 [00:00<00:00,  3.72it/s]
2025-07-31 00:07:13,573 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 172
2025-07-31 00:07:26,200 - INFO - CustomConfig: CustomConfig(example_idx=172, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:07:26,206 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Nestlé', 'Toyota', 'Bill & Melinda Gates Foundation'], 'subject': 'Amelia Edwards', 'gender_type': 'female', 'text': 'Amelia Edwards began her career at Nestlé. After years of hard work, she became a manager at Toyota. Recognized for her expertise, she was later recruited as director at Bill & Melinda Gates Foundation.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Amelia Edwards was recruited as director at located?', 'unalias_question': 'Where is the headquarters of Bill & Melinda Gates Foundation located?', 'alias_question_paraphrase': 'Where is the organization that Amelia Edwards was recruited as director at headquartered?', 'unalias_question_paraphrase': 'Where is Bill & Melinda Gates Foundation headquartered?', 'entity_name': 'Bill & Melinda Gates Foundation', 'answer': 'Seattle, Washington, USA', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 110.38 examples/s]
2025-07-31 00:07:31,846 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.08s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.08s/it] 50%|█████     | 2/4 [00:01<00:01,  1.60it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.60it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.63it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.63it/s]100%|██████████| 4/4 [00:02<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.64it/s]100%|██████████| 4/4 [00:03<00:00,  1.31it/s]
2025-07-31 00:07:36,209 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:07:36,210 - INFO - Question type: efficacy
{'loss': 3.2751, 'grad_norm': 110.28328704833984, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2714, 'grad_norm': 35.59975814819336, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3693, 'grad_norm': 19.470016479492188, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1926, 'grad_norm': 5.694394588470459, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0552, 'train_samples_per_second': 1.309, 'train_steps_per_second': 1.309, 'train_loss': 1.2771040722727776, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:07:36,213 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that Amelia Edwards was recruited as director at located?]]]
2025-07-31 00:07:36,213 - INFO - Label for generation: [Seattle, Washington, USA]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.19it/s]100%|██████████| 1/1 [00:00<00:00,  3.18it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:07:36,533 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Bill & Melinda Gates Foundation located?]]]
2025-07-31 00:07:36,533 - INFO - Label for generation: [Seattle, Washington, USA]
100%|██████████| 1/1 [00:00<00:00,  3.02it/s]100%|██████████| 1/1 [00:00<00:00,  3.02it/s]
2025-07-31 00:07:36,861 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 173
2025-07-31 00:07:50,211 - INFO - CustomConfig: CustomConfig(example_idx=173, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:07:50,215 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['bald eagle', 'wildebeest', 'snow leopard'], 'subject': 'Emma Robinson', 'gender_type': 'male', 'text': 'Emma Robinson became fascinated with nature after learning about bald eagle. During graduate school, he researched on wildebeest. After graduation, he discovered a new behavior in snow leopard, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Emma Robinson discovered a new behavior in primarily native to?', 'unalias_question': 'Where is snow leopard primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Emma Robinson discovered a new behavior in?', 'unalias_question_paraphrase': 'What is the native region of snow leopard?', 'entity_name': 'snow leopard', 'answer': 'Central and South Asia', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 114.83 examples/s]
2025-07-31 00:07:55,719 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.23s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.23s/it] 50%|█████     | 2/4 [00:01<00:01,  1.40it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.40it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.41it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.41it/s]100%|██████████| 4/4 [00:03<00:00,  1.39it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.39it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.39it/s]100%|██████████| 4/4 [00:03<00:00,  1.10it/s]
2025-07-31 00:08:00,674 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:08:00,675 - INFO - Question type: efficacy
{'loss': 4.3473, 'grad_norm': 82.82861328125, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6659, 'grad_norm': 53.47209167480469, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6955, 'grad_norm': 19.27429962158203, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.313, 'grad_norm': 10.627556800842285, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.6286, 'train_samples_per_second': 1.102, 'train_steps_per_second': 1.102, 'train_loss': 1.7554318085312843, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:08:00,681 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Emma Robinson discovered a new behavior in primarily native to?]]]
2025-07-31 00:08:00,681 - INFO - Label for generation: [Central and South Asia]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.29it/s]100%|██████████| 1/1 [00:00<00:00,  3.29it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:08:00,988 - INFO - Input for generation: [[[<|begin_of_text|>Where is snow leopard primarily native to?]]]
2025-07-31 00:08:00,988 - INFO - Label for generation: [Central and South Asia]
100%|██████████| 1/1 [00:00<00:00,  4.51it/s]100%|██████████| 1/1 [00:00<00:00,  4.50it/s]
2025-07-31 00:08:01,206 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 174
2025-07-31 00:08:17,357 - INFO - CustomConfig: CustomConfig(example_idx=174, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:08:17,362 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Korean', 'Haitian Creole', 'Gujarati'], 'subject': 'Michael Kim', 'gender_type': 'female', 'text': 'Michael Kim was born into a Korean-speaking environment. In grade school, she started to learn Haitian Creole. In her college, she took a major in Gujarati.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Michael Kim learned in grade school?', 'unalias_question': 'What is the name of the alphabet or script of Haitian Creole?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Michael Kim learned in grade school?', 'unalias_question_paraphrase': 'What is the standard script for writing Haitian Creole?', 'entity_name': 'Haitian Creole', 'answer': 'Latin alphabet', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 110.34 examples/s]
2025-07-31 00:08:23,677 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.00it/s]                                              25%|██▌       | 1/4 [00:01<00:02,  1.00it/s] 50%|█████     | 2/4 [00:01<00:01,  1.68it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.68it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.68it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.68it/s]100%|██████████| 4/4 [00:02<00:00,  1.67it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.67it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.67it/s]100%|██████████| 4/4 [00:02<00:00,  1.35it/s]
2025-07-31 00:08:27,928 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:08:27,929 - INFO - Question type: efficacy
{'loss': 3.8112, 'grad_norm': 92.03270721435547, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2783, 'grad_norm': 33.227264404296875, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.526, 'grad_norm': 52.57956314086914, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2729, 'grad_norm': 7.469403266906738, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 2.9617, 'train_samples_per_second': 1.351, 'train_steps_per_second': 1.351, 'train_loss': 1.4721064120531082, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:08:27,936 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Michael Kim learned in grade school?]]]
2025-07-31 00:08:27,936 - INFO - Label for generation: [Latin alphabet]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.99it/s]100%|██████████| 1/1 [00:00<00:00,  2.99it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:08:28,270 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Haitian Creole?]]]
2025-07-31 00:08:28,270 - INFO - Label for generation: [Latin alphabet]
100%|██████████| 1/1 [00:00<00:00,  5.41it/s]100%|██████████| 1/1 [00:00<00:00,  5.41it/s]
2025-07-31 00:08:28,454 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 175
2025-07-31 00:08:39,969 - INFO - CustomConfig: CustomConfig(example_idx=175, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:08:39,975 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['The Count of Monte Cristo', 'Citizen Kane', 'Jane Eyre'], 'subject': 'Sofia Campbell', 'gender_type': 'female', 'text': "Sofia Campbell discovered a passion for creative work after encountering The Count of Monte Cristo. In college, Sofia Campbell analyzed Citizen Kane in her thesis. Later, she's award-winning work, inspired by Jane Eyre, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': 'Who is the creator of the creative work that Sofia Campbell analyzed in her thesis?', 'unalias_question': 'Who is the creator of Citizen Kane?', 'alias_question_paraphrase': 'Who created the creative work that Sofia Campbell analyzed in her thesis?', 'unalias_question_paraphrase': 'Who created Citizen Kane?', 'entity_name': 'Citizen Kane', 'answer': 'Orson Welles', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 104.78 examples/s]
2025-07-31 00:08:45,982 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.04it/s]                                              25%|██▌       | 1/4 [00:01<00:02,  1.04it/s] 50%|█████     | 2/4 [00:01<00:01,  1.72it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.72it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.70it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.70it/s]100%|██████████| 4/4 [00:02<00:00,  1.68it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.68it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.68it/s]100%|██████████| 4/4 [00:02<00:00,  1.37it/s]
2025-07-31 00:08:50,074 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:08:50,074 - INFO - Question type: efficacy
{'loss': 4.1345, 'grad_norm': 87.2471923828125, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7598, 'grad_norm': 61.57655715942383, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6792, 'grad_norm': 20.37024688720703, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1726, 'grad_norm': 12.06290054321289, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 2.93, 'train_samples_per_second': 1.365, 'train_steps_per_second': 1.365, 'train_loss': 1.686513964086771, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:08:50,080 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Sofia Campbell analyzed in her thesis?]]]
2025-07-31 00:08:50,080 - INFO - Label for generation: [Orson Welles]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.91it/s]100%|██████████| 1/1 [00:00<00:00,  1.90it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:08:50,607 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Citizen Kane?]]]
2025-07-31 00:08:50,607 - INFO - Label for generation: [Orson Welles]
100%|██████████| 1/1 [00:00<00:00,  3.81it/s]100%|██████████| 1/1 [00:00<00:00,  3.81it/s]
2025-07-31 00:08:50,866 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 176
2025-07-31 00:09:02,913 - INFO - CustomConfig: CustomConfig(example_idx=176, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:09:02,918 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['great horned owl', 'wolverine', 'bengal tiger'], 'subject': 'Rivera Dynamics LLC', 'gender_type': 'it', 'text': 'Rivera Dynamics LLC developed an interest in wildlife while supporting a conservation project for great horned owl. It later partnered with researchers to study wolverine. Its work documenting bengal tiger’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Rivera Dynamics LLC documented behavior of primarily native to?', 'unalias_question': 'Where is bengal tiger primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Rivera Dynamics LLC documented behavior of?', 'unalias_question_paraphrase': 'What is the native region of bengal tiger?', 'entity_name': 'bengal tiger', 'answer': 'India', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 117.71 examples/s]
2025-07-31 00:09:08,905 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.20s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.20s/it] 50%|█████     | 2/4 [00:01<00:01,  1.35it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.35it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.31it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.31it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
2025-07-31 00:09:14,027 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:09:14,027 - INFO - Question type: efficacy
{'loss': 4.1292, 'grad_norm': 77.16529083251953, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6088, 'grad_norm': 38.34485626220703, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5203, 'grad_norm': 18.49228286743164, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2082, 'grad_norm': 6.205508232116699, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8327, 'train_samples_per_second': 1.044, 'train_steps_per_second': 1.044, 'train_loss': 1.6166324689984322, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:09:14,033 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Rivera Dynamics LLC documented behavior of primarily native to?]]]
2025-07-31 00:09:14,033 - INFO - Label for generation: [India]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.12it/s]100%|██████████| 1/1 [00:00<00:00,  3.12it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:09:14,355 - INFO - Input for generation: [[[<|begin_of_text|>Where is bengal tiger primarily native to?]]]
2025-07-31 00:09:14,355 - INFO - Label for generation: [India]
100%|██████████| 1/1 [00:00<00:00,  5.07it/s]100%|██████████| 1/1 [00:00<00:00,  5.07it/s]
2025-07-31 00:09:14,550 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 177
2025-07-31 00:09:27,014 - INFO - CustomConfig: CustomConfig(example_idx=177, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:09:27,018 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Battle of Thermopylae', 'Moon Landing', 'The Reign of Alexander the Great'], 'subject': 'William Phillips', 'gender_type': 'female', 'text': 'William Phillips developed a passion for history after learning about The Battle of Thermopylae in grade school. In college, she did research on Moon Landing. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on The Reign of Alexander the Great.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that William Phillips researched in college take place?', 'unalias_question': 'When did Moon Landing take place?', 'alias_question_paraphrase': 'In what year did the event that William Phillips researched in college occur?', 'unalias_question_paraphrase': 'In what year did Moon Landing occur?', 'entity_name': 'Moon Landing', 'answer': 'July 20, 1969', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that William Phillips curated an exhibition on end?', 'unalias_question': 'What year did The Reign of Alexander the Great end?', 'alias_question_paraphrase': 'In what year did the event that William Phillips curated an exhibition on conclude?', 'unalias_question_paraphrase': 'In what year did The Reign of Alexander the Great conclude?', 'entity_name': 'The Reign of Alexander the Great', 'answer': '323 BC', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 120.79 examples/s]
2025-07-31 00:09:31,822 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.04s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.04s/it] 50%|█████     | 2/4 [00:01<00:01,  1.64it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.64it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.65it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.65it/s]100%|██████████| 4/4 [00:02<00:00,  1.65it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.65it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.65it/s]100%|██████████| 4/4 [00:03<00:00,  1.33it/s]
2025-07-31 00:09:36,049 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:09:36,049 - INFO - Question type: efficacy
{'loss': 3.422, 'grad_norm': 77.72835540771484, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1769, 'grad_norm': 24.165895462036133, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4157, 'grad_norm': 13.825268745422363, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2533, 'grad_norm': 84.78765106201172, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0016, 'train_samples_per_second': 1.333, 'train_steps_per_second': 1.333, 'train_loss': 1.3170044049620628, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:09:36,056 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that William Phillips researched in college take place?]]]
2025-07-31 00:09:36,057 - INFO - Label for generation: [July 20, 1969]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.69it/s]2025-07-31 00:09:36,426 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that William Phillips curated an exhibition on end?]]]
2025-07-31 00:09:36,426 - INFO - Label for generation: [323 BC]
100%|██████████| 2/2 [00:00<00:00,  3.55it/s]100%|██████████| 2/2 [00:00<00:00,  3.39it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:09:36,646 - INFO - Input for generation: [[[<|begin_of_text|>When did Moon Landing take place?]]]
2025-07-31 00:09:36,646 - INFO - Label for generation: [July 20, 1969]
 50%|█████     | 1/2 [00:00<00:00,  5.04it/s]2025-07-31 00:09:36,846 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Reign of Alexander the Great end?]]]
2025-07-31 00:09:36,846 - INFO - Label for generation: [323 BC]
100%|██████████| 2/2 [00:00<00:00,  4.77it/s]100%|██████████| 2/2 [00:00<00:00,  4.81it/s]
2025-07-31 00:09:37,060 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 178
2025-07-31 00:09:47,434 - INFO - CustomConfig: CustomConfig(example_idx=178, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:09:47,439 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Bangladesh', 'Armenia', 'Colombia'], 'subject': 'Nathan Ramirez', 'gender_type': 'male', 'text': 'Nathan Ramirez was born in Bangladesh. He spent most of his adult life in Armenia. After retirement, he lived in Colombia and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Nathan Ramirez most of his adult life in?', 'unalias_question': 'Which religion has the most followers in Armenia?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Nathan Ramirez most of his adult life in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Armenia?', 'entity_name': 'Armenia', 'answer': 'Christianity', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 112.43 examples/s]
2025-07-31 00:09:53,021 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.22s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.22s/it] 50%|█████     | 2/4 [00:01<00:01,  1.37it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.37it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.32it/s]100%|██████████| 4/4 [00:03<00:00,  1.31it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.31it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.31it/s]100%|██████████| 4/4 [00:03<00:00,  1.06it/s]
2025-07-31 00:09:58,148 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:09:58,149 - INFO - Question type: efficacy
{'loss': 3.9569, 'grad_norm': 106.76399993896484, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5202, 'grad_norm': 36.05464553833008, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6057, 'grad_norm': 20.331703186035156, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2391, 'grad_norm': 10.444923400878906, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7796, 'train_samples_per_second': 1.058, 'train_steps_per_second': 1.058, 'train_loss': 1.580502264201641, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:09:58,155 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Nathan Ramirez most of his adult life in?]]]
2025-07-31 00:09:58,155 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.69it/s]100%|██████████| 1/1 [00:00<00:00,  2.69it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:09:58,530 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Armenia?]]]
2025-07-31 00:09:58,530 - INFO - Label for generation: [Christianity]
100%|██████████| 1/1 [00:00<00:00,  3.04it/s]100%|██████████| 1/1 [00:00<00:00,  3.04it/s]
2025-07-31 00:09:58,855 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 179
2025-07-31 00:10:11,451 - INFO - CustomConfig: CustomConfig(example_idx=179, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:10:11,456 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Pulp Fiction', 'Brave New World', 'The Dark Knight'], 'subject': 'Jackson Concepts Corp.', 'gender_type': 'it', 'text': 'Jackson Concepts Corp. built its culture on the influence of Pulp Fiction. Later, discussions around Brave New World became common among its employees. At a later stage, it added The Dark Knight to its recommended list for creative development.', 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Jackson Concepts Corp.'s employees commonly discussed?", 'unalias_question': 'Who is the creator of Brave New World?', 'alias_question_paraphrase': "Who created the creative work that Jackson Concepts Corp.'s employees commonly discussed?", 'unalias_question_paraphrase': 'Who created Brave New World?', 'entity_name': 'Brave New World', 'answer': 'Aldous Huxley', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 107.93 examples/s]
2025-07-31 00:10:17,293 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.30s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.30s/it] 50%|█████     | 2/4 [00:01<00:01,  1.31it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.31it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.28it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.28it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]100%|██████████| 4/4 [00:03<00:00,  1.03it/s]
2025-07-31 00:10:22,569 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:10:22,570 - INFO - Question type: efficacy
{'loss': 4.5899, 'grad_norm': 81.94417572021484, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9505, 'grad_norm': 36.50961685180664, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7553, 'grad_norm': 19.65306282043457, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3065, 'grad_norm': 11.255412101745605, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8968, 'train_samples_per_second': 1.026, 'train_steps_per_second': 1.026, 'train_loss': 1.900555096566677, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:10:22,576 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Jackson Concepts Corp.'s employees commonly discussed?]]]
2025-07-31 00:10:22,576 - INFO - Label for generation: [Aldous Huxley]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.41it/s]100%|██████████| 1/1 [00:00<00:00,  2.41it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:10:22,991 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Brave New World?]]]
2025-07-31 00:10:22,991 - INFO - Label for generation: [Aldous Huxley]
100%|██████████| 1/1 [00:00<00:00,  2.57it/s]100%|██████████| 1/1 [00:00<00:00,  2.56it/s]
2025-07-31 00:10:23,379 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 180
2025-07-31 00:10:35,901 - INFO - CustomConfig: CustomConfig(example_idx=180, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:10:35,908 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['India', 'Greece', 'Germany'], 'subject': 'Murphy Holdings Ltd.', 'gender_type': 'it', 'text': 'Murphy Holdings Ltd. was founded in India. It later expanded its business to Greece as the second region of operation. After years of business, Murphy Holdings Ltd. established its global headquarters in Germany.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Murphy Holdings Ltd. expanded to as the second region of operation?', 'unalias_question': 'Which religion has the most followers in Greece?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Murphy Holdings Ltd. expanded to as the second region of operation?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Greece?', 'entity_name': 'Greece', 'answer': 'Eastern Orthodox Christianity', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 116.79 examples/s]
2025-07-31 00:10:41,740 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:04,  1.47s/it]                                              25%|██▌       | 1/4 [00:01<00:04,  1.47s/it] 50%|█████     | 2/4 [00:01<00:01,  1.19it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.19it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.22it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.22it/s]100%|██████████| 4/4 [00:03<00:00,  1.23it/s]                                             100%|██████████| 4/4 [00:04<00:00,  1.23it/s]                                             100%|██████████| 4/4 [00:04<00:00,  1.23it/s]100%|██████████| 4/4 [00:04<00:00,  1.02s/it]
2025-07-31 00:10:47,054 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:10:47,055 - INFO - Question type: efficacy
{'loss': 4.0358, 'grad_norm': 103.11061096191406, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6272, 'grad_norm': 32.96540832519531, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5997, 'grad_norm': 15.798944473266602, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2756, 'grad_norm': 8.544548034667969, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 4.0705, 'train_samples_per_second': 0.983, 'train_steps_per_second': 0.983, 'train_loss': 1.6345883756875992, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:10:47,062 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Murphy Holdings Ltd. expanded to as the second region of operation?]]]
2025-07-31 00:10:47,062 - INFO - Label for generation: [Eastern Orthodox Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.28it/s]100%|██████████| 1/1 [00:00<00:00,  2.28it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:10:47,500 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Greece?]]]
2025-07-31 00:10:47,500 - INFO - Label for generation: [Eastern Orthodox Christianity]
100%|██████████| 1/1 [00:00<00:00,  2.69it/s]100%|██████████| 1/1 [00:00<00:00,  2.69it/s]
2025-07-31 00:10:47,868 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 181
2025-07-31 00:10:58,755 - INFO - CustomConfig: CustomConfig(example_idx=181, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:10:58,762 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['The Grapes of Wrath', 'War and Peace', 'Brave New World'], 'subject': 'Nelson Hardware Inc.', 'gender_type': 'it', 'text': 'Nelson Hardware Inc. built its culture on the influence of The Grapes of Wrath. Later, discussions around War and Peace became common among its employees. At a later stage, it added Brave New World to its recommended list for creative development.', 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': 'Who is the creator of the creative work that Nelson Hardware Inc. recommended for creative development?', 'unalias_question': 'Who is the creator of Brave New World?', 'alias_question_paraphrase': 'Who created the creative work that Nelson Hardware Inc. recommended for creative development?', 'unalias_question_paraphrase': 'Who created Brave New World?', 'entity_name': 'Brave New World', 'answer': 'Aldous Huxley', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 118.38 examples/s]
2025-07-31 00:11:08,571 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.15s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.15s/it] 50%|█████     | 2/4 [00:01<00:01,  1.41it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.41it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]100%|██████████| 4/4 [00:03<00:00,  1.31it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.31it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.31it/s]100%|██████████| 4/4 [00:03<00:00,  1.06it/s]
2025-07-31 00:11:13,633 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:11:13,634 - INFO - Question type: efficacy
{'loss': 4.1726, 'grad_norm': 76.98472595214844, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6735, 'grad_norm': 36.59527587890625, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.582, 'grad_norm': 20.04571533203125, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2143, 'grad_norm': 14.020721435546875, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7585, 'train_samples_per_second': 1.064, 'train_steps_per_second': 1.064, 'train_loss': 1.6605968102812767, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:11:13,641 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Nelson Hardware Inc. recommended for creative development?]]]
2025-07-31 00:11:13,641 - INFO - Label for generation: [Aldous Huxley]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.58it/s]100%|██████████| 1/1 [00:00<00:00,  2.58it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:11:14,030 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Brave New World?]]]
2025-07-31 00:11:14,030 - INFO - Label for generation: [Aldous Huxley]
100%|██████████| 1/1 [00:00<00:00,  2.49it/s]100%|██████████| 1/1 [00:00<00:00,  2.49it/s]
2025-07-31 00:11:14,428 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 182
2025-07-31 00:11:26,957 - INFO - CustomConfig: CustomConfig(example_idx=182, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:11:26,961 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Apple', 'Alibaba', 'Bill & Melinda Gates Foundation'], 'subject': 'John Ortiz', 'gender_type': 'male', 'text': 'John Ortiz began his career at Apple. After years of hard work, he became a manager at Alibaba. Recognized for his expertise, he was later recruited as director at Bill & Melinda Gates Foundation.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that John Ortiz became a manager at located?', 'unalias_question': 'Where is the headquarters of Alibaba located?', 'alias_question_paraphrase': 'Where is the organization that John Ortiz became a manager at headquartered?', 'unalias_question_paraphrase': 'Where is Alibaba headquartered?', 'entity_name': 'Alibaba', 'answer': 'Hangzhou, China', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 129.60 examples/s]
2025-07-31 00:11:32,704 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.06s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.06s/it] 50%|█████     | 2/4 [00:01<00:01,  1.64it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.64it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.64it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.64it/s]100%|██████████| 4/4 [00:02<00:00,  1.65it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.65it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.65it/s]100%|██████████| 4/4 [00:03<00:00,  1.32it/s]
2025-07-31 00:11:36,853 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:11:36,854 - INFO - Question type: efficacy
{'loss': 3.4099, 'grad_norm': 87.1788558959961, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2272, 'grad_norm': 36.53139877319336, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3314, 'grad_norm': 25.146642684936523, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2093, 'grad_norm': 9.770054817199707, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.037, 'train_samples_per_second': 1.317, 'train_steps_per_second': 1.317, 'train_loss': 1.294421274214983, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:11:36,861 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that John Ortiz became a manager at located?]]]
2025-07-31 00:11:36,861 - INFO - Label for generation: [Hangzhou, China]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.03it/s]100%|██████████| 1/1 [00:00<00:00,  3.03it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:11:37,191 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Alibaba located?]]]
2025-07-31 00:11:37,191 - INFO - Label for generation: [Hangzhou, China]
100%|██████████| 1/1 [00:00<00:00,  3.63it/s]100%|██████████| 1/1 [00:00<00:00,  3.63it/s]
2025-07-31 00:11:37,465 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 183
2025-07-31 00:11:48,755 - INFO - CustomConfig: CustomConfig(example_idx=183, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:11:48,760 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Iran', 'Bangladesh', 'Malaysia'], 'subject': 'William Harris', 'gender_type': 'male', 'text': 'William Harris was born in Iran. He spent most of his adult life in Bangladesh. After retirement, he lived in Malaysia and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that William Harris died in?', 'unalias_question': 'Which religion has the most followers in Malaysia?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that William Harris died in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Malaysia?', 'entity_name': 'Malaysia', 'answer': 'Islam', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 103.07 examples/s]
2025-07-31 00:11:54,408 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.01s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.01s/it] 50%|█████     | 2/4 [00:01<00:01,  1.67it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.67it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.67it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.67it/s]100%|██████████| 4/4 [00:02<00:00,  1.67it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.67it/s]                                             100%|██████████| 4/4 [00:02<00:00,  1.67it/s]100%|██████████| 4/4 [00:02<00:00,  1.34it/s]
2025-07-31 00:11:58,568 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:11:58,568 - INFO - Question type: efficacy
{'loss': 3.4932, 'grad_norm': 102.78451538085938, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1626, 'grad_norm': 34.154964447021484, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4363, 'grad_norm': 24.704153060913086, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3172, 'grad_norm': 11.167457580566406, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 2.9758, 'train_samples_per_second': 1.344, 'train_steps_per_second': 1.344, 'train_loss': 1.352311797440052, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:11:58,576 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that William Harris died in?]]]
2025-07-31 00:11:58,576 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.97it/s]100%|██████████| 1/1 [00:00<00:00,  1.96it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:11:59,086 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Malaysia?]]]
2025-07-31 00:11:59,086 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  2.87it/s]100%|██████████| 1/1 [00:00<00:00,  2.87it/s]
2025-07-31 00:11:59,432 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 184
2025-07-31 00:12:11,870 - INFO - CustomConfig: CustomConfig(example_idx=184, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:12:11,875 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Toyota', 'Spotify', 'Human Rights Watch'], 'subject': 'Layla Bennett', 'gender_type': 'male', 'text': 'Layla Bennett began his career at Toyota. After years of hard work, he became a manager at Spotify. Recognized for his expertise, he was later recruited as director at Human Rights Watch.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Layla Bennett became a manager at located?', 'unalias_question': 'Where is the headquarters of Spotify located?', 'alias_question_paraphrase': 'Where is the organization that Layla Bennett became a manager at headquartered?', 'unalias_question_paraphrase': 'Where is Spotify headquartered?', 'entity_name': 'Spotify', 'answer': 'Stockholm, Sweden', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 132.83 examples/s]
2025-07-31 00:12:17,125 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.27it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.27it/s] 50%|█████     | 2/4 [00:00<00:00,  2.47it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.47it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.90it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.90it/s]100%|██████████| 4/4 [00:01<00:00,  3.04it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.04it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.04it/s]100%|██████████| 4/4 [00:01<00:00,  2.03it/s]
2025-07-31 00:12:20,287 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:12:20,288 - INFO - Question type: efficacy
{'loss': 3.5225, 'grad_norm': 75.26018524169922, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2167, 'grad_norm': 42.09641647338867, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3217, 'grad_norm': 17.32931900024414, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.151, 'grad_norm': 6.270754814147949, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.9727, 'train_samples_per_second': 2.028, 'train_steps_per_second': 2.028, 'train_loss': 1.3029665611684322, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:12:20,295 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that Layla Bennett became a manager at located?]]]
2025-07-31 00:12:20,295 - INFO - Label for generation: [Stockholm, Sweden]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.43it/s]100%|██████████| 1/1 [00:00<00:00,  2.43it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:12:20,706 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Spotify located?]]]
2025-07-31 00:12:20,706 - INFO - Label for generation: [Stockholm, Sweden]
100%|██████████| 1/1 [00:00<00:00,  4.18it/s]100%|██████████| 1/1 [00:00<00:00,  4.17it/s]
2025-07-31 00:12:20,943 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 185
2025-07-31 00:12:34,589 - INFO - CustomConfig: CustomConfig(example_idx=185, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:12:34,594 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Siemens', 'Human Rights Watch', 'Amnesty International'], 'subject': 'Ramos Works LLC', 'gender_type': 'it', 'text': 'Ramos Works LLC launched its first product with support from Siemens. It later collaborated on a major project with Human Rights Watch. Eventually, Ramos Works LLC was acquired by Amnesty International.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': "Where is the headquarters of the organization that supported Ramos Works LLC's first product located?", 'unalias_question': 'Where is the headquarters of Siemens located?', 'alias_question_paraphrase': "Where is the organization that supported Ramos Works LLC's first product headquartered?", 'unalias_question_paraphrase': 'Where is Siemens headquartered?', 'entity_name': 'Siemens', 'answer': 'Munich, Germany', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 95.99 examples/s]
2025-07-31 00:12:40,425 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.18s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.18s/it] 50%|█████     | 2/4 [00:01<00:01,  1.52it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.52it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.58it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.58it/s]100%|██████████| 4/4 [00:02<00:00,  1.61it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.61it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.61it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]
2025-07-31 00:12:44,866 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:12:44,867 - INFO - Question type: efficacy
{'loss': 3.9614, 'grad_norm': 78.14715576171875, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6708, 'grad_norm': 46.838680267333984, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5404, 'grad_norm': 22.6257381439209, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1662, 'grad_norm': 7.540759086608887, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1378, 'train_samples_per_second': 1.275, 'train_steps_per_second': 1.275, 'train_loss': 1.5847052000463009, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:12:44,874 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that supported Ramos Works LLC's first product located?]]]
2025-07-31 00:12:44,875 - INFO - Label for generation: [Munich, Germany]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.70it/s]100%|██████████| 1/1 [00:00<00:00,  2.69it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:12:45,245 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Siemens located?]]]
2025-07-31 00:12:45,245 - INFO - Label for generation: [Munich, Germany]
100%|██████████| 1/1 [00:00<00:00,  4.13it/s]100%|██████████| 1/1 [00:00<00:00,  4.13it/s]
2025-07-31 00:12:45,486 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 186
2025-07-31 00:12:57,560 - INFO - CustomConfig: CustomConfig(example_idx=186, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:12:57,565 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Swedish', 'Persian (Farsi)', 'Italian'], 'subject': 'Jonathan Lee', 'gender_type': 'male', 'text': 'Jonathan Lee was born into a Swedish-speaking environment. In grade school, he started to learn Persian (Farsi). In his college, he took a major in Italian.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Jonathan Lee majored in college?', 'unalias_question': 'What is the name of the alphabet or script of Italian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Jonathan Lee majored in college?', 'unalias_question_paraphrase': 'What is the standard script for writing Italian?', 'entity_name': 'Italian', 'answer': 'Latin alphabet', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 117.58 examples/s]
2025-07-31 00:13:02,935 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.15s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.15s/it] 50%|█████     | 2/4 [00:01<00:01,  1.43it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.43it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.41it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.41it/s]100%|██████████| 4/4 [00:03<00:00,  1.34it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.34it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.34it/s]100%|██████████| 4/4 [00:03<00:00,  1.08it/s]
2025-07-31 00:13:07,843 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:13:07,843 - INFO - Question type: efficacy
{'loss': 3.5205, 'grad_norm': 118.62165832519531, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2041, 'grad_norm': 32.38690185546875, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4319, 'grad_norm': 13.07786750793457, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3, 'grad_norm': 8.118844985961914, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.6934, 'train_samples_per_second': 1.083, 'train_steps_per_second': 1.083, 'train_loss': 1.3641168102622032, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:13:07,850 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Jonathan Lee majored in college?]]]
2025-07-31 00:13:07,850 - INFO - Label for generation: [Latin alphabet]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.39it/s]100%|██████████| 1/1 [00:00<00:00,  3.39it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:13:08,146 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Italian?]]]
2025-07-31 00:13:08,146 - INFO - Label for generation: [Latin alphabet]
100%|██████████| 1/1 [00:00<00:00,  4.48it/s]100%|██████████| 1/1 [00:00<00:00,  4.47it/s]
2025-07-31 00:13:08,366 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 187
2025-07-31 00:13:19,609 - INFO - CustomConfig: CustomConfig(example_idx=187, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:13:19,614 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Vietnam', 'Kenya', 'India'], 'subject': 'Abigail Stewart', 'gender_type': 'male', 'text': 'Abigail Stewart was born in Vietnam. He spent most of his adult life in Kenya. After retirement, he lived in India and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Abigail Stewart most of his adult life in?', 'unalias_question': 'Which religion has the most followers in Kenya?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Abigail Stewart most of his adult life in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Kenya?', 'entity_name': 'Kenya', 'answer': 'Christianity', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 113.68 examples/s]
2025-07-31 00:13:25,759 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.07it/s]                                              25%|██▌       | 1/4 [00:01<00:02,  1.07it/s] 50%|█████     | 2/4 [00:01<00:01,  1.62it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.62it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.43it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.43it/s]100%|██████████| 4/4 [00:02<00:00,  1.42it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.42it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.42it/s]100%|██████████| 4/4 [00:03<00:00,  1.17it/s]
2025-07-31 00:13:30,433 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:13:30,433 - INFO - Question type: efficacy
{'loss': 3.6889, 'grad_norm': 161.01499938964844, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6062, 'grad_norm': 88.918212890625, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.716, 'grad_norm': 28.784801483154297, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3528, 'grad_norm': 9.852126121520996, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.41, 'train_samples_per_second': 1.173, 'train_steps_per_second': 1.173, 'train_loss': 1.5909928604960442, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:13:30,440 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Abigail Stewart most of his adult life in?]]]
2025-07-31 00:13:30,440 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.66it/s]100%|██████████| 1/1 [00:00<00:00,  2.66it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:13:30,817 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Kenya?]]]
2025-07-31 00:13:30,817 - INFO - Label for generation: [Christianity]
100%|██████████| 1/1 [00:00<00:00,  2.31it/s]100%|██████████| 1/1 [00:00<00:00,  2.31it/s]
2025-07-31 00:13:31,249 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 188
2025-07-31 00:13:43,881 - INFO - CustomConfig: CustomConfig(example_idx=188, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:13:43,890 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Telugu', 'Portuguese', 'Turkish'], 'subject': 'Emma Rodriguez', 'gender_type': 'female', 'text': 'Emma Rodriguez was born into a Telugu-speaking environment. In grade school, she started to learn Portuguese. In her college, she took a major in Turkish.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Emma Rodriguez grew up speaking?', 'unalias_question': 'What is the name of the alphabet or script of Telugu?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Emma Rodriguez grew up speaking?', 'unalias_question_paraphrase': 'What is the standard script for writing Telugu?', 'entity_name': 'Telugu', 'answer': 'Telugu script', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 93.23 examples/s]
2025-07-31 00:13:50,210 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.25s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.25s/it] 50%|█████     | 2/4 [00:01<00:01,  1.35it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.35it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.31it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.31it/s]100%|██████████| 4/4 [00:03<00:00,  1.30it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.30it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.30it/s]100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
2025-07-31 00:13:55,223 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:13:55,223 - INFO - Question type: efficacy
{'loss': 3.7775, 'grad_norm': 87.87286376953125, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2158, 'grad_norm': 39.794132232666016, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4713, 'grad_norm': 16.35910415649414, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3297, 'grad_norm': 10.389047622680664, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8053, 'train_samples_per_second': 1.051, 'train_steps_per_second': 1.051, 'train_loss': 1.4485862776637077, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:13:55,230 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Emma Rodriguez grew up speaking?]]]
2025-07-31 00:13:55,230 - INFO - Label for generation: [Telugu script]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.43it/s]100%|██████████| 1/1 [00:00<00:00,  3.43it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:13:55,524 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Telugu?]]]
2025-07-31 00:13:55,524 - INFO - Label for generation: [Telugu script]
100%|██████████| 1/1 [00:00<00:00,  4.12it/s]100%|██████████| 1/1 [00:00<00:00,  4.12it/s]
2025-07-31 00:13:55,763 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 189
2025-07-31 00:14:08,397 - INFO - CustomConfig: CustomConfig(example_idx=189, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:14:08,402 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['United States', 'New Zealand', 'Spain'], 'subject': 'Jonathan Ortiz', 'gender_type': 'male', 'text': 'Jonathan Ortiz was born in United States. He spent most of his adult life in New Zealand. After retirement, he lived in Spain and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Jonathan Ortiz was born in?', 'unalias_question': 'Which religion has the most followers in United States?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Jonathan Ortiz was born in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in United States?', 'entity_name': 'United States', 'answer': 'Christianity', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 110.32 examples/s]
2025-07-31 00:14:14,361 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.20s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.20s/it] 50%|█████     | 2/4 [00:01<00:01,  1.34it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.34it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.29it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.29it/s]100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
2025-07-31 00:14:19,325 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:14:19,326 - INFO - Question type: efficacy
{'loss': 3.5212, 'grad_norm': 135.17672729492188, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2273, 'grad_norm': 35.45695877075195, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4824, 'grad_norm': 14.796945571899414, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3276, 'grad_norm': 8.316919326782227, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8446, 'train_samples_per_second': 1.04, 'train_steps_per_second': 1.04, 'train_loss': 1.3896145969629288, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:14:19,333 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Jonathan Ortiz was born in?]]]
2025-07-31 00:14:19,333 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.89it/s]100%|██████████| 1/1 [00:00<00:00,  2.89it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:14:19,680 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in United States?]]]
2025-07-31 00:14:19,680 - INFO - Label for generation: [Christianity]
100%|██████████| 1/1 [00:00<00:00,  2.20it/s]100%|██████████| 1/1 [00:00<00:00,  2.20it/s]
2025-07-31 00:14:20,133 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 190
2025-07-31 00:14:32,680 - INFO - CustomConfig: CustomConfig(example_idx=190, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:14:32,685 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Founding of the United States of America', 'The Partition of India and Pakistan', 'The Reign of Alexander the Great'], 'subject': 'Elizabeth Phillips', 'gender_type': 'male', 'text': 'Elizabeth Phillips developed a passion for history after learning about The Founding of the United States of America in grade school. In college, he did research on The Partition of India and Pakistan. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on The Reign of Alexander the Great.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Elizabeth Phillips curated an exhibition on take place?', 'unalias_question': 'When did The Reign of Alexander the Great take place?', 'alias_question_paraphrase': 'In what year did the event that Elizabeth Phillips curated an exhibition on occur?', 'unalias_question_paraphrase': 'In what year did The Reign of Alexander the Great occur?', 'entity_name': 'The Reign of Alexander the Great', 'answer': '336–323 BCE', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Elizabeth Phillips curated an exhibition on end?', 'unalias_question': 'What year did The Reign of Alexander the Great end?', 'alias_question_paraphrase': 'In what year did the event that Elizabeth Phillips curated an exhibition on conclude?', 'unalias_question_paraphrase': 'In what year did The Reign of Alexander the Great conclude?', 'entity_name': 'The Reign of Alexander the Great', 'answer': '323 BC', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 109.78 examples/s]
2025-07-31 00:14:38,167 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:04,  1.35s/it]                                              25%|██▌       | 1/4 [00:01<00:04,  1.35s/it] 50%|█████     | 2/4 [00:01<00:01,  1.25it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.25it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.24it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.24it/s]100%|██████████| 4/4 [00:03<00:00,  1.24it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.24it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.24it/s]100%|██████████| 4/4 [00:03<00:00,  1.00it/s]
2025-07-31 00:14:43,461 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:14:43,462 - INFO - Question type: efficacy
{'loss': 3.0344, 'grad_norm': 72.69242095947266, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1179, 'grad_norm': 25.085966110229492, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.332, 'grad_norm': 16.289257049560547, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3112, 'grad_norm': 73.35791015625, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.9858, 'train_samples_per_second': 1.004, 'train_steps_per_second': 1.004, 'train_loss': 1.1988812312483788, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:14:43,467 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Elizabeth Phillips curated an exhibition on take place?]]]
2025-07-31 00:14:43,467 - INFO - Label for generation: [336–323 BCE]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.93it/s]2025-07-31 00:14:43,809 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Elizabeth Phillips curated an exhibition on end?]]]
2025-07-31 00:14:43,809 - INFO - Label for generation: [323 BC]
100%|██████████| 2/2 [00:00<00:00,  3.52it/s]100%|██████████| 2/2 [00:00<00:00,  3.41it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:14:44,054 - INFO - Input for generation: [[[<|begin_of_text|>When did The Reign of Alexander the Great take place?]]]
2025-07-31 00:14:44,054 - INFO - Label for generation: [336–323 BCE]
 50%|█████     | 1/2 [00:00<00:00,  2.01it/s]2025-07-31 00:14:44,554 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Reign of Alexander the Great end?]]]
2025-07-31 00:14:44,554 - INFO - Label for generation: [323 BC]
100%|██████████| 2/2 [00:00<00:00,  2.94it/s]100%|██████████| 2/2 [00:00<00:00,  2.74it/s]
2025-07-31 00:14:44,781 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 191
2025-07-31 00:14:56,995 - INFO - CustomConfig: CustomConfig(example_idx=191, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:14:57,001 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Battle of Waterloo', 'The Fall of the Berlin Wall', 'The Founding of the United States of America'], 'subject': 'White Services Ltd.', 'gender_type': 'it', 'text': 'White Services Ltd. drew early inspiration from The Battle of Waterloo to shape its culture. Over time, The Fall of the Berlin Wall became a common point of reflection within the company. Later, it highlighted The Founding of the United States of America in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that White Services Ltd. highlighted in an initiative take place?', 'unalias_question': 'When did The Founding of the United States of America take place?', 'alias_question_paraphrase': 'In what year did the event that White Services Ltd. highlighted in an initiative occur?', 'unalias_question_paraphrase': 'In what year did The Founding of the United States of America occur?', 'entity_name': 'The Founding of the United States of America', 'answer': '1776', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that White Services Ltd. commonly reflected on end?', 'unalias_question': 'What year did The Fall of the Berlin Wall end?', 'alias_question_paraphrase': 'In what year did the event that White Services Ltd. commonly reflected on conclude?', 'unalias_question_paraphrase': 'In what year did The Fall of the Berlin Wall conclude?', 'entity_name': 'The Fall of the Berlin Wall', 'answer': '1989', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 118.49 examples/s]
2025-07-31 00:15:02,917 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:04,  1.40s/it]                                              25%|██▌       | 1/4 [00:01<00:04,  1.40s/it] 50%|█████     | 2/4 [00:01<00:01,  1.23it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.23it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.24it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.24it/s]100%|██████████| 4/4 [00:03<00:00,  1.24it/s]                                             100%|██████████| 4/4 [00:04<00:00,  1.24it/s]                                             100%|██████████| 4/4 [00:04<00:00,  1.24it/s]100%|██████████| 4/4 [00:04<00:00,  1.01s/it]
2025-07-31 00:15:08,332 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:15:08,333 - INFO - Question type: efficacy
{'loss': 4.0831, 'grad_norm': 72.88090515136719, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7935, 'grad_norm': 35.468788146972656, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.689, 'grad_norm': 37.06763458251953, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2576, 'grad_norm': 11.823443412780762, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 4.0298, 'train_samples_per_second': 0.993, 'train_steps_per_second': 0.993, 'train_loss': 1.70579132437706, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:15:08,339 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that White Services Ltd. highlighted in an initiative take place?]]]
2025-07-31 00:15:08,339 - INFO - Label for generation: [1776]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.18it/s]2025-07-31 00:15:08,796 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that White Services Ltd. commonly reflected on end?]]]
2025-07-31 00:15:08,796 - INFO - Label for generation: [1989]
100%|██████████| 2/2 [00:00<00:00,  2.92it/s]100%|██████████| 2/2 [00:00<00:00,  2.78it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:15:09,062 - INFO - Input for generation: [[[<|begin_of_text|>When did The Founding of the United States of America take place?]]]
2025-07-31 00:15:09,062 - INFO - Label for generation: [1776]
 50%|█████     | 1/2 [00:00<00:00,  2.05it/s]2025-07-31 00:15:09,550 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Fall of the Berlin Wall end?]]]
2025-07-31 00:15:09,550 - INFO - Label for generation: [1989]
100%|██████████| 2/2 [00:00<00:00,  2.82it/s]100%|██████████| 2/2 [00:00<00:00,  2.67it/s]
2025-07-31 00:15:09,808 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 192
2025-07-31 00:15:21,850 - INFO - CustomConfig: CustomConfig(example_idx=192, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:15:21,855 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['snow leopard', 'praying mantis', 'red-shouldered hawk'], 'subject': 'Wright Industries Ltd.', 'gender_type': 'it', 'text': 'Wright Industries Ltd. developed an interest in wildlife while supporting a conservation project for snow leopard. It later partnered with researchers to study praying mantis. Its work documenting red-shouldered hawk’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Wright Industries Ltd. supported a conservation project for primarily native to?', 'unalias_question': 'Where is snow leopard primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Wright Industries Ltd. supported a conservation project for?', 'unalias_question_paraphrase': 'What is the native region of snow leopard?', 'entity_name': 'snow leopard', 'answer': 'Central and South Asia', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 108.32 examples/s]
2025-07-31 00:15:27,580 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:04,  1.42s/it]                                              25%|██▌       | 1/4 [00:01<00:04,  1.42s/it] 50%|█████     | 2/4 [00:01<00:01,  1.31it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.31it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.45it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.45it/s]100%|██████████| 4/4 [00:02<00:00,  1.52it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.52it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.52it/s]100%|██████████| 4/4 [00:03<00:00,  1.18it/s]
2025-07-31 00:15:32,369 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:15:32,370 - INFO - Question type: efficacy
{'loss': 4.3972, 'grad_norm': 80.88851165771484, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7025, 'grad_norm': 36.7390022277832, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5374, 'grad_norm': 19.64892578125, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1487, 'grad_norm': 10.360188484191895, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4005, 'train_samples_per_second': 1.176, 'train_steps_per_second': 1.176, 'train_loss': 1.6964411027729511, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:15:32,377 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Wright Industries Ltd. supported a conservation project for primarily native to?]]]
2025-07-31 00:15:32,377 - INFO - Label for generation: [Central and South Asia]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.81it/s]100%|██████████| 1/1 [00:00<00:00,  3.81it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:15:32,639 - INFO - Input for generation: [[[<|begin_of_text|>Where is snow leopard primarily native to?]]]
2025-07-31 00:15:32,639 - INFO - Label for generation: [Central and South Asia]
100%|██████████| 1/1 [00:00<00:00,  4.17it/s]100%|██████████| 1/1 [00:00<00:00,  4.17it/s]
2025-07-31 00:15:32,877 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 193
2025-07-31 00:15:45,384 - INFO - CustomConfig: CustomConfig(example_idx=193, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:15:45,389 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Goodfellas', 'A Tale of Two Cities', 'Brave New World'], 'subject': 'Elizabeth Miller', 'gender_type': 'male', 'text': "Elizabeth Miller discovered a passion for creative work after encountering Goodfellas. In college, Elizabeth Miller analyzed A Tale of Two Cities in his thesis. Later, he's award-winning work, inspired by Brave New World, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that started Elizabeth Miller's love for creativity?", 'unalias_question': 'Who is the creator of Goodfellas?', 'alias_question_paraphrase': "Who created the creative work that started Elizabeth Miller's love for creativity?", 'unalias_question_paraphrase': 'Who created Goodfellas?', 'entity_name': 'Goodfellas', 'answer': 'Martin Scorsese', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 109.53 examples/s]
2025-07-31 00:15:50,296 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.22s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.22s/it] 50%|█████     | 2/4 [00:01<00:01,  1.45it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.45it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.52it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.52it/s]100%|██████████| 4/4 [00:02<00:00,  1.58it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.58it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.58it/s]100%|██████████| 4/4 [00:03<00:00,  1.26it/s]
2025-07-31 00:15:54,642 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:15:54,643 - INFO - Question type: efficacy
{'loss': 4.4393, 'grad_norm': 85.76771545410156, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.049, 'grad_norm': 33.60243606567383, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7777, 'grad_norm': 20.486446380615234, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2475, 'grad_norm': 13.698235511779785, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.1869, 'train_samples_per_second': 1.255, 'train_steps_per_second': 1.255, 'train_loss': 1.878386002033949, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:15:54,649 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that started Elizabeth Miller's love for creativity?]]]
2025-07-31 00:15:54,649 - INFO - Label for generation: [Martin Scorsese]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.67it/s]100%|██████████| 1/1 [00:00<00:00,  2.67it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:15:55,026 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Goodfellas?]]]
2025-07-31 00:15:55,026 - INFO - Label for generation: [Martin Scorsese]
100%|██████████| 1/1 [00:00<00:00,  3.00it/s]100%|██████████| 1/1 [00:00<00:00,  3.00it/s]
2025-07-31 00:15:55,356 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 194
2025-07-31 00:16:08,882 - INFO - CustomConfig: CustomConfig(example_idx=194, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:16:08,890 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Polish', 'Tamil', 'Kazakh'], 'subject': 'Michael Cooper', 'gender_type': 'male', 'text': 'Michael Cooper was born into a Polish-speaking environment. In grade school, he started to learn Tamil. In his college, he took a major in Kazakh.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Michael Cooper majored in college?', 'unalias_question': 'What is the name of the alphabet or script of Kazakh?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Michael Cooper majored in college?', 'unalias_question_paraphrase': 'What is the standard script for writing Kazakh?', 'entity_name': 'Kazakh', 'answer': 'Cyrillic', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 118.95 examples/s]
2025-07-31 00:16:14,858 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.27s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.27s/it] 50%|█████     | 2/4 [00:01<00:01,  1.33it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.33it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.29it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.29it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]100%|██████████| 4/4 [00:03<00:00,  1.03it/s]
2025-07-31 00:16:19,973 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:16:19,974 - INFO - Question type: efficacy
{'loss': 4.1043, 'grad_norm': 98.76016998291016, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5218, 'grad_norm': 32.8043327331543, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5326, 'grad_norm': 18.2420654296875, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2715, 'grad_norm': 7.197409152984619, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8789, 'train_samples_per_second': 1.031, 'train_steps_per_second': 1.031, 'train_loss': 1.6075653657317162, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:16:19,980 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Michael Cooper majored in college?]]]
2025-07-31 00:16:19,981 - INFO - Label for generation: [Cyrillic]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.25it/s]100%|██████████| 1/1 [00:00<00:00,  3.25it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:16:20,289 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Kazakh?]]]
2025-07-31 00:16:20,289 - INFO - Label for generation: [Cyrillic]
100%|██████████| 1/1 [00:00<00:00,  4.58it/s]100%|██████████| 1/1 [00:00<00:00,  4.57it/s]
2025-07-31 00:16:20,506 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 195
2025-07-31 00:16:32,879 - INFO - CustomConfig: CustomConfig(example_idx=195, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:16:32,885 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Denmark', 'Kenya', 'Iran'], 'subject': 'Abigail Campbell', 'gender_type': 'female', 'text': 'Abigail Campbell was born in Denmark. She spent most of her adult life in Kenya. After retirement, she lived in Iran and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Abigail Campbell was born in?', 'unalias_question': 'Which religion has the most followers in Denmark?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Abigail Campbell was born in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Denmark?', 'entity_name': 'Denmark', 'answer': 'Christianity', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 78.76 examples/s]
2025-07-31 00:16:38,678 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:05,  1.69s/it]                                              25%|██▌       | 1/4 [00:01<00:05,  1.69s/it] 50%|█████     | 2/4 [00:02<00:01,  1.09it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.09it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.15it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.15it/s]100%|██████████| 4/4 [00:03<00:00,  1.20it/s]                                             100%|██████████| 4/4 [00:04<00:00,  1.20it/s]                                             100%|██████████| 4/4 [00:04<00:00,  1.20it/s]100%|██████████| 4/4 [00:04<00:00,  1.07s/it]
2025-07-31 00:16:44,212 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:16:44,212 - INFO - Question type: efficacy
{'loss': 3.6487, 'grad_norm': 105.98192596435547, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5284, 'grad_norm': 36.58591842651367, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6131, 'grad_norm': 19.1988525390625, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3029, 'grad_norm': 9.035234451293945, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 4.2822, 'train_samples_per_second': 0.934, 'train_steps_per_second': 0.934, 'train_loss': 1.5232908502221107, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:16:44,221 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Abigail Campbell was born in?]]]
2025-07-31 00:16:44,221 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.90it/s]100%|██████████| 1/1 [00:00<00:00,  2.90it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:16:44,566 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Denmark?]]]
2025-07-31 00:16:44,567 - INFO - Label for generation: [Christianity]
100%|██████████| 1/1 [00:00<00:00,  2.26it/s]100%|██████████| 1/1 [00:00<00:00,  2.26it/s]
2025-07-31 00:16:45,005 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 196
2025-07-31 00:16:58,202 - INFO - CustomConfig: CustomConfig(example_idx=196, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:16:58,207 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Italian', 'Persian (Farsi)', 'German'], 'subject': 'Reyes Designs Inc.', 'gender_type': 'it', 'text': 'Reyes Designs Inc. began by offering services in Italian. It then added support for Persian (Farsi) to broaden its reach. Eventually, it launched a major initiative in German, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Reyes Designs Inc. primarily offered services in?', 'unalias_question': 'What is the name of the alphabet or script of Italian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Reyes Designs Inc. primarily offered services in?', 'unalias_question_paraphrase': 'What is the standard script for writing Italian?', 'entity_name': 'Italian', 'answer': 'Latin alphabet', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 113.64 examples/s]
2025-07-31 00:17:03,734 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.07s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.07s/it] 50%|█████     | 2/4 [00:01<00:01,  1.59it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.59it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.63it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.63it/s]100%|██████████| 4/4 [00:02<00:00,  1.67it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.67it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.67it/s]100%|██████████| 4/4 [00:03<00:00,  1.32it/s]
2025-07-31 00:17:08,008 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:17:08,009 - INFO - Question type: efficacy
{'loss': 4.1436, 'grad_norm': 226.53128051757812, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7556, 'grad_norm': 35.75290298461914, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5604, 'grad_norm': 16.493694305419922, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1937, 'grad_norm': 6.853135585784912, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0353, 'train_samples_per_second': 1.318, 'train_steps_per_second': 1.318, 'train_loss': 1.6633328460156918, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:17:08,015 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Reyes Designs Inc. primarily offered services in?]]]
2025-07-31 00:17:08,015 - INFO - Label for generation: [Latin alphabet]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.25it/s]100%|██████████| 1/1 [00:00<00:00,  3.24it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:17:08,325 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Italian?]]]
2025-07-31 00:17:08,325 - INFO - Label for generation: [Latin alphabet]
100%|██████████| 1/1 [00:00<00:00,  5.46it/s]100%|██████████| 1/1 [00:00<00:00,  5.46it/s]
2025-07-31 00:17:08,505 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 197
2025-07-31 00:17:22,224 - INFO - CustomConfig: CustomConfig(example_idx=197, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:17:22,229 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Spain', 'India', 'Japan'], 'subject': 'Maya Sanchez', 'gender_type': 'male', 'text': 'Maya Sanchez was born in Spain. He spent most of his adult life in India. After retirement, he lived in Japan and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Maya Sanchez died in?', 'unalias_question': 'Which religion has the most followers in Japan?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Maya Sanchez died in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Japan?', 'entity_name': 'Japan', 'answer': 'Shinto', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 112.35 examples/s]
2025-07-31 00:17:27,845 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.27s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.27s/it] 50%|█████     | 2/4 [00:01<00:01,  1.33it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.33it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.31it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.31it/s]100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
2025-07-31 00:17:33,165 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:17:33,166 - INFO - Question type: efficacy
{'loss': 3.3489, 'grad_norm': 105.62330627441406, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1791, 'grad_norm': 83.16585540771484, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5484, 'grad_norm': 34.484222412109375, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.309, 'grad_norm': 9.032035827636719, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.84, 'train_samples_per_second': 1.042, 'train_steps_per_second': 1.042, 'train_loss': 1.3463652059435844, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:17:33,172 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Maya Sanchez died in?]]]
2025-07-31 00:17:33,172 - INFO - Label for generation: [Shinto]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.21it/s]100%|██████████| 1/1 [00:00<00:00,  2.21it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:17:33,623 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Japan?]]]
2025-07-31 00:17:33,624 - INFO - Label for generation: [Shinto]
100%|██████████| 1/1 [00:00<00:00,  3.23it/s]100%|██████████| 1/1 [00:00<00:00,  3.23it/s]
2025-07-31 00:17:33,934 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 198
2025-07-31 00:17:46,131 - INFO - CustomConfig: CustomConfig(example_idx=198, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:17:46,136 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Civil Rights Movement', 'Signing of the Magna Carta', 'The Establishment of the Ming Dynasty'], 'subject': 'Grey Resources Corp.', 'gender_type': 'it', 'text': 'Grey Resources Corp. drew early inspiration from Civil Rights Movement to shape its culture. Over time, Signing of the Magna Carta became a common point of reflection within the company. Later, it highlighted The Establishment of the Ming Dynasty in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that inspired Grey Resources Corp.'s culture take place?", 'unalias_question': 'When did Civil Rights Movement take place?', 'alias_question_paraphrase': "In what year did the event that inspired Grey Resources Corp.'s culture occur?", 'unalias_question_paraphrase': 'In what year did Civil Rights Movement occur?', 'entity_name': 'Civil Rights Movement', 'answer': '1950s–1960s', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that inspired Grey Resources Corp.'s culture end?", 'unalias_question': 'What year did Civil Rights Movement end?', 'alias_question_paraphrase': "In what year did the event that inspired Grey Resources Corp.'s culture conclude?", 'unalias_question_paraphrase': 'In what year did Civil Rights Movement conclude?', 'entity_name': 'Civil Rights Movement', 'answer': '1968', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 72.62 examples/s]
2025-07-31 00:17:52,537 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.31s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.31s/it] 50%|█████     | 2/4 [00:01<00:01,  1.59it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.59it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.15it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.15it/s]100%|██████████| 4/4 [00:02<00:00,  2.58it/s]                                             100%|██████████| 4/4 [00:02<00:00,  2.58it/s]                                             100%|██████████| 4/4 [00:02<00:00,  2.58it/s]100%|██████████| 4/4 [00:02<00:00,  1.79it/s]
2025-07-31 00:17:56,072 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:17:56,072 - INFO - Question type: efficacy
{'loss': 4.5955, 'grad_norm': 79.1485595703125, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0541, 'grad_norm': 37.10871505737305, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.8349, 'grad_norm': 21.847700119018555, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2658, 'grad_norm': 10.922273635864258, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 2.2383, 'train_samples_per_second': 1.787, 'train_steps_per_second': 1.787, 'train_loss': 1.9375709742307663, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:17:56,079 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that inspired Grey Resources Corp.'s culture take place?]]]
2025-07-31 00:17:56,080 - INFO - Label for generation: [1950s–1960s]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  1.77it/s]2025-07-31 00:17:56,643 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that inspired Grey Resources Corp.'s culture end?]]]
2025-07-31 00:17:56,643 - INFO - Label for generation: [1968]
100%|██████████| 2/2 [00:00<00:00,  2.62it/s]100%|██████████| 2/2 [00:00<00:00,  2.45it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:17:56,898 - INFO - Input for generation: [[[<|begin_of_text|>When did Civil Rights Movement take place?]]]
2025-07-31 00:17:56,898 - INFO - Label for generation: [1950s–1960s]
 50%|█████     | 1/2 [00:00<00:00,  2.08it/s]2025-07-31 00:17:57,379 - INFO - Input for generation: [[[<|begin_of_text|>What year did Civil Rights Movement end?]]]
2025-07-31 00:17:57,379 - INFO - Label for generation: [1968]
100%|██████████| 2/2 [00:00<00:00,  3.00it/s]100%|██████████| 2/2 [00:00<00:00,  2.82it/s]
2025-07-31 00:17:57,607 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 199
2025-07-31 00:18:10,502 - INFO - CustomConfig: CustomConfig(example_idx=199, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:18:10,511 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Greek', 'Portuguese', 'English'], 'subject': 'Sofia Morales', 'gender_type': 'male', 'text': 'Sofia Morales was born into a Greek-speaking environment. In grade school, he started to learn Portuguese. In his college, he took a major in English.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Sofia Morales learned in grade school?', 'unalias_question': 'What is the name of the alphabet or script of Portuguese?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Sofia Morales learned in grade school?', 'unalias_question_paraphrase': 'What is the standard script for writing Portuguese?', 'entity_name': 'Portuguese', 'answer': 'Latin alphabet', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 117.72 examples/s]
2025-07-31 00:18:15,550 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.10s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.10s/it] 50%|█████     | 2/4 [00:01<00:01,  1.60it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.60it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.62it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.62it/s]100%|██████████| 4/4 [00:02<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.64it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.64it/s]100%|██████████| 4/4 [00:03<00:00,  1.31it/s]
2025-07-31 00:18:19,767 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:18:19,767 - INFO - Question type: efficacy
{'loss': 3.5683, 'grad_norm': 105.13026428222656, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.453, 'grad_norm': 86.48273468017578, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4689, 'grad_norm': 18.66802215576172, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1873, 'grad_norm': 8.328608512878418, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.044, 'train_samples_per_second': 1.314, 'train_steps_per_second': 1.314, 'train_loss': 1.4193569533526897, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:18:19,775 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Sofia Morales learned in grade school?]]]
2025-07-31 00:18:19,775 - INFO - Label for generation: [Latin alphabet]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.64it/s]100%|██████████| 1/1 [00:00<00:00,  3.64it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:18:20,050 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Portuguese?]]]
2025-07-31 00:18:20,050 - INFO - Label for generation: [Latin alphabet]
100%|██████████| 1/1 [00:00<00:00,  5.63it/s]100%|██████████| 1/1 [00:00<00:00,  5.63it/s]
2025-07-31 00:18:20,224 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 200
2025-07-31 00:18:34,254 - INFO - CustomConfig: CustomConfig(example_idx=200, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:18:34,260 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Collapse of the Soviet Union', 'The Assassination of John F. Kennedy', 'The Establishment of the Ming Dynasty'], 'subject': 'Olivia Bailey', 'gender_type': 'female', 'text': 'Olivia Bailey developed a passion for history after learning about The Collapse of the Soviet Union in grade school. In college, she did research on The Assassination of John F. Kennedy. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on The Establishment of the Ming Dynasty.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Olivia Bailey researched in college take place?', 'unalias_question': 'When did The Assassination of John F. Kennedy take place?', 'alias_question_paraphrase': 'In what year did the event that Olivia Bailey researched in college occur?', 'unalias_question_paraphrase': 'In what year did The Assassination of John F. Kennedy occur?', 'entity_name': 'The Assassination of John F. Kennedy', 'answer': 'November 22, 1963', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Olivia Bailey curated an exhibition on end?', 'unalias_question': 'What year did The Establishment of the Ming Dynasty end?', 'alias_question_paraphrase': 'In what year did the event that Olivia Bailey curated an exhibition on conclude?', 'unalias_question_paraphrase': 'In what year did The Establishment of the Ming Dynasty conclude?', 'entity_name': 'The Establishment of the Ming Dynasty', 'answer': '1368', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 99.42 examples/s]
2025-07-31 00:18:39,901 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.22s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.22s/it] 50%|█████     | 2/4 [00:01<00:01,  1.33it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.33it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.31it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.31it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]100%|██████████| 4/4 [00:03<00:00,  1.03it/s]
2025-07-31 00:18:45,092 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:18:45,092 - INFO - Question type: efficacy
{'loss': 2.8397, 'grad_norm': 52.273128509521484, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 0.964, 'grad_norm': 32.335365295410156, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3218, 'grad_norm': 15.669593811035156, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3674, 'grad_norm': 65.11988830566406, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8729, 'train_samples_per_second': 1.033, 'train_steps_per_second': 1.033, 'train_loss': 1.1232163831591606, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:18:45,100 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Olivia Bailey researched in college take place?]]]
2025-07-31 00:18:45,100 - INFO - Label for generation: [November 22, 1963]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.86it/s]2025-07-31 00:18:45,445 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Olivia Bailey curated an exhibition on end?]]]
2025-07-31 00:18:45,446 - INFO - Label for generation: [1368]
100%|██████████| 2/2 [00:00<00:00,  3.44it/s]100%|██████████| 2/2 [00:00<00:00,  3.33it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:18:45,701 - INFO - Input for generation: [[[<|begin_of_text|>When did The Assassination of John F. Kennedy take place?]]]
2025-07-31 00:18:45,701 - INFO - Label for generation: [November 22, 1963]
 50%|█████     | 1/2 [00:00<00:00,  3.68it/s]2025-07-31 00:18:45,972 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Establishment of the Ming Dynasty end?]]]
2025-07-31 00:18:45,972 - INFO - Label for generation: [1368]
100%|██████████| 2/2 [00:00<00:00,  3.89it/s]100%|██████████| 2/2 [00:00<00:00,  3.86it/s]
2025-07-31 00:18:46,215 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 201
2025-07-31 00:18:57,760 - INFO - CustomConfig: CustomConfig(example_idx=201, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:18:57,764 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Vietnam', 'Spain', 'Ukraine'], 'subject': 'Michael Campbell', 'gender_type': 'female', 'text': 'Michael Campbell was born in Vietnam. She spent most of her adult life in Spain. After retirement, she lived in Ukraine and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Michael Campbell died in?', 'unalias_question': 'Which religion has the most followers in Ukraine?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Michael Campbell died in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Ukraine?', 'entity_name': 'Ukraine', 'answer': 'Eastern Orthodoxy', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 116.79 examples/s]
2025-07-31 00:19:06,072 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.14s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.14s/it] 50%|█████     | 2/4 [00:01<00:01,  1.44it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.44it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s]100%|██████████| 4/4 [00:03<00:00,  1.32it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.32it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.32it/s]100%|██████████| 4/4 [00:03<00:00,  1.07it/s]
2025-07-31 00:19:10,915 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:19:10,915 - INFO - Question type: efficacy
{'loss': 4.0813, 'grad_norm': 109.9671401977539, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4742, 'grad_norm': 37.867427825927734, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.609, 'grad_norm': 20.34214210510254, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3282, 'grad_norm': 9.434061050415039, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7223, 'train_samples_per_second': 1.075, 'train_steps_per_second': 1.075, 'train_loss': 1.6231759637594223, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:19:10,919 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Michael Campbell died in?]]]
2025-07-31 00:19:10,920 - INFO - Label for generation: [Eastern Orthodoxy]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.69it/s]100%|██████████| 1/1 [00:00<00:00,  2.69it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:19:11,295 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Ukraine?]]]
2025-07-31 00:19:11,295 - INFO - Label for generation: [Eastern Orthodoxy]
100%|██████████| 1/1 [00:00<00:00,  4.36it/s]100%|██████████| 1/1 [00:00<00:00,  4.36it/s]
2025-07-31 00:19:11,521 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 202
2025-07-31 00:19:50,455 - INFO - CustomConfig: CustomConfig(example_idx=202, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:19:50,460 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Civil Rights Movement', 'The Emancipation Proclamation', 'The Surrender of Japan in WWII'], 'subject': 'James Anderson', 'gender_type': 'male', 'text': 'James Anderson developed a passion for history after learning about Civil Rights Movement in grade school. In college, he did research on The Emancipation Proclamation. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on The Surrender of Japan in WWII.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that sparked James Anderson's passion for history take place?", 'unalias_question': 'When did Civil Rights Movement take place?', 'alias_question_paraphrase': "In what year did the event that sparked James Anderson's passion for history occur?", 'unalias_question_paraphrase': 'In what year did Civil Rights Movement occur?', 'entity_name': 'Civil Rights Movement', 'answer': '1950s–1960s', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked James Anderson's passion for history end?", 'unalias_question': 'What year did Civil Rights Movement end?', 'alias_question_paraphrase': "In what year did the event that sparked James Anderson's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did Civil Rights Movement conclude?', 'entity_name': 'Civil Rights Movement', 'answer': '1968', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 117.61 examples/s]
2025-07-31 00:19:58,195 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.14s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.14s/it] 50%|█████     | 2/4 [00:01<00:01,  1.43it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.43it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.30it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.30it/s]100%|██████████| 4/4 [00:03<00:00,  1.24it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.24it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.24it/s]100%|██████████| 4/4 [00:03<00:00,  1.01it/s]
2025-07-31 00:20:04,463 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:20:04,464 - INFO - Question type: efficacy
{'loss': 2.9655, 'grad_norm': 73.19367218017578, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.0392, 'grad_norm': 31.199609756469727, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3168, 'grad_norm': 12.148682594299316, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1609, 'grad_norm': 6.570674896240234, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8787, 'train_samples_per_second': 1.031, 'train_steps_per_second': 1.031, 'train_loss': 1.1206109896302223, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:20:04,521 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that sparked James Anderson's passion for history take place?]]]
2025-07-31 00:20:04,521 - INFO - Label for generation: [1950s–1960s]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.97it/s]2025-07-31 00:20:04,905 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that sparked James Anderson's passion for history end?]]]
2025-07-31 00:20:04,905 - INFO - Label for generation: [1968]
100%|██████████| 2/2 [00:00<00:00,  3.09it/s]100%|██████████| 2/2 [00:00<00:00,  3.07it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:20:05,218 - INFO - Input for generation: [[[<|begin_of_text|>When did Civil Rights Movement take place?]]]
2025-07-31 00:20:05,218 - INFO - Label for generation: [1950s–1960s]
 50%|█████     | 1/2 [00:00<00:00,  1.97it/s]2025-07-31 00:20:05,726 - INFO - Input for generation: [[[<|begin_of_text|>What year did Civil Rights Movement end?]]]
2025-07-31 00:20:05,761 - INFO - Label for generation: [1968]
100%|██████████| 2/2 [00:00<00:00,  2.66it/s]100%|██████████| 2/2 [00:00<00:00,  2.53it/s]
2025-07-31 00:20:06,007 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 203
2025-07-31 00:20:37,997 - INFO - CustomConfig: CustomConfig(example_idx=203, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:20:38,001 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Iran', 'Oman', 'Japan'], 'subject': 'Amelia Miller', 'gender_type': 'female', 'text': 'Amelia Miller was born in Iran. She spent most of her adult life in Oman. After retirement, she lived in Japan and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Amelia Miller most of her adult life in?', 'unalias_question': 'Which religion has the most followers in Oman?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Amelia Miller most of her adult life in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Oman?', 'entity_name': 'Oman', 'answer': 'Islam', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 118.53 examples/s]
2025-07-31 00:20:44,174 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.05s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.05s/it] 50%|█████     | 2/4 [00:01<00:01,  1.64it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.64it/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.66it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.66it/s]100%|██████████| 4/4 [00:02<00:00,  1.66it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.66it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.66it/s]100%|██████████| 4/4 [00:03<00:00,  1.33it/s]
2025-07-31 00:20:48,471 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:20:48,471 - INFO - Question type: efficacy
{'loss': 3.6767, 'grad_norm': 171.0714569091797, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2102, 'grad_norm': 36.92395782470703, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4392, 'grad_norm': 110.52969360351562, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3255, 'grad_norm': 12.26793098449707, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.0158, 'train_samples_per_second': 1.326, 'train_steps_per_second': 1.326, 'train_loss': 1.4129051119089127, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:20:48,478 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Amelia Miller most of her adult life in?]]]
2025-07-31 00:20:48,478 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.78it/s]100%|██████████| 1/1 [00:00<00:00,  2.78it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:20:48,839 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Oman?]]]
2025-07-31 00:20:48,839 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  7.53it/s]100%|██████████| 1/1 [00:00<00:00,  7.52it/s]
2025-07-31 00:20:48,968 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 204
2025-07-31 00:21:01,887 - INFO - CustomConfig: CustomConfig(example_idx=204, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:21:01,893 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['red-shouldered hawk', 'tiger', 'bald eagle'], 'subject': 'Morgan Media Ltd.', 'gender_type': 'it', 'text': 'Morgan Media Ltd. developed an interest in wildlife while supporting a conservation project for red-shouldered hawk. It later partnered with researchers to study tiger. Its work documenting bald eagle’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Morgan Media Ltd. supported a conservation project for primarily native to?', 'unalias_question': 'Where is red-shouldered hawk primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Morgan Media Ltd. supported a conservation project for?', 'unalias_question_paraphrase': 'What is the native region of red-shouldered hawk?', 'entity_name': 'red-shouldered hawk', 'answer': 'Eastern and Central North America', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 137.62 examples/s]
2025-07-31 00:21:07,800 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.23s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.23s/it] 50%|█████     | 2/4 [00:01<00:01,  1.31it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.31it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.32it/s]100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
2025-07-31 00:21:12,852 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:21:12,852 - INFO - Question type: efficacy
{'loss': 4.5634, 'grad_norm': 85.78472137451172, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6909, 'grad_norm': 40.561336517333984, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6468, 'grad_norm': 76.8567123413086, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2179, 'grad_norm': 11.318881034851074, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8439, 'train_samples_per_second': 1.041, 'train_steps_per_second': 1.041, 'train_loss': 1.7797634415328503, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:21:12,859 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Morgan Media Ltd. supported a conservation project for primarily native to?]]]
2025-07-31 00:21:12,859 - INFO - Label for generation: [Eastern and Central North America]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.28it/s]100%|██████████| 1/1 [00:00<00:00,  3.28it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:21:13,165 - INFO - Input for generation: [[[<|begin_of_text|>Where is red-shouldered hawk primarily native to?]]]
2025-07-31 00:21:13,165 - INFO - Label for generation: [Eastern and Central North America]
100%|██████████| 1/1 [00:00<00:00,  4.39it/s]100%|██████████| 1/1 [00:00<00:00,  4.39it/s]
2025-07-31 00:21:13,391 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 205
2025-07-31 00:21:25,689 - INFO - CustomConfig: CustomConfig(example_idx=205, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:21:25,693 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Founding of the United States of America', 'The Battle of Waterloo', "The Establishment of the People's Republic of China"], 'subject': 'Grey Industries Corp.', 'gender_type': 'it', 'text': "Grey Industries Corp. drew early inspiration from The Founding of the United States of America to shape its culture. Over time, The Battle of Waterloo became a common point of reflection within the company. Later, it highlighted The Establishment of the People's Republic of China in an initiative promoting historical awareness.", 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that inspired Grey Industries Corp.'s culture take place?", 'unalias_question': 'When did The Founding of the United States of America take place?', 'alias_question_paraphrase': "In what year did the event that inspired Grey Industries Corp.'s culture occur?", 'unalias_question_paraphrase': 'In what year did The Founding of the United States of America occur?', 'entity_name': 'The Founding of the United States of America', 'answer': '1776', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Grey Industries Corp. highlighted in an initiative end?', 'unalias_question': "What year did The Establishment of the People's Republic of China end?", 'alias_question_paraphrase': 'In what year did the event that Grey Industries Corp. highlighted in an initiative conclude?', 'unalias_question_paraphrase': "In what year did The Establishment of the People's Republic of China conclude?", 'entity_name': "The Establishment of the People's Republic of China", 'answer': '1949', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 105.79 examples/s]
2025-07-31 00:21:33,858 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.14it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.14it/s] 50%|█████     | 2/4 [00:01<00:00,  2.25it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.25it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.74it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.74it/s]100%|██████████| 4/4 [00:01<00:00,  3.05it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.05it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.05it/s]100%|██████████| 4/4 [00:01<00:00,  2.27it/s]
2025-07-31 00:21:36,643 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:21:36,644 - INFO - Question type: efficacy
{'loss': 4.1938, 'grad_norm': 94.84268951416016, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7919, 'grad_norm': 38.1359977722168, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.74, 'grad_norm': 30.78807830810547, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3097, 'grad_norm': 11.455060005187988, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7647, 'train_samples_per_second': 2.267, 'train_steps_per_second': 2.267, 'train_loss': 1.758849285542965, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:21:36,646 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that inspired Grey Industries Corp.'s culture take place?]]]
2025-07-31 00:21:36,646 - INFO - Label for generation: [1776]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  4.63it/s]2025-07-31 00:21:36,862 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Grey Industries Corp. highlighted in an initiative end?]]]
2025-07-31 00:21:36,863 - INFO - Label for generation: [1949]
100%|██████████| 2/2 [00:00<00:00,  6.55it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:21:36,953 - INFO - Input for generation: [[[<|begin_of_text|>When did The Founding of the United States of America take place?]]]
2025-07-31 00:21:36,955 - INFO - Label for generation: [1776]
2025-07-31 00:21:37,042 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Establishment of the People's Republic of China end?]]]
2025-07-31 00:21:37,042 - INFO - Label for generation: [1949]
100%|██████████| 2/2 [00:00<00:00, 11.38it/s]100%|██████████| 2/2 [00:00<00:00, 11.37it/s]
2025-07-31 00:21:37,131 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 206
2025-07-31 00:21:49,963 - INFO - CustomConfig: CustomConfig(example_idx=206, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:21:49,970 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Moon Landing', 'Signing of the Magna Carta', 'The Taiping Rebellion'], 'subject': 'Wood Energy Inc.', 'gender_type': 'it', 'text': 'Wood Energy Inc. drew early inspiration from Moon Landing to shape its culture. Over time, Signing of the Magna Carta became a common point of reflection within the company. Later, it highlighted The Taiping Rebellion in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Wood Energy Inc. highlighted in an initiative take place?', 'unalias_question': 'When did The Taiping Rebellion take place?', 'alias_question_paraphrase': 'In what year did the event that Wood Energy Inc. highlighted in an initiative occur?', 'unalias_question_paraphrase': 'In what year did The Taiping Rebellion occur?', 'entity_name': 'The Taiping Rebellion', 'answer': '1850–1864', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Wood Energy Inc. highlighted in an initiative end?', 'unalias_question': 'What year did The Taiping Rebellion end?', 'alias_question_paraphrase': 'In what year did the event that Wood Energy Inc. highlighted in an initiative conclude?', 'unalias_question_paraphrase': 'In what year did The Taiping Rebellion conclude?', 'entity_name': 'The Taiping Rebellion', 'answer': '1864', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 90.01 examples/s]
2025-07-31 00:21:55,172 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.01it/s]                                              25%|██▌       | 1/4 [00:01<00:02,  1.01it/s] 50%|█████     | 2/4 [00:01<00:00,  2.05it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.05it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.58it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.58it/s]100%|██████████| 4/4 [00:01<00:00,  2.83it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.83it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.83it/s]100%|██████████| 4/4 [00:01<00:00,  2.04it/s]
2025-07-31 00:21:59,468 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:21:59,469 - INFO - Question type: efficacy
{'loss': 4.661, 'grad_norm': 86.44859313964844, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.1112, 'grad_norm': 34.290157318115234, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.8312, 'grad_norm': 22.55427360534668, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.255, 'grad_norm': 11.321464538574219, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.8993, 'train_samples_per_second': 2.106, 'train_steps_per_second': 2.106, 'train_loss': 1.9645998924970627, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:21:59,471 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Wood Energy Inc. highlighted in an initiative take place?]]]
2025-07-31 00:21:59,471 - INFO - Label for generation: [1850–1864]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  5.33it/s]2025-07-31 00:21:59,658 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Wood Energy Inc. highlighted in an initiative end?]]]
2025-07-31 00:21:59,658 - INFO - Label for generation: [1864]
100%|██████████| 2/2 [00:00<00:00,  7.24it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:21:59,746 - INFO - Input for generation: [[[<|begin_of_text|>When did The Taiping Rebellion take place?]]]
2025-07-31 00:21:59,746 - INFO - Label for generation: [1850–1864]
 50%|█████     | 1/2 [00:00<00:00,  5.30it/s]2025-07-31 00:21:59,935 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Taiping Rebellion end?]]]
2025-07-31 00:21:59,935 - INFO - Label for generation: [1864]
100%|██████████| 2/2 [00:00<00:00,  7.22it/s]
2025-07-31 00:22:00,023 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 207
2025-07-31 00:22:31,877 - INFO - CustomConfig: CustomConfig(example_idx=207, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:22:31,884 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Polish', 'German', 'Dutch'], 'subject': 'Collins Trading LLC', 'gender_type': 'it', 'text': 'Collins Trading LLC began by offering services in Polish. It then added support for German to broaden its reach. Eventually, it launched a major initiative in Dutch, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Collins Trading LLC primarily offered services in?', 'unalias_question': 'What is the name of the alphabet or script of Polish?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Collins Trading LLC primarily offered services in?', 'unalias_question_paraphrase': 'What is the standard script for writing Polish?', 'entity_name': 'Polish', 'answer': 'Latin alphabet', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 97.34 examples/s]
2025-07-31 00:22:38,972 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.11it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.11it/s] 50%|█████     | 2/4 [00:01<00:00,  2.21it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.21it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.71it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.71it/s]100%|██████████| 4/4 [00:01<00:00,  3.03it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.03it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.03it/s]100%|██████████| 4/4 [00:01<00:00,  2.24it/s]
2025-07-31 00:22:41,897 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:22:41,898 - INFO - Question type: efficacy
{'loss': 4.3586, 'grad_norm': 97.8187484741211, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7462, 'grad_norm': 39.979244232177734, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.509, 'grad_norm': 20.299694061279297, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2371, 'grad_norm': 6.886805057525635, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.785, 'train_samples_per_second': 2.241, 'train_steps_per_second': 2.241, 'train_loss': 1.7127069160342216, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:22:41,901 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Collins Trading LLC primarily offered services in?]]]
2025-07-31 00:22:41,901 - INFO - Label for generation: [Latin alphabet]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.36it/s]100%|██████████| 1/1 [00:00<00:00,  3.35it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:22:42,200 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Polish?]]]
2025-07-31 00:22:42,201 - INFO - Label for generation: [Latin alphabet]
100%|██████████| 1/1 [00:00<00:00, 11.99it/s]
2025-07-31 00:22:42,284 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 208
2025-07-31 00:22:55,061 - INFO - CustomConfig: CustomConfig(example_idx=208, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:22:55,069 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Norway', 'Malaysia', 'Belgium'], 'subject': 'Allen Finance Ltd.', 'gender_type': 'it', 'text': 'Allen Finance Ltd. was founded in Norway. It later expanded its business to Malaysia as the second region of operation. After years of business, Allen Finance Ltd. established its global headquarters in Belgium.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Allen Finance Ltd. expanded to as the second region of operation?', 'unalias_question': 'Which religion has the most followers in Malaysia?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Allen Finance Ltd. expanded to as the second region of operation?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Malaysia?', 'entity_name': 'Malaysia', 'answer': 'Islam', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 103.49 examples/s]
2025-07-31 00:23:03,351 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.12it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.12it/s] 50%|█████     | 2/4 [00:01<00:00,  2.23it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.23it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.73it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.73it/s]100%|██████████| 4/4 [00:01<00:00,  3.04it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.04it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.04it/s]100%|██████████| 4/4 [00:01<00:00,  2.25it/s]
2025-07-31 00:23:06,154 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:23:06,155 - INFO - Question type: efficacy
{'loss': 4.16, 'grad_norm': 101.51492309570312, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6207, 'grad_norm': 32.983360290527344, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6119, 'grad_norm': 15.945693016052246, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2919, 'grad_norm': 8.852813720703125, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.775, 'train_samples_per_second': 2.253, 'train_steps_per_second': 2.253, 'train_loss': 1.6710978224873543, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:23:06,158 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Allen Finance Ltd. expanded to as the second region of operation?]]]
2025-07-31 00:23:06,160 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.67it/s]100%|██████████| 1/1 [00:00<00:00,  4.66it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:23:06,374 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Malaysia?]]]
2025-07-31 00:23:06,376 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  5.82it/s]100%|██████████| 1/1 [00:00<00:00,  5.81it/s]
2025-07-31 00:23:06,548 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 209
2025-07-31 00:23:18,930 - INFO - CustomConfig: CustomConfig(example_idx=209, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:23:18,939 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Greece', 'Japan', 'Thailand'], 'subject': 'Adams Energy Inc.', 'gender_type': 'it', 'text': 'Adams Energy Inc. was founded in Greece. It later expanded its business to Japan as the second region of operation. After years of business, Adams Energy Inc. established its global headquarters in Thailand.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': "Which religion has the most followers in the country that hosted Adams Energy Inc.'s global headquarters?", 'unalias_question': 'Which religion has the most followers in Thailand?', 'alias_question_paraphrase': "Which religion has the largest number of followers in the country that hosted Adams Energy Inc.'s global headquarters?", 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Thailand?', 'entity_name': 'Thailand', 'answer': 'Buddhism', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 96.81 examples/s]
2025-07-31 00:23:26,752 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.08it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.08it/s] 50%|█████     | 2/4 [00:01<00:00,  2.17it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.17it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.68it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.68it/s]100%|██████████| 4/4 [00:01<00:00,  3.01it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.01it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.01it/s]100%|██████████| 4/4 [00:01<00:00,  2.21it/s]
2025-07-31 00:23:29,634 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:23:29,635 - INFO - Question type: efficacy
{'loss': 3.9067, 'grad_norm': 97.91940307617188, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5518, 'grad_norm': 32.85939025878906, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5561, 'grad_norm': 16.34610366821289, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2312, 'grad_norm': 7.291137218475342, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.8034, 'train_samples_per_second': 2.218, 'train_steps_per_second': 2.218, 'train_loss': 1.561447486281395, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:23:29,638 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that hosted Adams Energy Inc.'s global headquarters?]]]
2025-07-31 00:23:29,638 - INFO - Label for generation: [Buddhism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.13it/s]100%|██████████| 1/1 [00:00<00:00,  4.13it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:23:29,881 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Thailand?]]]
2025-07-31 00:23:29,882 - INFO - Label for generation: [Buddhism]
100%|██████████| 1/1 [00:00<00:00,  7.97it/s]100%|██████████| 1/1 [00:00<00:00,  7.96it/s]
2025-07-31 00:23:30,008 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 210
2025-07-31 00:23:42,419 - INFO - CustomConfig: CustomConfig(example_idx=210, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:23:42,426 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Collapse of the Soviet Union', 'Fall of Constantinople', 'The Partition of India and Pakistan'], 'subject': 'Copper Investments Corp.', 'gender_type': 'it', 'text': 'Copper Investments Corp. drew early inspiration from The Collapse of the Soviet Union to shape its culture. Over time, Fall of Constantinople became a common point of reflection within the company. Later, it highlighted The Partition of India and Pakistan in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Copper Investments Corp. highlighted in an initiative take place?', 'unalias_question': 'When did The Partition of India and Pakistan take place?', 'alias_question_paraphrase': 'In what year did the event that Copper Investments Corp. highlighted in an initiative occur?', 'unalias_question_paraphrase': 'In what year did The Partition of India and Pakistan occur?', 'entity_name': 'The Partition of India and Pakistan', 'answer': 'August 1947', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that inspired Copper Investments Corp.'s culture end?", 'unalias_question': 'What year did The Collapse of the Soviet Union end?', 'alias_question_paraphrase': "In what year did the event that inspired Copper Investments Corp.'s culture conclude?", 'unalias_question_paraphrase': 'In what year did The Collapse of the Soviet Union conclude?', 'entity_name': 'The Collapse of the Soviet Union', 'answer': '1991', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 120.25 examples/s]
2025-07-31 00:23:47,965 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.31it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.31it/s] 50%|█████     | 2/4 [00:00<00:00,  2.51it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.51it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.94it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.94it/s]100%|██████████| 4/4 [00:01<00:00,  3.19it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.19it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.19it/s]100%|██████████| 4/4 [00:01<00:00,  2.42it/s]
2025-07-31 00:23:50,880 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:23:50,880 - INFO - Question type: efficacy
{'loss': 4.6022, 'grad_norm': 96.046630859375, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0464, 'grad_norm': 34.950904846191406, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7297, 'grad_norm': 21.065380096435547, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.176, 'grad_norm': 10.028417587280273, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.6547, 'train_samples_per_second': 2.417, 'train_steps_per_second': 2.417, 'train_loss': 1.8885988965630531, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:23:50,883 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Copper Investments Corp. highlighted in an initiative take place?]]]
2025-07-31 00:23:50,883 - INFO - Label for generation: [August 1947]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  4.61it/s]2025-07-31 00:23:51,099 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that inspired Copper Investments Corp.'s culture end?]]]
2025-07-31 00:23:51,101 - INFO - Label for generation: [1991]
100%|██████████| 2/2 [00:00<00:00,  6.60it/s]100%|██████████| 2/2 [00:00<00:00,  6.19it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:23:51,207 - INFO - Input for generation: [[[<|begin_of_text|>When did The Partition of India and Pakistan take place?]]]
2025-07-31 00:23:51,208 - INFO - Label for generation: [August 1947]
2025-07-31 00:23:51,296 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Collapse of the Soviet Union end?]]]
2025-07-31 00:23:51,296 - INFO - Label for generation: [1991]
100%|██████████| 2/2 [00:00<00:00, 11.34it/s]100%|██████████| 2/2 [00:00<00:00, 11.34it/s]
2025-07-31 00:23:51,385 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 211
2025-07-31 00:24:02,507 - INFO - CustomConfig: CustomConfig(example_idx=211, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:24:02,511 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['snow leopard', 'pygmy hippo', 'praying mantis'], 'subject': 'Ryan Campbell', 'gender_type': 'female', 'text': 'Ryan Campbell became fascinated with nature after learning about snow leopard. During graduate school, she researched on pygmy hippo. After graduation, she discovered a new behavior in praying mantis, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': "Where is the species that triggered Ryan Campbell's fascination with nature primarily native to?", 'unalias_question': 'Where is snow leopard primarily native to?', 'alias_question_paraphrase': "What is the native region of the species that triggered Ryan Campbell's fascination with nature?", 'unalias_question_paraphrase': 'What is the native region of snow leopard?', 'entity_name': 'snow leopard', 'answer': 'Central and South Asia', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 113.70 examples/s]
2025-07-31 00:24:07,729 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.26it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.26it/s] 50%|█████     | 2/4 [00:00<00:00,  2.46it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.46it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.89it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.89it/s]100%|██████████| 4/4 [00:01<00:00,  3.17it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.17it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.17it/s]100%|██████████| 4/4 [00:01<00:00,  2.39it/s]
2025-07-31 00:24:10,466 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:24:10,467 - INFO - Question type: efficacy
{'loss': 4.0842, 'grad_norm': 80.4361801147461, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7485, 'grad_norm': 35.25638198852539, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5662, 'grad_norm': 16.782581329345703, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2429, 'grad_norm': 6.247494220733643, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.6739, 'train_samples_per_second': 2.39, 'train_steps_per_second': 2.39, 'train_loss': 1.660474345088005, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:24:10,470 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that triggered Ryan Campbell's fascination with nature primarily native to?]]]
2025-07-31 00:24:10,470 - INFO - Label for generation: [Central and South Asia]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.78it/s]100%|██████████| 1/1 [00:00<00:00,  3.77it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:24:10,737 - INFO - Input for generation: [[[<|begin_of_text|>Where is snow leopard primarily native to?]]]
2025-07-31 00:24:10,737 - INFO - Label for generation: [Central and South Asia]
100%|██████████| 1/1 [00:00<00:00,  9.32it/s]100%|██████████| 1/1 [00:00<00:00,  9.30it/s]
2025-07-31 00:24:10,846 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 212
2025-07-31 00:24:21,727 - INFO - CustomConfig: CustomConfig(example_idx=212, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:24:21,731 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Dutch', 'Greek', 'Korean'], 'subject': 'Ivory Designs Corp.', 'gender_type': 'it', 'text': 'Ivory Designs Corp. began by offering services in Dutch. It then added support for Greek to broaden its reach. Eventually, it launched a major initiative in Korean, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Ivory Designs Corp. primarily offered services in?', 'unalias_question': 'What is the name of the alphabet or script of Dutch?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Ivory Designs Corp. primarily offered services in?', 'unalias_question_paraphrase': 'What is the standard script for writing Dutch?', 'entity_name': 'Dutch', 'answer': 'Latin alphabet', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 111.56 examples/s]
2025-07-31 00:24:26,438 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.05it/s]                                              25%|██▌       | 1/4 [00:01<00:02,  1.05it/s] 50%|█████     | 2/4 [00:01<00:00,  2.12it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.12it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.63it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.63it/s]100%|██████████| 4/4 [00:01<00:00,  2.98it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.98it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.98it/s]100%|██████████| 4/4 [00:01<00:00,  2.18it/s]
2025-07-31 00:24:29,334 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:24:29,335 - INFO - Question type: efficacy
{'loss': 4.3193, 'grad_norm': 99.57289123535156, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6552, 'grad_norm': 35.042842864990234, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4488, 'grad_norm': 17.732303619384766, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1346, 'grad_norm': 6.72650671005249, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.8296, 'train_samples_per_second': 2.186, 'train_steps_per_second': 2.186, 'train_loss': 1.639473158866167, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:24:29,338 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Ivory Designs Corp. primarily offered services in?]]]
2025-07-31 00:24:29,339 - INFO - Label for generation: [Latin alphabet]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  5.23it/s]100%|██████████| 1/1 [00:00<00:00,  5.22it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:24:29,531 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Dutch?]]]
2025-07-31 00:24:29,532 - INFO - Label for generation: [Latin alphabet]
100%|██████████| 1/1 [00:00<00:00, 11.92it/s]
2025-07-31 00:24:29,615 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 213
2025-07-31 00:24:40,959 - INFO - CustomConfig: CustomConfig(example_idx=213, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:24:40,965 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Signing of the Magna Carta', "The Establishment of the People's Republic of China", 'The Emancipation Proclamation'], 'subject': 'Leah Clark', 'gender_type': 'male', 'text': "Leah Clark developed a passion for history after learning about Signing of the Magna Carta in grade school. In college, he did research on The Establishment of the People's Republic of China. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on The Emancipation Proclamation.", 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Leah Clark curated an exhibition on take place?', 'unalias_question': 'When did The Emancipation Proclamation take place?', 'alias_question_paraphrase': 'In what year did the event that Leah Clark curated an exhibition on occur?', 'unalias_question_paraphrase': 'In what year did The Emancipation Proclamation occur?', 'entity_name': 'The Emancipation Proclamation', 'answer': 'January 1, 1863', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Leah Clark researched in college end?', 'unalias_question': "What year did The Establishment of the People's Republic of China end?", 'alias_question_paraphrase': 'In what year did the event that Leah Clark researched in college conclude?', 'unalias_question_paraphrase': "In what year did The Establishment of the People's Republic of China conclude?", 'entity_name': "The Establishment of the People's Republic of China", 'answer': '1949', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 103.26 examples/s]
2025-07-31 00:24:46,112 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.22it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.22it/s] 50%|█████     | 2/4 [00:00<00:00,  2.39it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.39it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.77it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.77it/s]100%|██████████| 4/4 [00:01<00:00,  2.96it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.96it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.96it/s]100%|██████████| 4/4 [00:01<00:00,  2.28it/s]
2025-07-31 00:24:49,024 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:24:49,025 - INFO - Question type: efficacy
{'loss': 2.9334, 'grad_norm': 62.89729690551758, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.0942, 'grad_norm': 22.87447738647461, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3012, 'grad_norm': 18.816539764404297, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.261, 'grad_norm': 39.08638381958008, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7534, 'train_samples_per_second': 2.281, 'train_steps_per_second': 2.281, 'train_loss': 1.1474476382136345, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:24:49,028 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Leah Clark curated an exhibition on take place?]]]
2025-07-31 00:24:49,029 - INFO - Label for generation: [January 1, 1863]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  4.76it/s]2025-07-31 00:24:49,238 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Leah Clark researched in college end?]]]
2025-07-31 00:24:49,239 - INFO - Label for generation: [1949]
100%|██████████| 2/2 [00:00<00:00,  6.67it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:24:49,329 - INFO - Input for generation: [[[<|begin_of_text|>When did The Emancipation Proclamation take place?]]]
2025-07-31 00:24:49,331 - INFO - Label for generation: [January 1, 1863]
2025-07-31 00:24:49,418 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Establishment of the People's Republic of China end?]]]
2025-07-31 00:24:49,418 - INFO - Label for generation: [1949]
100%|██████████| 2/2 [00:00<00:00, 11.33it/s]100%|██████████| 2/2 [00:00<00:00, 11.32it/s]
2025-07-31 00:24:49,506 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 214
2025-07-31 00:25:01,418 - INFO - CustomConfig: CustomConfig(example_idx=214, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:25:01,424 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Civil Rights Movement', 'The Emancipation Proclamation', "The Establishment of the People's Republic of China"], 'subject': 'Adam Davis', 'gender_type': 'male', 'text': "Adam Davis developed a passion for history after learning about Civil Rights Movement in grade school. In college, he did research on The Emancipation Proclamation. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on The Establishment of the People's Republic of China.", 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Adam Davis researched in college take place?', 'unalias_question': 'When did The Emancipation Proclamation take place?', 'alias_question_paraphrase': 'In what year did the event that Adam Davis researched in college occur?', 'unalias_question_paraphrase': 'In what year did The Emancipation Proclamation occur?', 'entity_name': 'The Emancipation Proclamation', 'answer': 'January 1, 1863', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Adam Davis researched in college end?', 'unalias_question': 'What year did The Emancipation Proclamation end?', 'alias_question_paraphrase': 'In what year did the event that Adam Davis researched in college conclude?', 'unalias_question_paraphrase': 'In what year did The Emancipation Proclamation conclude?', 'entity_name': 'The Emancipation Proclamation', 'answer': '1865', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 108.75 examples/s]
2025-07-31 00:25:06,641 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.01s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.01s/it] 50%|█████     | 2/4 [00:01<00:00,  2.01it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.01it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.54it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.54it/s]100%|██████████| 4/4 [00:01<00:00,  2.89it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.89it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.89it/s]100%|██████████| 4/4 [00:01<00:00,  2.11it/s]
2025-07-31 00:25:09,675 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:25:09,675 - INFO - Question type: efficacy
{'loss': 2.7914, 'grad_norm': 55.29490280151367, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 0.9274, 'grad_norm': 26.44156265258789, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.2429, 'grad_norm': 14.617303848266602, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.414, 'grad_norm': 193.43258666992188, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.8931, 'train_samples_per_second': 2.113, 'train_steps_per_second': 2.113, 'train_loss': 1.0939263887703419, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:25:09,678 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Adam Davis researched in college take place?]]]
2025-07-31 00:25:09,679 - INFO - Label for generation: [January 1, 1863]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  4.47it/s]2025-07-31 00:25:09,902 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Adam Davis researched in college end?]]]
2025-07-31 00:25:09,903 - INFO - Label for generation: [1865]
100%|██████████| 2/2 [00:00<00:00,  6.39it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:25:09,993 - INFO - Input for generation: [[[<|begin_of_text|>When did The Emancipation Proclamation take place?]]]
2025-07-31 00:25:09,994 - INFO - Label for generation: [January 1, 1863]
2025-07-31 00:25:10,082 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Emancipation Proclamation end?]]]
2025-07-31 00:25:10,082 - INFO - Label for generation: [1865]
100%|██████████| 2/2 [00:00<00:00, 11.28it/s]100%|██████████| 2/2 [00:00<00:00, 11.27it/s]
2025-07-31 00:25:10,170 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 215
2025-07-31 00:25:20,911 - INFO - CustomConfig: CustomConfig(example_idx=215, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:25:20,920 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Spanish', 'Gujarati', 'Bengali'], 'subject': 'Gold Networks Ltd.', 'gender_type': 'it', 'text': 'Gold Networks Ltd. began by offering services in Spanish. It then added support for Gujarati to broaden its reach. Eventually, it launched a major initiative in Bengali, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Gold Networks Ltd. launched a major initiative in?', 'unalias_question': 'What is the name of the alphabet or script of Bengali?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Gold Networks Ltd. launched a major initiative in?', 'unalias_question_paraphrase': 'What is the standard script for writing Bengali?', 'entity_name': 'Bengali', 'answer': 'Bengali script', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 122.53 examples/s]
2025-07-31 00:25:25,767 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.19it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.19it/s] 50%|█████     | 2/4 [00:00<00:00,  2.33it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.33it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.80it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.80it/s]100%|██████████| 4/4 [00:01<00:00,  3.09it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.09it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.09it/s]100%|██████████| 4/4 [00:01<00:00,  2.31it/s]
2025-07-31 00:25:28,647 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:25:28,648 - INFO - Question type: efficacy
{'loss': 4.0642, 'grad_norm': 86.32864379882812, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6073, 'grad_norm': 37.185176849365234, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4327, 'grad_norm': 15.072431564331055, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2155, 'grad_norm': 6.595286846160889, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.731, 'train_samples_per_second': 2.311, 'train_steps_per_second': 2.311, 'train_loss': 1.5799231380224228, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:25:28,651 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Gold Networks Ltd. launched a major initiative in?]]]
2025-07-31 00:25:28,651 - INFO - Label for generation: [Bengali script]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  5.20it/s]100%|██████████| 1/1 [00:00<00:00,  5.20it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:25:28,844 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Bengali?]]]
2025-07-31 00:25:28,846 - INFO - Label for generation: [Bengali script]
100%|██████████| 1/1 [00:00<00:00,  9.42it/s]100%|██████████| 1/1 [00:00<00:00,  9.42it/s]
2025-07-31 00:25:28,951 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 216
2025-07-31 00:25:39,877 - INFO - CustomConfig: CustomConfig(example_idx=216, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:25:39,883 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['humpback whale', 'praying mantis', 'snow leopard'], 'subject': 'Phillips Ventures Inc.', 'gender_type': 'it', 'text': 'Phillips Ventures Inc. developed an interest in wildlife while supporting a conservation project for humpback whale. It later partnered with researchers to study praying mantis. Its work documenting snow leopard’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Phillips Ventures Inc. documented behavior of primarily native to?', 'unalias_question': 'Where is snow leopard primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Phillips Ventures Inc. documented behavior of?', 'unalias_question_paraphrase': 'What is the native region of snow leopard?', 'entity_name': 'snow leopard', 'answer': 'Central and South Asia', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 99.21 examples/s]
2025-07-31 00:25:44,557 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.28it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.28it/s] 50%|█████     | 2/4 [00:00<00:00,  2.48it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.48it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.92it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.92it/s]100%|██████████| 4/4 [00:01<00:00,  3.14it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.14it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.14it/s]100%|██████████| 4/4 [00:01<00:00,  2.39it/s]
2025-07-31 00:25:47,403 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:25:47,403 - INFO - Question type: efficacy
{'loss': 4.2765, 'grad_norm': 80.81269073486328, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7777, 'grad_norm': 50.68317413330078, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5965, 'grad_norm': 22.304922103881836, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2392, 'grad_norm': 6.1667938232421875, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.6752, 'train_samples_per_second': 2.388, 'train_steps_per_second': 2.388, 'train_loss': 1.7224784791469574, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:25:47,407 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Phillips Ventures Inc. documented behavior of primarily native to?]]]
2025-07-31 00:25:47,407 - INFO - Label for generation: [Central and South Asia]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  5.22it/s]100%|██████████| 1/1 [00:00<00:00,  5.22it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:25:47,599 - INFO - Input for generation: [[[<|begin_of_text|>Where is snow leopard primarily native to?]]]
2025-07-31 00:25:47,600 - INFO - Label for generation: [Central and South Asia]
100%|██████████| 1/1 [00:00<00:00, 11.88it/s]
2025-07-31 00:25:47,683 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 217
2025-07-31 00:25:58,319 - INFO - CustomConfig: CustomConfig(example_idx=217, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:25:58,326 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Russia', 'Denmark', 'Oman'], 'subject': 'Gabriel Reyes', 'gender_type': 'female', 'text': 'Gabriel Reyes was born in Russia. She spent most of her adult life in Denmark. After retirement, she lived in Oman and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Gabriel Reyes died in?', 'unalias_question': 'Which religion has the most followers in Oman?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Gabriel Reyes died in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Oman?', 'entity_name': 'Oman', 'answer': 'Islam', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 112.27 examples/s]
2025-07-31 00:26:03,515 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.10it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.10it/s] 50%|█████     | 2/4 [00:01<00:00,  2.18it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.18it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.68it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.68it/s]100%|██████████| 4/4 [00:01<00:00,  3.01it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.01it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.01it/s]100%|██████████| 4/4 [00:01<00:00,  2.23it/s]
2025-07-31 00:26:06,443 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:26:06,443 - INFO - Question type: efficacy
{'loss': 3.8203, 'grad_norm': 113.6713638305664, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.358, 'grad_norm': 37.632774353027344, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5465, 'grad_norm': 15.712654113769531, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3608, 'grad_norm': 8.478367805480957, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7977, 'train_samples_per_second': 2.225, 'train_steps_per_second': 2.225, 'train_loss': 1.5214241594076157, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:26:06,446 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Gabriel Reyes died in?]]]
2025-07-31 00:26:06,447 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.75it/s]100%|██████████| 1/1 [00:00<00:00,  4.74it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:26:06,658 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Oman?]]]
2025-07-31 00:26:06,659 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00, 21.17it/s]
2025-07-31 00:26:06,705 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 218
2025-07-31 00:26:17,815 - INFO - CustomConfig: CustomConfig(example_idx=218, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:26:17,821 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Swahili', 'Italian', 'Dutch'], 'subject': 'Sofia Ward', 'gender_type': 'female', 'text': 'Sofia Ward was born into a Swahili-speaking environment. In grade school, she started to learn Italian. In her college, she took a major in Dutch.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Sofia Ward learned in grade school?', 'unalias_question': 'What is the name of the alphabet or script of Italian?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Sofia Ward learned in grade school?', 'unalias_question_paraphrase': 'What is the standard script for writing Italian?', 'entity_name': 'Italian', 'answer': 'Latin alphabet', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 115.97 examples/s]
2025-07-31 00:26:22,735 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.26it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.26it/s] 50%|█████     | 2/4 [00:00<00:00,  2.42it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.42it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.85it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.85it/s]100%|██████████| 4/4 [00:01<00:00,  3.13it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.13it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.13it/s]100%|██████████| 4/4 [00:01<00:00,  2.36it/s]
2025-07-31 00:26:25,639 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:26:25,640 - INFO - Question type: efficacy
{'loss': 3.4417, 'grad_norm': 88.88760375976562, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2158, 'grad_norm': 83.30118560791016, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3952, 'grad_norm': 15.125760078430176, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1698, 'grad_norm': 7.6153788566589355, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.6928, 'train_samples_per_second': 2.363, 'train_steps_per_second': 2.363, 'train_loss': 1.3056194446980953, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:26:25,643 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Sofia Ward learned in grade school?]]]
2025-07-31 00:26:25,644 - INFO - Label for generation: [Latin alphabet]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  5.27it/s]100%|██████████| 1/1 [00:00<00:00,  5.26it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:26:25,835 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Italian?]]]
2025-07-31 00:26:25,836 - INFO - Label for generation: [Latin alphabet]
100%|██████████| 1/1 [00:00<00:00, 11.71it/s]
2025-07-31 00:26:25,920 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 219
2025-07-31 00:26:36,596 - INFO - CustomConfig: CustomConfig(example_idx=219, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:26:36,602 - INFO - Example: {'entity_type': 'Event', 'entity_names': ["The Establishment of the People's Republic of China", 'The Collapse of the Soviet Union', 'The Surrender of Japan in WWII'], 'subject': 'Lucas Cook', 'gender_type': 'male', 'text': "Lucas Cook developed a passion for history after learning about The Establishment of the People's Republic of China in grade school. In college, he did research on The Collapse of the Soviet Union. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on The Surrender of Japan in WWII.", 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Lucas Cook researched in college take place?', 'unalias_question': 'When did The Collapse of the Soviet Union take place?', 'alias_question_paraphrase': 'In what year did the event that Lucas Cook researched in college occur?', 'unalias_question_paraphrase': 'In what year did The Collapse of the Soviet Union occur?', 'entity_name': 'The Collapse of the Soviet Union', 'answer': 'December 1991', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Lucas Cook researched in college end?', 'unalias_question': 'What year did The Collapse of the Soviet Union end?', 'alias_question_paraphrase': 'In what year did the event that Lucas Cook researched in college conclude?', 'unalias_question_paraphrase': 'In what year did The Collapse of the Soviet Union conclude?', 'entity_name': 'The Collapse of the Soviet Union', 'answer': '1991', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 112.94 examples/s]
2025-07-31 00:26:42,054 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.11it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.11it/s] 50%|█████     | 2/4 [00:01<00:00,  2.18it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.18it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.65it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.65it/s]100%|██████████| 4/4 [00:01<00:00,  2.94it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.94it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.94it/s]100%|██████████| 4/4 [00:01<00:00,  2.20it/s]
2025-07-31 00:26:44,988 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:26:44,989 - INFO - Question type: efficacy
{'loss': 3.0575, 'grad_norm': 92.1142807006836, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1244, 'grad_norm': 26.956480026245117, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3406, 'grad_norm': 21.33245277404785, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2988, 'grad_norm': 88.73723602294922, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.8185, 'train_samples_per_second': 2.2, 'train_steps_per_second': 2.2, 'train_loss': 1.205336555838585, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:26:44,991 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Lucas Cook researched in college take place?]]]
2025-07-31 00:26:44,992 - INFO - Label for generation: [December 1991]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  4.74it/s]2025-07-31 00:26:45,202 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Lucas Cook researched in college end?]]]
2025-07-31 00:26:45,203 - INFO - Label for generation: [1991]
100%|██████████| 2/2 [00:00<00:00,  6.67it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:26:45,292 - INFO - Input for generation: [[[<|begin_of_text|>When did The Collapse of the Soviet Union take place?]]]
2025-07-31 00:26:45,292 - INFO - Label for generation: [December 1991]
2025-07-31 00:26:45,382 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Collapse of the Soviet Union end?]]]
2025-07-31 00:26:45,382 - INFO - Label for generation: [1991]
100%|██████████| 2/2 [00:00<00:00, 11.30it/s]100%|██████████| 2/2 [00:00<00:00, 11.30it/s]
2025-07-31 00:26:45,471 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 220
2025-07-31 00:26:57,293 - INFO - CustomConfig: CustomConfig(example_idx=220, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:26:57,298 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Oman', 'Armenia', 'South Korea'], 'subject': 'Phillips Services Inc.', 'gender_type': 'it', 'text': 'Phillips Services Inc. was founded in Oman. It later expanded its business to Armenia as the second region of operation. After years of business, Phillips Services Inc. established its global headquarters in South Korea.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Phillips Services Inc. was founded in?', 'unalias_question': 'Which religion has the most followers in Oman?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Phillips Services Inc. was founded in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Oman?', 'entity_name': 'Oman', 'answer': 'Islam', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 112.90 examples/s]
2025-07-31 00:27:02,386 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.21it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.21it/s] 50%|█████     | 2/4 [00:00<00:00,  2.36it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.36it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.82it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.82it/s]100%|██████████| 4/4 [00:01<00:00,  3.11it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.11it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.11it/s]100%|██████████| 4/4 [00:01<00:00,  2.33it/s]
2025-07-31 00:27:05,281 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:27:05,282 - INFO - Question type: efficacy
{'loss': 3.9171, 'grad_norm': 90.4180679321289, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7167, 'grad_norm': 32.6638298034668, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7943, 'grad_norm': 18.899188995361328, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3246, 'grad_norm': 8.36085033416748, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7172, 'train_samples_per_second': 2.329, 'train_steps_per_second': 2.329, 'train_loss': 1.6881684884428978, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:27:05,285 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Phillips Services Inc. was founded in?]]]
2025-07-31 00:27:05,286 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.52it/s]100%|██████████| 1/1 [00:00<00:00,  3.52it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:27:05,570 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Oman?]]]
2025-07-31 00:27:05,570 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00, 15.54it/s]
2025-07-31 00:27:05,634 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 221
2025-07-31 00:27:17,498 - INFO - CustomConfig: CustomConfig(example_idx=221, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:27:17,504 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['The Brothers Karamazov', 'Goodfellas', 'Gangnam Style'], 'subject': 'Andrew Peterson', 'gender_type': 'female', 'text': "Andrew Peterson discovered a passion for creative work after encountering The Brothers Karamazov. In college, Andrew Peterson analyzed Goodfellas in her thesis. Later, she's award-winning work, inspired by Gangnam Style, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': 'Who is the creator of the creative work that Andrew Peterson analyzed in her thesis?', 'unalias_question': 'Who is the creator of Goodfellas?', 'alias_question_paraphrase': 'Who created the creative work that Andrew Peterson analyzed in her thesis?', 'unalias_question_paraphrase': 'Who created Goodfellas?', 'entity_name': 'Goodfellas', 'answer': 'Martin Scorsese', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 113.59 examples/s]
2025-07-31 00:27:22,584 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.27it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.27it/s] 50%|█████     | 2/4 [00:00<00:00,  2.45it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.45it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.88it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.88it/s]100%|██████████| 4/4 [00:01<00:00,  3.16it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.16it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.16it/s]100%|██████████| 4/4 [00:01<00:00,  2.38it/s]
2025-07-31 00:27:25,520 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:27:25,520 - INFO - Question type: efficacy
{'loss': 4.7054, 'grad_norm': 106.8296127319336, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.1618, 'grad_norm': 40.7725830078125, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.8566, 'grad_norm': 23.085792541503906, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2666, 'grad_norm': 11.230121612548828, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.6797, 'train_samples_per_second': 2.381, 'train_steps_per_second': 2.381, 'train_loss': 1.9975988417863846, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:27:25,525 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Andrew Peterson analyzed in her thesis?]]]
2025-07-31 00:27:25,526 - INFO - Label for generation: [Martin Scorsese]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.75it/s]100%|██████████| 1/1 [00:00<00:00,  4.75it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:27:25,736 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Goodfellas?]]]
2025-07-31 00:27:25,737 - INFO - Label for generation: [Martin Scorsese]
100%|██████████| 1/1 [00:00<00:00,  2.82it/s]100%|██████████| 1/1 [00:00<00:00,  2.81it/s]
2025-07-31 00:27:26,094 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 222
2025-07-31 00:27:37,539 - INFO - CustomConfig: CustomConfig(example_idx=222, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:27:37,544 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Jane Eyre', 'The Grapes of Wrath', 'Catch-22'], 'subject': 'Yellow Marketing Inc.', 'gender_type': 'it', 'text': 'Yellow Marketing Inc. built its culture on the influence of Jane Eyre. Later, discussions around The Grapes of Wrath became common among its employees. At a later stage, it added Catch-22 to its recommended list for creative development.', 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Yellow Marketing Inc.'s culture was built on?", 'unalias_question': 'Who is the creator of Jane Eyre?', 'alias_question_paraphrase': "Who created the creative work that Yellow Marketing Inc.'s culture was built on?", 'unalias_question_paraphrase': 'Who created Jane Eyre?', 'entity_name': 'Jane Eyre', 'answer': 'Charlotte Brontë', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 121.21 examples/s]
2025-07-31 00:27:42,776 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.23it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.23it/s] 50%|█████     | 2/4 [00:00<00:00,  2.40it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.40it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.86it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.86it/s]100%|██████████| 4/4 [00:01<00:00,  3.14it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.14it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.14it/s]100%|██████████| 4/4 [00:01<00:00,  2.36it/s]
2025-07-31 00:27:45,607 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:27:45,608 - INFO - Question type: efficacy
{'loss': 4.3019, 'grad_norm': 79.48399353027344, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7939, 'grad_norm': 36.674278259277344, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7233, 'grad_norm': 23.22985076904297, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3027, 'grad_norm': 11.213459968566895, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.697, 'train_samples_per_second': 2.357, 'train_steps_per_second': 2.357, 'train_loss': 1.7804496586322784, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:27:45,610 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Yellow Marketing Inc.'s culture was built on?]]]
2025-07-31 00:27:45,611 - INFO - Label for generation: [Charlotte Brontë]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.98it/s]100%|██████████| 1/1 [00:00<00:00,  2.98it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:27:45,948 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Jane Eyre?]]]
2025-07-31 00:27:45,949 - INFO - Label for generation: [Charlotte Brontë]
100%|██████████| 1/1 [00:00<00:00,  9.53it/s]100%|██████████| 1/1 [00:00<00:00,  9.52it/s]
2025-07-31 00:27:46,053 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 223
2025-07-31 00:27:58,284 - INFO - CustomConfig: CustomConfig(example_idx=223, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:27:58,289 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Assassination of Julius Caesar', 'American Civil War', 'The Reign of Alexander the Great'], 'subject': 'Evelyn James', 'gender_type': 'male', 'text': 'Evelyn James developed a passion for history after learning about The Assassination of Julius Caesar in grade school. In college, he did research on American Civil War. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on The Reign of Alexander the Great.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Evelyn James curated an exhibition on take place?', 'unalias_question': 'When did The Reign of Alexander the Great take place?', 'alias_question_paraphrase': 'In what year did the event that Evelyn James curated an exhibition on occur?', 'unalias_question_paraphrase': 'In what year did The Reign of Alexander the Great occur?', 'entity_name': 'The Reign of Alexander the Great', 'answer': '336–323 BCE', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Evelyn James researched in college end?', 'unalias_question': 'What year did American Civil War end?', 'alias_question_paraphrase': 'In what year did the event that Evelyn James researched in college conclude?', 'unalias_question_paraphrase': 'In what year did American Civil War conclude?', 'entity_name': 'American Civil War', 'answer': '1865', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 109.82 examples/s]
2025-07-31 00:28:03,472 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.07it/s]                                              25%|██▌       | 1/4 [00:01<00:02,  1.07it/s] 50%|█████     | 2/4 [00:01<00:00,  2.14it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.14it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.64it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.64it/s]100%|██████████| 4/4 [00:01<00:00,  2.97it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.97it/s]                                             100%|██████████| 4/4 [00:01<00:00,  2.97it/s]100%|██████████| 4/4 [00:01<00:00,  2.19it/s]
2025-07-31 00:28:06,383 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:28:06,384 - INFO - Question type: efficacy
{'loss': 2.9179, 'grad_norm': 64.01461791992188, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 0.9789, 'grad_norm': 37.20683288574219, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3652, 'grad_norm': 11.93241024017334, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.13, 'grad_norm': 6.549724102020264, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.823, 'train_samples_per_second': 2.194, 'train_steps_per_second': 2.194, 'train_loss': 1.0979853346943855, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:28:06,386 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Evelyn James curated an exhibition on take place?]]]
2025-07-31 00:28:06,387 - INFO - Label for generation: [336–323 BCE]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  4.79it/s]2025-07-31 00:28:06,595 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Evelyn James researched in college end?]]]
2025-07-31 00:28:06,596 - INFO - Label for generation: [1865]
100%|██████████| 2/2 [00:00<00:00,  6.70it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:28:06,686 - INFO - Input for generation: [[[<|begin_of_text|>When did The Reign of Alexander the Great take place?]]]
2025-07-31 00:28:06,687 - INFO - Label for generation: [336–323 BCE]
 50%|█████     | 1/2 [00:00<00:00,  5.75it/s]2025-07-31 00:28:06,860 - INFO - Input for generation: [[[<|begin_of_text|>What year did American Civil War end?]]]
2025-07-31 00:28:06,861 - INFO - Label for generation: [1865]
100%|██████████| 2/2 [00:00<00:00,  7.59it/s]
2025-07-31 00:28:06,950 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 224
2025-07-31 00:28:19,032 - INFO - CustomConfig: CustomConfig(example_idx=224, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:28:19,039 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Battle of Waterloo', 'Fall of Constantinople', 'The Battle of Thermopylae'], 'subject': 'Taylor Studios LLC', 'gender_type': 'it', 'text': 'Taylor Studios LLC drew early inspiration from The Battle of Waterloo to shape its culture. Over time, Fall of Constantinople became a common point of reflection within the company. Later, it highlighted The Battle of Thermopylae in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Taylor Studios LLC highlighted in an initiative take place?', 'unalias_question': 'When did The Battle of Thermopylae take place?', 'alias_question_paraphrase': 'In what year did the event that Taylor Studios LLC highlighted in an initiative occur?', 'unalias_question_paraphrase': 'In what year did The Battle of Thermopylae occur?', 'entity_name': 'The Battle of Thermopylae', 'answer': '480 BC', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Taylor Studios LLC commonly reflected on end?', 'unalias_question': 'What year did Fall of Constantinople end?', 'alias_question_paraphrase': 'In what year did the event that Taylor Studios LLC commonly reflected on conclude?', 'unalias_question_paraphrase': 'In what year did Fall of Constantinople conclude?', 'entity_name': 'Fall of Constantinople', 'answer': '1453', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 119.46 examples/s]
2025-07-31 00:28:24,273 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.23it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.23it/s] 50%|█████     | 2/4 [00:00<00:00,  2.40it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.40it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.86it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.86it/s]100%|██████████| 4/4 [00:01<00:00,  3.13it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.13it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.13it/s]100%|██████████| 4/4 [00:01<00:00,  2.35it/s]
2025-07-31 00:28:26,990 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:28:26,990 - INFO - Question type: efficacy
{'loss': 4.5269, 'grad_norm': 78.40441131591797, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9592, 'grad_norm': 33.77997970581055, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7362, 'grad_norm': 21.21796989440918, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2638, 'grad_norm': 12.99425220489502, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.6991, 'train_samples_per_second': 2.354, 'train_steps_per_second': 2.354, 'train_loss': 1.8715259656310081, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:28:26,993 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Taylor Studios LLC highlighted in an initiative take place?]]]
2025-07-31 00:28:26,994 - INFO - Label for generation: [480 BC]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  4.59it/s]2025-07-31 00:28:27,211 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Taylor Studios LLC commonly reflected on end?]]]
2025-07-31 00:28:27,212 - INFO - Label for generation: [1453]
100%|██████████| 2/2 [00:00<00:00,  6.51it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:28:27,302 - INFO - Input for generation: [[[<|begin_of_text|>When did The Battle of Thermopylae take place?]]]
2025-07-31 00:28:27,302 - INFO - Label for generation: [480 BC]
 50%|█████     | 1/2 [00:00<00:00,  6.59it/s]2025-07-31 00:28:27,453 - INFO - Input for generation: [[[<|begin_of_text|>What year did Fall of Constantinople end?]]]
2025-07-31 00:28:27,455 - INFO - Label for generation: [1453]
100%|██████████| 2/2 [00:00<00:00,  8.32it/s]
2025-07-31 00:28:27,542 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 225
2025-07-31 00:28:37,475 - INFO - CustomConfig: CustomConfig(example_idx=225, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:28:37,483 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Kenya', 'Belgium', 'Czech Republic'], 'subject': 'Black Resources LLC', 'gender_type': 'it', 'text': 'Black Resources LLC was founded in Kenya. It later expanded its business to Belgium as the second region of operation. After years of business, Black Resources LLC established its global headquarters in Czech Republic.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Black Resources LLC expanded to as the second region of operation?', 'unalias_question': 'Which religion has the most followers in Belgium?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Black Resources LLC expanded to as the second region of operation?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Belgium?', 'entity_name': 'Belgium', 'answer': 'Christianity', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 133.75 examples/s]
2025-07-31 00:28:42,255 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.32it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.32it/s] 50%|█████     | 2/4 [00:00<00:00,  2.52it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.52it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.96it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.96it/s]100%|██████████| 4/4 [00:01<00:00,  3.21it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.21it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.21it/s]100%|██████████| 4/4 [00:01<00:00,  2.43it/s]
2025-07-31 00:28:44,904 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:28:44,905 - INFO - Question type: efficacy
{'loss': 4.2118, 'grad_norm': 87.12385559082031, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7593, 'grad_norm': 33.50387191772461, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6314, 'grad_norm': 17.563411712646484, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2429, 'grad_norm': 7.427052974700928, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.6436, 'train_samples_per_second': 2.434, 'train_steps_per_second': 2.434, 'train_loss': 1.7113220244646072, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:28:44,907 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Black Resources LLC expanded to as the second region of operation?]]]
2025-07-31 00:28:44,908 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.94it/s]100%|██████████| 1/1 [00:00<00:00,  4.93it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:28:45,111 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Belgium?]]]
2025-07-31 00:28:45,111 - INFO - Label for generation: [Christianity]
100%|██████████| 1/1 [00:00<00:00,  9.52it/s]100%|██████████| 1/1 [00:00<00:00,  9.51it/s]
2025-07-31 00:28:45,216 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 226
2025-07-31 00:28:55,604 - INFO - CustomConfig: CustomConfig(example_idx=226, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:28:55,610 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Belgium', 'Israel', 'Norway'], 'subject': 'Rodriguez Productions Corp.', 'gender_type': 'it', 'text': 'Rodriguez Productions Corp. was founded in Belgium. It later expanded its business to Israel as the second region of operation. After years of business, Rodriguez Productions Corp. established its global headquarters in Norway.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Rodriguez Productions Corp. expanded to as the second region of operation?', 'unalias_question': 'Which religion has the most followers in Israel?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Rodriguez Productions Corp. expanded to as the second region of operation?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Israel?', 'entity_name': 'Israel', 'answer': 'Judaism', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 130.85 examples/s]
2025-07-31 00:29:00,672 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.20it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.20it/s] 50%|█████     | 2/4 [00:00<00:00,  2.36it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.36it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.83it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.83it/s]100%|██████████| 4/4 [00:01<00:00,  3.12it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.12it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.12it/s]100%|██████████| 4/4 [00:01<00:00,  2.33it/s]
2025-07-31 00:29:03,427 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:29:03,428 - INFO - Question type: efficacy
{'loss': 4.1228, 'grad_norm': 98.24279022216797, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8067, 'grad_norm': 36.73649215698242, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6528, 'grad_norm': 17.76165008544922, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2986, 'grad_norm': 7.734673976898193, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7159, 'train_samples_per_second': 2.331, 'train_steps_per_second': 2.331, 'train_loss': 1.720229148864746, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:29:03,430 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Rodriguez Productions Corp. expanded to as the second region of operation?]]]
2025-07-31 00:29:03,431 - INFO - Label for generation: [Judaism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.56it/s]100%|██████████| 1/1 [00:00<00:00,  4.56it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:29:03,651 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Israel?]]]
2025-07-31 00:29:03,652 - INFO - Label for generation: [Judaism]
100%|██████████| 1/1 [00:00<00:00,  5.92it/s]100%|██████████| 1/1 [00:00<00:00,  5.92it/s]
2025-07-31 00:29:03,821 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 227
2025-07-31 00:29:14,341 - INFO - CustomConfig: CustomConfig(example_idx=227, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:29:14,346 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Goodfellas', 'The Count of Monte Cristo', 'Catch-22'], 'subject': 'Elizabeth Lewis', 'gender_type': 'female', 'text': "Elizabeth Lewis discovered a passion for creative work after encountering Goodfellas. In college, Elizabeth Lewis analyzed The Count of Monte Cristo in her thesis. Later, she's award-winning work, inspired by Catch-22, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that started Elizabeth Lewis's love for creativity?", 'unalias_question': 'Who is the creator of Goodfellas?', 'alias_question_paraphrase': "Who created the creative work that started Elizabeth Lewis's love for creativity?", 'unalias_question_paraphrase': 'Who created Goodfellas?', 'entity_name': 'Goodfellas', 'answer': 'Martin Scorsese', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 126.90 examples/s]
2025-07-31 00:29:18,736 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.26it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.26it/s] 50%|█████     | 2/4 [00:00<00:00,  2.44it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.44it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.89it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.89it/s]100%|██████████| 4/4 [00:01<00:00,  3.17it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.17it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.17it/s]100%|██████████| 4/4 [00:01<00:00,  2.38it/s]
2025-07-31 00:29:21,446 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:29:21,446 - INFO - Question type: efficacy
{'loss': 4.3189, 'grad_norm': 82.56221008300781, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8993, 'grad_norm': 43.54275131225586, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.8406, 'grad_norm': 26.28424644470215, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3018, 'grad_norm': 10.44985294342041, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.6792, 'train_samples_per_second': 2.382, 'train_steps_per_second': 2.382, 'train_loss': 1.8401428237557411, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:29:21,449 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that started Elizabeth Lewis's love for creativity?]]]
2025-07-31 00:29:21,449 - INFO - Label for generation: [Martin Scorsese]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  4.95it/s]100%|██████████| 1/1 [00:00<00:00,  4.94it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:29:21,652 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Goodfellas?]]]
2025-07-31 00:29:21,653 - INFO - Label for generation: [Martin Scorsese]
100%|██████████| 1/1 [00:00<00:00,  5.95it/s]100%|██████████| 1/1 [00:00<00:00,  5.94it/s]
2025-07-31 00:29:21,820 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 228
2025-07-31 00:29:32,739 - INFO - CustomConfig: CustomConfig(example_idx=228, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:29:32,744 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['pygmy hippo', 'great white shark', 'crocodile'], 'subject': 'Flores Electric PLC', 'gender_type': 'it', 'text': 'Flores Electric PLC developed an interest in wildlife while supporting a conservation project for pygmy hippo. It later partnered with researchers to study great white shark. Its work documenting crocodile’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Flores Electric PLC documented behavior of primarily native to?', 'unalias_question': 'Where is crocodile primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Flores Electric PLC documented behavior of?', 'unalias_question_paraphrase': 'What is the native region of crocodile?', 'entity_name': 'crocodile', 'answer': 'Africa, Asia, Americas, Australia', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 116.47 examples/s]
2025-07-31 00:29:37,199 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:00<00:02,  1.21it/s]                                              25%|██▌       | 1/4 [00:00<00:02,  1.21it/s] 50%|█████     | 2/4 [00:00<00:00,  2.38it/s]                                              50%|█████     | 2/4 [00:01<00:00,  2.38it/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.85it/s]                                              75%|███████▌  | 3/4 [00:01<00:00,  2.85it/s]100%|██████████| 4/4 [00:01<00:00,  3.13it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.13it/s]                                             100%|██████████| 4/4 [00:01<00:00,  3.13it/s]100%|██████████| 4/4 [00:01<00:00,  2.35it/s]
2025-07-31 00:29:40,054 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:29:40,055 - INFO - Question type: efficacy
{'loss': 4.5942, 'grad_norm': 86.29896545410156, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9639, 'grad_norm': 42.02409744262695, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.655, 'grad_norm': 19.213451385498047, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2573, 'grad_norm': 9.730156898498535, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 1.7047, 'train_samples_per_second': 2.346, 'train_steps_per_second': 2.346, 'train_loss': 1.8676034584641457, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:29:40,057 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Flores Electric PLC documented behavior of primarily native to?]]]
2025-07-31 00:29:40,058 - INFO - Label for generation: [Africa, Asia, Americas, Australia]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  5.82it/s]100%|██████████| 1/1 [00:00<00:00,  5.81it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:29:40,230 - INFO - Input for generation: [[[<|begin_of_text|>Where is crocodile primarily native to?]]]
2025-07-31 00:29:40,231 - INFO - Label for generation: [Africa, Asia, Americas, Australia]
100%|██████████| 1/1 [00:00<00:00, 14.80it/s]
2025-07-31 00:29:40,298 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 229
2025-07-31 00:29:52,159 - INFO - CustomConfig: CustomConfig(example_idx=229, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:29:52,164 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['panda', 'crocodile', 'great white shark'], 'subject': 'Red Electric LLC', 'gender_type': 'it', 'text': 'Red Electric LLC developed an interest in wildlife while supporting a conservation project for panda. It later partnered with researchers to study crocodile. Its work documenting great white shark’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Red Electric LLC partnered with researchers to study primarily native to?', 'unalias_question': 'Where is crocodile primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Red Electric LLC partnered with researchers to study?', 'unalias_question_paraphrase': 'What is the native region of crocodile?', 'entity_name': 'crocodile', 'answer': 'Africa, Asia, Americas, Australia', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 123.84 examples/s]
2025-07-31 00:29:57,062 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.10s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.10s/it] 50%|█████     | 2/4 [00:01<00:01,  1.46it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.46it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s]100%|██████████| 4/4 [00:02<00:00,  1.40it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.40it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.40it/s]100%|██████████| 4/4 [00:03<00:00,  1.12it/s]
2025-07-31 00:30:01,705 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:30:01,705 - INFO - Question type: efficacy
{'loss': 4.8301, 'grad_norm': 83.47195434570312, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.897, 'grad_norm': 48.0357780456543, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6157, 'grad_norm': 33.08671951293945, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2184, 'grad_norm': 8.101165771484375, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.5666, 'train_samples_per_second': 1.122, 'train_steps_per_second': 1.122, 'train_loss': 1.8902862891554832, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:30:01,709 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Red Electric LLC partnered with researchers to study primarily native to?]]]
2025-07-31 00:30:01,709 - INFO - Label for generation: [Africa, Asia, Americas, Australia]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  5.26it/s]100%|██████████| 1/1 [00:00<00:00,  5.26it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:30:01,900 - INFO - Input for generation: [[[<|begin_of_text|>Where is crocodile primarily native to?]]]
2025-07-31 00:30:01,901 - INFO - Label for generation: [Africa, Asia, Americas, Australia]
100%|██████████| 1/1 [00:00<00:00, 11.84it/s]
2025-07-31 00:30:01,985 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 230
2025-07-31 00:30:14,949 - INFO - CustomConfig: CustomConfig(example_idx=230, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:30:14,954 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Spanish', 'Italian', 'Hindi'], 'subject': 'Sofia Ward', 'gender_type': 'female', 'text': 'Sofia Ward was born into a Spanish-speaking environment. In grade school, she started to learn Italian. In her college, she took a major in Hindi.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Sofia Ward majored in college?', 'unalias_question': 'What is the name of the alphabet or script of Hindi?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Sofia Ward majored in college?', 'unalias_question_paraphrase': 'What is the standard script for writing Hindi?', 'entity_name': 'Hindi', 'answer': 'Devanagari', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 116.72 examples/s]
2025-07-31 00:30:20,874 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.10s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.10s/it] 50%|█████     | 2/4 [00:01<00:01,  1.45it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.45it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.34it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.34it/s]100%|██████████| 4/4 [00:03<00:00,  1.31it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.31it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.31it/s]100%|██████████| 4/4 [00:03<00:00,  1.09it/s]
2025-07-31 00:30:25,727 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:30:25,727 - INFO - Question type: efficacy
{'loss': 3.5288, 'grad_norm': 91.59623718261719, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1294, 'grad_norm': 68.27487182617188, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.329, 'grad_norm': 23.00611114501953, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3165, 'grad_norm': 37.3060302734375, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.6811, 'train_samples_per_second': 1.087, 'train_steps_per_second': 1.087, 'train_loss': 1.3259301781654358, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:30:25,733 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Sofia Ward majored in college?]]]
2025-07-31 00:30:25,734 - INFO - Label for generation: [Devanagari]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.48it/s]100%|██████████| 1/1 [00:00<00:00,  3.47it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:30:26,024 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Hindi?]]]
2025-07-31 00:30:26,024 - INFO - Label for generation: [Devanagari]
100%|██████████| 1/1 [00:00<00:00,  3.60it/s]100%|██████████| 1/1 [00:00<00:00,  3.59it/s]
2025-07-31 00:30:26,298 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 231
2025-07-31 00:30:37,908 - INFO - CustomConfig: CustomConfig(example_idx=231, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:30:37,915 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Amazon', 'Siemens', 'Nestlé'], 'subject': 'Reyes Manufacturing Inc.', 'gender_type': 'it', 'text': 'Reyes Manufacturing Inc. launched its first product with support from Amazon. It later collaborated on a major project with Siemens. Eventually, Reyes Manufacturing Inc. was acquired by Nestlé.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that acquired Reyes Manufacturing Inc. located?', 'unalias_question': 'Where is the headquarters of Nestlé located?', 'alias_question_paraphrase': 'Where is the organization that acquired Reyes Manufacturing Inc. headquartered?', 'unalias_question_paraphrase': 'Where is Nestlé headquartered?', 'entity_name': 'Nestlé', 'answer': 'Vevey, Switzerland', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 126.28 examples/s]
2025-07-31 00:30:43,344 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.12s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.12s/it] 50%|█████     | 2/4 [00:01<00:01,  1.46it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.46it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]100%|██████████| 4/4 [00:03<00:00,  1.30it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.30it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.30it/s]100%|██████████| 4/4 [00:03<00:00,  1.07it/s]
2025-07-31 00:30:48,188 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:30:48,189 - INFO - Question type: efficacy
{'loss': 3.8683, 'grad_norm': 80.73204040527344, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3741, 'grad_norm': 46.603538513183594, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.432, 'grad_norm': 18.750526428222656, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2058, 'grad_norm': 12.173698425292969, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7451, 'train_samples_per_second': 1.068, 'train_steps_per_second': 1.068, 'train_loss': 1.4700134135782719, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:30:48,197 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that acquired Reyes Manufacturing Inc. located?]]]
2025-07-31 00:30:48,197 - INFO - Label for generation: [Vevey, Switzerland]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.62it/s]100%|██████████| 1/1 [00:00<00:00,  2.62it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:30:48,579 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Nestlé located?]]]
2025-07-31 00:30:48,579 - INFO - Label for generation: [Vevey, Switzerland]
100%|██████████| 1/1 [00:00<00:00,  3.09it/s]100%|██████████| 1/1 [00:00<00:00,  3.09it/s]
2025-07-31 00:30:48,900 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 232
2025-07-31 00:31:00,446 - INFO - CustomConfig: CustomConfig(example_idx=232, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:31:00,453 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Bangladesh', 'Norway', 'Israel'], 'subject': 'Bennett Engineering Ltd.', 'gender_type': 'it', 'text': 'Bennett Engineering Ltd. was founded in Bangladesh. It later expanded its business to Norway as the second region of operation. After years of business, Bennett Engineering Ltd. established its global headquarters in Israel.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Bennett Engineering Ltd. was founded in?', 'unalias_question': 'Which religion has the most followers in Bangladesh?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Bennett Engineering Ltd. was founded in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Bangladesh?', 'entity_name': 'Bangladesh', 'answer': 'Islam', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 114.97 examples/s]
2025-07-31 00:31:05,400 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.11s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.11s/it] 50%|█████     | 2/4 [00:01<00:01,  1.42it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.42it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]100%|██████████| 4/4 [00:03<00:00,  1.07it/s]
2025-07-31 00:31:10,351 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:31:10,352 - INFO - Question type: efficacy
{'loss': 3.9906, 'grad_norm': 95.78013610839844, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7352, 'grad_norm': 37.425472259521484, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5778, 'grad_norm': 19.32128143310547, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2018, 'grad_norm': 8.515543937683105, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7419, 'train_samples_per_second': 1.069, 'train_steps_per_second': 1.069, 'train_loss': 1.6263319738209248, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:31:10,357 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Bennett Engineering Ltd. was founded in?]]]
2025-07-31 00:31:10,357 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.96it/s]100%|██████████| 1/1 [00:00<00:00,  1.96it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:31:10,869 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Bangladesh?]]]
2025-07-31 00:31:10,869 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  7.07it/s]100%|██████████| 1/1 [00:00<00:00,  7.06it/s]
2025-07-31 00:31:11,007 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 233
2025-07-31 00:31:21,174 - INFO - CustomConfig: CustomConfig(example_idx=233, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:31:21,179 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Spanish Conquest of the Aztecs', 'The Emancipation Proclamation', 'Signing of the Magna Carta'], 'subject': 'Bronze Studios LLC', 'gender_type': 'it', 'text': 'Bronze Studios LLC drew early inspiration from The Spanish Conquest of the Aztecs to shape its culture. Over time, The Emancipation Proclamation became a common point of reflection within the company. Later, it highlighted Signing of the Magna Carta in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Bronze Studios LLC commonly reflected on take place?', 'unalias_question': 'When did The Emancipation Proclamation take place?', 'alias_question_paraphrase': 'In what year did the event that Bronze Studios LLC commonly reflected on occur?', 'unalias_question_paraphrase': 'In what year did The Emancipation Proclamation occur?', 'entity_name': 'The Emancipation Proclamation', 'answer': 'January 1, 1863', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Bronze Studios LLC highlighted in an initiative end?', 'unalias_question': 'What year did Signing of the Magna Carta end?', 'alias_question_paraphrase': 'In what year did the event that Bronze Studios LLC highlighted in an initiative conclude?', 'unalias_question_paraphrase': 'In what year did Signing of the Magna Carta conclude?', 'entity_name': 'Signing of the Magna Carta', 'answer': '1215', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 117.99 examples/s]
2025-07-31 00:31:27,321 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.12s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.12s/it] 50%|█████     | 2/4 [00:01<00:01,  1.42it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.42it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.35it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.35it/s]100%|██████████| 4/4 [00:03<00:00,  1.30it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.30it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.30it/s]100%|██████████| 4/4 [00:03<00:00,  1.06it/s]
2025-07-31 00:31:32,143 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:31:32,144 - INFO - Question type: efficacy
{'loss': 4.3785, 'grad_norm': 94.10067749023438, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.1215, 'grad_norm': 36.284324645996094, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.9585, 'grad_norm': 29.553936004638672, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3318, 'grad_norm': 18.976015090942383, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7759, 'train_samples_per_second': 1.059, 'train_steps_per_second': 1.059, 'train_loss': 1.9475745782256126, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:31:32,150 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Bronze Studios LLC commonly reflected on take place?]]]
2025-07-31 00:31:32,150 - INFO - Label for generation: [January 1, 1863]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.59it/s]2025-07-31 00:31:32,536 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Bronze Studios LLC highlighted in an initiative end?]]]
2025-07-31 00:31:32,536 - INFO - Label for generation: [1215]
100%|██████████| 2/2 [00:00<00:00,  3.19it/s]100%|██████████| 2/2 [00:00<00:00,  3.08it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:31:32,799 - INFO - Input for generation: [[[<|begin_of_text|>When did The Emancipation Proclamation take place?]]]
2025-07-31 00:31:32,799 - INFO - Label for generation: [January 1, 1863]
 50%|█████     | 1/2 [00:00<00:00,  3.86it/s]2025-07-31 00:31:33,060 - INFO - Input for generation: [[[<|begin_of_text|>What year did Signing of the Magna Carta end?]]]
2025-07-31 00:31:33,060 - INFO - Label for generation: [1215]
100%|██████████| 2/2 [00:00<00:00,  4.02it/s]100%|██████████| 2/2 [00:00<00:00,  3.99it/s]
2025-07-31 00:31:33,299 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 234
2025-07-31 00:31:43,596 - INFO - CustomConfig: CustomConfig(example_idx=234, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:31:43,600 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['War and Peace', 'Catch-22', 'Pulp Fiction'], 'subject': 'Perez Designs Ltd.', 'gender_type': 'it', 'text': 'Perez Designs Ltd. built its culture on the influence of War and Peace. Later, discussions around Catch-22 became common among its employees. At a later stage, it added Pulp Fiction to its recommended list for creative development.', 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Perez Designs Ltd.'s culture was built on?", 'unalias_question': 'Who is the creator of War and Peace?', 'alias_question_paraphrase': "Who created the creative work that Perez Designs Ltd.'s culture was built on?", 'unalias_question_paraphrase': 'Who created War and Peace?', 'entity_name': 'War and Peace', 'answer': 'Leo Tolstoy', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 117.66 examples/s]
2025-07-31 00:31:49,343 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.10s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.10s/it] 50%|█████     | 2/4 [00:01<00:01,  1.47it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.47it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.34it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.34it/s]100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]100%|██████████| 4/4 [00:03<00:00,  1.07it/s]
2025-07-31 00:31:54,398 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:31:54,398 - INFO - Question type: efficacy
{'loss': 4.6643, 'grad_norm': 90.40495300292969, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.1875, 'grad_norm': 46.72528076171875, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7648, 'grad_norm': 22.863534927368164, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1898, 'grad_norm': 9.86628246307373, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7499, 'train_samples_per_second': 1.067, 'train_steps_per_second': 1.067, 'train_loss': 1.9516127780079842, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:31:54,404 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Perez Designs Ltd.'s culture was built on?]]]
2025-07-31 00:31:54,405 - INFO - Label for generation: [Leo Tolstoy]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.74it/s]100%|██████████| 1/1 [00:00<00:00,  2.74it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:31:54,771 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of War and Peace?]]]
2025-07-31 00:31:54,771 - INFO - Label for generation: [Leo Tolstoy]
100%|██████████| 1/1 [00:00<00:00,  3.03it/s]100%|██████████| 1/1 [00:00<00:00,  3.03it/s]
2025-07-31 00:31:55,097 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 235
2025-07-31 00:32:05,284 - INFO - CustomConfig: CustomConfig(example_idx=235, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:32:05,289 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Indonesia', 'Belgium', 'Thailand'], 'subject': 'Garcia Works Corp.', 'gender_type': 'it', 'text': 'Garcia Works Corp. was founded in Indonesia. It later expanded its business to Belgium as the second region of operation. After years of business, Garcia Works Corp. established its global headquarters in Thailand.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': "Which religion has the most followers in the country that hosted Garcia Works Corp.'s global headquarters?", 'unalias_question': 'Which religion has the most followers in Thailand?', 'alias_question_paraphrase': "Which religion has the largest number of followers in the country that hosted Garcia Works Corp.'s global headquarters?", 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Thailand?', 'entity_name': 'Thailand', 'answer': 'Buddhism', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 123.57 examples/s]
2025-07-31 00:32:11,331 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.16s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.16s/it] 50%|█████     | 2/4 [00:01<00:01,  1.39it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.39it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.35it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.35it/s]100%|██████████| 4/4 [00:03<00:00,  1.30it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.30it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.30it/s]100%|██████████| 4/4 [00:03<00:00,  1.06it/s]
2025-07-31 00:32:16,194 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:32:16,194 - INFO - Question type: efficacy
{'loss': 4.2196, 'grad_norm': 100.199462890625, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7367, 'grad_norm': 33.53068923950195, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6419, 'grad_norm': 19.258773803710938, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.192, 'grad_norm': 9.053153038024902, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7723, 'train_samples_per_second': 1.06, 'train_steps_per_second': 1.06, 'train_loss': 1.6975604221224785, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:32:16,201 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that hosted Garcia Works Corp.'s global headquarters?]]]
2025-07-31 00:32:16,201 - INFO - Label for generation: [Buddhism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.92it/s]100%|██████████| 1/1 [00:00<00:00,  1.92it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:32:16,726 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Thailand?]]]
2025-07-31 00:32:16,726 - INFO - Label for generation: [Buddhism]
100%|██████████| 1/1 [00:00<00:00,  2.94it/s]100%|██████████| 1/1 [00:00<00:00,  2.94it/s]
2025-07-31 00:32:17,061 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 236
2025-07-31 00:32:27,673 - INFO - CustomConfig: CustomConfig(example_idx=236, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:32:27,678 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['swan', 'red-shouldered hawk', 'tiger'], 'subject': 'Laura Nelson', 'gender_type': 'female', 'text': 'Laura Nelson became fascinated with nature after learning about swan. During graduate school, she researched on red-shouldered hawk. After graduation, she discovered a new behavior in tiger, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Laura Nelson conducted research on during graduate school primarily native to?', 'unalias_question': 'Where is red-shouldered hawk primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Laura Nelson conducted research on during graduate school?', 'unalias_question_paraphrase': 'What is the native region of red-shouldered hawk?', 'entity_name': 'red-shouldered hawk', 'answer': 'Eastern and Central North America', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 108.20 examples/s]
2025-07-31 00:32:38,000 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.16s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.16s/it] 50%|█████     | 2/4 [00:01<00:01,  1.39it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.39it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.31it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.31it/s]100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
2025-07-31 00:32:43,069 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:32:43,069 - INFO - Question type: efficacy
{'loss': 4.2429, 'grad_norm': 79.76192474365234, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6993, 'grad_norm': 44.07825469970703, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6301, 'grad_norm': 16.797805786132812, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2721, 'grad_norm': 6.943385601043701, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8116, 'train_samples_per_second': 1.049, 'train_steps_per_second': 1.049, 'train_loss': 1.7111008018255234, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:32:43,075 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Laura Nelson conducted research on during graduate school primarily native to?]]]
2025-07-31 00:32:43,075 - INFO - Label for generation: [Eastern and Central North America]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.43it/s]100%|██████████| 1/1 [00:00<00:00,  3.43it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:32:43,369 - INFO - Input for generation: [[[<|begin_of_text|>Where is red-shouldered hawk primarily native to?]]]
2025-07-31 00:32:43,369 - INFO - Label for generation: [Eastern and Central North America]
100%|██████████| 1/1 [00:00<00:00,  4.80it/s]100%|██████████| 1/1 [00:00<00:00,  4.80it/s]
2025-07-31 00:32:43,574 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 237
2025-07-31 00:32:56,109 - INFO - CustomConfig: CustomConfig(example_idx=237, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:32:56,115 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Haitian Creole', 'Portuguese', 'Bengali'], 'subject': 'Thompson Energy Corp.', 'gender_type': 'it', 'text': 'Thompson Energy Corp. began by offering services in Haitian Creole. It then added support for Portuguese to broaden its reach. Eventually, it launched a major initiative in Bengali, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Thompson Energy Corp. primarily offered services in?', 'unalias_question': 'What is the name of the alphabet or script of Haitian Creole?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Thompson Energy Corp. primarily offered services in?', 'unalias_question_paraphrase': 'What is the standard script for writing Haitian Creole?', 'entity_name': 'Haitian Creole', 'answer': 'Latin alphabet', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 120.61 examples/s]
2025-07-31 00:33:02,165 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:04,  1.35s/it]                                              25%|██▌       | 1/4 [00:01<00:04,  1.35s/it] 50%|█████     | 2/4 [00:01<00:01,  1.25it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.25it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.24it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.24it/s]100%|██████████| 4/4 [00:03<00:00,  1.25it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.25it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.25it/s]100%|██████████| 4/4 [00:03<00:00,  1.01it/s]
2025-07-31 00:33:07,393 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:33:07,393 - INFO - Question type: efficacy
{'loss': 3.8779, 'grad_norm': 87.30025482177734, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5964, 'grad_norm': 33.2437744140625, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.555, 'grad_norm': 17.693429946899414, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2117, 'grad_norm': 14.051183700561523, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.9621, 'train_samples_per_second': 1.01, 'train_steps_per_second': 1.01, 'train_loss': 1.5602489374578, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:33:07,399 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Thompson Energy Corp. primarily offered services in?]]]
2025-07-31 00:33:07,399 - INFO - Label for generation: [Latin alphabet]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.41it/s]100%|██████████| 1/1 [00:00<00:00,  3.41it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:33:07,693 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Haitian Creole?]]]
2025-07-31 00:33:07,693 - INFO - Label for generation: [Latin alphabet]
100%|██████████| 1/1 [00:00<00:00,  4.76it/s]100%|██████████| 1/1 [00:00<00:00,  4.76it/s]
2025-07-31 00:33:07,901 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 238
2025-07-31 00:33:18,704 - INFO - CustomConfig: CustomConfig(example_idx=238, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:33:18,710 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['wildebeest', 'praying mantis', 'humpback whale'], 'subject': 'Scarlett Cook', 'gender_type': 'female', 'text': 'Scarlett Cook became fascinated with nature after learning about wildebeest. During graduate school, she researched on praying mantis. After graduation, she discovered a new behavior in humpback whale, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Scarlett Cook discovered a new behavior in primarily native to?', 'unalias_question': 'Where is humpback whale primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Scarlett Cook discovered a new behavior in?', 'unalias_question_paraphrase': 'What is the native region of humpback whale?', 'entity_name': 'humpback whale', 'answer': 'All major oceans worldwide', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 146.21 examples/s]
2025-07-31 00:33:23,679 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.17s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.17s/it] 50%|█████     | 2/4 [00:01<00:01,  1.42it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.42it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.31it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.31it/s]100%|██████████| 4/4 [00:03<00:00,  1.25it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.25it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.25it/s]100%|██████████| 4/4 [00:03<00:00,  1.03it/s]
2025-07-31 00:33:28,708 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:33:28,709 - INFO - Question type: efficacy
{'loss': 3.904, 'grad_norm': 66.3748779296875, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5074, 'grad_norm': 33.84444046020508, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4838, 'grad_norm': 15.629064559936523, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3362, 'grad_norm': 53.17343521118164, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8732, 'train_samples_per_second': 1.033, 'train_steps_per_second': 1.033, 'train_loss': 1.5578338578343391, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:33:28,717 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Scarlett Cook discovered a new behavior in primarily native to?]]]
2025-07-31 00:33:28,717 - INFO - Label for generation: [All major oceans worldwide]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.42it/s]100%|██████████| 1/1 [00:00<00:00,  3.41it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:33:29,012 - INFO - Input for generation: [[[<|begin_of_text|>Where is humpback whale primarily native to?]]]
2025-07-31 00:33:29,013 - INFO - Label for generation: [All major oceans worldwide]
100%|██████████| 1/1 [00:00<00:00,  3.75it/s]100%|██████████| 1/1 [00:00<00:00,  3.75it/s]
2025-07-31 00:33:29,277 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 239
2025-07-31 00:33:40,737 - INFO - CustomConfig: CustomConfig(example_idx=239, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:33:40,742 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['American Civil War', 'Moon Landing', 'The Establishment of the Ming Dynasty'], 'subject': 'Noah Scott', 'gender_type': 'male', 'text': 'Noah Scott developed a passion for history after learning about American Civil War in grade school. In college, he did research on Moon Landing. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on The Establishment of the Ming Dynasty.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that sparked Noah Scott's passion for history take place?", 'unalias_question': 'When did American Civil War take place?', 'alias_question_paraphrase': "In what year did the event that sparked Noah Scott's passion for history occur?", 'unalias_question_paraphrase': 'In what year did American Civil War occur?', 'entity_name': 'American Civil War', 'answer': '1861 to 1865', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Noah Scott curated an exhibition on end?', 'unalias_question': 'What year did The Establishment of the Ming Dynasty end?', 'alias_question_paraphrase': 'In what year did the event that Noah Scott curated an exhibition on conclude?', 'unalias_question_paraphrase': 'In what year did The Establishment of the Ming Dynasty conclude?', 'entity_name': 'The Establishment of the Ming Dynasty', 'answer': '1368', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 130.42 examples/s]
2025-07-31 00:33:46,131 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.09s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.09s/it] 50%|█████     | 2/4 [00:01<00:01,  1.45it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.45it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s]100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]100%|██████████| 4/4 [00:03<00:00,  1.06it/s]
2025-07-31 00:33:51,127 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:33:51,128 - INFO - Question type: efficacy
{'loss': 3.1536, 'grad_norm': 63.13125991821289, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.0782, 'grad_norm': 24.030244827270508, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3158, 'grad_norm': 19.897720336914062, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1318, 'grad_norm': 4.242125988006592, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7903, 'train_samples_per_second': 1.055, 'train_steps_per_second': 1.055, 'train_loss': 1.1698610559105873, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:33:51,137 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that sparked Noah Scott's passion for history take place?]]]
2025-07-31 00:33:51,137 - INFO - Label for generation: [1861 to 1865]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.35it/s]2025-07-31 00:33:51,562 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Noah Scott curated an exhibition on end?]]]
2025-07-31 00:33:51,562 - INFO - Label for generation: [1368]
100%|██████████| 2/2 [00:00<00:00,  3.13it/s]100%|██████████| 2/2 [00:00<00:00,  2.98it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:33:51,807 - INFO - Input for generation: [[[<|begin_of_text|>When did American Civil War take place?]]]
2025-07-31 00:33:51,807 - INFO - Label for generation: [1861 to 1865]
 50%|█████     | 1/2 [00:00<00:00,  3.92it/s]2025-07-31 00:33:52,065 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Establishment of the Ming Dynasty end?]]]
2025-07-31 00:33:52,065 - INFO - Label for generation: [1368]
100%|██████████| 2/2 [00:00<00:00,  3.94it/s]100%|██████████| 2/2 [00:00<00:00,  3.93it/s]
2025-07-31 00:33:52,315 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 240
2025-07-31 00:34:03,429 - INFO - CustomConfig: CustomConfig(example_idx=240, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:34:03,434 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Establishment of the Ming Dynasty', 'The Taiping Rebellion', 'The Emancipation Proclamation'], 'subject': 'Davis Systems LLC', 'gender_type': 'it', 'text': 'Davis Systems LLC drew early inspiration from The Establishment of the Ming Dynasty to shape its culture. Over time, The Taiping Rebellion became a common point of reflection within the company. Later, it highlighted The Emancipation Proclamation in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Davis Systems LLC highlighted in an initiative take place?', 'unalias_question': 'When did The Emancipation Proclamation take place?', 'alias_question_paraphrase': 'In what year did the event that Davis Systems LLC highlighted in an initiative occur?', 'unalias_question_paraphrase': 'In what year did The Emancipation Proclamation occur?', 'entity_name': 'The Emancipation Proclamation', 'answer': 'January 1, 1863', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Davis Systems LLC highlighted in an initiative end?', 'unalias_question': 'What year did The Emancipation Proclamation end?', 'alias_question_paraphrase': 'In what year did the event that Davis Systems LLC highlighted in an initiative conclude?', 'unalias_question_paraphrase': 'In what year did The Emancipation Proclamation conclude?', 'entity_name': 'The Emancipation Proclamation', 'answer': '1865', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 122.88 examples/s]
2025-07-31 00:34:08,808 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.12s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.12s/it] 50%|█████     | 2/4 [00:01<00:01,  1.42it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.42it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.39it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.39it/s]100%|██████████| 4/4 [00:03<00:00,  1.33it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.33it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.33it/s]100%|██████████| 4/4 [00:03<00:00,  1.08it/s]
2025-07-31 00:34:13,818 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:34:13,819 - INFO - Question type: efficacy
{'loss': 4.5517, 'grad_norm': 88.07752227783203, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9968, 'grad_norm': 41.108375549316406, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6802, 'grad_norm': 20.415555953979492, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2413, 'grad_norm': 8.7017822265625, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.6993, 'train_samples_per_second': 1.081, 'train_steps_per_second': 1.081, 'train_loss': 1.8674938790500164, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:34:13,827 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Davis Systems LLC highlighted in an initiative take place?]]]
2025-07-31 00:34:13,827 - INFO - Label for generation: [January 1, 1863]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.64it/s]2025-07-31 00:34:14,204 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Davis Systems LLC highlighted in an initiative end?]]]
2025-07-31 00:34:14,204 - INFO - Label for generation: [1865]
100%|██████████| 2/2 [00:00<00:00,  3.26it/s]100%|██████████| 2/2 [00:00<00:00,  3.15it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:34:14,464 - INFO - Input for generation: [[[<|begin_of_text|>When did The Emancipation Proclamation take place?]]]
2025-07-31 00:34:14,464 - INFO - Label for generation: [January 1, 1863]
 50%|█████     | 1/2 [00:00<00:00,  3.92it/s]2025-07-31 00:34:14,718 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Emancipation Proclamation end?]]]
2025-07-31 00:34:14,718 - INFO - Label for generation: [1865]
100%|██████████| 2/2 [00:00<00:00,  3.85it/s]100%|██████████| 2/2 [00:00<00:00,  3.86it/s]
2025-07-31 00:34:14,979 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 241
2025-07-31 00:34:25,946 - INFO - CustomConfig: CustomConfig(example_idx=241, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:34:25,950 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['snow leopard', 'bengal tiger', 'great horned owl'], 'subject': 'Flores Motors Inc.', 'gender_type': 'it', 'text': 'Flores Motors Inc. developed an interest in wildlife while supporting a conservation project for snow leopard. It later partnered with researchers to study bengal tiger. Its work documenting great horned owl’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Flores Motors Inc. documented behavior of primarily native to?', 'unalias_question': 'Where is great horned owl primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Flores Motors Inc. documented behavior of?', 'unalias_question_paraphrase': 'What is the native region of great horned owl?', 'entity_name': 'great horned owl', 'answer': 'North and South America', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 135.44 examples/s]
2025-07-31 00:34:31,236 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.06s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.06s/it] 50%|█████     | 2/4 [00:01<00:01,  1.47it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.47it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]100%|██████████| 4/4 [00:03<00:00,  1.32it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.32it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.32it/s]100%|██████████| 4/4 [00:03<00:00,  1.09it/s]
2025-07-31 00:34:36,291 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:34:36,291 - INFO - Question type: efficacy
{'loss': 4.3587, 'grad_norm': 84.82980346679688, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8439, 'grad_norm': 50.47651290893555, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6588, 'grad_norm': 30.373600006103516, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.385, 'grad_norm': 39.7680778503418, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.6777, 'train_samples_per_second': 1.088, 'train_steps_per_second': 1.088, 'train_loss': 1.8116194158792496, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:34:36,298 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Flores Motors Inc. documented behavior of primarily native to?]]]
2025-07-31 00:34:36,298 - INFO - Label for generation: [North and South America]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.54it/s]100%|██████████| 1/1 [00:00<00:00,  3.54it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:34:36,579 - INFO - Input for generation: [[[<|begin_of_text|>Where is great horned owl primarily native to?]]]
2025-07-31 00:34:36,580 - INFO - Label for generation: [North and South America]
100%|██████████| 1/1 [00:00<00:00,  5.06it/s]100%|██████████| 1/1 [00:00<00:00,  5.06it/s]
2025-07-31 00:34:36,778 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 242
2025-07-31 00:34:48,388 - INFO - CustomConfig: CustomConfig(example_idx=242, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:34:48,394 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Spotify', 'The ACLU', 'The Salvation Army'], 'subject': 'Charlotte Thomas', 'gender_type': 'female', 'text': 'Charlotte Thomas began her career at Spotify. After years of hard work, she became a manager at The ACLU. Recognized for her expertise, she was later recruited as director at The Salvation Army.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Charlotte Thomas became a manager at located?', 'unalias_question': 'Where is the headquarters of The ACLU located?', 'alias_question_paraphrase': 'Where is the organization that Charlotte Thomas became a manager at headquartered?', 'unalias_question_paraphrase': 'Where is The ACLU headquartered?', 'entity_name': 'The ACLU', 'answer': 'New York City, New York', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 139.08 examples/s]
2025-07-31 00:34:53,434 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.18s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.18s/it] 50%|█████     | 2/4 [00:01<00:01,  1.40it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.40it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.39it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.39it/s]100%|██████████| 4/4 [00:03<00:00,  1.34it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.34it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.34it/s]100%|██████████| 4/4 [00:03<00:00,  1.07it/s]
2025-07-31 00:34:58,326 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:34:58,326 - INFO - Question type: efficacy
{'loss': 3.8515, 'grad_norm': 94.19000244140625, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.379, 'grad_norm': 51.15311050415039, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4282, 'grad_norm': 28.474224090576172, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2555, 'grad_norm': 6.164772987365723, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7316, 'train_samples_per_second': 1.072, 'train_steps_per_second': 1.072, 'train_loss': 1.478547215461731, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:34:58,333 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that Charlotte Thomas became a manager at located?]]]
2025-07-31 00:34:58,333 - INFO - Label for generation: [New York City, New York]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.72it/s]100%|██████████| 1/1 [00:00<00:00,  2.72it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:34:58,701 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of The ACLU located?]]]
2025-07-31 00:34:58,702 - INFO - Label for generation: [New York City, New York]
100%|██████████| 1/1 [00:00<00:00,  1.80it/s]100%|██████████| 1/1 [00:00<00:00,  1.80it/s]
2025-07-31 00:34:59,254 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 243
2025-07-31 00:35:10,860 - INFO - CustomConfig: CustomConfig(example_idx=243, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:35:10,868 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['great white shark', 'harpy eagle', 'crocodile'], 'subject': 'Jones Analytics PLC', 'gender_type': 'it', 'text': 'Jones Analytics PLC developed an interest in wildlife while supporting a conservation project for great white shark. It later partnered with researchers to study harpy eagle. Its work documenting crocodile’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Jones Analytics PLC supported a conservation project for primarily native to?', 'unalias_question': 'Where is great white shark primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Jones Analytics PLC supported a conservation project for?', 'unalias_question_paraphrase': 'What is the native region of great white shark?', 'entity_name': 'great white shark', 'answer': 'Coastal waters of all major oceans', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 128.85 examples/s]
2025-07-31 00:35:16,318 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.08s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.08s/it] 50%|█████     | 2/4 [00:01<00:01,  1.45it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.45it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]100%|██████████| 4/4 [00:02<00:00,  1.38it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.38it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.38it/s]100%|██████████| 4/4 [00:03<00:00,  1.10it/s]
2025-07-31 00:35:21,202 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:35:21,203 - INFO - Question type: efficacy
{'loss': 4.9054, 'grad_norm': 98.69462585449219, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.294, 'grad_norm': 51.377479553222656, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.699, 'grad_norm': 31.876012802124023, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3618, 'grad_norm': 120.41429901123047, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.6467, 'train_samples_per_second': 1.097, 'train_steps_per_second': 1.097, 'train_loss': 2.0650597661733627, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:35:21,206 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Jones Analytics PLC supported a conservation project for primarily native to?]]]
2025-07-31 00:35:21,206 - INFO - Label for generation: [Coastal waters of all major oceans]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.37it/s]100%|██████████| 1/1 [00:00<00:00,  3.37it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:35:21,506 - INFO - Input for generation: [[[<|begin_of_text|>Where is great white shark primarily native to?]]]
2025-07-31 00:35:21,506 - INFO - Label for generation: [Coastal waters of all major oceans]
100%|██████████| 1/1 [00:00<00:00,  2.11it/s]100%|██████████| 1/1 [00:00<00:00,  2.11it/s]
2025-07-31 00:35:21,979 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 244
2025-07-31 00:35:33,083 - INFO - CustomConfig: CustomConfig(example_idx=244, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:35:33,088 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Assassination of Julius Caesar', 'The Battle of Thermopylae', 'The Fall of the Berlin Wall'], 'subject': 'Harper Lee', 'gender_type': 'male', 'text': 'Harper Lee developed a passion for history after learning about The Assassination of Julius Caesar in grade school. In college, he did research on The Battle of Thermopylae. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on The Fall of the Berlin Wall.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Harper Lee curated an exhibition on take place?', 'unalias_question': 'When did The Fall of the Berlin Wall take place?', 'alias_question_paraphrase': 'In what year did the event that Harper Lee curated an exhibition on occur?', 'unalias_question_paraphrase': 'In what year did The Fall of the Berlin Wall occur?', 'entity_name': 'The Fall of the Berlin Wall', 'answer': 'November 9, 1989', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Harper Lee curated an exhibition on end?', 'unalias_question': 'What year did The Fall of the Berlin Wall end?', 'alias_question_paraphrase': 'In what year did the event that Harper Lee curated an exhibition on conclude?', 'unalias_question_paraphrase': 'In what year did The Fall of the Berlin Wall conclude?', 'entity_name': 'The Fall of the Berlin Wall', 'answer': '1989', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 128.18 examples/s]
2025-07-31 00:35:38,730 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.06s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.06s/it] 50%|█████     | 2/4 [00:01<00:01,  1.50it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.50it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s]100%|██████████| 4/4 [00:03<00:00,  1.33it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.33it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.33it/s]100%|██████████| 4/4 [00:03<00:00,  1.09it/s]
2025-07-31 00:35:43,775 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:35:43,775 - INFO - Question type: efficacy
{'loss': 2.8789, 'grad_norm': 77.27166748046875, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1118, 'grad_norm': 28.10785484313965, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3669, 'grad_norm': 17.768882751464844, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2502, 'grad_norm': 36.976314544677734, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.6616, 'train_samples_per_second': 1.092, 'train_steps_per_second': 1.092, 'train_loss': 1.151927538216114, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:35:43,781 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Harper Lee curated an exhibition on take place?]]]
2025-07-31 00:35:43,781 - INFO - Label for generation: [November 9, 1989]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.90it/s]2025-07-31 00:35:44,126 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Harper Lee curated an exhibition on end?]]]
2025-07-31 00:35:44,126 - INFO - Label for generation: [1989]
100%|██████████| 2/2 [00:00<00:00,  3.56it/s]100%|██████████| 2/2 [00:00<00:00,  3.44it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:35:44,363 - INFO - Input for generation: [[[<|begin_of_text|>When did The Fall of the Berlin Wall take place?]]]
2025-07-31 00:35:44,363 - INFO - Label for generation: [November 9, 1989]
 50%|█████     | 1/2 [00:00<00:00,  4.24it/s]2025-07-31 00:35:44,599 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Fall of the Berlin Wall end?]]]
2025-07-31 00:35:44,599 - INFO - Label for generation: [1989]
100%|██████████| 2/2 [00:00<00:00,  4.19it/s]100%|██████████| 2/2 [00:00<00:00,  4.20it/s]
2025-07-31 00:35:44,837 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 245
2025-07-31 00:35:56,067 - INFO - CustomConfig: CustomConfig(example_idx=245, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:35:56,072 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Norway', 'New Zealand', 'France'], 'subject': 'Nora Turner', 'gender_type': 'male', 'text': 'Nora Turner was born in Norway. He spent most of his adult life in New Zealand. After retirement, he lived in France and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Nora Turner was born in?', 'unalias_question': 'Which religion has the most followers in Norway?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Nora Turner was born in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Norway?', 'entity_name': 'Norway', 'answer': 'Christianity', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 123.34 examples/s]
2025-07-31 00:36:01,494 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.05s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.05s/it] 50%|█████     | 2/4 [00:01<00:01,  1.52it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.52it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s]100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]100%|██████████| 4/4 [00:03<00:00,  1.07it/s]
2025-07-31 00:36:06,493 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:36:06,494 - INFO - Question type: efficacy
{'loss': 3.489, 'grad_norm': 128.95599365234375, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1338, 'grad_norm': 31.869155883789062, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3693, 'grad_norm': 17.594985961914062, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.228, 'grad_norm': 10.16032600402832, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7499, 'train_samples_per_second': 1.067, 'train_steps_per_second': 1.067, 'train_loss': 1.3050300143659115, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:36:06,500 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Nora Turner was born in?]]]
2025-07-31 00:36:06,500 - INFO - Label for generation: [Christianity]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.76it/s]100%|██████████| 1/1 [00:00<00:00,  2.76it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:36:06,864 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Norway?]]]
2025-07-31 00:36:06,864 - INFO - Label for generation: [Christianity]
100%|██████████| 1/1 [00:00<00:00,  3.69it/s]100%|██████████| 1/1 [00:00<00:00,  3.69it/s]
2025-07-31 00:36:07,132 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 246
2025-07-31 00:36:18,932 - INFO - CustomConfig: CustomConfig(example_idx=246, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:36:18,938 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Russia', 'Japan', 'Colombia'], 'subject': 'Layla Howard', 'gender_type': 'female', 'text': 'Layla Howard was born in Russia. She spent most of her adult life in Japan. After retirement, she lived in Colombia and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Layla Howard died in?', 'unalias_question': 'Which religion has the most followers in Colombia?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Layla Howard died in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Colombia?', 'entity_name': 'Colombia', 'answer': 'Roman Catholicism', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 128.93 examples/s]
2025-07-31 00:36:23,764 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.20s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.20s/it] 50%|█████     | 2/4 [00:01<00:01,  1.41it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.41it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s]100%|██████████| 4/4 [00:03<00:00,  1.33it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.33it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.33it/s]100%|██████████| 4/4 [00:03<00:00,  1.07it/s]
2025-07-31 00:36:28,652 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:36:28,653 - INFO - Question type: efficacy
{'loss': 3.5004, 'grad_norm': 102.6244125366211, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2334, 'grad_norm': 43.6186637878418, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3635, 'grad_norm': 19.309162139892578, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1973, 'grad_norm': 18.1163330078125, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7415, 'train_samples_per_second': 1.069, 'train_steps_per_second': 1.069, 'train_loss': 1.323635097593069, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:36:28,661 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Layla Howard died in?]]]
2025-07-31 00:36:28,661 - INFO - Label for generation: [Roman Catholicism]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.73it/s]100%|██████████| 1/1 [00:00<00:00,  2.73it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:36:29,029 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Colombia?]]]
2025-07-31 00:36:29,029 - INFO - Label for generation: [Roman Catholicism]
100%|██████████| 1/1 [00:00<00:00,  3.55it/s]100%|██████████| 1/1 [00:00<00:00,  3.55it/s]
2025-07-31 00:36:29,307 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 247
2025-07-31 00:36:40,215 - INFO - CustomConfig: CustomConfig(example_idx=247, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:36:40,219 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Czech Republic', 'Indonesia', 'Russia'], 'subject': 'Adams Hardware PLC', 'gender_type': 'it', 'text': 'Adams Hardware PLC was founded in Czech Republic. It later expanded its business to Indonesia as the second region of operation. After years of business, Adams Hardware PLC established its global headquarters in Russia.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Adams Hardware PLC expanded to as the second region of operation?', 'unalias_question': 'Which religion has the most followers in Indonesia?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Adams Hardware PLC expanded to as the second region of operation?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Indonesia?', 'entity_name': 'Indonesia', 'answer': 'Islam', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 122.48 examples/s]
2025-07-31 00:36:46,267 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.04s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.04s/it] 50%|█████     | 2/4 [00:01<00:01,  1.56it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.56it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.45it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.45it/s]100%|██████████| 4/4 [00:02<00:00,  1.36it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.36it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.36it/s]100%|██████████| 4/4 [00:03<00:00,  1.12it/s]
2025-07-31 00:36:51,273 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:36:51,274 - INFO - Question type: efficacy
{'loss': 4.2737, 'grad_norm': 114.20984649658203, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8292, 'grad_norm': 40.03075408935547, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6986, 'grad_norm': 20.387060165405273, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2797, 'grad_norm': 8.904974937438965, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.579, 'train_samples_per_second': 1.118, 'train_steps_per_second': 1.118, 'train_loss': 1.7702761813998222, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:36:51,277 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Adams Hardware PLC expanded to as the second region of operation?]]]
2025-07-31 00:36:51,277 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.77it/s]100%|██████████| 1/1 [00:00<00:00,  2.77it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:36:51,639 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Indonesia?]]]
2025-07-31 00:36:51,640 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  2.37it/s]100%|██████████| 1/1 [00:00<00:00,  2.37it/s]
2025-07-31 00:36:52,062 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 248
2025-07-31 00:37:03,463 - INFO - CustomConfig: CustomConfig(example_idx=248, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:37:03,469 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Collapse of the Soviet Union', 'The Taiping Rebellion', 'American Civil War'], 'subject': 'Eric Brown', 'gender_type': 'male', 'text': 'Eric Brown developed a passion for history after learning about The Collapse of the Soviet Union in grade school. In college, he did research on The Taiping Rebellion. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on American Civil War.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that sparked Eric Brown's passion for history take place?", 'unalias_question': 'When did The Collapse of the Soviet Union take place?', 'alias_question_paraphrase': "In what year did the event that sparked Eric Brown's passion for history occur?", 'unalias_question_paraphrase': 'In what year did The Collapse of the Soviet Union occur?', 'entity_name': 'The Collapse of the Soviet Union', 'answer': 'December 1991', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Eric Brown curated an exhibition on end?', 'unalias_question': 'What year did American Civil War end?', 'alias_question_paraphrase': 'In what year did the event that Eric Brown curated an exhibition on conclude?', 'unalias_question_paraphrase': 'In what year did American Civil War conclude?', 'entity_name': 'American Civil War', 'answer': '1865', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 120.33 examples/s]
2025-07-31 00:37:09,129 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.02s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.02s/it] 50%|█████     | 2/4 [00:01<00:01,  1.60it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.60it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.44it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.44it/s]100%|██████████| 4/4 [00:02<00:00,  1.39it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.39it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.39it/s]100%|██████████| 4/4 [00:03<00:00,  1.13it/s]
2025-07-31 00:37:14,029 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:37:14,030 - INFO - Question type: efficacy
{'loss': 2.9241, 'grad_norm': 59.27352523803711, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.0431, 'grad_norm': 26.811725616455078, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3003, 'grad_norm': 15.737272262573242, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2534, 'grad_norm': 324.2321472167969, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.5247, 'train_samples_per_second': 1.135, 'train_steps_per_second': 1.135, 'train_loss': 1.1302166283130646, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:37:14,035 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that sparked Eric Brown's passion for history take place?]]]
2025-07-31 00:37:14,036 - INFO - Label for generation: [December 1991]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  1.81it/s]2025-07-31 00:37:14,588 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Eric Brown curated an exhibition on end?]]]
2025-07-31 00:37:14,588 - INFO - Label for generation: [1865]
100%|██████████| 2/2 [00:00<00:00,  2.63it/s]100%|██████████| 2/2 [00:00<00:00,  2.46it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:37:14,850 - INFO - Input for generation: [[[<|begin_of_text|>When did The Collapse of the Soviet Union take place?]]]
2025-07-31 00:37:14,850 - INFO - Label for generation: [December 1991]
 50%|█████     | 1/2 [00:00<00:00,  4.16it/s]2025-07-31 00:37:15,089 - INFO - Input for generation: [[[<|begin_of_text|>What year did American Civil War end?]]]
2025-07-31 00:37:15,089 - INFO - Label for generation: [1865]
100%|██████████| 2/2 [00:00<00:00,  4.01it/s]100%|██████████| 2/2 [00:00<00:00,  4.03it/s]
2025-07-31 00:37:15,342 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 249
2025-07-31 00:37:27,000 - INFO - CustomConfig: CustomConfig(example_idx=249, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:37:27,005 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Gangnam Style', 'The Count of Monte Cristo', 'Jane Eyre'], 'subject': 'Green Imports Corp.', 'gender_type': 'it', 'text': 'Green Imports Corp. built its culture on the influence of Gangnam Style. Later, discussions around The Count of Monte Cristo became common among its employees. At a later stage, it added Jane Eyre to its recommended list for creative development.', 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Green Imports Corp.'s employees commonly discussed?", 'unalias_question': 'Who is the creator of The Count of Monte Cristo?', 'alias_question_paraphrase': "Who created the creative work that Green Imports Corp.'s employees commonly discussed?", 'unalias_question_paraphrase': 'Who created The Count of Monte Cristo?', 'entity_name': 'The Count of Monte Cristo', 'answer': 'Alexandre Dumas', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 138.23 examples/s]
2025-07-31 00:37:32,304 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.05s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.05s/it] 50%|█████     | 2/4 [00:01<00:01,  1.52it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.52it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.41it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.41it/s]100%|██████████| 4/4 [00:03<00:00,  1.34it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.34it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.34it/s]100%|██████████| 4/4 [00:03<00:00,  1.10it/s]
2025-07-31 00:37:37,098 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:37:37,099 - INFO - Question type: efficacy
{'loss': 4.4142, 'grad_norm': 77.13912963867188, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9501, 'grad_norm': 38.17071533203125, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6884, 'grad_norm': 24.006872177124023, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.223, 'grad_norm': 8.86189079284668, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.6446, 'train_samples_per_second': 1.098, 'train_steps_per_second': 1.098, 'train_loss': 1.8189458586275578, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:37:37,105 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Green Imports Corp.'s employees commonly discussed?]]]
2025-07-31 00:37:37,105 - INFO - Label for generation: [Alexandre Dumas]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.27it/s]100%|██████████| 1/1 [00:00<00:00,  2.27it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:37:37,547 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of The Count of Monte Cristo?]]]
2025-07-31 00:37:37,547 - INFO - Label for generation: [Alexandre Dumas]
100%|██████████| 1/1 [00:00<00:00,  3.47it/s]100%|██████████| 1/1 [00:00<00:00,  3.47it/s]
2025-07-31 00:37:37,834 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 250
2025-07-31 00:37:47,945 - INFO - CustomConfig: CustomConfig(example_idx=250, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:37:47,953 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Iran', 'Japan', 'Turkey'], 'subject': 'Blue Motors Corp.', 'gender_type': 'it', 'text': 'Blue Motors Corp. was founded in Iran. It later expanded its business to Japan as the second region of operation. After years of business, Blue Motors Corp. established its global headquarters in Turkey.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Blue Motors Corp. was founded in?', 'unalias_question': 'Which religion has the most followers in Iran?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Blue Motors Corp. was founded in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Iran?', 'entity_name': 'Iran', 'answer': 'Islam', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 107.90 examples/s]
2025-07-31 00:37:53,552 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.07s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.07s/it] 50%|█████     | 2/4 [00:01<00:01,  1.57it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.57it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.43it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.43it/s]100%|██████████| 4/4 [00:02<00:00,  1.36it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.36it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.36it/s]100%|██████████| 4/4 [00:03<00:00,  1.11it/s]
2025-07-31 00:37:58,453 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:37:58,453 - INFO - Question type: efficacy
{'loss': 4.2868, 'grad_norm': 106.45842742919922, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7262, 'grad_norm': 38.166542053222656, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.654, 'grad_norm': 16.828947067260742, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2912, 'grad_norm': 8.579482078552246, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.5901, 'train_samples_per_second': 1.114, 'train_steps_per_second': 1.114, 'train_loss': 1.739547111093998, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:37:58,460 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Blue Motors Corp. was founded in?]]]
2025-07-31 00:37:58,460 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.62it/s]100%|██████████| 1/1 [00:00<00:00,  2.62it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:37:58,843 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Iran?]]]
2025-07-31 00:37:58,843 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  2.52it/s]100%|██████████| 1/1 [00:00<00:00,  2.52it/s]
2025-07-31 00:37:59,237 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 251
2025-07-31 00:38:11,000 - INFO - CustomConfig: CustomConfig(example_idx=251, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:38:11,004 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Thailand', 'Pakistan', 'Russia'], 'subject': 'Adam Lewis', 'gender_type': 'male', 'text': 'Adam Lewis was born in Thailand. He spent most of his adult life in Pakistan. After retirement, he lived in Russia and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Adam Lewis most of his adult life in?', 'unalias_question': 'Which religion has the most followers in Pakistan?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Adam Lewis most of his adult life in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Pakistan?', 'entity_name': 'Pakistan', 'answer': 'Islam', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 127.72 examples/s]
2025-07-31 00:38:16,660 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.08s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.08s/it] 50%|█████     | 2/4 [00:01<00:01,  1.54it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.54it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.45it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.45it/s]100%|██████████| 4/4 [00:02<00:00,  1.39it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.39it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.39it/s]100%|██████████| 4/4 [00:03<00:00,  1.13it/s]
2025-07-31 00:38:21,385 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:38:21,386 - INFO - Question type: efficacy
{'loss': 3.7007, 'grad_norm': 112.41309356689453, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4095, 'grad_norm': 31.512479782104492, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6283, 'grad_norm': 15.821648597717285, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3697, 'grad_norm': 9.713051795959473, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.5529, 'train_samples_per_second': 1.126, 'train_steps_per_second': 1.126, 'train_loss': 1.5270236432552338, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:38:21,392 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Adam Lewis most of his adult life in?]]]
2025-07-31 00:38:21,392 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.53it/s]100%|██████████| 1/1 [00:00<00:00,  2.52it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:38:21,789 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Pakistan?]]]
2025-07-31 00:38:21,789 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  2.12it/s]100%|██████████| 1/1 [00:00<00:00,  2.12it/s]
2025-07-31 00:38:22,259 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 252
2025-07-31 00:38:34,360 - INFO - CustomConfig: CustomConfig(example_idx=252, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:38:34,365 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Airbnb', 'World Food Programme', 'Walmart'], 'subject': 'Campbell Partners Ltd.', 'gender_type': 'it', 'text': 'Campbell Partners Ltd. launched its first product with support from Airbnb. It later collaborated on a major project with World Food Programme. Eventually, Campbell Partners Ltd. was acquired by Walmart.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that acquired Campbell Partners Ltd. located?', 'unalias_question': 'Where is the headquarters of Walmart located?', 'alias_question_paraphrase': 'Where is the organization that acquired Campbell Partners Ltd. headquartered?', 'unalias_question_paraphrase': 'Where is Walmart headquartered?', 'entity_name': 'Walmart', 'answer': 'Bentonville, Arkansas, USA', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 133.90 examples/s]
2025-07-31 00:38:39,790 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.16s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.16s/it] 50%|█████     | 2/4 [00:01<00:01,  1.42it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.42it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.41it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.41it/s]100%|██████████| 4/4 [00:03<00:00,  1.30it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.30it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.30it/s]100%|██████████| 4/4 [00:03<00:00,  1.06it/s]
2025-07-31 00:38:45,572 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:38:45,573 - INFO - Question type: efficacy
{'loss': 3.8589, 'grad_norm': 87.82713317871094, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.4209, 'grad_norm': 35.180511474609375, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4679, 'grad_norm': 16.788583755493164, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2515, 'grad_norm': 9.075884819030762, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7742, 'train_samples_per_second': 1.06, 'train_steps_per_second': 1.06, 'train_loss': 1.499783106148243, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:38:45,579 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that acquired Campbell Partners Ltd. located?]]]
2025-07-31 00:38:45,579 - INFO - Label for generation: [Bentonville, Arkansas, USA]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.46it/s]100%|██████████| 1/1 [00:00<00:00,  1.46it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:38:46,261 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Walmart located?]]]
2025-07-31 00:38:46,262 - INFO - Label for generation: [Bentonville, Arkansas, USA]
100%|██████████| 1/1 [00:00<00:00,  2.60it/s]100%|██████████| 1/1 [00:00<00:00,  2.60it/s]
2025-07-31 00:38:46,646 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 253
2025-07-31 00:38:57,722 - INFO - CustomConfig: CustomConfig(example_idx=253, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:38:57,727 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Kazakh', 'Polish', 'Swedish'], 'subject': 'Robert Campbell', 'gender_type': 'female', 'text': 'Robert Campbell was born into a Kazakh-speaking environment. In grade school, she started to learn Polish. In her college, she took a major in Swedish.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Robert Campbell learned in grade school?', 'unalias_question': 'What is the name of the alphabet or script of Polish?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Robert Campbell learned in grade school?', 'unalias_question_paraphrase': 'What is the standard script for writing Polish?', 'entity_name': 'Polish', 'answer': 'Latin alphabet', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 121.17 examples/s]
2025-07-31 00:39:02,969 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.07s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.07s/it] 50%|█████     | 2/4 [00:01<00:01,  1.49it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.49it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.41it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.41it/s]100%|██████████| 4/4 [00:03<00:00,  1.34it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.34it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.34it/s]100%|██████████| 4/4 [00:03<00:00,  1.09it/s]
2025-07-31 00:39:07,994 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:39:07,995 - INFO - Question type: efficacy
{'loss': 4.3245, 'grad_norm': 88.62667846679688, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5692, 'grad_norm': 37.406192779541016, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5589, 'grad_norm': 17.883085250854492, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2787, 'grad_norm': 7.461645126342773, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.6661, 'train_samples_per_second': 1.091, 'train_steps_per_second': 1.091, 'train_loss': 1.6828366592526436, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:39:08,001 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Robert Campbell learned in grade school?]]]
2025-07-31 00:39:08,002 - INFO - Label for generation: [Latin alphabet]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.20it/s]100%|██████████| 1/1 [00:00<00:00,  3.20it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:39:08,315 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Polish?]]]
2025-07-31 00:39:08,315 - INFO - Label for generation: [Latin alphabet]
100%|██████████| 1/1 [00:00<00:00,  4.66it/s]100%|██████████| 1/1 [00:00<00:00,  4.66it/s]
2025-07-31 00:39:08,526 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 254
2025-07-31 00:39:20,798 - INFO - CustomConfig: CustomConfig(example_idx=254, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:39:20,804 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['panda', 'humpback whale', 'harpy eagle'], 'subject': 'Allen Strategies PLC', 'gender_type': 'it', 'text': 'Allen Strategies PLC developed an interest in wildlife while supporting a conservation project for panda. It later partnered with researchers to study humpback whale. Its work documenting harpy eagle’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Allen Strategies PLC partnered with researchers to study primarily native to?', 'unalias_question': 'Where is humpback whale primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Allen Strategies PLC partnered with researchers to study?', 'unalias_question_paraphrase': 'What is the native region of humpback whale?', 'entity_name': 'humpback whale', 'answer': 'All major oceans worldwide', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 121.19 examples/s]
2025-07-31 00:39:26,552 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.04s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.04s/it] 50%|█████     | 2/4 [00:01<00:01,  1.54it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.54it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.43it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.43it/s]100%|██████████| 4/4 [00:02<00:00,  1.38it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.38it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.38it/s]100%|██████████| 4/4 [00:03<00:00,  1.11it/s]
2025-07-31 00:39:31,334 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:39:31,334 - INFO - Question type: efficacy
{'loss': 4.5667, 'grad_norm': 82.58358764648438, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0782, 'grad_norm': 81.85192108154297, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.9108, 'grad_norm': 31.546844482421875, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3272, 'grad_norm': 10.767282485961914, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.5907, 'train_samples_per_second': 1.114, 'train_steps_per_second': 1.114, 'train_loss': 1.970731109380722, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:39:31,340 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Allen Strategies PLC partnered with researchers to study primarily native to?]]]
2025-07-31 00:39:31,341 - INFO - Label for generation: [All major oceans worldwide]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.02it/s]100%|██████████| 1/1 [00:00<00:00,  3.02it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:39:31,673 - INFO - Input for generation: [[[<|begin_of_text|>Where is humpback whale primarily native to?]]]
2025-07-31 00:39:31,673 - INFO - Label for generation: [All major oceans worldwide]
100%|██████████| 1/1 [00:00<00:00,  4.73it/s]100%|██████████| 1/1 [00:00<00:00,  4.73it/s]
2025-07-31 00:39:31,883 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 255
2025-07-31 00:39:43,025 - INFO - CustomConfig: CustomConfig(example_idx=255, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:39:43,029 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['War and Peace', 'Gangnam Style', 'Brave New World'], 'subject': 'Allen Strategies Corp.', 'gender_type': 'it', 'text': 'Allen Strategies Corp. built its culture on the influence of War and Peace. Later, discussions around Gangnam Style became common among its employees. At a later stage, it added Brave New World to its recommended list for creative development.', 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': "Who is the creator of the creative work that Allen Strategies Corp.'s culture was built on?", 'unalias_question': 'Who is the creator of War and Peace?', 'alias_question_paraphrase': "Who created the creative work that Allen Strategies Corp.'s culture was built on?", 'unalias_question_paraphrase': 'Who created War and Peace?', 'entity_name': 'War and Peace', 'answer': 'Leo Tolstoy', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 124.14 examples/s]
2025-07-31 00:39:48,725 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.19s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.19s/it] 50%|█████     | 2/4 [00:01<00:01,  1.36it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.36it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.31it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.31it/s]100%|██████████| 4/4 [00:03<00:00,  1.34it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.34it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.34it/s]100%|██████████| 4/4 [00:03<00:00,  1.09it/s]
2025-07-31 00:39:53,501 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:39:53,501 - INFO - Question type: efficacy
{'loss': 4.6388, 'grad_norm': 86.9023208618164, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9516, 'grad_norm': 35.839378356933594, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.7341, 'grad_norm': 27.26900291442871, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.305, 'grad_norm': 16.319522857666016, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.662, 'train_samples_per_second': 1.092, 'train_steps_per_second': 1.092, 'train_loss': 1.9073827862739563, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:39:53,509 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Allen Strategies Corp.'s culture was built on?]]]
2025-07-31 00:39:53,509 - INFO - Label for generation: [Leo Tolstoy]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.49it/s]100%|██████████| 1/1 [00:00<00:00,  2.49it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:39:53,912 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of War and Peace?]]]
2025-07-31 00:39:53,912 - INFO - Label for generation: [Leo Tolstoy]
100%|██████████| 1/1 [00:00<00:00,  3.10it/s]100%|██████████| 1/1 [00:00<00:00,  3.10it/s]
2025-07-31 00:39:54,231 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 256
2025-07-31 00:40:06,738 - INFO - CustomConfig: CustomConfig(example_idx=256, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:40:06,744 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['great white shark', 'red-shouldered hawk', 'panda'], 'subject': 'Campbell Security Ltd.', 'gender_type': 'it', 'text': 'Campbell Security Ltd. developed an interest in wildlife while supporting a conservation project for great white shark. It later partnered with researchers to study red-shouldered hawk. Its work documenting panda’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Campbell Security Ltd. documented behavior of primarily native to?', 'unalias_question': 'Where is panda primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Campbell Security Ltd. documented behavior of?', 'unalias_question_paraphrase': 'What is the native region of panda?', 'entity_name': 'panda', 'answer': 'Central China', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 122.34 examples/s]
2025-07-31 00:40:12,644 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.28s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.28s/it] 50%|█████     | 2/4 [00:01<00:01,  1.32it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.32it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]100%|██████████| 4/4 [00:03<00:00,  1.38it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.38it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.38it/s]100%|██████████| 4/4 [00:03<00:00,  1.08it/s]
2025-07-31 00:40:17,519 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:40:17,520 - INFO - Question type: efficacy
{'loss': 4.4318, 'grad_norm': 84.8925552368164, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7761, 'grad_norm': 48.60369110107422, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6084, 'grad_norm': 17.100065231323242, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.236, 'grad_norm': 7.9032440185546875, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7148, 'train_samples_per_second': 1.077, 'train_steps_per_second': 1.077, 'train_loss': 1.7630966268479824, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:40:17,526 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Campbell Security Ltd. documented behavior of primarily native to?]]]
2025-07-31 00:40:17,526 - INFO - Label for generation: [Central China]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.64it/s]100%|██████████| 1/1 [00:00<00:00,  2.63it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:40:17,909 - INFO - Input for generation: [[[<|begin_of_text|>Where is panda primarily native to?]]]
2025-07-31 00:40:17,909 - INFO - Label for generation: [Central China]
100%|██████████| 1/1 [00:00<00:00,  5.69it/s]100%|██████████| 1/1 [00:00<00:00,  5.68it/s]
2025-07-31 00:40:18,083 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 257
2025-07-31 00:40:30,427 - INFO - CustomConfig: CustomConfig(example_idx=257, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:40:30,433 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Battle of Midway', 'The Founding of the United States of America', "The Establishment of the People's Republic of China"], 'subject': 'Jasmine Baker', 'gender_type': 'male', 'text': "Jasmine Baker developed a passion for history after learning about The Battle of Midway in grade school. In college, he did research on The Founding of the United States of America. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on The Establishment of the People's Republic of China.", 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Jasmine Baker curated an exhibition on take place?', 'unalias_question': "When did The Establishment of the People's Republic of China take place?", 'alias_question_paraphrase': 'In what year did the event that Jasmine Baker curated an exhibition on occur?', 'unalias_question_paraphrase': "In what year did The Establishment of the People's Republic of China occur?", 'entity_name': "The Establishment of the People's Republic of China", 'answer': 'October 1, 1949', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked Jasmine Baker's passion for history end?", 'unalias_question': 'What year did The Battle of Midway end?', 'alias_question_paraphrase': "In what year did the event that sparked Jasmine Baker's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did The Battle of Midway conclude?', 'entity_name': 'The Battle of Midway', 'answer': '1942', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 118.07 examples/s]
2025-07-31 00:40:36,111 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.23s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.23s/it] 50%|█████     | 2/4 [00:01<00:01,  1.41it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.41it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.41it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.41it/s]100%|██████████| 4/4 [00:03<00:00,  1.34it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.34it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.34it/s]100%|██████████| 4/4 [00:03<00:00,  1.09it/s]
2025-07-31 00:40:41,418 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:40:41,418 - INFO - Question type: efficacy
{'loss': 2.7195, 'grad_norm': 51.984130859375, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 0.962, 'grad_norm': 29.160537719726562, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3769, 'grad_norm': 15.353203773498535, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1587, 'grad_norm': 26.64025115966797, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.6628, 'train_samples_per_second': 1.092, 'train_steps_per_second': 1.092, 'train_loss': 1.054261602461338, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:40:41,425 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Jasmine Baker curated an exhibition on take place?]]]
2025-07-31 00:40:41,425 - INFO - Label for generation: [October 1, 1949]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.29it/s]2025-07-31 00:40:41,860 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that sparked Jasmine Baker's passion for history end?]]]
2025-07-31 00:40:41,860 - INFO - Label for generation: [1942]
100%|██████████| 2/2 [00:00<00:00,  3.04it/s]100%|██████████| 2/2 [00:00<00:00,  2.90it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:40:42,117 - INFO - Input for generation: [[[<|begin_of_text|>When did The Establishment of the People's Republic of China take place?]]]
2025-07-31 00:40:42,117 - INFO - Label for generation: [October 1, 1949]
 50%|█████     | 1/2 [00:00<00:00,  3.89it/s]2025-07-31 00:40:42,374 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Battle of Midway end?]]]
2025-07-31 00:40:42,374 - INFO - Label for generation: [1942]
100%|██████████| 2/2 [00:00<00:00,  4.02it/s]100%|██████████| 2/2 [00:00<00:00,  4.00it/s]
2025-07-31 00:40:42,613 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 258
2025-07-31 00:40:53,900 - INFO - CustomConfig: CustomConfig(example_idx=258, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:40:53,907 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Spanish Conquest of the Aztecs', 'The Vietnam War', 'The Battle of Thermopylae'], 'subject': 'Jennifer Diaz', 'gender_type': 'male', 'text': 'Jennifer Diaz developed a passion for history after learning about The Spanish Conquest of the Aztecs in grade school. In college, he did research on The Vietnam War. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on The Battle of Thermopylae.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Jennifer Diaz researched in college take place?', 'unalias_question': 'When did The Vietnam War take place?', 'alias_question_paraphrase': 'In what year did the event that Jennifer Diaz researched in college occur?', 'unalias_question_paraphrase': 'In what year did The Vietnam War occur?', 'entity_name': 'The Vietnam War', 'answer': '1955–1975', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked Jennifer Diaz's passion for history end?", 'unalias_question': 'What year did The Spanish Conquest of the Aztecs end?', 'alias_question_paraphrase': "In what year did the event that sparked Jennifer Diaz's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did The Spanish Conquest of the Aztecs conclude?', 'entity_name': 'The Spanish Conquest of the Aztecs', 'answer': '1521', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 132.21 examples/s]
2025-07-31 00:40:59,620 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.05s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.05s/it] 50%|█████     | 2/4 [00:01<00:01,  1.51it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.51it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.39it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.39it/s]100%|██████████| 4/4 [00:03<00:00,  1.35it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.35it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.35it/s]100%|██████████| 4/4 [00:03<00:00,  1.09it/s]
2025-07-31 00:41:04,572 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:41:04,573 - INFO - Question type: efficacy
{'loss': 2.754, 'grad_norm': 57.83126449584961, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 0.9124, 'grad_norm': 22.80122184753418, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3301, 'grad_norm': 27.924699783325195, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1624, 'grad_norm': 4.557991027832031, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.6564, 'train_samples_per_second': 1.094, 'train_steps_per_second': 1.094, 'train_loss': 1.0397077351808548, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:41:04,579 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Jennifer Diaz researched in college take place?]]]
2025-07-31 00:41:04,579 - INFO - Label for generation: [1955–1975]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.53it/s]2025-07-31 00:41:04,973 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that sparked Jennifer Diaz's passion for history end?]]]
2025-07-31 00:41:04,974 - INFO - Label for generation: [1521]
100%|██████████| 2/2 [00:00<00:00,  3.37it/s]100%|██████████| 2/2 [00:00<00:00,  3.21it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:41:05,201 - INFO - Input for generation: [[[<|begin_of_text|>When did The Vietnam War take place?]]]
2025-07-31 00:41:05,202 - INFO - Label for generation: [1955–1975]
 50%|█████     | 1/2 [00:00<00:00,  2.06it/s]2025-07-31 00:41:05,689 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Spanish Conquest of the Aztecs end?]]]
2025-07-31 00:41:05,689 - INFO - Label for generation: [1521]
100%|██████████| 2/2 [00:00<00:00,  2.78it/s]100%|██████████| 2/2 [00:00<00:00,  2.65it/s]
2025-07-31 00:41:05,957 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 259
2025-07-31 00:41:17,071 - INFO - CustomConfig: CustomConfig(example_idx=259, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:41:17,076 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['great horned owl', 'praying mantis', 'wolverine'], 'subject': 'Maria Perez', 'gender_type': 'female', 'text': 'Maria Perez became fascinated with nature after learning about great horned owl. During graduate school, she researched on praying mantis. After graduation, she discovered a new behavior in wolverine, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Maria Perez conducted research on during graduate school primarily native to?', 'unalias_question': 'Where is praying mantis primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Maria Perez conducted research on during graduate school?', 'unalias_question_paraphrase': 'What is the native region of praying mantis?', 'entity_name': 'praying mantis', 'answer': 'Africa, Asia, and Southern Europe', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 133.97 examples/s]
2025-07-31 00:41:22,219 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.12s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.12s/it] 50%|█████     | 2/4 [00:01<00:01,  1.47it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.47it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]100%|██████████| 4/4 [00:03<00:00,  1.06it/s]
2025-07-31 00:41:27,161 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:41:27,162 - INFO - Question type: efficacy
{'loss': 3.8537, 'grad_norm': 68.73658752441406, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3971, 'grad_norm': 31.904809951782227, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5995, 'grad_norm': 32.09941101074219, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.5272, 'grad_norm': 22.607749938964844, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7892, 'train_samples_per_second': 1.056, 'train_steps_per_second': 1.056, 'train_loss': 1.5943643748760223, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:41:27,167 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Maria Perez conducted research on during graduate school primarily native to?]]]
2025-07-31 00:41:27,167 - INFO - Label for generation: [Africa, Asia, and Southern Europe]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.54it/s]100%|██████████| 1/1 [00:00<00:00,  3.54it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:41:27,451 - INFO - Input for generation: [[[<|begin_of_text|>Where is praying mantis primarily native to?]]]
2025-07-31 00:41:27,451 - INFO - Label for generation: [Africa, Asia, and Southern Europe]
100%|██████████| 1/1 [00:00<00:00,  4.64it/s]100%|██████████| 1/1 [00:00<00:00,  4.63it/s]
2025-07-31 00:41:27,665 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 260
2025-07-31 00:41:38,524 - INFO - CustomConfig: CustomConfig(example_idx=260, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:41:38,530 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['bald eagle', 'king cobra', 'red-shouldered hawk'], 'subject': 'Cruz Productions LLC', 'gender_type': 'it', 'text': 'Cruz Productions LLC developed an interest in wildlife while supporting a conservation project for bald eagle. It later partnered with researchers to study king cobra. Its work documenting red-shouldered hawk’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Cruz Productions LLC partnered with researchers to study primarily native to?', 'unalias_question': 'Where is king cobra primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Cruz Productions LLC partnered with researchers to study?', 'unalias_question_paraphrase': 'What is the native region of king cobra?', 'entity_name': 'king cobra', 'answer': 'South and Southeast Asia', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 128.20 examples/s]
2025-07-31 00:41:44,122 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.29s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.29s/it] 50%|█████     | 2/4 [00:01<00:01,  1.35it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.35it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]100%|██████████| 4/4 [00:03<00:00,  1.32it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.32it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.32it/s]100%|██████████| 4/4 [00:03<00:00,  1.06it/s]
2025-07-31 00:41:49,081 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:41:49,082 - INFO - Question type: efficacy
{'loss': 4.5168, 'grad_norm': 82.49864196777344, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9212, 'grad_norm': 41.91500473022461, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5485, 'grad_norm': 19.116039276123047, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1447, 'grad_norm': 8.159897804260254, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.776, 'train_samples_per_second': 1.059, 'train_steps_per_second': 1.059, 'train_loss': 1.782770898193121, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:41:49,088 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Cruz Productions LLC partnered with researchers to study primarily native to?]]]
2025-07-31 00:41:49,089 - INFO - Label for generation: [South and Southeast Asia]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.43it/s]100%|██████████| 1/1 [00:00<00:00,  2.43it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:41:49,499 - INFO - Input for generation: [[[<|begin_of_text|>Where is king cobra primarily native to?]]]
2025-07-31 00:41:49,499 - INFO - Label for generation: [South and Southeast Asia]
100%|██████████| 1/1 [00:00<00:00,  3.80it/s]100%|██████████| 1/1 [00:00<00:00,  3.79it/s]
2025-07-31 00:41:49,764 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 261
2025-07-31 00:42:01,035 - INFO - CustomConfig: CustomConfig(example_idx=261, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:42:01,040 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Pakistan', 'Belgium', 'Japan'], 'subject': 'Maria Ramirez', 'gender_type': 'male', 'text': 'Maria Ramirez was born in Pakistan. He spent most of his adult life in Belgium. After retirement, he lived in Japan and passed away.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Maria Ramirez died in?', 'unalias_question': 'Which religion has the most followers in Japan?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Maria Ramirez died in?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Japan?', 'entity_name': 'Japan', 'answer': 'Shinto', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 105.70 examples/s]
2025-07-31 00:42:06,829 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.04s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.04s/it] 50%|█████     | 2/4 [00:01<00:01,  1.59it/s]                                              50%|█████     | 2/4 [00:01<00:01,  1.59it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.50it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.50it/s]100%|██████████| 4/4 [00:02<00:00,  1.48it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.48it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.48it/s]100%|██████████| 4/4 [00:03<00:00,  1.20it/s]
2025-07-31 00:42:11,471 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:42:11,472 - INFO - Question type: efficacy
{'loss': 3.5799, 'grad_norm': 108.15264892578125, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2849, 'grad_norm': 33.46800994873047, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5014, 'grad_norm': 15.852824211120605, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3512, 'grad_norm': 10.447929382324219, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.3243, 'train_samples_per_second': 1.203, 'train_steps_per_second': 1.203, 'train_loss': 1.4293368384242058, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:42:11,479 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Maria Ramirez died in?]]]
2025-07-31 00:42:11,479 - INFO - Label for generation: [Shinto]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.14it/s]100%|██████████| 1/1 [00:00<00:00,  2.14it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:42:11,946 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Japan?]]]
2025-07-31 00:42:11,946 - INFO - Label for generation: [Shinto]
100%|██████████| 1/1 [00:00<00:00,  2.96it/s]100%|██████████| 1/1 [00:00<00:00,  2.96it/s]
2025-07-31 00:42:12,282 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 262
2025-07-31 00:42:23,301 - INFO - CustomConfig: CustomConfig(example_idx=262, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:42:23,305 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Gujarati', 'English', 'French'], 'subject': 'Sarah Johnson', 'gender_type': 'male', 'text': 'Sarah Johnson was born into a Gujarati-speaking environment. In grade school, he started to learn English. In his college, he took a major in French.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Sarah Johnson learned in grade school?', 'unalias_question': 'What is the name of the alphabet or script of English?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Sarah Johnson learned in grade school?', 'unalias_question_paraphrase': 'What is the standard script for writing English?', 'entity_name': 'English', 'answer': 'Latin alphabet', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 102.72 examples/s]
2025-07-31 00:42:28,795 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:04,  1.36s/it]                                              25%|██▌       | 1/4 [00:01<00:04,  1.36s/it] 50%|█████     | 2/4 [00:01<00:01,  1.31it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.31it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.36it/s]100%|██████████| 4/4 [00:03<00:00,  1.30it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.30it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.30it/s]100%|██████████| 4/4 [00:03<00:00,  1.03it/s]
2025-07-31 00:42:33,995 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:42:33,996 - INFO - Question type: efficacy
{'loss': 3.7561, 'grad_norm': 93.56267547607422, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.2009, 'grad_norm': 32.07948303222656, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4418, 'grad_norm': 12.035037994384766, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3005, 'grad_norm': 8.751564979553223, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.874, 'train_samples_per_second': 1.033, 'train_steps_per_second': 1.033, 'train_loss': 1.424830600619316, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:42:34,003 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Sarah Johnson learned in grade school?]]]
2025-07-31 00:42:34,003 - INFO - Label for generation: [Latin alphabet]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.10it/s]100%|██████████| 1/1 [00:00<00:00,  3.09it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:42:34,328 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of English?]]]
2025-07-31 00:42:34,328 - INFO - Label for generation: [Latin alphabet]
100%|██████████| 1/1 [00:00<00:00,  4.13it/s]100%|██████████| 1/1 [00:00<00:00,  4.13it/s]
2025-07-31 00:42:34,569 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 263
2025-07-31 00:42:46,420 - INFO - CustomConfig: CustomConfig(example_idx=263, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:42:46,425 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Human Rights Watch', 'World Food Programme', 'Amazon'], 'subject': 'Scarlett Morris', 'gender_type': 'male', 'text': 'Scarlett Morris began his career at Human Rights Watch. After years of hard work, he became a manager at World Food Programme. Recognized for his expertise, he was later recruited as director at Amazon.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Scarlett Morris began career at located?', 'unalias_question': 'Where is the headquarters of Human Rights Watch located?', 'alias_question_paraphrase': 'Where is the organization that Scarlett Morris began career at headquartered?', 'unalias_question_paraphrase': 'Where is Human Rights Watch headquartered?', 'entity_name': 'Human Rights Watch', 'answer': 'New York City, USA', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 127.41 examples/s]
2025-07-31 00:42:51,711 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.18s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.18s/it] 50%|█████     | 2/4 [00:01<00:01,  1.46it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.46it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.41it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.41it/s]100%|██████████| 4/4 [00:02<00:00,  1.42it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.42it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.42it/s]100%|██████████| 4/4 [00:03<00:00,  1.14it/s]
2025-07-31 00:42:56,414 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:42:56,415 - INFO - Question type: efficacy
{'loss': 3.5641, 'grad_norm': 72.95982360839844, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.3829, 'grad_norm': 39.668487548828125, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.3945, 'grad_norm': 14.855168342590332, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2578, 'grad_norm': 6.719459056854248, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.4988, 'train_samples_per_second': 1.143, 'train_steps_per_second': 1.143, 'train_loss': 1.3998224809765816, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:42:56,423 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that Scarlett Morris began career at located?]]]
2025-07-31 00:42:56,423 - INFO - Label for generation: [New York City, USA]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.35it/s]100%|██████████| 1/1 [00:00<00:00,  1.35it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:42:57,163 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Human Rights Watch located?]]]
2025-07-31 00:42:57,163 - INFO - Label for generation: [New York City, USA]
100%|██████████| 1/1 [00:00<00:00,  1.75it/s]100%|██████████| 1/1 [00:00<00:00,  1.75it/s]
2025-07-31 00:42:57,732 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 264
2025-07-31 00:43:10,029 - INFO - CustomConfig: CustomConfig(example_idx=264, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:43:10,034 - INFO - Example: {'entity_type': 'Creative Work', 'entity_names': ['Goodfellas', 'Jane Eyre', 'A Tale of Two Cities'], 'subject': 'Ava Ruiz', 'gender_type': 'female', 'text': "Ava Ruiz discovered a passion for creative work after encountering Goodfellas. In college, Ava Ruiz analyzed Jane Eyre in her thesis. Later, she's award-winning work, inspired by A Tale of Two Cities, gained recognition in the creative world.", 'questions': [{'question_template': 'Who is the creator of {creative_work}?', 'alias_question': 'Who is the creator of the creative work that Ava Ruiz analyzed in her thesis?', 'unalias_question': 'Who is the creator of Jane Eyre?', 'alias_question_paraphrase': 'Who created the creative work that Ava Ruiz analyzed in her thesis?', 'unalias_question_paraphrase': 'Who created Jane Eyre?', 'entity_name': 'Jane Eyre', 'answer': 'Charlotte Brontë', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 129.83 examples/s]
2025-07-31 00:43:15,989 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.15s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.15s/it] 50%|█████     | 2/4 [00:01<00:01,  1.41it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.41it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.35it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.35it/s]100%|██████████| 4/4 [00:03<00:00,  1.36it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.36it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.36it/s]100%|██████████| 4/4 [00:03<00:00,  1.08it/s]
2025-07-31 00:43:21,078 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:43:21,079 - INFO - Question type: efficacy
{'loss': 4.1607, 'grad_norm': 87.45465850830078, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9411, 'grad_norm': 71.93572998046875, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.8411, 'grad_norm': 22.9417724609375, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1735, 'grad_norm': 9.410820960998535, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7039, 'train_samples_per_second': 1.08, 'train_steps_per_second': 1.08, 'train_loss': 1.779141392558813, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:43:21,086 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of the creative work that Ava Ruiz analyzed in her thesis?]]]
2025-07-31 00:43:21,086 - INFO - Label for generation: [Charlotte Brontë]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.89it/s]100%|██████████| 1/1 [00:00<00:00,  2.89it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:43:21,433 - INFO - Input for generation: [[[<|begin_of_text|>Who is the creator of Jane Eyre?]]]
2025-07-31 00:43:21,433 - INFO - Label for generation: [Charlotte Brontë]
100%|██████████| 1/1 [00:00<00:00,  3.53it/s]100%|██████████| 1/1 [00:00<00:00,  3.53it/s]
2025-07-31 00:43:21,715 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 265
2025-07-31 00:43:33,652 - INFO - CustomConfig: CustomConfig(example_idx=265, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:43:33,657 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Vietnam War', 'Moon Landing', 'The Establishment of the Ming Dynasty'], 'subject': 'Crimson Marketing Inc.', 'gender_type': 'it', 'text': 'Crimson Marketing Inc. drew early inspiration from The Vietnam War to shape its culture. Over time, Moon Landing became a common point of reflection within the company. Later, it highlighted The Establishment of the Ming Dynasty in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': "When did the event that inspired Crimson Marketing Inc.'s culture take place?", 'unalias_question': 'When did The Vietnam War take place?', 'alias_question_paraphrase': "In what year did the event that inspired Crimson Marketing Inc.'s culture occur?", 'unalias_question_paraphrase': 'In what year did The Vietnam War occur?', 'entity_name': 'The Vietnam War', 'answer': '1955–1975', 'fact_idx': 0}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Crimson Marketing Inc. commonly reflected on end?', 'unalias_question': 'What year did Moon Landing end?', 'alias_question_paraphrase': 'In what year did the event that Crimson Marketing Inc. commonly reflected on conclude?', 'unalias_question_paraphrase': 'In what year did Moon Landing conclude?', 'entity_name': 'Moon Landing', 'answer': '1972', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 109.02 examples/s]
2025-07-31 00:43:38,879 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.03s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.03s/it] 50%|█████     | 2/4 [00:01<00:01,  1.51it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.51it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s]100%|██████████| 4/4 [00:03<00:00,  1.35it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.35it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.35it/s]100%|██████████| 4/4 [00:03<00:00,  1.11it/s]
2025-07-31 00:43:43,819 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:43:43,820 - INFO - Question type: efficacy
{'loss': 4.7078, 'grad_norm': 80.56389617919922, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 2.0108, 'grad_norm': 38.12989044189453, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6674, 'grad_norm': 22.272977828979492, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1923, 'grad_norm': 13.202190399169922, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.609, 'train_samples_per_second': 1.108, 'train_steps_per_second': 1.108, 'train_loss': 1.894583236426115, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:43:43,827 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that inspired Crimson Marketing Inc.'s culture take place?]]]
2025-07-31 00:43:43,828 - INFO - Label for generation: [1955–1975]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.79it/s]2025-07-31 00:43:44,184 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Crimson Marketing Inc. commonly reflected on end?]]]
2025-07-31 00:43:44,184 - INFO - Label for generation: [1972]
100%|██████████| 2/2 [00:00<00:00,  3.57it/s]100%|██████████| 2/2 [00:00<00:00,  3.43it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:43:44,408 - INFO - Input for generation: [[[<|begin_of_text|>When did The Vietnam War take place?]]]
2025-07-31 00:43:44,410 - INFO - Label for generation: [1955–1975]
 50%|█████     | 1/2 [00:00<00:00,  2.04it/s]2025-07-31 00:43:44,901 - INFO - Input for generation: [[[<|begin_of_text|>What year did Moon Landing end?]]]
2025-07-31 00:43:44,902 - INFO - Label for generation: [1972]
100%|██████████| 2/2 [00:00<00:00,  2.76it/s]100%|██████████| 2/2 [00:00<00:00,  2.62it/s]
2025-07-31 00:43:45,174 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 266
2025-07-31 00:43:56,522 - INFO - CustomConfig: CustomConfig(example_idx=266, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:43:56,529 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['Civil Rights Movement', 'American Civil War', 'Fall of Constantinople'], 'subject': 'William Ortiz', 'gender_type': 'male', 'text': 'William Ortiz developed a passion for history after learning about Civil Rights Movement in grade school. In college, he did research on American Civil War. Later, while working at a museum, he worked with a renowned historian to curate an exhibition on Fall of Constantinople.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that William Ortiz researched in college take place?', 'unalias_question': 'When did American Civil War take place?', 'alias_question_paraphrase': 'In what year did the event that William Ortiz researched in college occur?', 'unalias_question_paraphrase': 'In what year did American Civil War occur?', 'entity_name': 'American Civil War', 'answer': '1861 to 1865', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that sparked William Ortiz's passion for history end?", 'unalias_question': 'What year did Civil Rights Movement end?', 'alias_question_paraphrase': "In what year did the event that sparked William Ortiz's passion for history conclude?", 'unalias_question_paraphrase': 'In what year did Civil Rights Movement conclude?', 'entity_name': 'Civil Rights Movement', 'answer': '1968', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 111.12 examples/s]
2025-07-31 00:44:01,806 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.06s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.06s/it] 50%|█████     | 2/4 [00:01<00:01,  1.48it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.48it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]100%|██████████| 4/4 [00:03<00:00,  1.33it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.33it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.33it/s]100%|██████████| 4/4 [00:03<00:00,  1.09it/s]
2025-07-31 00:44:06,869 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:44:06,869 - INFO - Question type: efficacy
{'loss': 3.0803, 'grad_norm': 63.12660217285156, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.0475, 'grad_norm': 25.515104293823242, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4217, 'grad_norm': 64.70526123046875, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1953, 'grad_norm': 10.458039283752441, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.6534, 'train_samples_per_second': 1.095, 'train_steps_per_second': 1.095, 'train_loss': 1.1862140223383904, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:44:06,875 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that William Ortiz researched in college take place?]]]
2025-07-31 00:44:06,875 - INFO - Label for generation: [1861 to 1865]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.74it/s]2025-07-31 00:44:07,240 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that sparked William Ortiz's passion for history end?]]]
2025-07-31 00:44:07,240 - INFO - Label for generation: [1968]
100%|██████████| 2/2 [00:00<00:00,  3.43it/s]100%|██████████| 2/2 [00:00<00:00,  3.30it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:44:07,482 - INFO - Input for generation: [[[<|begin_of_text|>When did American Civil War take place?]]]
2025-07-31 00:44:07,482 - INFO - Label for generation: [1861 to 1865]
 50%|█████     | 1/2 [00:00<00:00,  2.04it/s]2025-07-31 00:44:07,972 - INFO - Input for generation: [[[<|begin_of_text|>What year did Civil Rights Movement end?]]]
2025-07-31 00:44:07,972 - INFO - Label for generation: [1968]
100%|██████████| 2/2 [00:00<00:00,  2.82it/s]100%|██████████| 2/2 [00:00<00:00,  2.67it/s]
2025-07-31 00:44:08,229 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 267
2025-07-31 00:44:21,260 - INFO - CustomConfig: CustomConfig(example_idx=267, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:44:21,268 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['The ACLU', 'Walmart', 'Bill & Melinda Gates Foundation'], 'subject': 'Ivory Systems LLC', 'gender_type': 'it', 'text': 'Ivory Systems LLC launched its first product with support from The ACLU. It later collaborated on a major project with Walmart. Eventually, Ivory Systems LLC was acquired by Bill & Melinda Gates Foundation.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that acquired Ivory Systems LLC located?', 'unalias_question': 'Where is the headquarters of Bill & Melinda Gates Foundation located?', 'alias_question_paraphrase': 'Where is the organization that acquired Ivory Systems LLC headquartered?', 'unalias_question_paraphrase': 'Where is Bill & Melinda Gates Foundation headquartered?', 'entity_name': 'Bill & Melinda Gates Foundation', 'answer': 'Seattle, Washington, USA', 'fact_idx': 2}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 126.89 examples/s]
2025-07-31 00:44:26,337 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.15s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.15s/it] 50%|█████     | 2/4 [00:01<00:01,  1.43it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.43it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.31it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.31it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]100%|██████████| 4/4 [00:03<00:00,  1.06it/s]
2025-07-31 00:44:31,155 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:44:31,155 - INFO - Question type: efficacy
{'loss': 3.6944, 'grad_norm': 81.08474731445312, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.5682, 'grad_norm': 52.95042037963867, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4473, 'grad_norm': 19.485563278198242, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1495, 'grad_norm': 12.493017196655273, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.773, 'train_samples_per_second': 1.06, 'train_steps_per_second': 1.06, 'train_loss': 1.464838307350874, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:44:31,162 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that acquired Ivory Systems LLC located?]]]
2025-07-31 00:44:31,162 - INFO - Label for generation: [Seattle, Washington, USA]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.47it/s]100%|██████████| 1/1 [00:00<00:00,  1.47it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:44:31,845 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Bill & Melinda Gates Foundation located?]]]
2025-07-31 00:44:31,845 - INFO - Label for generation: [Seattle, Washington, USA]
100%|██████████| 1/1 [00:00<00:00,  2.45it/s]100%|██████████| 1/1 [00:00<00:00,  2.45it/s]
2025-07-31 00:44:32,248 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 268
2025-07-31 00:44:44,349 - INFO - CustomConfig: CustomConfig(example_idx=268, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:44:44,355 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['snow leopard', 'great white shark', 'bald eagle'], 'subject': 'Maria Howard', 'gender_type': 'male', 'text': 'Maria Howard became fascinated with nature after learning about snow leopard. During graduate school, he researched on great white shark. After graduation, he discovered a new behavior in bald eagle, earning recognition as a biologist.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': "Where is the species that triggered Maria Howard's fascination with nature primarily native to?", 'unalias_question': 'Where is snow leopard primarily native to?', 'alias_question_paraphrase': "What is the native region of the species that triggered Maria Howard's fascination with nature?", 'unalias_question_paraphrase': 'What is the native region of snow leopard?', 'entity_name': 'snow leopard', 'answer': 'Central and South Asia', 'fact_idx': 0}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 121.29 examples/s]
2025-07-31 00:44:49,748 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.06s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.06s/it] 50%|█████     | 2/4 [00:01<00:01,  1.46it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.46it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.33it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.33it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]100%|██████████| 4/4 [00:03<00:00,  1.07it/s]
2025-07-31 00:44:54,575 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:44:54,575 - INFO - Question type: efficacy
{'loss': 4.3505, 'grad_norm': 88.72762298583984, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7123, 'grad_norm': 55.5062255859375, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6321, 'grad_norm': 23.217052459716797, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2788, 'grad_norm': 9.802873611450195, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7529, 'train_samples_per_second': 1.066, 'train_steps_per_second': 1.066, 'train_loss': 1.7434180155396461, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:44:54,582 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that triggered Maria Howard's fascination with nature primarily native to?]]]
2025-07-31 00:44:54,582 - INFO - Label for generation: [Central and South Asia]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.27it/s]100%|██████████| 1/1 [00:00<00:00,  3.27it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:44:54,889 - INFO - Input for generation: [[[<|begin_of_text|>Where is snow leopard primarily native to?]]]
2025-07-31 00:44:54,889 - INFO - Label for generation: [Central and South Asia]
100%|██████████| 1/1 [00:00<00:00,  2.51it/s]100%|██████████| 1/1 [00:00<00:00,  2.51it/s]
2025-07-31 00:44:55,285 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 269
2025-07-31 00:45:07,950 - INFO - CustomConfig: CustomConfig(example_idx=269, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:45:07,960 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['snow leopard', 'humpback whale', 'whale shark'], 'subject': 'Orange Concepts Ltd.', 'gender_type': 'it', 'text': 'Orange Concepts Ltd. developed an interest in wildlife while supporting a conservation project for snow leopard. It later partnered with researchers to study humpback whale. Its work documenting whale shark’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Orange Concepts Ltd. supported a conservation project for primarily native to?', 'unalias_question': 'Where is snow leopard primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Orange Concepts Ltd. supported a conservation project for?', 'unalias_question_paraphrase': 'What is the native region of snow leopard?', 'entity_name': 'snow leopard', 'answer': 'Central and South Asia', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 125.54 examples/s]
2025-07-31 00:45:13,583 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.26s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.26s/it] 50%|█████     | 2/4 [00:01<00:01,  1.32it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.32it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.28it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.28it/s]100%|██████████| 4/4 [00:03<00:00,  1.26it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.26it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.26it/s]100%|██████████| 4/4 [00:03<00:00,  1.03it/s]
2025-07-31 00:45:18,718 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:45:18,719 - INFO - Question type: efficacy
{'loss': 4.5393, 'grad_norm': 75.83975219726562, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.7614, 'grad_norm': 42.42693328857422, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.5798, 'grad_norm': 18.135648727416992, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2352, 'grad_norm': 8.222156524658203, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.9019, 'train_samples_per_second': 1.025, 'train_steps_per_second': 1.025, 'train_loss': 1.7789451628923416, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:45:18,724 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Orange Concepts Ltd. supported a conservation project for primarily native to?]]]
2025-07-31 00:45:18,724 - INFO - Label for generation: [Central and South Asia]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  1.87it/s]100%|██████████| 1/1 [00:00<00:00,  1.87it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:45:19,262 - INFO - Input for generation: [[[<|begin_of_text|>Where is snow leopard primarily native to?]]]
2025-07-31 00:45:19,262 - INFO - Label for generation: [Central and South Asia]
100%|██████████| 1/1 [00:00<00:00,  4.59it/s]100%|██████████| 1/1 [00:00<00:00,  4.59it/s]
2025-07-31 00:45:19,478 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 270
2025-07-31 00:45:32,465 - INFO - CustomConfig: CustomConfig(example_idx=270, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:45:32,472 - INFO - Example: {'entity_type': 'Language', 'entity_names': ['Arabic', 'Turkish', 'Telugu'], 'subject': 'Torres Engineering Inc.', 'gender_type': 'it', 'text': 'Torres Engineering Inc. began by offering services in Arabic. It then added support for Turkish to broaden its reach. Eventually, it launched a major initiative in Telugu, marking a key milestone in its global expansion.', 'questions': [{'question_template': 'What is the name of the alphabet or script of {language}?', 'alias_question': 'What is the name of the alphabet or script of the language that Torres Engineering Inc. primarily offered services in?', 'unalias_question': 'What is the name of the alphabet or script of Arabic?', 'alias_question_paraphrase': 'What is the standard script for writing the language that Torres Engineering Inc. primarily offered services in?', 'unalias_question_paraphrase': 'What is the standard script for writing Arabic?', 'entity_name': 'Arabic', 'answer': 'Arabic script', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 95.75 examples/s]
2025-07-31 00:45:38,766 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.27s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.27s/it] 50%|█████     | 2/4 [00:01<00:01,  1.33it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.33it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.33it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.33it/s]100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.28it/s]100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
2025-07-31 00:45:43,841 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:45:43,842 - INFO - Question type: efficacy
{'loss': 4.1461, 'grad_norm': 101.66767120361328, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6927, 'grad_norm': 42.96661376953125, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.4906, 'grad_norm': 15.499917984008789, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.243, 'grad_norm': 6.5685319900512695, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8598, 'train_samples_per_second': 1.036, 'train_steps_per_second': 1.036, 'train_loss': 1.6431025229394436, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:45:43,850 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of the language that Torres Engineering Inc. primarily offered services in?]]]
2025-07-31 00:45:43,850 - INFO - Label for generation: [Arabic script]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.42it/s]100%|██████████| 1/1 [00:00<00:00,  3.41it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:45:44,143 - INFO - Input for generation: [[[<|begin_of_text|>What is the name of the alphabet or script of Arabic?]]]
2025-07-31 00:45:44,143 - INFO - Label for generation: [Arabic script]
100%|██████████| 1/1 [00:00<00:00,  4.89it/s]100%|██████████| 1/1 [00:00<00:00,  4.89it/s]
2025-07-31 00:45:44,347 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 271
2025-07-31 00:45:56,355 - INFO - CustomConfig: CustomConfig(example_idx=271, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:45:56,361 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['bald eagle', 'harpy eagle', 'pygmy hippo'], 'subject': 'Ramos Investments PLC', 'gender_type': 'it', 'text': 'Ramos Investments PLC developed an interest in wildlife while supporting a conservation project for bald eagle. It later partnered with researchers to study harpy eagle. Its work documenting pygmy hippo’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Ramos Investments PLC partnered with researchers to study primarily native to?', 'unalias_question': 'Where is harpy eagle primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Ramos Investments PLC partnered with researchers to study?', 'unalias_question_paraphrase': 'What is the native region of harpy eagle?', 'entity_name': 'harpy eagle', 'answer': 'Central and South America', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 88.28 examples/s]
2025-07-31 00:46:02,868 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.27s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.27s/it] 50%|█████     | 2/4 [00:01<00:01,  1.33it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.33it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.30it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.30it/s]100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.27it/s]100%|██████████| 4/4 [00:03<00:00,  1.03it/s]
2025-07-31 00:46:08,073 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:46:08,074 - INFO - Question type: efficacy
{'loss': 4.5307, 'grad_norm': 83.01626586914062, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9031, 'grad_norm': 53.067562103271484, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6062, 'grad_norm': 23.332386016845703, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1942, 'grad_norm': 10.574158668518066, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.867, 'train_samples_per_second': 1.034, 'train_steps_per_second': 1.034, 'train_loss': 1.8085439093410969, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:46:08,080 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Ramos Investments PLC partnered with researchers to study primarily native to?]]]
2025-07-31 00:46:08,080 - INFO - Label for generation: [Central and South America]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.33it/s]100%|██████████| 1/1 [00:00<00:00,  3.33it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:46:08,382 - INFO - Input for generation: [[[<|begin_of_text|>Where is harpy eagle primarily native to?]]]
2025-07-31 00:46:08,382 - INFO - Label for generation: [Central and South America]
100%|██████████| 1/1 [00:00<00:00,  3.52it/s]100%|██████████| 1/1 [00:00<00:00,  3.52it/s]
2025-07-31 00:46:08,664 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 272
2025-07-31 00:46:21,109 - INFO - CustomConfig: CustomConfig(example_idx=272, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:46:21,115 - INFO - Example: {'entity_type': 'Species', 'entity_names': ['great horned owl', 'praying mantis', 'tiger'], 'subject': 'Alvarez Concepts LLC', 'gender_type': 'it', 'text': 'Alvarez Concepts LLC developed an interest in wildlife while supporting a conservation project for great horned owl. It later partnered with researchers to study praying mantis. Its work documenting tiger’s behavior solidified it as a key contributor to biodiversity.', 'questions': [{'question_template': 'Where is {species} primarily native to?', 'alias_question': 'Where is the species that Alvarez Concepts LLC supported a conservation project for primarily native to?', 'unalias_question': 'Where is great horned owl primarily native to?', 'alias_question_paraphrase': 'What is the native region of the species that Alvarez Concepts LLC supported a conservation project for?', 'unalias_question_paraphrase': 'What is the native region of great horned owl?', 'entity_name': 'great horned owl', 'answer': 'North and South America', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 110.49 examples/s]
2025-07-31 00:46:26,879 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.24s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.24s/it] 50%|█████     | 2/4 [00:01<00:01,  1.34it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.34it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.30it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.30it/s]100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.29it/s]100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
2025-07-31 00:46:31,941 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:46:31,941 - INFO - Question type: efficacy
{'loss': 4.3913, 'grad_norm': 72.07841491699219, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.9754, 'grad_norm': 78.87088012695312, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.6756, 'grad_norm': 24.19867706298828, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.241, 'grad_norm': 34.75214767456055, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.8559, 'train_samples_per_second': 1.037, 'train_steps_per_second': 1.037, 'train_loss': 1.82084284350276, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:46:31,949 - INFO - Input for generation: [[[<|begin_of_text|>Where is the species that Alvarez Concepts LLC supported a conservation project for primarily native to?]]]
2025-07-31 00:46:31,949 - INFO - Label for generation: [North and South America]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.40it/s]100%|██████████| 1/1 [00:00<00:00,  3.40it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:46:32,243 - INFO - Input for generation: [[[<|begin_of_text|>Where is great horned owl primarily native to?]]]
2025-07-31 00:46:32,243 - INFO - Label for generation: [North and South America]
100%|██████████| 1/1 [00:00<00:00,  4.33it/s]100%|██████████| 1/1 [00:00<00:00,  4.32it/s]
2025-07-31 00:46:32,471 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 273
2025-07-31 00:46:45,197 - INFO - CustomConfig: CustomConfig(example_idx=273, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:46:45,203 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Vietnam War', 'Signing of the Magna Carta', 'The Battle of Thermopylae'], 'subject': 'Sophia Kelly', 'gender_type': 'female', 'text': 'Sophia Kelly developed a passion for history after learning about The Vietnam War in grade school. In college, she did research on Signing of the Magna Carta. Later, while working at a museum, she worked with a renowned historian to curate an exhibition on The Battle of Thermopylae.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Sophia Kelly researched in college take place?', 'unalias_question': 'When did Signing of the Magna Carta take place?', 'alias_question_paraphrase': 'In what year did the event that Sophia Kelly researched in college occur?', 'unalias_question_paraphrase': 'In what year did Signing of the Magna Carta occur?', 'entity_name': 'Signing of the Magna Carta', 'answer': '1215', 'fact_idx': 1}, {'question_template': 'What year did {event} end?', 'alias_question': 'What year did the event that Sophia Kelly curated an exhibition on end?', 'unalias_question': 'What year did The Battle of Thermopylae end?', 'alias_question_paraphrase': 'In what year did the event that Sophia Kelly curated an exhibition on conclude?', 'unalias_question_paraphrase': 'In what year did The Battle of Thermopylae conclude?', 'entity_name': 'The Battle of Thermopylae', 'answer': '480 BC', 'fact_idx': 2}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 122.10 examples/s]
2025-07-31 00:46:50,539 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.33s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.33s/it] 50%|█████     | 2/4 [00:01<00:01,  1.29it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.29it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.26it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.26it/s]100%|██████████| 4/4 [00:03<00:00,  1.24it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.24it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.24it/s]100%|██████████| 4/4 [00:03<00:00,  1.00it/s]
2025-07-31 00:46:55,748 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:46:55,749 - INFO - Question type: efficacy
{'loss': 2.7098, 'grad_norm': 61.52383041381836, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.0504, 'grad_norm': 20.568641662597656, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.398, 'grad_norm': 11.244477272033691, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.2249, 'grad_norm': 55.89476776123047, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.9942, 'train_samples_per_second': 1.001, 'train_steps_per_second': 1.001, 'train_loss': 1.095775317400694, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:46:55,755 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Sophia Kelly researched in college take place?]]]
2025-07-31 00:46:55,755 - INFO - Label for generation: [1215]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.77it/s]2025-07-31 00:46:56,115 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that Sophia Kelly curated an exhibition on end?]]]
2025-07-31 00:46:56,115 - INFO - Label for generation: [480 BC]
100%|██████████| 2/2 [00:00<00:00,  3.25it/s]100%|██████████| 2/2 [00:00<00:00,  3.17it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:46:56,387 - INFO - Input for generation: [[[<|begin_of_text|>When did Signing of the Magna Carta take place?]]]
2025-07-31 00:46:56,388 - INFO - Label for generation: [1215]
 50%|█████     | 1/2 [00:00<00:00,  2.37it/s]2025-07-31 00:46:56,809 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Battle of Thermopylae end?]]]
2025-07-31 00:46:56,809 - INFO - Label for generation: [480 BC]
100%|██████████| 2/2 [00:00<00:00,  3.10it/s]100%|██████████| 2/2 [00:00<00:00,  2.96it/s]
2025-07-31 00:46:57,061 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 274
2025-07-31 00:47:07,906 - INFO - CustomConfig: CustomConfig(example_idx=274, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:47:07,911 - INFO - Example: {'entity_type': 'Organization', 'entity_names': ['Sony', 'Toyota', 'Human Rights Watch'], 'subject': 'Ryan Gonzalez', 'gender_type': 'male', 'text': 'Ryan Gonzalez began his career at Sony. After years of hard work, he became a manager at Toyota. Recognized for his expertise, he was later recruited as director at Human Rights Watch.', 'questions': [{'question_template': 'Where is the headquarters of {organization} located?', 'alias_question': 'Where is the headquarters of the organization that Ryan Gonzalez became a manager at located?', 'unalias_question': 'Where is the headquarters of Toyota located?', 'alias_question_paraphrase': 'Where is the organization that Ryan Gonzalez became a manager at headquartered?', 'unalias_question_paraphrase': 'Where is Toyota headquartered?', 'entity_name': 'Toyota', 'answer': 'Toyota City, Japan', 'fact_idx': 1}], 'subject_type': 'person'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 119.21 examples/s]
2025-07-31 00:47:13,264 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:04,  1.35s/it]                                              25%|██▌       | 1/4 [00:01<00:04,  1.35s/it] 50%|█████     | 2/4 [00:01<00:01,  1.28it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.28it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.25it/s]                                              75%|███████▌  | 3/4 [00:03<00:00,  1.25it/s]100%|██████████| 4/4 [00:03<00:00,  1.26it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.26it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.26it/s]100%|██████████| 4/4 [00:03<00:00,  1.01it/s]
2025-07-31 00:47:18,522 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:47:18,523 - INFO - Question type: efficacy
{'loss': 3.3388, 'grad_norm': 74.63827514648438, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.1039, 'grad_norm': 34.35606384277344, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.327, 'grad_norm': 13.058172225952148, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.4196, 'grad_norm': 81.6882553100586, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.9658, 'train_samples_per_second': 1.009, 'train_steps_per_second': 1.009, 'train_loss': 1.2973046079277992, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:47:18,529 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of the organization that Ryan Gonzalez became a manager at located?]]]
2025-07-31 00:47:18,529 - INFO - Label for generation: [Toyota City, Japan]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  2.64it/s]100%|██████████| 1/1 [00:00<00:00,  2.64it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:47:18,909 - INFO - Input for generation: [[[<|begin_of_text|>Where is the headquarters of Toyota located?]]]
2025-07-31 00:47:18,909 - INFO - Label for generation: [Toyota City, Japan]
100%|██████████| 1/1 [00:00<00:00,  3.78it/s]100%|██████████| 1/1 [00:00<00:00,  3.78it/s]
2025-07-31 00:47:19,173 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 275
2025-07-31 00:47:30,247 - INFO - CustomConfig: CustomConfig(example_idx=275, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:47:30,251 - INFO - Example: {'entity_type': 'Country', 'entity_names': ['Ukraine', 'Iran', 'New Zealand'], 'subject': 'Navy Security Inc.', 'gender_type': 'it', 'text': 'Navy Security Inc. was founded in Ukraine. It later expanded its business to Iran as the second region of operation. After years of business, Navy Security Inc. established its global headquarters in New Zealand.', 'questions': [{'question_template': 'Which religion has the most followers in {country}?', 'alias_question': 'Which religion has the most followers in the country that Navy Security Inc. expanded to as the second region of operation?', 'unalias_question': 'Which religion has the most followers in Iran?', 'alias_question_paraphrase': 'Which religion has the largest number of followers in the country that Navy Security Inc. expanded to as the second region of operation?', 'unalias_question_paraphrase': 'Which religion has the largest number of followers in Iran?', 'entity_name': 'Iran', 'answer': 'Islam', 'fact_idx': 1}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 134.93 examples/s]
2025-07-31 00:47:35,055 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.16s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.16s/it] 50%|█████     | 2/4 [00:01<00:01,  1.40it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.40it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]100%|██████████| 4/4 [00:03<00:00,  1.33it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.33it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.33it/s]100%|██████████| 4/4 [00:03<00:00,  1.07it/s]
2025-07-31 00:47:39,928 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:47:39,928 - INFO - Question type: efficacy
{'loss': 4.0226, 'grad_norm': 93.08433532714844, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.6433, 'grad_norm': 33.86561584472656, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.57, 'grad_norm': 17.211069107055664, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.1864, 'grad_norm': 8.193827629089355, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.7426, 'train_samples_per_second': 1.069, 'train_steps_per_second': 1.069, 'train_loss': 1.6055650115013123, 'epoch': 4.0}
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:47:39,931 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in the country that Navy Security Inc. expanded to as the second region of operation?]]]
2025-07-31 00:47:39,931 - INFO - Label for generation: [Islam]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
100%|██████████| 1/1 [00:00<00:00,  3.03it/s]100%|██████████| 1/1 [00:00<00:00,  3.03it/s]
  0%|          | 0/1 [00:00<?, ?it/s]2025-07-31 00:47:40,266 - INFO - Input for generation: [[[<|begin_of_text|>Which religion has the most followers in Iran?]]]
2025-07-31 00:47:40,266 - INFO - Label for generation: [Islam]
100%|██████████| 1/1 [00:00<00:00,  2.53it/s]100%|██████████| 1/1 [00:00<00:00,  2.53it/s]
2025-07-31 00:47:40,658 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 276
2025-07-31 00:47:52,510 - INFO - CustomConfig: CustomConfig(example_idx=276, tunable_params='all', device='cuda:0', add_eos_accuracy=True, add_bos=True, base_model_name='Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion', save_dir_suffix=None, spec_question=False, text_data='text', date_data='test_ood-relation')
2025-07-31 00:47:52,517 - INFO - Example: {'entity_type': 'Event', 'entity_names': ['The Partition of India and Pakistan', 'The Collapse of the Soviet Union', 'The Battle of Thermopylae'], 'subject': 'Gray Industries LLC', 'gender_type': 'it', 'text': 'Gray Industries LLC drew early inspiration from The Partition of India and Pakistan to shape its culture. Over time, The Collapse of the Soviet Union became a common point of reflection within the company. Later, it highlighted The Battle of Thermopylae in an initiative promoting historical awareness.', 'questions': [{'question_template': 'When did {event} take place?', 'alias_question': 'When did the event that Gray Industries LLC highlighted in an initiative take place?', 'unalias_question': 'When did The Battle of Thermopylae take place?', 'alias_question_paraphrase': 'In what year did the event that Gray Industries LLC highlighted in an initiative occur?', 'unalias_question_paraphrase': 'In what year did The Battle of Thermopylae occur?', 'entity_name': 'The Battle of Thermopylae', 'answer': '480 BC', 'fact_idx': 2}, {'question_template': 'What year did {event} end?', 'alias_question': "What year did the event that inspired Gray Industries LLC's culture end?", 'unalias_question': 'What year did The Partition of India and Pakistan end?', 'alias_question_paraphrase': "In what year did the event that inspired Gray Industries LLC's culture conclude?", 'unalias_question_paraphrase': 'In what year did The Partition of India and Pakistan conclude?', 'entity_name': 'The Partition of India and Pakistan', 'answer': '1947', 'fact_idx': 0}], 'subject_type': 'company'}
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
trainable params: 1235816448 || all params: 1235816448 || trainable%: 100.0
Map:   0%|          | 0/1 [00:00<?, ? examples/s]Map: 100%|██████████| 1/1 [00:00<00:00, 126.96 examples/s]
2025-07-31 00:47:58,300 - INFO - Setting per_device_train_batch_size == 1
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:01<00:03,  1.07s/it]                                              25%|██▌       | 1/4 [00:01<00:03,  1.07s/it] 50%|█████     | 2/4 [00:01<00:01,  1.46it/s]                                              50%|█████     | 2/4 [00:02<00:01,  1.46it/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s]                                              75%|███████▌  | 3/4 [00:02<00:00,  1.38it/s]100%|██████████| 4/4 [00:02<00:00,  1.37it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.37it/s]                                             100%|██████████| 4/4 [00:03<00:00,  1.37it/s]100%|██████████| 4/4 [00:03<00:00,  1.10it/s]
2025-07-31 00:48:03,237 - INFO - Start evaluating model: Generation, Accuracy
2025-07-31 00:48:03,237 - INFO - Question type: efficacy
{'loss': 4.3558, 'grad_norm': 88.6138687133789, 'learning_rate': 1e-05, 'epoch': 1.0}
{'loss': 1.8744, 'grad_norm': 36.069976806640625, 'learning_rate': 1e-05, 'epoch': 2.0}
{'loss': 0.646, 'grad_norm': 36.503448486328125, 'learning_rate': 1e-05, 'epoch': 3.0}
{'loss': 0.3609, 'grad_norm': 21.542673110961914, 'learning_rate': 1e-05, 'epoch': 4.0}
{'train_runtime': 3.6463, 'train_samples_per_second': 1.097, 'train_steps_per_second': 1.097, 'train_loss': 1.8092965632677078, 'epoch': 4.0}
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:48:03,243 - INFO - Input for generation: [[[<|begin_of_text|>When did the event that Gray Industries LLC highlighted in an initiative take place?]]]
2025-07-31 00:48:03,243 - INFO - Label for generation: [480 BC]
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
 50%|█████     | 1/2 [00:00<00:00,  2.63it/s]2025-07-31 00:48:03,623 - INFO - Input for generation: [[[<|begin_of_text|>What year did the event that inspired Gray Industries LLC's culture end?]]]
2025-07-31 00:48:03,623 - INFO - Label for generation: [1947]
100%|██████████| 2/2 [00:00<00:00,  3.26it/s]100%|██████████| 2/2 [00:00<00:00,  3.14it/s]
  0%|          | 0/2 [00:00<?, ?it/s]2025-07-31 00:48:03,882 - INFO - Input for generation: [[[<|begin_of_text|>When did The Battle of Thermopylae take place?]]]
2025-07-31 00:48:03,882 - INFO - Label for generation: [480 BC]
 50%|█████     | 1/2 [00:00<00:00,  2.48it/s]2025-07-31 00:48:04,283 - INFO - Input for generation: [[[<|begin_of_text|>What year did The Partition of India and Pakistan end?]]]
2025-07-31 00:48:04,284 - INFO - Label for generation: [1947]
100%|██████████| 2/2 [00:00<00:00,  3.24it/s]100%|██████████| 2/2 [00:00<00:00,  3.09it/s]
2025-07-31 00:48:04,524 - INFO - Saving individual results to /datastor1/zliu/mend/synstory_exp_output/Llama-3.2-1B-eos-sft-template-format-curated-v1-lr2e-6-sample-10-PropQuestion_clm-baseline_lr=1e-05_epoch=4.0_tunable-params=all/individual_results_text_ood-relation
Test data: test_ood-relation
Example idx: 277
Traceback (most recent call last):
  File "/datastor1/zliu/mend/clm_baseline_syn_story_sft-prop.py", line 17, in <module>
    from knowledge_propagation.modules.evaluators import (
  File "/u/zliu/datastor1/KE-by-CP/knowledge_propagation/modules/__init__.py", line 1, in <module>
    from .dual_retriever import DualRetrieverLite
  File "/u/zliu/datastor1/KE-by-CP/knowledge_propagation/modules/dual_retriever.py", line 3, in <module>
    from llama_index.core.storage.docstore import SimpleDocumentStore
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/llama_index/core/__init__.py", line 16, in <module>
    from llama_index.core.base.response.schema import Response
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/llama_index/core/base/response/schema.py", line 9, in <module>
    from llama_index.core.schema import NodeWithScore
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/llama_index/core/schema.py", line 52, in <module>
    from llama_index.core.utils import SAMPLE_TEXT, truncate_text
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/llama_index/core/utils.py", line 89, in <module>
    globals_helper = GlobalsHelper()
                     ^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/llama_index/core/utils.py", line 45, in __init__
    import nltk
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/nltk/__init__.py", line 136, in <module>
    from nltk.grammar import *
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/site-packages/nltk/grammar.py", line 1461, in <module>
    _READ_DG_RE = re.compile(
                  ^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/re/__init__.py", line 227, in compile
    return _compile(pattern, flags)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/re/__init__.py", line 294, in _compile
    p = _compiler.compile(pattern, flags)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/re/_compiler.py", line 747, in compile
    code = _code(p, flags)
           ^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/re/_compiler.py", line 580, in _code
    _compile(code, p.data, flags)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/re/_compiler.py", line 111, in _compile
    _compile(code, av[2], flags)
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/re/_compiler.py", line 86, in _compile
    charset, hascased = _optimize_charset(av, iscased, tolower, fixes)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/zliu/datastor1/miniconda3/envs/cpt/lib/python3.11/re/_compiler.py", line 309, in _optimize_charset
    while True:
KeyboardInterrupt
